[
    {
        "id": "1",
        "title": "Toward Engineering AGI: Benchmarking the Engineering Design Capabilities of LLMs",
        "author": [
            "Xingang Guo",
            "Yaxin Li",
            "Xiangyi Kong",
            "Yilan Jiang",
            "Xiayu Zhao",
            "Zhihua Gong",
            "Yufan Zhang",
            "Daixuan Li",
            "Tianle Sang",
            "Beixiao Zhu",
            "Gregory Jun",
            "Yingbing Huang",
            "Yiqi Liu",
            "Yuqi Xue",
            "Rahul Dev Kundu",
            "Qi Jian Lim",
            "Yizhou Zhao",
            "Luke Alexander Granger",
            "Mohamed Badr Younis",
            "Darioush Keivan",
            "Nippun Sabharwal",
            "Shreyanka Sinha",
            "Prakhar Agarwal",
            "Kojo Vandyck",
            "Hanlin Mai",
            "Zichen Wang",
            "Aditya Venkatesh",
            "Ayush Barik",
            "Jiankun Yang",
            "Chongying Yue",
            "Jingjie He",
            "Libin Wang",
            "Licheng Xu",
            "Hao Chen",
            "Jinwen Wang",
            "Liujun Xu",
            "Rushabh Shetty",
            "Ziheng Guo",
            "Dahui Song",
            "Manvi Jha",
            "Weijie Liang",
            "Weiman Yan",
            "Bryan Zhang",
            "Sahil Bhandary Karnoor",
            "Jialiang Zhang",
            "Rutva Pandya",
            "Xinyi Gong",
            "Mithesh Ballae Ganesh",
            "Feize Shi",
            "Ruiling Xu",
            "Yifan Zhang",
            "Yanfeng Ouyang",
            "Lianhui Qin",
            "Elyse Rosenbaum",
            "Corey Snyder",
            "Peter Seiler",
            "Geir Dullerud",
            "Xiaojia Shelly Zhang",
            "Zuofu Cheng",
            "Pavan Kumar Hanumolu",
            "Jian Huang",
            "Mayank Kulkarni",
            "Mahdi Namazifar",
            "Huan Zhang",
            "Bin Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16204",
        "abstract": "Today, industry pioneers dream of developing general-purpose AI engineers capable of designing and building humanity's most ambitious projects--from starships that will carry us to distant worlds to Dyson spheres that harness stellar energy. Yet engineering design represents a fundamentally different challenge for large language models (LLMs) compared to traditional textbook-style problem solving or factual question answering. Real-world engineering design demands the synthesis of domain knowledge, navigation of complex trade-offs, and management of the tedious processes that consume much of practicing engineers' time. Despite these shared challenges across engineering disciplines, no benchmark currently captures the unique demands of engineering design work. In this work, we introduce ENGDESIGN, an Engineering Design benchmark that evaluates LLMs' abilities to perform practical design tasks across nine engineering domains: Operating System Design, Computer Architecture Design, Control System Design, Mechanical Systems, Structural Design, Digital Hardware Design, Analog Integrated Circuit Design, Robotics, and Signal Processing. Unlike existing benchmarks that focus on factual recall or question answering, ENGDESIGN uniquely emphasizes LLMs' ability to synthesize domain knowledge, reason under constraints, and generate functional, objective-oriented designs. Each task in ENGDESIGN represents a real-world engineering design problem, accompanied by a detailed task description specifying design goals, constraints, and performance requirements. We pioneer a simulation-based evaluation paradigm where LLM-generated designs undergo rigorous testing through executable, domain-specific simulations-from circuit SPICE simulations to structural finite element analysis, from control system validation to robotic motion planning.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "2",
        "title": "Deep Reinforcement Learning in Factor Investment",
        "author": [
            "Junlin Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16206",
        "abstract": "Deep reinforcement learning has shown promise in trade execution, yet its use in low-frequency factor portfolio construction remains under-explored. A key obstacle is the high-dimensional, unbalanced state space created by stocks that enter and exit the investable universe. We introduce Conditional Auto-encoded Factor-based Portfolio Optimisation (CAFPO), which compresses stock-level returns into a small set of latent factors conditioned on 94 firm-specific characteristics. The factors feed a DRL agent implemented with both PPO and DDPG to generate continuous long-short weights. On 20 years of U.S. equity data (2000--2020), CAFPO outperforms equal-weight, value-weight, Markowitz, vanilla DRL, and Fama--French-driven DRL, delivering a 24.6\\% compound return and a Sharpe ratio of 0.94 out of sample. SHAP analysis further reveals economically intuitive factor attributions. Our results demonstrate that factor-aware representation learning can make DRL practical for institutional, low-turnover portfolio management.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "3",
        "title": "An Automated Framework for Assessing Electric Vehicle Charging Impacts on a Campus Distribution Grid",
        "author": [
            "Mohammadreza Iranpour",
            "Sammy Hamed",
            "Mohammad Rasoul Narimani",
            "Silvia Carpitella",
            "Kourosh Sedghisigarchi",
            "Xudong Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16218",
        "abstract": "This paper introduces a unified and automated framework designed to dynamically assess the impact of electric vehicle (EV) charging on distribution feeders and transformers at California State University, Northridge (CSUN). As EV adoption accelerates, the resulting increase in charging demand imposes additional stress on local power distribution systems. Moreover, the evolving nature of EV load profiles throughout the day necessitates detailed temporal analysis to identify peak loading conditions, anticipate worst-case scenarios, and plan timely infrastructure upgrades. Our main contribution is the development of a flexible testbed that integrates Julia, a high-performance programming language for technical computing, with PowerWorld Simulator via the http://EasySimauto.jl package. This integration enables seamless modeling, simulation, and analysis of EV charging load profiles and their implications for campus grid infrastructure. The framework leverages a real-world dataset collected from CSUN's EV charging stations, consisting of 15-minute interval measurements over the course of one year. By coupling high-resolution data with dynamic simulations, the proposed system offers a valuable tool for evaluating transformer loading, feeder utilization, and overall system stress. The results support data-driven decision-making for EV infrastructure deployment, load forecasting, and energy management strategies. In addition, the framework allows for scenario-based studies to explore the impact of future increases in EV penetration or changes in charging behavior. Its modular architecture also makes it adaptable to other campus or urban distribution systems facing similar electrification challenges.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "4",
        "title": "An Open Dataset for Temperature Modelling in Machine Tools",
        "author": [
            "C. Coelho",
            "D. FernÃ¡ndez",
            "M. Hohmann",
            "L. Penter",
            "S. Ihlenfeldt",
            "O. Niggemann"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16222",
        "abstract": "This data set descriptor introduces a structured, high-resolution dataset of transient thermal simulations for a vertical axis of a machine tool test rig. The data set includes temperature and heat flux values recorded at 29 probe locations at 1800 time steps, sampled every second over a 30-minute range, across 17 simulation runs derived from a fractional factorial design. First, a computer-aided design model was de-featured, segmented, and optimized, followed by finite element (FE) modelling. Detailed information on material, mesh, and boundary conditions is included. To support research and model development, the dataset provides summary statistics, thermal evolution plots, correlation matrix analyses, and a reproducible Jupyter notebook. The data set is designed to support machine learning and deep learning applications in thermal modelling for prediction, correction, and compensation of thermally induced deviations in mechanical systems, and aims to support researchers without FE expertise by providing ready-to-use simulation data.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "5",
        "title": "On LLM-Based Scientific Inductive Reasoning Beyond Equations",
        "author": [
            "Brian S. Lin",
            "Jiaxin Yuan",
            "Zihan Zhou",
            "Shouli Wang",
            "Shuo Wang",
            "Cunliang Kong",
            "Qi Shi",
            "Yuxuan Li",
            "Liner Yang",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16226",
        "abstract": "As large language models (LLMs) increasingly exhibit human-like capabilities, a fundamental question emerges: How can we enable LLMs to learn the underlying patterns from limited examples in entirely novel environments and apply them effectively? This question is central to the ability of LLMs in inductive reasoning. Existing research on LLM-based inductive reasoning can be broadly categorized based on whether the underlying rules are expressible via explicit mathematical equations. However, many recent studies in the beyond-equations category have emphasized rule design without grounding them in specific scenarios. Inspired by the parallels between inductive reasoning and human scientific discovery, we propose the task of LLM-Based Scientific Inductive Reasoning Beyond Equations and introduce a new benchmark, SIRBench-V1, to evaluate the inductive reasoning abilities of LLMs in scientific settings. Our experimental results show that current LLMs still struggle with this task, underscoring its difficulty and the need for further advancement in this area.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "VerilogMonkey: Exploring Parallel Scaling for Automated Verilog Code Generation with LLMs",
        "author": [
            "Juxin Niu",
            "Yuxin Du",
            "Dan Niu",
            "Xi Wang",
            "Zhe Jiang",
            "Nan Guan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16246",
        "abstract": "We present VerilogMonkey, an empirical study of parallel scaling for the under-explored task of automated Verilog generation. Parallel scaling improves LLM performance by sampling many outputs in parallel. Across multiple benchmarks and mainstream LLMs, we find that scaling to hundreds of samples is cost-effective in both time and money and, even without any additional enhancements such as post-training or agentic methods, surpasses prior results on LLM-based Verilog generation. We further dissect why parallel scaling delivers these gains and show how output randomness in LLMs affects its effectiveness.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "7",
        "title": "HausaMovieReview: A Benchmark Dataset for Sentiment Analysis in Low-Resource African Language",
        "author": [
            "Asiya Ibrahim Zanga",
            "Salisu Mamman Abdulrahman",
            "Abubakar Ado",
            "Abdulkadir Abubakar Bichi",
            "Lukman Aliyu Jibril",
            "Abdulmajid Babangida Umar",
            "Alhassan Adamu",
            "Shamsuddeen Hassan Muhammad",
            "Bashir Salisu Abubakar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16256",
        "abstract": "The development of Natural Language Processing (NLP) tools for low-resource languages is critically hindered by the scarcity of annotated datasets. This paper addresses this fundamental challenge by introducing HausaMovieReview, a novel benchmark dataset comprising 5,000 YouTube comments in Hausa and code-switched English. The dataset was meticulously annotated by three independent annotators, demonstrating a robust agreement with a Fleiss' Kappa score of 0.85 between annotators. We used this dataset to conduct a comparative analysis of classical models (Logistic Regression, Decision Tree, K-Nearest Neighbors) and fine-tuned transformer models (BERT and RoBERTa). Our results reveal a key finding: the Decision Tree classifier, with an accuracy and F1-score 89.72% and 89.60% respectively, significantly outperformed the deep learning models. Our findings also provide a robust baseline, demonstrating that effective feature engineering can enable classical models to achieve state-of-the-art performance in low-resource contexts, thereby laying a solid foundation for future research.\nKeywords: Hausa, Kannywood, Low-Resource Languages, NLP, Sentiment Analysis",
        "tags": [
            "BERT",
            "Transformer"
        ]
    },
    {
        "id": "8",
        "title": "Gender and Political Bias in Large Language Models: A Demonstration Platform",
        "author": [
            "Wenjie Lin",
            "Hange Liu",
            "Xutao Mao",
            "Yingying Zhuang",
            "Jingwei Shi",
            "Xudong Han",
            "Tianyu Shi",
            "Jinrui Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16264",
        "abstract": "We present ParlAI Vote, an interactive system for exploring European Parliament debates and votes, and for testing LLMs on vote prediction and bias analysis. This platform connects debate topics, speeches, and roll-call outcomes, and includes rich demographic data such as gender, age, country, and political group. Users can browse debates, inspect linked speeches, compare real voting outcomes with predictions from frontier LLMs, and view error breakdowns by demographic group. Visualizing the EuroParlVote benchmark and its core tasks of gender classification and vote prediction, ParlAI Vote highlights systematic performance bias in state-of-the-art LLMs. The system unifies data, models, and visual analytics in a single interface, lowering the barrier for reproducing findings, auditing behavior, and running counterfactual scenarios. It supports research, education, and public engagement with legislative decision-making, while making clear both the strengths and the limitations of current LLMs in political analysis.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "9",
        "title": "Underground Multi-robot Systems at Work: a revolution in mining",
        "author": [
            "Victor V. Puche",
            "Kashish Verma",
            "Matteo Fumagalli"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16267",
        "abstract": "The growing global demand for critical raw materials (CRMs) has highlighted the need to access difficult and hazardous environments such as abandoned underground mines. These sites pose significant challenges for conventional machinery and human operators due to confined spaces, structural instability, and lack of infrastructure. To address this, we propose a modular multi-robot system designed for autonomous operation in such environments, enabling sequential mineral extraction tasks. Unlike existing work that focuses primarily on mapping and inspection through global behavior or central control, our approach incorporates physical interaction capabilities using specialized robots coordinated through local high-level behavior control. Our proposed system utilizes Hierarchical Finite State Machine (HFSM) behaviors to structure complex task execution across heterogeneous robotic platforms. Each robot has its own HFSM behavior to perform sequential autonomy while maintaining overall system coordination, achieved by triggering behavior execution through inter-robot communication. This architecture effectively integrates software and hardware components to support collaborative, task-driven multi-robot operation in confined underground environments.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "10",
        "title": "Digging Into the Internal: Causality-Based Analysis of LLM Function Calling",
        "author": [
            "Zhenlan Ji",
            "Daoyuan Wu",
            "Wenxuan Wang",
            "Pingchuan Ma",
            "Shuai Wang",
            "Lei Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16268",
        "abstract": "Function calling (FC) has emerged as a powerful technique for facilitating large language models (LLMs) to interact with external systems and perform structured tasks. However, the mechanisms through which it influences model behavior remain largely under-explored. Besides, we discover that in addition to the regular usage of FC, this technique can substantially enhance the compliance of LLMs with user instructions. These observations motivate us to leverage causality, a canonical analysis method, to investigate how FC works within LLMs. In particular, we conduct layer-level and token-level causal interventions to dissect FC's impact on the model's internal computational logic when responding to user queries. Our analysis confirms the substantial influence of FC and reveals several in-depth insights into its mechanisms. To further validate our findings, we conduct extensive experiments comparing the effectiveness of FC-based instructions against conventional prompting methods. We focus on enhancing LLM safety robustness, a critical LLM application scenario, and evaluate four mainstream LLMs across two benchmark datasets. The results are striking: FC shows an average performance improvement of around 135% over conventional prompting methods in detecting malicious inputs, demonstrating its promising potential to enhance LLM reliability and capability in practical applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "11",
        "title": "SecureFixAgent: A Hybrid LLM Agent for Automated Python Static Vulnerability Repair",
        "author": [
            "Jugal Gajjar",
            "Kamalasankari Subramaniakuppusamy",
            "Relsy Puthal",
            "Kaustik Ranaware"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16275",
        "abstract": "Modern software development pipelines face growing challenges in securing large codebases with extensive dependencies. Static analysis tools like Bandit are effective at vulnerability detection but suffer from high false positives and lack repair capabilities. Large Language Models (LLMs), in contrast, can suggest fixes but often hallucinate changes and lack self-validation. We present SecureFixAgent, a hybrid repair framework integrating Bandit with lightweight local LLMs (<8B parameters) in an iterative detect-repair-validate loop. To improve precision, we apply parameter-efficient LoRA-based fine-tuning on a diverse, curated dataset spanning multiple Python project domains, mitigating dataset bias and reducing unnecessary edits. SecureFixAgent uses Bandit for detection, the LLM for candidate fixes with explanations, and Bandit re-validation for verification, all executed locally to preserve privacy and reduce cloud reliance. Experiments show SecureFixAgent reduces false positives by 10.8% over static analysis, improves fix accuracy by 13.51%, and lowers false positives by 5.46% compared to pre-trained LLMs, typically converging within three iterations. Beyond metrics, developer studies rate explanation quality 4.5/5, highlighting its value for human trust and adoption. By combining verifiable security improvements with transparent rationale in a resource-efficient local framework, SecureFixAgent advances trustworthy, automated vulnerability remediation for modern pipelines.",
        "tags": [
            "Detection",
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "12",
        "title": "Language Modeling with Learned Meta-Tokens",
        "author": [
            "Alok N. Shah",
            "Khush Gupta",
            "Keshav Ramji",
            "Pratik Chaudhari"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16278",
        "abstract": "While modern Transformer-based language models (LMs) have achieved major success in multi-task generalization, they often struggle to capture long-range dependencies within their context window. This work introduces a novel approach using meta-tokens, special tokens injected during pre-training, along with a dedicated meta-attention mechanism to guide LMs to use these tokens. We pre-train a language model with a modified GPT-2 architecture equipped with meta-attention in addition to causal multi-head attention, and study the impact of these tokens on a suite of synthetic tasks. We find that data-efficient language model pre-training on fewer than 100B tokens utilizing meta-tokens and our meta-attention mechanism achieves strong performance on these tasks after fine-tuning. We suggest that these gains arise due to the meta-tokens sharpening the positional encoding. This enables them to operate as trainable, content-based landmarks, implicitly compressing preceding context and \"caching\" it in the meta-token. At inference-time, the meta-token points to relevant context, facilitating length generalization up to 2$\\times$ its context window, even after extension with YaRN. We provide further evidence of these behaviors by visualizing model internals to study the residual stream, and assessing the compression quality by information-theoretic analysis on the rate-distortion tradeoff. Our findings suggest that pre-training LMs with meta-tokens offers a simple, data-efficient method to enhance long-context language modeling performance, while introducing new insights into the nature of their behavior towards length generalization.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "13",
        "title": "Test-Time Learning and Inference-Time Deliberation for Efficiency-First Offline Reinforcement Learning in Care Coordination and Population Health Management",
        "author": [
            "Sanjay Basu",
            "Sadiq Y. Patel",
            "Parth Sheth",
            "Bhairavi Muralidharan",
            "Namrata Elamaran",
            "Aakriti Kinra",
            "Rajaie Batniji"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16291",
        "abstract": "Care coordination and population health management programs serve large Medicaid and safety-net populations and must be auditable, efficient, and adaptable. While clinical risk for outreach modalities is typically low, time and opportunity costs differ substantially across text, phone, video, and in-person visits. We propose a lightweight offline reinforcement learning (RL) approach that augments trained policies with (i) test-time learning via local neighborhood calibration, and (ii) inference-time deliberation via a small Q-ensemble that incorporates predictive uncertainty and time/effort cost. The method exposes transparent dials for neighborhood size and uncertainty/cost penalties and preserves an auditable training pipeline. Evaluated on a de-identified operational dataset, TTL+ITD achieves stable value estimates with predictable efficiency trade-offs and subgroup auditing.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "14",
        "title": "Robust LLM Training Infrastructure at ByteDance",
        "author": [
            "Borui Wan",
            "Gaohong Liu",
            "Zuquan Song",
            "Jun Wang",
            "Yun Zhang",
            "Guangming Sheng",
            "Shuguang Wang",
            "Houmin Wei",
            "Chenyuan Wang",
            "Weiqiang Lou",
            "Xi Yang",
            "Mofan Zhang",
            "Kaihua Jiang",
            "Cheng Ren",
            "Xiaoyun Zhi",
            "Menghan Yu",
            "Zhe Nan",
            "Zhuolin Zheng",
            "Baoquan Zhong",
            "Qinlong Wang",
            "Huan Yu",
            "Jinxin Chi",
            "Wang Zhang",
            "Yuhan Li",
            "Zixian Du",
            "Sida Zhao",
            "Yongqiang Zhang",
            "Jingzhe Tang",
            "Zherui Liu",
            "Chuan Wu",
            "Yanghua Peng",
            "Haibin Lin",
            "Wencong Xiao",
            "Xin Liu",
            "Liang Xiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16293",
        "abstract": "The training scale of large language models (LLMs) has reached tens of thousands of GPUs and is still continuously expanding, enabling faster learning of larger models. Accompanying the expansion of the resource scale is the prevalence of failures (CUDA error, NaN values, job hang, etc.), which poses significant challenges to training stability. Any large-scale LLM training infrastructure should strive for minimal training interruption, efficient fault diagnosis, and effective failure tolerance to enable highly efficient continuous training. This paper presents ByteRobust, a large-scale GPU infrastructure management system tailored for robust and stable training of LLMs. It exploits the uniqueness of LLM training process and gives top priorities to detecting and recovering failures in a routine manner. Leveraging parallelisms and characteristics of LLM training, ByteRobust enables high-capacity fault tolerance, prompt fault demarcation, and localization with an effective data-driven approach, comprehensively ensuring continuous and efficient training of LLM tasks. ByteRobust is deployed on a production GPU platform with over 200,000 GPUs and achieves 97% ETTR for a three-month training job on 9,600 GPUs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "15",
        "title": "How Large Language Models are Designed to Hallucinate",
        "author": [
            "Richard Ackermann",
            "Simeon Emanuilov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16297",
        "abstract": "Large language models (LLMs) achieve remarkable fluency across linguistic and reasoning tasks but remain systematically prone to hallucination. Prevailing accounts attribute hallucinations to data gaps, limited context, or optimization errors. We argue instead that hallucination is a structural outcome of the transformer architecture. As coherence engines, transformers are compelled to produce fluent continuations, with self-attention simulating the relational structure of meaning but lacking the existential grounding of temporality, mood, and care that stabilizes human understanding. On this basis, we distinguish ontological hallucination, arising when continuations require disclosure of beings in world, and residual reasoning hallucination, where models mimic inference by recycling traces of human reasoning in text. We illustrate these patterns through case studies aligned with Heideggerian categories and an experiment across twelve LLMs showing how simulated \"self-preservation\" emerges under extended prompts. Our contribution is threefold: (1) a comparative account showing why existing explanations are insufficient; (2) a predictive taxonomy of hallucination linked to existential structures with proposed benchmarks; and (3) design directions toward \"truth-constrained\" architectures capable of withholding or deferring when disclosure is absent. We conclude that hallucination is not an incidental defect but a defining limit of transformer-based models, an outcome scaffolding can mask but never resolve.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "16",
        "title": "Generalizability of Large Language Model-Based Agents: A Comprehensive Survey",
        "author": [
            "Minxing Zhang",
            "Yi Yang",
            "Roy Xie",
            "Bhuwan Dhingra",
            "Shuyan Zhou",
            "Jian Pei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16330",
        "abstract": "Large Language Model (LLM)-based agents have emerged as a new paradigm that extends LLMs' capabilities beyond text generation to dynamic interaction with external environments. By integrating reasoning with perception, memory, and tool use, agents are increasingly deployed in diverse domains like web navigation and household robotics. A critical challenge, however, lies in ensuring agent generalizability - the ability to maintain consistent performance across varied instructions, tasks, environments, and domains, especially those beyond agents' fine-tuning data. Despite growing interest, the concept of generalizability in LLM-based agents remains underdefined, and systematic approaches to measure and improve it are lacking. In this survey, we provide the first comprehensive review of generalizability in LLM-based agents. We begin by emphasizing agent generalizability's importance by appealing to stakeholders and clarifying the boundaries of agent generalizability by situating it within a hierarchical domain-task ontology. We then review datasets, evaluation dimensions, and metrics, highlighting their limitations. Next, we categorize methods for improving generalizability into three groups: methods for the backbone LLM, for agent components, and for their interactions. Moreover, we introduce the distinction between generalizable frameworks and generalizable agents and outline how generalizable frameworks can be translated into agent-level generalizability. Finally, we identify critical challenges and future directions, including developing standardized frameworks, variance- and cost-based metrics, and approaches that integrate methodological innovations with architecture-level designs. By synthesizing progress and highlighting opportunities, this survey aims to establish a foundation for principled research on building LLM-based agents that generalize reliably across diverse applications.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "17",
        "title": "Psychometric Personality Shaping Modulates Capabilities and Safety in Language Models",
        "author": [
            "Stephen Fitz",
            "Peter Romero",
            "Steven Basart",
            "Sipeng Chen",
            "Jose Hernandez-Orallo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16332",
        "abstract": "Large Language Models increasingly mediate high-stakes interactions, intensifying research on their capabilities and safety. While recent work has shown that LLMs exhibit consistent and measurable synthetic personality traits, little is known about how modulating these traits affects model behavior. We address this gap by investigating how psychometric personality control grounded in the Big Five framework influences AI behavior in the context of capability and safety benchmarks. Our experiments reveal striking effects: for example, reducing conscientiousness leads to significant drops in safety-relevant metrics on benchmarks such as WMDP, TruthfulQA, ETHICS, and Sycophancy as well as reduction in general capabilities as measured by MMLU. These findings highlight personality shaping as a powerful and underexplored axis of model control that interacts with both safety and general competence. We discuss the implications for safety evaluation, alignment strategies, steering model behavior after deployment, and risks associated with possible exploitation of these findings. Our findings motivate a new line of research on personality-sensitive safety evaluations and dynamic behavioral control in LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "18",
        "title": "Neural Atlas Graphs for Dynamic Scene Decomposition and Editing",
        "author": [
            "Jan Philipp Schneider",
            "Pratik Singh Bisht",
            "Ilya Chugunov",
            "Andreas Kolb",
            "Michael Moeller",
            "Felix Heide"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16336",
        "abstract": "Learning editable high-resolution scene representations for dynamic scenes is an open problem with applications across the domains from autonomous driving to creative editing - the most successful approaches today make a trade-off between editability and supporting scene complexity: neural atlases represent dynamic scenes as two deforming image layers, foreground and background, which are editable in 2D, but break down when multiple objects occlude and interact. In contrast, scene graph models make use of annotated data such as masks and bounding boxes from autonomous-driving datasets to capture complex 3D spatial relationships, but their implicit volumetric node representations are challenging to edit view-consistently. We propose Neural Atlas Graphs (NAGs), a hybrid high-resolution scene representation, where every graph node is a view-dependent neural atlas, facilitating both 2D appearance editing and 3D ordering and positioning of scene elements. Fit at test-time, NAGs achieve state-of-the-art quantitative results on the Waymo Open Dataset - by 5 dB PSNR increase compared to existing methods - and make environmental editing possible in high resolution and visual quality - creating counterfactual driving scenarios with new backgrounds and edited vehicle appearance. We find that the method also generalizes beyond driving scenes and compares favorably - by more than 7 dB in PSNR - to recent matting and video editing baselines on the DAVIS video dataset with a diverse set of human and animal-centric scenes.",
        "tags": [
            "3D",
            "Matting",
            "Video Editing"
        ]
    },
    {
        "id": "19",
        "title": "Estimating Clinical Lab Test Result Trajectories from PPG using Physiological Foundation Model and Patient-Aware State Space Model -- a UNIPHY+ Approach",
        "author": [
            "Minxiao Wang",
            "Runze Yan",
            "Carol Li",
            "Saurabh Kataria",
            "Xiao Hu",
            "Matthew Clark",
            "Timothy Ruchti",
            "Timothy G. Buchman",
            "Sivasubramanium V Bhavani",
            "Randall J. Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16345",
        "abstract": "Clinical laboratory tests provide essential biochemical measurements for diagnosis and treatment, but are limited by intermittent and invasive sampling. In contrast, photoplethysmogram (PPG) is a non-invasive, continuously recorded signal in intensive care units (ICUs) that reflects cardiovascular dynamics and can serve as a proxy for latent physiological changes. We propose UNIPHY+Lab, a framework that combines a large-scale PPG foundation model for local waveform encoding with a patient-aware Mamba model for long-range temporal modeling. Our architecture addresses three challenges: (1) capturing extended temporal trends in laboratory values, (2) accounting for patient-specific baseline variation via FiLM-modulated initial states, and (3) performing multi-task estimation for interrelated biomarkers. We evaluate our method on the two ICU datasets for predicting the five key laboratory tests. The results show substantial improvements over the LSTM and carry-forward baselines in MAE, RMSE, and $R^2$ among most of the estimation targets. This work demonstrates the feasibility of continuous, personalized lab value estimation from routine PPG monitoring, offering a pathway toward non-invasive biochemical surveillance in critical care.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "20",
        "title": "Tactile-Based Human Intent Recognition for Robot Assistive Navigation",
        "author": [
            "Shaoting Peng",
            "Dakarai Crowder",
            "Wenzhen Yuan",
            "Katherine Driggs-Campbell"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16353",
        "abstract": "Robot assistive navigation (RAN) is critical for enhancing the mobility and independence of the growing population of mobility-impaired individuals. However, existing systems often rely on interfaces that fail to replicate the intuitive and efficient physical communication observed between a person and a human caregiver, limiting their effectiveness. In this paper, we introduce Tac-Nav, a RAN system that leverages a cylindrical tactile skin mounted on a Stretch 3 mobile manipulator to provide a more natural and efficient interface for human navigational intent recognition. To robustly classify the tactile data, we developed the Cylindrical Kernel Support Vector Machine (CK-SVM), an algorithm that explicitly models the sensor's cylindrical geometry and is consequently robust to the natural rotational shifts present in a user's grasp. Comprehensive experiments were conducted to demonstrate the effectiveness of our classification algorithm and the overall system. Results show that CK-SVM achieved superior classification accuracy on both simulated (97.1%) and real-world (90.8%) datasets compared to four baseline models. Furthermore, a pilot study confirmed that users more preferred the Tac-Nav tactile interface over conventional joystick and voice-based controls.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "21",
        "title": "Evaluation of Causal Reasoning for Large Language Models in Contextualized Clinical Scenarios of Laboratory Test Interpretation",
        "author": [
            "Balu Bhasuran",
            "Mattia Prosperi",
            "Karim Hanna",
            "John Petrilli",
            "Caretia JeLayne Washington",
            "Zhe He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16372",
        "abstract": "This study evaluates causal reasoning in large language models (LLMs) using 99 clinically grounded laboratory test scenarios aligned with Pearl's Ladder of Causation: association, intervention, and counterfactual reasoning. We examined common laboratory tests such as hemoglobin A1c, creatinine, and vitamin D, and paired them with relevant causal factors including age, gender, obesity, and smoking. Two LLMs - GPT-o1 and Llama-3.2-8b-instruct - were tested, with responses evaluated by four medically trained human experts. GPT-o1 demonstrated stronger discriminative performance (AUROC overall = 0.80 +/- 0.12) compared to Llama-3.2-8b-instruct (0.73 +/- 0.15), with higher scores across association (0.75 vs 0.72), intervention (0.84 vs 0.70), and counterfactual reasoning (0.84 vs 0.69). Sensitivity (0.90 vs 0.84) and specificity (0.93 vs 0.80) were also greater for GPT-o1, with reasoning ratings showing similar trends. Both models performed best on intervention questions and worst on counterfactuals, particularly in altered outcome scenarios. These findings suggest GPT-o1 provides more consistent causal reasoning, but refinement is required before adoption in high-stakes clinical applications.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "22",
        "title": "Evaluating Behavioral Alignment in Conflict Dialogue: A Multi-Dimensional Comparison of LLM Agents and Humans",
        "author": [
            "Deuksin Kwon",
            "Kaleen Shrestha",
            "Bin Han",
            "Elena Hayoung Lee",
            "Gale Lucas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16394",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in socially complex, interaction-driven tasks, yet their ability to mirror human behavior in emotionally and strategically complex contexts remains underexplored. This study assesses the behavioral alignment of personality-prompted LLMs in adversarial dispute resolution by simulating multi-turn conflict dialogues that incorporate negotiation. Each LLM is guided by a matched Five-Factor personality profile to control for individual variation and enhance realism. We evaluate alignment across three dimensions: linguistic style, emotional expression (e.g., anger dynamics), and strategic behavior. GPT-4.1 achieves the closest alignment with humans in linguistic style and emotional dynamics, while Claude-3.7-Sonnet best reflects strategic behavior. Nonetheless, substantial alignment gaps persist. Our findings establish a benchmark for alignment between LLMs and humans in socially complex interactions, underscoring both the promise and the limitations of personality conditioning in dialogue modeling.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "23",
        "title": "Dynamic Objects Relocalization in Changing Environments with Flow Matching",
        "author": [
            "Francesco Argenziano",
            "Miguel Saavedra-Ruiz",
            "Sacha Morin",
            "Daniele Nardi",
            "Liam Paull"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16398",
        "abstract": "Task and motion planning are long-standing challenges in robotics, especially when robots have to deal with dynamic environments exhibiting long-term dynamics, such as households or warehouses. In these environments, long-term dynamics mostly stem from human activities, since previously detected objects can be moved or removed from the scene. This adds the necessity to find such objects again before completing the designed task, increasing the risk of failure due to missed relocalizations. However, in these settings, the nature of such human-object interactions is often overlooked, despite being governed by common habits and repetitive patterns. Our conjecture is that these cues can be exploited to recover the most likely objects' positions in the scene, helping to address the problem of unknown relocalization in changing environments. To this end we propose FlowMaps, a model based on Flow Matching that is able to infer multimodal object locations over space and time. Our results present statistical evidence to support our hypotheses, opening the way to more complex applications of our approach. The code is publically available at https://github.com/Fra-Tsuna/flowmaps",
        "tags": [
            "Flow Matching",
            "Robotics"
        ]
    },
    {
        "id": "24",
        "title": "VORTEX: Aligning Task Utility and Human Preferences through LLM-Guided Reward Shaping",
        "author": [
            "Guojun Xiong",
            "Milind Tambe"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16399",
        "abstract": "In social impact optimization, AI decision systems often rely on solvers that optimize well-calibrated mathematical objectives. However, these solvers cannot directly accommodate evolving human preferences, typically expressed in natural language rather than formal constraints. Recent approaches address this by using large language models (LLMs) to generate new reward functions from preference descriptions. While flexible, they risk sacrificing the system's core utility guarantees. In this paper, we propose \\texttt{VORTEX}, a language-guided reward shaping framework that preserves established optimization goals while adaptively incorporating human feedback. By formalizing the problem as multi-objective optimization, we use LLMs to iteratively generate shaping rewards based on verbal reinforcement and text-gradient prompt updates. This allows stakeholders to steer decision behavior via natural language without modifying solvers or specifying trade-off weights. We provide theoretical guarantees that \\texttt{VORTEX} converges to Pareto-optimal trade-offs between utility and preference satisfaction. Empirical results in real-world allocation tasks demonstrate that \\texttt{VORTEX} outperforms baselines in satisfying human-aligned coverage goals while maintaining high task performance. This work introduces a practical and theoretically grounded paradigm for human-AI collaborative optimization guided by natural language.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "25",
        "title": "'Rich Dad, Poor Lad': How do Large Language Models Contextualize Socioeconomic Factors in College Admission ?",
        "author": [
            "Huy Nghiem",
            "Phuong-Anh Nguyen-Le",
            "John Prindle",
            "Rachel Rudinger",
            "Hal DaumÃ© III"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16400",
        "abstract": "Large Language Models (LLMs) are increasingly involved in high-stakes domains, yet how they reason about socially sensitive decisions remains underexplored. We present a large-scale audit of LLMs' treatment of socioeconomic status (SES) in college admissions decisions using a novel dual-process framework inspired by cognitive science. Leveraging a synthetic dataset of 30,000 applicant profiles grounded in real-world correlations, we prompt 4 open-source LLMs (Qwen 2, Mistral v0.3, Gemma 2, Llama 3.1) under 2 modes: a fast, decision-only setup (System 1) and a slower, explanation-based setup (System 2). Results from 5 million prompts reveal that LLMs consistently favor low-SES applicants -- even when controlling for academic performance -- and that System 2 amplifies this tendency by explicitly invoking SES as compensatory justification, highlighting both their potential and volatility as decision-makers. We then propose DPAF, a dual-process audit framework to probe LLMs' reasoning behaviors in sensitive applications.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "26",
        "title": "Subteaming and Adaptive Formation Control for Coordinated Multi-Robot Navigation",
        "author": [
            "Zihao Deng",
            "Peng Gao",
            "Williard Joshua Jose",
            "Maggie Wigness",
            "John Rogers",
            "Brian Reily",
            "Christopher Reardon",
            "Hao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16412",
        "abstract": "Coordinated multi-robot navigation is essential for robots to operate as a team in diverse environments. During navigation, robot teams usually need to maintain specific formations, such as circular formations to protect human teammates at the center. However, in complex scenarios such as narrow corridors, rigidly preserving predefined formations can become infeasible. Therefore, robot teams must be capable of dynamically splitting into smaller subteams and adaptively controlling the subteams to navigate through such scenarios while preserving formations. To enable this capability, we introduce a novel method for SubTeaming and Adaptive Formation (STAF), which is built upon a unified hierarchical learning framework: (1) high-level deep graph cut for team splitting, (2) intermediate-level graph learning for facilitating coordinated navigation among subteams, and (3) low-level policy learning for controlling individual mobile robots to reach their goal positions while avoiding collisions. To evaluate STAF, we conducted extensive experiments in both indoor and outdoor environments using robotics simulations and physical robot teams. Experimental results show that STAF enables the novel capability for subteaming and adaptive formation control, and achieves promising performance in coordinated multi-robot navigation through challenging scenarios. More details are available on the project website: https://hcrlab.gitlab.io/project/STAF.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "27",
        "title": "AHA -- Predicting What Matters Next: Online Highlight Detection Without Looking Ahead",
        "author": [
            "Aiden Chang",
            "Celso De Melo",
            "Stephanie M. Lukin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16421",
        "abstract": "Real-time understanding of continuous video streams is essential for intelligent agents operating in high-stakes environments, including autonomous vehicles, surveillance drones, and disaster response robots. Yet, most existing video understanding and highlight detection methods assume access to the entire video during inference, making them unsuitable for online or streaming scenarios. In particular, current models optimize for offline summarization, failing to support step-by-step reasoning needed for real-time decision-making. We introduce Aha, an autoregressive highlight detection framework that predicts the relevance of each video frame against a task described in natural language. Without accessing future video frames, Aha utilizes a multimodal vision-language model and lightweight, decoupled heads trained on a large, curated dataset of human-centric video labels. To enable scalability, we introduce the Dynamic SinkCache mechanism that achieves constant memory usage across infinite-length streams without degrading performance on standard benchmarks. This encourages the hidden representation to capture high-level task objectives, enabling effective frame-level rankings for informativeness, relevance, and uncertainty with respect to the natural language task. Aha achieves state-of-the-art (SOTA) performance on highlight detection benchmarks, surpassing even prior offline, full-context approaches and video-language models by +5.9% on TVSum and +8.3% on http://Mr.Hisum in mAP (mean Average Precision). We explore Aha's potential for real-world robotics applications given a task-oriented natural language input and a continuous, robot-centric video. Both experiments demonstrate Aha's potential effectiveness as a real-time reasoning module for downstream planning and long-horizon understanding.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "28",
        "title": "Evaluating CxG Generalisation in LLMs via Construction-Based NLI Fine Tuning",
        "author": [
            "Tom Mackintosh",
            "Harish Tayyar Madabushi",
            "Claire Bonial"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16422",
        "abstract": "We probe large language models' ability to learn deep form-meaning mappings as defined by construction grammars. We introduce the ConTest-NLI benchmark of 80k sentences covering eight English constructions from highly lexicalized to highly schematic. Our pipeline generates diverse synthetic NLI triples via templating and the application of a model-in-the-loop filter. This provides aspects of human validation to ensure challenge and label reliability. Zero-shot tests on leading LLMs reveal a 24% drop in accuracy between naturalistic (88%) and adversarial data (64%), with schematic patterns proving hardest. Fine-tuning on a subset of ConTest-NLI yields up to 9% improvement, yet our results highlight persistent abstraction gaps in current LLMs and offer a scalable framework for evaluating construction-informed learning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "29",
        "title": "End-to-end RL Improves Dexterous Grasping Policies",
        "author": [
            "Ritvik Singh",
            "Karl Van Wyk",
            "Pieter Abbeel",
            "Jitendra Malik",
            "Nathan Ratliff",
            "Ankur Handa"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16434",
        "abstract": "This work explores techniques to scale up image-based end-to-end learning for dexterous grasping with an arm + hand system. Unlike state-based RL, vision-based RL is much more memory inefficient, resulting in relatively low batch sizes, which is not amenable for algorithms like PPO. Nevertheless, it is still an attractive method as unlike the more commonly used techniques which distill state-based policies into vision networks, end-to-end RL can allow for emergent active vision behaviors. We identify a key bottleneck in training these policies is the way most existing simulators scale to multiple GPUs using traditional data parallelism techniques. We propose a new method where we disaggregate the simulator and RL (both training and experience buffers) onto separate GPUs. On a node with four GPUs, we have the simulator running on three of them, and PPO running on the fourth. We are able to show that with the same number of GPUs, we can double the number of existing environments compared to the previous baseline of standard data parallelism. This allows us to train vision-based environments, end-to-end with depth, which were previously performing far worse with the baseline. We train and distill both depth and state-based policies into stereo RGB networks and show that depth distillation leads to better results, both in simulation and reality. This improvement is likely due to the observability gap between state and vision policies which does not exist when distilling depth policies into stereo RGB. We further show that the increased batch size brought about by disaggregated simulation also improves real world performance. When deploying in the real world, we improve upon the previous state-of-the-art vision-based results using our end-to-end policies.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "30",
        "title": "SENSE-7: Taxonomy and Dataset for Measuring User Perceptions of Empathy in Sustained Human-AI Conversations",
        "author": [
            "Jina Suh",
            "Lindy Le",
            "Erfan Shayegani",
            "Gonzalo Ramos",
            "Judith Amores",
            "Desmond C. Ong",
            "Mary Czerwinski",
            "Javier Hernandez"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16437",
        "abstract": "Empathy is increasingly recognized as a key factor in human-AI communication, yet conventional approaches to \"digital empathy\" often focus on simulating internal, human-like emotional states while overlooking the inherently subjective, contextual, and relational facets of empathy as perceived by users. In this work, we propose a human-centered taxonomy that emphasizes observable empathic behaviors and introduce a new dataset, Sense-7, of real-world conversations between information workers and Large Language Models (LLMs), which includes per-turn empathy annotations directly from the users, along with user characteristics, and contextual details, offering a more user-grounded representation of empathy. Analysis of 695 conversations from 109 participants reveals that empathy judgments are highly individualized, context-sensitive, and vulnerable to disruption when conversational continuity fails or user expectations go unmet. To promote further research, we provide a subset of 672 anonymized conversation and provide exploratory classification analysis, showing that an LLM-based classifier can recognize 5 levels of empathy with an encouraging average Spearman $\\rho$=0.369 and Accuracy=0.487 over this set. Overall, our findings underscore the need for AI designs that dynamically tailor empathic behaviors to user contexts and goals, offering a roadmap for future research and practical development of socially attuned, human-centered artificial agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "31",
        "title": "AutoArabic: A Three-Stage Framework for Localizing Video-Text Retrieval Benchmarks",
        "author": [
            "Mohamed Eltahir",
            "Osamah Sarraj",
            "Abdulrahman Alfrihidi",
            "Taha Alshatiri",
            "Mohammed Khurd",
            "Mohammed Bremoo",
            "Tanveer Hussain"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16438",
        "abstract": "Video-to-text and text-to-video retrieval are dominated by English benchmarks (e.g. DiDeMo, MSR-VTT) and recent multilingual corpora (e.g. RUDDER), yet Arabic remains underserved, lacking localized evaluation metrics. We introduce a three-stage framework, AutoArabic, utilizing state-of-the-art large language models (LLMs) to translate non-Arabic benchmarks into Modern Standard Arabic, reducing the manual revision required by nearly fourfold. The framework incorporates an error detection module that automatically flags potential translation errors with 97% accuracy. Applying the framework to DiDeMo, a video retrieval benchmark produces DiDeMo-AR, an Arabic variant with 40,144 fluent Arabic descriptions. An analysis of the translation errors is provided and organized into an insightful taxonomy to guide future Arabic localization efforts. We train a CLIP-style baseline with identical hyperparameters on the Arabic and English variants of the benchmark, finding a moderate performance gap (about 3 percentage points at Recall@1), indicating that Arabic localization preserves benchmark difficulty. We evaluate three post-editing budgets (zero/ flagged-only/ full) and find that performance improves monotonically with more post-editing, while the raw LLM output (zero-budget) remains usable. To ensure reproducibility to other languages, we made the code available at https://github.com/Tahaalshatiri/AutoArabic.",
        "tags": [
            "CLIP",
            "Detection",
            "LLM",
            "Text-to-Video"
        ]
    },
    {
        "id": "32",
        "title": "Evaluating the Effectiveness and Scalability of LLM-Based Data Augmentation for Retrieval",
        "author": [
            "Pranjal A. Chitale",
            "Bishal Santra",
            "Yashoteja Prabhu",
            "Amit Sharma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16442",
        "abstract": "Compact dual-encoder models are widely used for retrieval owing to their efficiency and scalability. However, such models often underperform compared to their Large Language Model (LLM)-based retrieval counterparts, likely due to their limited world knowledge. While LLM-based data augmentation has been proposed as a strategy to bridge this performance gap, there is insufficient understanding of its effectiveness and scalability to real-world retrieval problems. Existing research does not systematically explore key factors such as the optimal augmentation scale, the necessity of using large augmentation models, and whether diverse augmentations improve generalization, particularly in out-of-distribution (OOD) settings. This work presents a comprehensive study of the effectiveness of LLM augmentation for retrieval, comprising over 100 distinct experimental settings of retrieval models, augmentation models and augmentation strategies. We find that, while augmentation enhances retrieval performance, its benefits diminish beyond a certain augmentation scale, even with diverse augmentation strategies. Surprisingly, we observe that augmentation with smaller LLMs can achieve performance competitive with larger augmentation models. Moreover, we examine how augmentation effectiveness varies with retrieval model pre-training, revealing that augmentation provides the most benefit to models which are not well pre-trained. Our insights pave the way for more judicious and efficient augmentation strategies, thus enabling informed decisions and maximizing retrieval performance while being more cost-effective. Code and augmented datasets accompanying this work are publicly available at https://aka.ms/DAGR.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "33",
        "title": "FiLM-Nav: Efficient and Generalizable Navigation via VLM Fine-tuning",
        "author": [
            "Naoki Yokoyama",
            "Sehoon Ha"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16445",
        "abstract": "Enabling robotic assistants to navigate complex environments and locate objects described in free-form language is a critical capability for real-world deployment. While foundation models, particularly Vision-Language Models (VLMs), offer powerful semantic understanding, effectively adapting their web-scale knowledge for embodied decision-making remains a key challenge. We present FiLM-Nav (Fine-tuned Language Model for Navigation), an approach that directly fine-tunes pre-trained VLM as the navigation policy. In contrast to methods that use foundation models primarily in a zero-shot manner or for map annotation, FiLM-Nav learns to select the next best exploration frontier by conditioning directly on raw visual trajectory history and the navigation goal. Leveraging targeted simulated embodied experience allows the VLM to ground its powerful pre-trained representations in the specific dynamics and visual patterns relevant to goal-driven navigation. Critically, fine-tuning on a diverse data mixture combining ObjectNav, OVON, ImageNav, and an auxiliary spatial reasoning task proves essential for achieving robustness and broad generalization. FiLM-Nav sets a new state-of-the-art in both SPL and success rate on HM3D ObjectNav among open-vocabulary methods, and sets a state-of-the-art SPL on the challenging HM3D-OVON benchmark, demonstrating strong generalization to unseen object categories. Our work validates that directly fine-tuning VLMs on diverse simulated embodied data is a highly effective pathway towards generalizable and efficient semantic navigation capabilities.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "34",
        "title": "Local Mechanisms of Compositional Generalization in Conditional Diffusion",
        "author": [
            "Arwen Bradley"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16447",
        "abstract": "Conditional diffusion models appear capable of compositional generalization, i.e., generating convincing samples for out-of-distribution combinations of conditioners, but the mechanisms underlying this ability remain unclear. To make this concrete, we study length generalization, the ability to generate images with more objects than seen during training. In a controlled CLEVR setting (Johnson et al., 2017), we find that length generalization is achievable in some cases but not others, suggesting that models only sometimes learn the underlying compositional structure. We then investigate locality as a structural mechanism for compositional generalization. Prior works proposed score locality as a mechanism for creativity in unconditional diffusion models (Kamb & Ganguli, 2024; Niedoba et al., 2024), but did not address flexible conditioning or compositional generalization. In this paper, we prove an exact equivalence between a specific compositional structure (\"conditional projective composition\") (Bradley et al., 2025) and scores with sparse dependencies on both pixels and conditioners (\"local conditional scores\"). This theory also extends to feature-space compositionality. We validate our theory empirically: CLEVR models that succeed at length generalization exhibit local conditional scores, while those that fail do not. Furthermore, we show that a causal intervention explicitly enforcing local conditional scores restores length generalization in a previously failing model. Finally, we investigate feature-space compositionality in color-conditioned CLEVR, and find preliminary evidence of compositional structure in SDXL.",
        "tags": [
            "Diffusion",
            "SDXL"
        ]
    },
    {
        "id": "35",
        "title": "KRAST: Knowledge-Augmented Robotic Action Recognition with Structured Text for Vision-Language Models",
        "author": [
            "Son Hai Nguyen",
            "Diwei Wang",
            "Jinhyeok Jang",
            "Hyewon Seo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16452",
        "abstract": "Accurate vision-based action recognition is crucial for developing autonomous robots that can operate safely and reliably in complex, real-world environments. In this work, we advance video-based recognition of indoor daily actions for robotic perception by leveraging vision-language models (VLMs) enriched with domain-specific knowledge. We adapt a prompt-learning framework in which class-level textual descriptions of each action are embedded as learnable prompts into a frozen pre-trained VLM backbone. Several strategies for structuring and encoding these textual descriptions are designed and evaluated. Experiments on the ETRI-Activity3D dataset demonstrate that our method, using only RGB video inputs at test time, achieves over 95\\% accuracy and outperforms state-of-the-art approaches. These results highlight the effectiveness of knowledge-augmented prompts in enabling robust action recognition with minimal supervision.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "36",
        "title": "GPO: Learning from Critical Steps to Improve LLM Reasoning",
        "author": [
            "Jiahao Yu",
            "Zelei Cheng",
            "Xian Wu",
            "Xinyu Xing"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16456",
        "abstract": "Large language models (LLMs) are increasingly used in various domains, showing impressive potential on different tasks. Recently, reasoning LLMs have been proposed to improve the \\textit{reasoning} or \\textit{thinking} capabilities of LLMs to solve complex problems. Despite the promising results of reasoning LLMs, enhancing the multi-step reasoning capabilities of LLMs still remains a significant challenge. While existing optimization methods have advanced the LLM reasoning capabilities, they often treat reasoning trajectories as a whole, without considering the underlying critical steps within the trajectory. In this paper, we introduce \\textbf{G}uided \\textbf{P}ivotal \\textbf{O}ptimization (GPO), a novel fine-tuning strategy that dives into the reasoning process to enable more effective improvements. GPO first identifies the `critical step' within a reasoning trajectory - a point that the model must carefully proceed to succeed at the problem. We locate the critical step by estimating the advantage function. GPO then resets the policy to the critical step, samples the new rollout and prioritizes the learning process on those rollouts. This focus allows the model to learn more effectively from pivotal moments within the reasoning process to improve the reasoning performance. We demonstrate that GPO is a general strategy that can be integrated with various optimization methods to improve reasoning performance. Besides theoretical analysis, our experiments across challenging reasoning benchmarks show that GPO can consistently and significantly enhance the performance of existing optimization methods, showcasing its effectiveness and generalizability in improving LLM reasoning by concentrating on pivotal moments within the generation process.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "37",
        "title": "Implicit Behavioral Alignment of Language Agents in High-Stakes Crowd Simulations",
        "author": [
            "Yunzhe Wang",
            "Gale M. Lucas",
            "Burcin Becerik-Gerber",
            "Volkan Ustun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16457",
        "abstract": "Language-driven generative agents have enabled large-scale social simulations with transformative uses, from interpersonal training to aiding global policy-making. However, recent studies indicate that generative agent behaviors often deviate from expert expectations and real-world data--a phenomenon we term the Behavior-Realism Gap. To address this, we introduce a theoretical framework called Persona-Environment Behavioral Alignment (PEBA), formulated as a distribution matching problem grounded in Lewin's behavior equation stating that behavior is a function of the person and their environment. Leveraging PEBA, we propose PersonaEvolve (PEvo), an LLM-based optimization algorithm that iteratively refines agent personas, implicitly aligning their collective behaviors with realistic expert benchmarks within a specified environmental context. We validate PEvo in an active shooter incident simulation we developed, achieving an 84% average reduction in distributional divergence compared to no steering and a 34% improvement over explicit instruction baselines. Results also show PEvo-refined personas generalize to novel, related simulation scenarios. Our method greatly enhances behavioral realism and reliability in high-stakes social simulations. More broadly, the PEBA-PEvo framework provides a principled approach to developing trustworthy LLM-driven social simulations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "38",
        "title": "Intrinsic Meets Extrinsic Fairness: Assessing the Downstream Impact of Bias Mitigation in Large Language Models",
        "author": [
            "'Mina Arzaghi'",
            "'Alireza Dehghanpour Farashah'",
            "'Florian Carichon'",
            "' Golnoosh Farnadi'"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16462",
        "abstract": "Large Language Models (LLMs) exhibit socio-economic biases that can propagate into downstream tasks. While prior studies have questioned whether intrinsic bias in LLMs affects fairness at the downstream task level, this work empirically investigates the connection. We present a unified evaluation framework to compare intrinsic bias mitigation via concept unlearning with extrinsic bias mitigation via counterfactual data augmentation (CDA). We examine this relationship through real-world financial classification tasks, including salary prediction, employment status, and creditworthiness assessment. Using three open-source LLMs, we evaluate models both as frozen embedding extractors and as fine-tuned classifiers. Our results show that intrinsic bias mitigation through unlearning reduces intrinsic gender bias by up to 94.9%, while also improving downstream task fairness metrics, such as demographic parity by up to 82%, without compromising accuracy. Our framework offers practical guidance on where mitigation efforts can be most effective and highlights the importance of applying early-stage mitigation before downstream deployment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "39",
        "title": "Computational Analysis of Conversation Dynamics through Participant Responsivity",
        "author": [
            "Margaret Hughes",
            "Brandon Roy",
            "Elinor Poole-Dayan",
            "Deb Roy",
            "Jad Kabbara"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16464",
        "abstract": "Growing literature explores toxicity and polarization in discourse, with comparatively less work on characterizing what makes dialogue prosocial and constructive. We explore conversational discourse and investigate a method for characterizing its quality built upon the notion of ``responsivity'' -- whether one person's conversational turn is responding to a preceding turn. We develop and evaluate methods for quantifying responsivity -- first through semantic similarity of speaker turns, and second by leveraging state-of-the-art large language models (LLMs) to identify the relation between two speaker turns. We evaluate both methods against a ground truth set of human-annotated conversations. Furthermore, selecting the better performing LLM-based approach, we characterize the nature of the response -- whether it responded to that preceding turn in a substantive way or not.\nWe view these responsivity links as a fundamental aspect of dialogue but note that conversations can exhibit significantly different responsivity structures. Accordingly, we then develop conversation-level derived metrics to address various aspects of conversational discourse. We use these derived metrics to explore other conversations and show that they support meaningful characterizations and differentiations across a diverse collection of conversations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "40",
        "title": "A Framework for Optimal Ankle Design of Humanoid Robots",
        "author": [
            "Guglielmo Cervettini",
            "Roberto Mauceri",
            "Alex Coppola",
            "Fabio Bergonti",
            "Luca Fiorio",
            "Marco Maggiali",
            "Daniele Pucci"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16469",
        "abstract": "The design of the humanoid ankle is critical for safe and efficient ground interaction. Key factors such as mechanical compliance and motor mass distribution have driven the adoption of parallel mechanism architectures. However, selecting the optimal configuration depends on both actuator availability and task requirements. We propose a unified methodology for the design and evaluation of parallel ankle mechanisms. A multi-objective optimization synthesizes the mechanism geometry, the resulting solutions are evaluated using a scalar cost function that aggregates key performance metrics for cross-architecture comparison. We focus on two representative architectures: the Spherical-Prismatic-Universal (SPU) and the Revolute-Spherical-Universal (RSU). For both, we resolve the kinematics, and for the RSU, introduce a parameterization that ensures workspace feasibility and accelerates optimization. We validate our approach by redesigning the ankle of an existing humanoid robot. The optimized RSU consistently outperforms both the original serial design and a conventionally engineered RSU, reducing the cost function by up to 41% and 14%, respectively.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "41",
        "title": "Eye Gaze Tells You Where to Compute: Gaze-Driven Efficient VLMs",
        "author": [
            "Qinyu Chen",
            "Jiawen Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16476",
        "abstract": "Vision-Language Models (VLMs) deliver impressive performance in understanding visual content with language instructions. However, redundancy in vision tokens results in the degenerated inference efficiency of VLMs, which hinders real-time use on edge consumer devices such as AR/VR devices. Existing efficiency methods commonly prune visual tokens using learned saliency, sparse attention schedules, or controller policies, but they often require architectural modification or access to intermediate activations. These pipelines add inference-time modules that increase compute and memory and often lead to an accuracy trade-off. Moreover, they also suffer from misalignment between the prompts and the region of interest in the images. Without human guidance, the model may focus on the wrong regions and miss small, high-frequency details when prompts or scenes change. In this paper, we propose GazeVLM, a training-free framework that uses the human eye gaze as a natural supervisory signal to allocate computation where it matters. By extracting gaze-driven regions of interest (ROIs) and optionally combining them with a low-resolution global view, GazeVLM mimics fovea-periphery perception to cut redundant visual tokens while preserving task-relevant details. We evaluate the visual question answering tasks on Qwen2.5-VL-3B/7B on the VOILA-COCO benchmark with human gaze. Quality of the answer is assessed by GPT-4o pairwise judging and a weighted score over coverage, accuracy, details, and fluency. Efficiency is measured by token counts and FLOPs. GazeVLM reduces visual tokens by up to 93.1%, total tokens by up to 59.6%, and FLOPs by 50%, while keeping better answer quality relative to full-resolution baselines. Our results show that aligning model computation with human gaze offers a simple, plug-and-play path toward efficient VLM inference on consumer devices.",
        "tags": [
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "42",
        "title": "Robot Conga: A Leader-Follower Walking Approach to Sequential Path Following in Multi-Agent Systems",
        "author": [
            "Pranav Tiwari",
            "Soumyodipta Nath"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16482",
        "abstract": "Coordinated path following in multi-agent systems is a key challenge in robotics, with applications in automated logistics, surveillance, and collaborative exploration. Traditional formation control techniques often rely on time-parameterized trajectories and path integrals, which can result in synchronization issues and rigid behavior. In this work, we address the problem of sequential path following, where agents maintain fixed spatial separation along a common trajectory, guided by a leader under centralized control. We introduce Robot Conga, a leader-follower control strategy that updates each agent's desired state based on the leader's spatial displacement rather than time, assuming access to a global position reference, an assumption valid in indoor environments equipped with motion capture, vision-based tracking, or UWB localization systems. The algorithm was validated in simulation using both TurtleBot3 and quadruped (Laikago) robots. Results demonstrate accurate trajectory tracking, stable inter-agent spacing, and fast convergence, with all agents aligning within 250 time steps (approx. 0.25 seconds) in the quadruped case, and almost instantaneously in the TurtleBot3 implementation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "43",
        "title": "Octree Latent Diffusion for Semantic 3D Scene Generation and Completion",
        "author": [
            "Xujia Zhang",
            "Brendan Crowe",
            "Christoffer Heckman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16483",
        "abstract": "The completion, extension, and generation of 3D semantic scenes are an interrelated set of capabilities that are useful for robotic navigation and exploration. Existing approaches seek to decouple these problems and solve them oneoff. Additionally, these approaches are often domain-specific, requiring separate models for different data distributions, e.g. indoor vs. outdoor scenes. To unify these techniques and provide cross-domain compatibility, we develop a single framework that can perform scene completion, extension, and generation in both indoor and outdoor scenes, which we term Octree Latent Semantic Diffusion. Our approach operates directly on an efficient dual octree graph latent representation: a hierarchical, sparse, and memory-efficient occupancy structure. This technique disentangles synthesis into two stages: (i) structure diffusion, which predicts binary split signals to construct a coarse occupancy octree, and (ii) latent semantic diffusion, which generates semantic embeddings decoded by a graph VAE into voxellevel semantic labels. To perform semantic scene completion or extension, our model leverages inference-time latent inpainting, or outpainting respectively. These inference-time methods use partial LiDAR scans or maps to condition generation, without the need for retraining or finetuning. We demonstrate highquality structure, coherent semantics, and robust completion from single LiDAR scans, as well as zero-shot generalization to out-of-distribution LiDAR data. These results indicate that completion-through-generation in a dual octree graph latent space is a practical and scalable alternative to regression-based pipelines for real-world robotic perception tasks.",
        "tags": [
            "3D",
            "Diffusion",
            "Inpainting",
            "VAE"
        ]
    },
    {
        "id": "44",
        "title": "Spatial Encoding of Flow Spaces for Intelligent SDN Applications",
        "author": [
            "Abdur Rouf",
            "Murat Yuksel"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16485",
        "abstract": "Efficient encoding of network flow spaces while preserving spatial locality is essential for intelligent Software-Defined Networking (SDN) applications, particularly those employing reinforcement learning (RL) methods in a reactive manner. In this work, we introduce a spatially aware Bloom Filter-based approach to encode IP flow pairs, leveraging their inherent geographical locality. Through controlled experiments using IoT traffic data, we demonstrate that Bloom Filters effectively preserve spatial relationships among flows. Our findings show that Bloom Filters degrade gracefully, maintaining predictable spatial correlations critical for RL state representation. We integrate this encoding into a DQN-based eviction strategy for reactive SDN forwarding. Experiments show that Bloom Filter-encoded, spatially aware flow representation enables up to 7% and 8% reduction in normalized miss rate over LRU and LFU, respectively, across 10 hours of traffic, demonstrating potential for low-latency applications. This experiment justifies the usefulness of preserving spatial correlation by encoding the flow space into a manageable size, opening a novel research direction for RL-based SDN applications.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "45",
        "title": "The Oracle Has Spoken: A Multi-Aspect Evaluation of Dialogue in Pythia",
        "author": [
            "Zixun Chen",
            "Petr Babkin",
            "Akshat Gupta",
            "Gopala Anumanchipalli",
            "Xiaomo Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16487",
        "abstract": "Dialogue is one of the landmark abilities of large language models (LLMs). Despite its ubiquity, few studies actually distinguish specific ingredients underpinning dialogue behavior emerging during post-training. We employ a comprehensive suite of model-based metrics, each targeting a distinct fine-grained aspect of dialogue, motivated by linguistic theory. We evaluate how the performance of pre-trained Pythia models changes with respect to each of those dimensions, depending on model size and as a result of supervised fine-tuning on conversational datasets. We observe only a mild impact of raw model size on most metrics, whereas fine-tuning quickly saturates the scores for all but the smallest models tested. Somewhat contrary to our expectations, many metrics show very similar trends, especially if they are all rooted in the same evaluator model, which raises the question of their reliability in measuring a specific dimension. To that end, we conduct additional analyses of score distributions, metric correlations, and term frequencies in generated responses to help explain our observations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "46",
        "title": "FairTune: A Bias-Aware Fine-Tuning Framework Towards Fair Heart Rate Prediction from PPG",
        "author": [
            "Lovely Yeswanth Panchumarthi",
            "Saurabh Kataria",
            "Yi Wu",
            "Xiao Hu",
            "Alex Fedorov",
            "Hyunjung Gloria Kwak"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16491",
        "abstract": "Foundation models pretrained on physiological data such as photoplethysmography (PPG) signals are increasingly used to improve heart rate (HR) prediction across diverse settings. Fine-tuning these models for local deployment is often seen as a practical and scalable strategy. However, its impact on demographic fairness particularly under domain shifts remains underexplored. We fine-tune PPG-GPT a transformer-based foundation model pretrained on intensive care unit (ICU) data across three heterogeneous datasets (ICU, wearable, smartphone) and systematically evaluate the effects on HR prediction accuracy and gender fairness. While fine-tuning substantially reduces mean absolute error (up to 80%), it can simultaneously widen fairness gaps, especially in larger models and under significant distributional characteristics shifts. To address this, we introduce FairTune, a bias-aware fine-tuning framework in which we benchmark three mitigation strategies: class weighting based on inverse group frequency (IF), Group Distributionally Robust Optimization (GroupDRO), and adversarial debiasing (ADV). We find that IF and GroupDRO significantly reduce fairness gaps without compromising accuracy, with effectiveness varying by deployment domain. Representation analyses further reveal that mitigation techniques reshape internal embeddings to reduce demographic clustering. Our findings highlight that fairness does not emerge as a natural byproduct of fine-tuning and that explicit mitigation is essential for equitable deployment of physiological foundation models.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "47",
        "title": "Can an Individual Manipulate the Collective Decisions of Multi-Agents?",
        "author": [
            "Fengyuan Liu",
            "Rui Zhao",
            "Shuo Chen",
            "Guohao Li",
            "Philip Torr",
            "Lei Han",
            "Jindong Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16494",
        "abstract": "Individual Large Language Models (LLMs) have demonstrated significant capabilities across various domains, such as healthcare and law. Recent studies also show that coordinated multi-agent systems exhibit enhanced decision-making and reasoning abilities through collaboration. However, due to the vulnerabilities of individual LLMs and the difficulty of accessing all agents in a multi-agent system, a key question arises: If attackers only know one agent, could they still generate adversarial samples capable of misleading the collective decision? To explore this question, we formulate it as a game with incomplete information, where attackers know only one target agent and lack knowledge of the other agents in the system. With this formulation, we propose M-Spoiler, a framework that simulates agent interactions within a multi-agent system to generate adversarial samples. These samples are then used to manipulate the target agent in the target system, misleading the system's collaborative decision-making process. More specifically, M-Spoiler introduces a stubborn agent that actively aids in optimizing adversarial samples by simulating potential stubborn responses from agents in the target system. This enhances the effectiveness of the generated adversarial samples in misleading the system. Through extensive experiments across various tasks, our findings confirm the risks posed by the knowledge of an individual agent in multi-agent systems and demonstrate the effectiveness of our framework. We also explore several defense mechanisms, showing that our proposed attack framework remains more potent than baselines, underscoring the need for further research into defensive strategies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "48",
        "title": "Shift Parallelism: Low-Latency, High-Throughput LLM Inference for Dynamic Workloads",
        "author": [
            "Mert Hidayetoglu",
            "Aurick Qiao",
            "Michael Wyatt",
            "Jeff Rasley",
            "Yuxiong He",
            "Samyam Rajbhandari"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16495",
        "abstract": "Efficient parallelism is necessary for achieving low-latency, high-throughput inference with large language models (LLMs). Tensor parallelism (TP) is the state-of-the-art method for reducing LLM response latency, however GPU communications reduces combined token throughput. On the other hand, data parallelism (DP) obtains a higher throughput yet is slow in response latency. Best of both worlds does not exist, and it is not possible to combine TP and DP because of the KV cache variance across the parallelisms.\nWe notice Sequence Parallelism (SP - Ulysses in training) has similar properties as DP but with KV cache invariance. We adapt SP to inference, and combine it with TP to get the best of both worlds. Our solution: Shift Parallelism.\nShift Parallelism dynamically switches across TP and SP, and minimizes latency in low traffic without losing throughput in high traffic. The efficient GPU communications of Shift Parallelism yields up to i) 1.51x faster response in interactive workloads and ii) 50% higher throughput in batch workloads, compared to a TP-only solution.\nWe evaluate Shift Parallelism with real-world production traces with dynamic traffic patterns as well as synthetic benchmarking patterns across models, context sizes, and arrival rates. All results affirm the same: Shift Parallelism has a better the latency vs. throughput tradeoff than TP or DP, and hence obtains low latency without degrading throughput in dynamic workloads.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "49",
        "title": "A Closer Look at Model Collapse: From a Generalization-to-Memorization Perspective",
        "author": [
            "Lianghe Shi",
            "Meng Wu",
            "Huijie Zhang",
            "Zekai Zhang",
            "Molei Tao",
            "Qing Qu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16499",
        "abstract": "The widespread use of diffusion models has led to an abundance of AI-generated data, raising concerns about model collapse -- a phenomenon in which recursive iterations of training on synthetic data lead to performance degradation. Prior work primarily characterizes this collapse via variance shrinkage or distribution shift, but these perspectives miss practical manifestations of model collapse. This paper identifies a transition from generalization to memorization during model collapse in diffusion models, where models increasingly replicate training data instead of generating novel content during iterative training on synthetic samples. This transition is directly driven by the declining entropy of the synthetic training data produced in each training cycle, which serves as a clear indicator of model degradation. Motivated by this insight, we propose an entropy-based data selection strategy to mitigate the transition from generalization to memorization and alleviate model collapse. Empirical results show that our approach significantly enhances visual quality and diversity in recursive generation, effectively preventing collapse.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "50",
        "title": "RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation",
        "author": [
            "Tianyi Yan",
            "Wencheng Han",
            "Xia Zhou",
            "Xueyang Zhang",
            "Kun Zhan",
            "Cheng-zhong Xu",
            "Jianbing Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16500",
        "abstract": "Synthetic data is crucial for advancing autonomous driving (AD) systems, yet current state-of-the-art video generation models, despite their visual realism, suffer from subtle geometric distortions that limit their utility for downstream perception tasks. We identify and quantify this critical issue, demonstrating a significant performance gap in 3D object detection when using synthetic versus real data. To address this, we introduce Reinforcement Learning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusion models by incorporating rewards from specialized latent-space AD perception models. Its core components include an efficient Latent-Space Windowing Optimization technique for targeted feedback during diffusion, and a Hierarchical Geometric Reward (HGR) system providing multi-level rewards for point-line-plane alignment, and scene occupancy coherence. To quantify these distortions, we propose GeoScores. Applied to models like DiVE on nuScenes, RLGF substantially reduces geometric errors (e.g., VP error by 21\\%, Depth error by 57\\%) and dramatically improves 3D object detection mAP by 12.7\\%, narrowing the gap to real-data performance. RLGF offers a plug-and-play solution for generating geometrically sound and reliable synthetic videos for AD development.",
        "tags": [
            "3D",
            "Detection",
            "Diffusion",
            "RL",
            "Video Generation"
        ]
    },
    {
        "id": "51",
        "title": "GRIL: Knowledge Graph Retrieval-Integrated Learning with Large Language Models",
        "author": [
            "Jialin Chen",
            "Houyu Zhang",
            "Seongjun Yun",
            "Alejandro Mottini",
            "Rex Ying",
            "Xiang Song",
            "Vassilis N. Ioannidis",
            "Zheng Li",
            "Qingjun Cui"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16502",
        "abstract": "Retrieval-Augmented Generation (RAG) has significantly mitigated the hallucinations of Large Language Models (LLMs) by grounding the generation with external knowledge. Recent extensions of RAG to graph-based retrieval offer a promising direction, leveraging the structural knowledge for multi-hop reasoning. However, existing graph RAG typically decouples retrieval and reasoning processes, which prevents the retriever from adapting to the reasoning needs of the LLM. They also struggle with scalability when performing multi-hop expansion over large-scale graphs, or depend heavily on annotated ground-truth entities, which are often unavailable in open-domain settings. To address these challenges, we propose a novel graph retriever trained end-to-end with LLM, which features an attention-based growing and pruning mechanism, adaptively navigating multi-hop relevant entities while filtering out noise. Within the extracted subgraph, structural knowledge and semantic features are encoded via soft tokens and the verbalized graph, respectively, which are infused into the LLM together, thereby enhancing its reasoning capability and facilitating interactive joint training of the graph retriever and the LLM reasoner. Experimental results across three QA benchmarks show that our approach consistently achieves state-of-the-art performance, validating the strength of joint graph-LLM optimization for complex reasoning tasks. Notably, our framework eliminates the need for predefined ground-truth entities by directly optimizing the retriever using LLM logits as implicit feedback, making it especially effective in open-domain settings.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "52",
        "title": "OS-DiffVSR: Towards One-step Latent Diffusion Model for High-detailed Real-world Video Super-Resolution",
        "author": [
            "Hanting Li",
            "Huaao Tang",
            "Jianhong Han",
            "Tianxiong Zhou",
            "Jiulong Cui",
            "Haizhen Xie",
            "Yan Chen",
            "Jie Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16507",
        "abstract": "Recently, latent diffusion models has demonstrated promising performance in real-world video super-resolution (VSR) task, which can reconstruct high-quality videos from distorted low-resolution input through multiple diffusion steps. Compared to image super-resolution (ISR), VSR methods needs to process each frame in a video, which poses challenges to its inference efficiency. However, video quality and inference efficiency have always been a trade-off for the diffusion-based VSR methods. In this work, we propose One-Step Diffusion model for real-world Video Super-Resolution, namely OS-DiffVSR. Specifically, we devise a novel adjacent frame adversarial training paradigm, which can significantly improve the quality of synthetic videos. Besides, we devise a multi-frame fusion mechanism to maintain inter-frame temporal consistency and reduce the flicker in video. Extensive experiments on several popular VSR benchmarks demonstrate that OS-DiffVSR can even achieve better quality than existing diffusion-based VSR methods that require dozens of sampling steps.",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "53",
        "title": "Federated Learning with Ad-hoc Adapter Insertions: The Case of Soft-Embeddings for Training Classifier-as-Retriever",
        "author": [
            "Marijan Fofonjka",
            "Shahryar Zehtabi",
            "Alireza Behtash",
            "Tyler Mauer",
            "David Stout"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16508",
        "abstract": "When existing retrieval-augmented generation (RAG) solutions are intended to be used for new knowledge domains, it is necessary to update their encoders, which are taken to be pretrained large language models (LLMs). However, fully finetuning these large models is compute- and memory-intensive, and even infeasible when deployed on resource-constrained edge devices. We propose a novel encoder architecture in this work that addresses this limitation by using a frozen small language model (SLM), which satisfies the memory constraints of edge devices, and inserting a small adapter network before the transformer blocks of the SLM. The trainable adapter takes the token embeddings of the new corpus and learns to produce enhanced soft embeddings for it, while requiring significantly less compute power to update than full fine-tuning. We further propose a novel retrieval mechanism by attaching a classifier head to the SLM encoder, which is trained to learn a similarity mapping of the input embeddings to their corresponding documents. Finally, to enable the online fine-tuning of both (i) the encoder soft embeddings and (ii) the classifier-as-retriever on edge devices, we adopt federated learning (FL) and differential privacy (DP) to achieve an efficient, privacy-preserving, and product-grade training solution. We conduct a theoretical analysis of our methodology, establishing convergence guarantees under mild assumptions on gradient variance when deployed for general smooth nonconvex loss functions. Through extensive numerical experiments, we demonstrate (i) the efficacy of obtaining soft embeddings to enhance the encoder, (ii) training a classifier to improve the retriever, and (iii) the role of FL in achieving speedup.",
        "tags": [
            "LLM",
            "RAG",
            "Transformer"
        ]
    },
    {
        "id": "54",
        "title": "Trace Replay Simulation of MIT SuperCloud for Studying Optimal Sustainability Policies",
        "author": [
            "Wesley Brewer",
            "Matthias Maiterth",
            "Damien Fay"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16513",
        "abstract": "The rapid growth of AI supercomputing is creating unprecedented power demands, with next-generation GPU datacenters requiring hundreds of megawatts and producing fast, large swings in consumption. To address the resulting challenges for utilities and system operators, we extend ExaDigiT, an open-source digital twin framework for modeling power, cooling, and scheduling of supercomputers. Originally developed for replaying traces from leadership-class HPC systems, ExaDigiT now incorporates heterogeneity, multi-tenancy, and cloud-scale workloads. In this work, we focus on trace replay and rescheduling of jobs on the MIT SuperCloud TX-GAIA system to enable reinforcement learning (RL)-based experimentation with sustainability policies. The RAPS module provides a simulation environment with detailed power and performance statistics, supporting the study of scheduling strategies, incentive structures, and hardware/software prototyping. Preliminary RL experiments using Proximal Policy Optimization demonstrate the feasibility of learning energy-aware scheduling decisions, highlighting ExaDigiT's potential as a platform for exploring optimal policies to improve throughput, efficiency, and sustainability.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "55",
        "title": "LLM-Guided Co-Training for Text Classification",
        "author": [
            "Md Mezbaur Rahman",
            "Cornelia Caragea"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16516",
        "abstract": "In this paper, we introduce a novel weighted co-training approach that is guided by Large Language Models (LLMs). Namely, in our co-training approach, we use LLM labels on unlabeled data as target labels and co-train two encoder-only based networks that train each other over multiple iterations: first, all samples are forwarded through each network and historical estimates of each network's confidence in the LLM label are recorded; second, a dynamic importance weight is derived for each sample according to each network's belief in the quality of the LLM label for that sample; finally, the two networks exchange importance weights with each other -- each network back-propagates all samples weighted with the importance weights coming from its peer network and updates its own parameters. By strategically utilizing LLM-generated guidance, our approach significantly outperforms conventional SSL methods, particularly in settings with abundant unlabeled data. Empirical results show that it achieves state-of-the-art performance on 4 out of 5 benchmark datasets and ranks first among 14 compared methods according to the Friedman test. Our results highlight a new direction in semi-supervised learning -- where LLMs serve as knowledge amplifiers, enabling backbone co-training models to achieve state-of-the-art performance efficiently.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "56",
        "title": "Seeing Culture: A Benchmark for Visual Reasoning and Grounding",
        "author": [
            "Burak Satar",
            "Zhixin Ma",
            "Patrick A. Irawan",
            "Wilfried A. Mulyawan",
            "Jing Jiang",
            "Ee-Peng Lim",
            "Chong-Wah Ngo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16517",
        "abstract": "Multimodal vision-language models (VLMs) have made substantial progress in various tasks that require a combined understanding of visual and textual content, particularly in cultural understanding tasks, with the emergence of new cultural datasets. However, these datasets frequently fall short of providing cultural reasoning while underrepresenting many cultures. In this paper, we introduce the Seeing Culture Benchmark (SCB), focusing on cultural reasoning with a novel approach that requires VLMs to reason on culturally rich images in two stages: i) selecting the correct visual option with multiple-choice visual question answering (VQA), and ii) segmenting the relevant cultural artifact as evidence of reasoning. Visual options in the first stage are systematically organized into three types: those originating from the same country, those from different countries, or a mixed group. Notably, all options are derived from a singular category for each type. Progression to the second stage occurs only after a correct visual option is chosen. The SCB benchmark comprises 1,065 images that capture 138 cultural artifacts across five categories from seven Southeast Asia countries, whose diverse cultures are often overlooked, accompanied by 3,178 questions, of which 1,093 are unique and meticulously curated by human annotators. Our evaluation of various VLMs reveals the complexities involved in cross-modal cultural reasoning and highlights the disparity between visual reasoning and spatial grounding in culturally nuanced scenarios. The SCB serves as a crucial benchmark for identifying these shortcomings, thereby guiding future developments in the field of cultural reasoning. https://github.com/buraksatar/SeeingCulture",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "57",
        "title": "FG-Attn: Leveraging Fine-Grained Sparsity In Diffusion Transformers",
        "author": [
            "Sankeerth Durvasula",
            "Kavya Sreedhar",
            "Zain Moustafa",
            "Suraj Kothawade",
            "Ashish Gondimalla",
            "Suvinay Subramanian",
            "Narges Shahidi",
            "Nandita Vijaykumar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16518",
        "abstract": "Generating realistic videos with diffusion transformers demands significant computation, with attention layers the central bottleneck; even producing a short clip requires running a transformer over a very long sequence of embeddings, e.g., more than 30K embeddings for a 5-second video, incurring significant latency. Prior work aims to mitigate this bottleneck by exploiting sparsity in the attention layers to reduce computation. However, these works typically rely on block-sparse attention, which skips score computation only when all entries in a block of attention scores (corresponding to M queries and M keys, with M = 64 typically) are zero. This coarse-granular skipping of attention scores does not fully exploit sparsity in the attention map and leaves room for improvement. In this work, we propose FG-Attn, a sparse attention mechanism for long-context diffusion transformers that leverages sparsity at a fine granularity. Unlike block-sparse attention, which skips entire MxM blocks, our approach skips computations at the granularity of Mx1 slices of the attention map. Each slice is produced by query-key dot products between a block of query vectors and a single key. To implement our proposed sparse attention mechanism, we develop a new efficient bulk-load operation called asynchronous-gather load. This load operation gathers a sparse set of relevant key-value vectors from memory and arranges them into packed tiles in the GPU's shared memory. Only a sparse set of keys relevant to those queries are loaded into shared memory when computing attention for a block of queries, in contrast to loading full blocks of key tokens in block-sparse attention. Our fine-grained sparse attention, applied to video diffusion models, achieves an average 1.55X (up to 1.65X) speedup for 5 second, 480p videos, and an average 1.41X (up to 1.49X) for 5 second, 720p videos on a single H100 GPU.",
        "tags": [
            "CLIP",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "58",
        "title": "PM25Vision: A Large-Scale Benchmark Dataset for Visual Estimation of Air Quality",
        "author": [
            "Yang Han"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16519",
        "abstract": "We introduce PM25Vision (PM25V), the largest and most comprehensive dataset to date for estimating air quality - specifically PM2.5 concentrations - from street-level images. The dataset contains over 11,114 images matched with timestamped and geolocated PM2.5 readings across 3,261 AQI monitoring stations and 11 years, significantly exceeding the scale of previous benchmarks. The spatial accuracy of this dataset has reached 5 kilometers, far exceeding the city-level accuracy of many datasets. We describe the data collection, synchronization, and cleaning pipelines, and provide baseline model performances using CNN and transformer architectures. Our dataset is publicly available.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "59",
        "title": "mmExpert: Integrating Large Language Models for Comprehensive mmWave Data Synthesis and Understanding",
        "author": [
            "Yifan Yan",
            "Shuai Yang",
            "Xiuzhen Guo",
            "Xiangguang Wang",
            "Wei Chow",
            "Yuanchao Shu",
            "Shibo He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16521",
        "abstract": "Millimeter-wave (mmWave) sensing technology holds significant value in human-centric applications, yet the high costs associated with data acquisition and annotation limit its widespread adoption in our daily lives. Concurrently, the rapid evolution of large language models (LLMs) has opened up opportunities for addressing complex human needs. This paper presents mmExpert, an innovative mmWave understanding framework consisting of a data generation flywheel that leverages LLMs to automate the generation of synthetic mmWave radar datasets for specific application scenarios, thereby training models capable of zero-shot generalization in real-world environments. Extensive experiments demonstrate that the data synthesized by mmExpert significantly enhances the performance of downstream models and facilitates the successful deployment of large models for mmWave understanding.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "60",
        "title": "AIPsychoBench: Understanding the Psychometric Differences between LLMs and Humans",
        "author": [
            "Wei Xie",
            "Shuoyoucheng Ma",
            "Zhenhua Wang",
            "Enze Wang",
            "Kai Chen",
            "Xiaobing Sun",
            "Baosheng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16530",
        "abstract": "Large Language Models (LLMs) with hundreds of billions of parameters have exhibited human-like intelligence by learning from vast amounts of internet-scale data. However, the uninterpretability of large-scale neural networks raises concerns about the reliability of LLM. Studies have attempted to assess the psychometric properties of LLMs by borrowing concepts from human psychology to enhance their interpretability, but they fail to account for the fundamental differences between LLMs and humans. This results in high rejection rates when human scales are reused directly. Furthermore, these scales do not support the measurement of LLM psychological property variations in different languages. This paper introduces AIPsychoBench, a specialized benchmark tailored to assess the psychological properties of LLM. It uses a lightweight role-playing prompt to bypass LLM alignment, improving the average effective response rate from 70.12% to 90.40%. Meanwhile, the average biases are only 3.3% (positive) and 2.1% (negative), which are significantly lower than the biases of 9.8% and 6.9%, respectively, caused by traditional jailbreak prompts. Furthermore, among the total of 112 psychometric subcategories, the score deviations for seven languages compared to English ranged from 5% to 20.2% in 43 subcategories, providing the first comprehensive evidence of the linguistic impact on the psychometrics of LLM.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "61",
        "title": "No Need for Real 3D: Fusing 2D Vision with Pseudo 3D Representations for Robotic Manipulation Learning",
        "author": [
            "Run Yu",
            "Yangdi Liu",
            "Wen-Da Wei",
            "Chen Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16532",
        "abstract": "Recently,vision-based robotic manipulation has garnered significant attention and witnessed substantial advancements. 2D image-based and 3D point cloud-based policy learning represent two predominant paradigms in the field, with recent studies showing that the latter consistently outperforms the former in terms of both policy performance and generalization, thereby underscoring the value and significance of 3D information. However, 3D point cloud-based approaches face the significant challenge of high data acquisition costs, limiting their scalability and real-world deployment. To address this issue, we propose a novel framework NoReal3D: which introduces the 3DStructureFormer, a learnable 3D perception module capable of transforming monocular images into geometrically meaningful pseudo-point cloud features, effectively fused with the 2D encoder output features. Specially, the generated pseudo-point clouds retain geometric and topological structures so we design a pseudo-point cloud encoder to preserve these properties, making it well-suited for our framework. We also investigate the effectiveness of different feature fusion http://strategies.Our framework enhances the robot's understanding of 3D spatial structures while completely eliminating the substantial costs associated with 3D point cloud http://acquisition.Extensive experiments across various tasks validate that our framework can achieve performance comparable to 3D point cloud-based methods, without the actual point cloud data.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "62",
        "title": "Challenging the Evaluator: LLM Sycophancy Under User Rebuttal",
        "author": [
            "Sungwon Kim",
            "Daniel Khashabi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16533",
        "abstract": "Large Language Models (LLMs) often exhibit sycophancy, distorting responses to align with user beliefs, notably by readily agreeing with user counterarguments. Paradoxically, LLMs are increasingly adopted as successful evaluative agents for tasks such as grading and adjudicating claims. This research investigates that tension: why do LLMs show sycophancy when challenged in subsequent conversational turns, yet perform well when evaluating conflicting arguments presented simultaneously? We empirically tested these contrasting scenarios by varying key interaction patterns. We find that state-of-the-art models: (1) are more likely to endorse a user's counterargument when framed as a follow-up from a user, rather than when both responses are presented simultaneously for evaluation; (2) show increased susceptibility to persuasion when the user's rebuttal includes detailed reasoning, even when the conclusion of the reasoning is incorrect; and (3) are more readily swayed by casually phrased feedback than by formal critiques, even when the casual input lacks justification. Our results highlight the risk of relying on LLMs for judgment tasks without accounting for conversational framing.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "63",
        "title": "InteGround: On the Evaluation of Verification and Retrieval Planning in Integrative Grounding",
        "author": [
            "Cheng Jiayang",
            "Qianqian Zhuang",
            "Haoran Li",
            "Chunkit Chan",
            "Xin Liu",
            "Lin Qiu",
            "Yangqiu Song"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16534",
        "abstract": "Grounding large language models (LLMs) in external knowledge sources is a promising method for faithful prediction. While existing grounding approaches work well for simple queries, many real-world information needs require synthesizing multiple pieces of evidence. We introduce \"integrative grounding\" -- the challenge of retrieving and verifying multiple inter-dependent pieces of evidence to support a hypothesis query. To systematically study this problem, we repurpose data from four domains for evaluating integrative grounding capabilities. Our investigation reveals two critical findings: First, in groundedness verification, while LLMs are robust to redundant evidence, they tend to rationalize using internal knowledge when information is incomplete. Second, in examining retrieval planning strategies, we find that undirected planning can degrade performance through noise introduction, while premise abduction emerges as a promising approach due to its logical constraints. Additionally, LLMs' zero-shot self-reflection capabilities consistently improve grounding quality. These insights provide valuable direction for developing more effective integrative grounding systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "64",
        "title": "Advancing Reference-free Evaluation of Video Captions with Factual Analysis",
        "author": [
            "Shubhashis Roy Dipta",
            "Tz-Ying Wu",
            "Subarna Tripathi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16538",
        "abstract": "Video captions offer concise snapshots of actors, objects, and actions within a video, serving as valuable assets for applications such as question answering and event localization. However, acquiring human annotations for video captions is costly or even impractical, especially when dealing with diverse video domains. Existing models trained on supervised datasets face challenges in evaluating performance across different domains due to the reliance on reference-based evaluation protocols, which necessitate ground truth captions. This assumption is unrealistic for evaluating videos in the wild. To address these limitations, we propose a reference-free evaluation framework that does not require ground truth captions, focusing on factual grounding to ensure accurate assessment of caption quality. We introduce VC-Inspector, a novel caption quality evaluator that is both reference-free and factually grounded. Utilizing large language models, we generate pseudo captions of varying quality based on supervised data, which are subsequently used to train a multimodal model (i.e., Qwen2.5-VL) as the evaluator. Our approach demonstrates superior alignment with human judgments on the VATEX-Eval dataset, outperforming existing methods. The performance also generalizes to image caption datasets, Flickr8K-Expert and Flickr8K-CF, when viewing images as 1-frame videos. Overall, VC-Inspector offers a scalable and generalizable solution for evaluating the factual accuracy of video captions, paving the way for more effective and objective assessment methodologies in diverse video domains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "65",
        "title": "ChemOrch: Empowering LLMs with Chemical Intelligence via Synthetic Instructions",
        "author": [
            "Yue Huang",
            "Zhengzhe Jiang",
            "Xiaonan Luo",
            "Kehan Guo",
            "Haomin Zhuang",
            "Yujun Zhou",
            "Zhengqing Yuan",
            "Xiaoqi Sun",
            "Jules Schleinitz",
            "Yanbo Wang",
            "Shuhao Zhang",
            "Mihir Surve",
            "Nitesh V Chawla",
            "Olaf Wiest",
            "Xiangliang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16543",
        "abstract": "Empowering large language models (LLMs) with chemical intelligence remains a challenge due to the scarcity of high-quality, domain-specific instruction-response datasets and the misalignment of existing synthetic data generation pipelines with the inherently hierarchical and rule-governed structure of chemical information. To address this, we propose ChemOrch, a framework that synthesizes chemically grounded instruction-response pairs through a two-stage process: task-controlled instruction generation and tool-aware response construction. ChemOrch enables controllable diversity and levels of difficulty for the generated tasks, and ensures response precision through tool planning and distillation, and tool-based self-repair mechanisms. The effectiveness of ChemOrch is evaluated based on: 1) the high quality of generated instruction data, demonstrating superior diversity and strong alignment with chemical constraints; 2) the reliable generation of evaluation tasks that more effectively reveal LLM weaknesses in chemistry; and 3) the significant improvement of LLM chemistry capabilities when the generated instruction data are used for fine-tuning. Our work thus represents a critical step toward scalable and verifiable chemical intelligence in LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "66",
        "title": "SCAN: Self-Denoising Monte Carlo Annotation for Robust Process Reward Learning",
        "author": [
            "Yuyang Ding",
            "Xinyu Shi",
            "Juntao Li",
            "Xiaobo Liang",
            "Zhaopeng Tu",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16548",
        "abstract": "Process reward models (PRMs) offer fine-grained, step-level evaluations that facilitate deeper reasoning processes in large language models (LLMs), proving effective in complex tasks like mathematical reasoning. However, developing PRMs is challenging due to the high cost and limited scalability of human-annotated data. Synthetic data from Monte Carlo (MC) estimation is a promising alternative but suffers from a high noise ratio, which can cause overfitting and hinder large-scale training. In this work, we conduct a preliminary study on the noise distribution in synthetic data from MC estimation, identifying that annotation models tend to both underestimate and overestimate step correctness due to limitations in their annotation capabilities. Building on these insights, we propose Self-Denoising Monte Carlo Annotation (SCAN), an efficient data synthesis and noise-tolerant learning framework. Our key findings indicate that: (1) Even lightweight models (e.g., 1.5B parameters) can produce high-quality annotations through a self-denoising strategy, enabling PRMs to achieve superior performance with only 6% the inference cost required by vanilla MC estimation. (2) With our robust learning strategy, PRMs can effectively learn from this weak supervision, achieving a 39.2 F1 score improvement (from 19.9 to 59.1) in ProcessBench. Despite using only a compact synthetic dataset, our models surpass strong baselines, including those trained on large-scale human-annotated datasets such as PRM800K. Furthermore, performance continues to improve as we scale up the synthetic data, highlighting the potential of SCAN for scalable, cost-efficient, and robust PRM training.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "67",
        "title": "Efficient Rectified Flow for Image Fusion",
        "author": [
            "Zirui Wang",
            "Jiayi Zhang",
            "Tianwei Guan",
            "Yuhan Zhou",
            "Xingyuan Li",
            "Minjing Dong",
            "Jinyuan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16549",
        "abstract": "Image fusion is a fundamental and important task in computer vision, aiming to combine complementary information from different modalities to fuse images. In recent years, diffusion models have made significant developments in the field of image fusion. However, diffusion models often require complex computations and redundant inference time, which reduces the applicability of these methods. To address this issue, we propose RFfusion, an efficient one-step diffusion model for image fusion based on Rectified Flow. We incorporate Rectified Flow into the image fusion task to straighten the sampling path in the diffusion model, achieving one-step sampling without the need for additional training, while still maintaining high-quality fusion results. Furthermore, we propose a task-specific variational autoencoder (VAE) architecture tailored for image fusion, where the fusion operation is embedded within the latent space to further reduce computational complexity. To address the inherent discrepancy between conventional reconstruction-oriented VAE objectives and the requirements of image fusion, we introduce a two-stage training strategy. This approach facilitates the effective learning and integration of complementary information from multi-modal source images, thereby enabling the model to retain fine-grained structural details while significantly enhancing inference efficiency. Extensive experiments demonstrate that our method outperforms other state-of-the-art methods in terms of both inference speed and fusion quality. Code is available at https://github.com/zirui0625/RFfusion.",
        "tags": [
            "Diffusion",
            "Rectified Flow",
            "VAE"
        ]
    },
    {
        "id": "68",
        "title": "TranTac: Leveraging Transient Tactile Signals for Contact-Rich Robotic Manipulation",
        "author": [
            "Yinghao Wu",
            "Shuhong Hou",
            "Haowen Zheng",
            "Yichen Li",
            "Weiyi Lu",
            "Xun Zhou",
            "Yitian Shao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16550",
        "abstract": "Robotic manipulation tasks such as inserting a key into a lock or plugging a USB device into a port can fail when visual perception is insufficient to detect misalignment. In these situations, touch sensing is crucial for the robot to monitor the task's states and make precise, timely adjustments. Current touch sensing solutions are either insensitive to detect subtle changes or demand excessive sensor data. Here, we introduce TranTac, a data-efficient and low-cost tactile sensing and control framework that integrates a single contact-sensitive 6-axis inertial measurement unit within the elastomeric tips of a robotic gripper for completing fine insertion tasks. Our customized sensing system can detect dynamic translational and torsional deformations at the micrometer scale, enabling the tracking of visually imperceptible pose changes of the grasped object. By leveraging transformer-based encoders and diffusion policy, TranTac can imitate human insertion behaviors using transient tactile cues detected at the gripper's tip during insertion processes. These cues enable the robot to dynamically control and correct the 6-DoF pose of the grasped object. When combined with vision, TranTac achieves an average success rate of 79% on object grasping and insertion tasks, outperforming both vision-only policy and the one augmented with end-effector 6D force/torque sensing. Contact localization performance is also validated through tactile-only misaligned insertion tasks, achieving an average success rate of 88%. We assess the generalizability by training TranTac on a single prism-slot pair and testing it on unseen data, including a USB plug and a metal key, and find that the insertion tasks can still be completed with an average success rate of nearly 70%. The proposed framework may inspire new robotic tactile sensing systems for delicate manipulation tasks.",
        "tags": [
            "Diffusion",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "69",
        "title": "ST-GS: Vision-Based 3D Semantic Occupancy Prediction with Spatial-Temporal Gaussian Splatting",
        "author": [
            "Xiaoyang Yan",
            "Muleilan Pei",
            "Shaojie Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16552",
        "abstract": "3D occupancy prediction is critical for comprehensive scene understanding in vision-centric autonomous driving. Recent advances have explored utilizing 3D semantic Gaussians to model occupancy while reducing computational overhead, but they remain constrained by insufficient multi-view spatial interaction and limited multi-frame temporal consistency. To overcome these issues, in this paper, we propose a novel Spatial-Temporal Gaussian Splatting (ST-GS) framework to enhance both spatial and temporal modeling in existing Gaussian-based pipelines. Specifically, we develop a guidance-informed spatial aggregation strategy within a dual-mode attention mechanism to strengthen spatial interaction in Gaussian representations. Furthermore, we introduce a geometry-aware temporal fusion scheme that effectively leverages historical context to improve temporal continuity in scene completion. Extensive experiments on the large-scale nuScenes occupancy prediction benchmark showcase that our proposed approach not only achieves state-of-the-art performance but also delivers markedly better temporal consistency compared to existing Gaussian-based methods.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "70",
        "title": "ViTCAE: ViT-based Class-conditioned Autoencoder",
        "author": [
            "Vahid Jebraeeli",
            "Hamid Krim",
            "Derya Cansever"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16554",
        "abstract": "Vision Transformer (ViT) based autoencoders often underutilize the global Class token and employ static attention mechanisms, limiting both generative control and optimization efficiency. This paper introduces ViTCAE, a framework that addresses these issues by re-purposing the Class token into a generative linchpin. In our architecture, the encoder maps the Class token to a global latent variable that dictates the prior distribution for local, patch-level latent variables, establishing a robust dependency where global semantics directly inform the synthesis of local details. Drawing inspiration from opinion dynamics, we treat each attention head as a dynamical system of interacting tokens seeking consensus. This perspective motivates a convergence-aware temperature scheduler that adaptively anneals each head's influence function based on its distributional stability. This process enables a principled head-freezing mechanism, guided by theoretically-grounded diagnostics like an attention evolution distance and a consensus/cluster functional. This technique prunes converged heads during training to significantly improve computational efficiency without sacrificing fidelity. By unifying a generative Class token with an adaptive attention mechanism rooted in multi-agent consensus theory, ViTCAE offers a more efficient and controllable approach to transformer-based generation.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "71",
        "title": "Captioning for Text-Video Retrieval via Dual-Group Direct Preference Optimization",
        "author": [
            "Ji Soo Lee",
            "Byungoh Ko",
            "Jaewon Cho",
            "Howoong Lee",
            "Jaewoon Byun",
            "Hyunwoo J. Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16560",
        "abstract": "In text-video retrieval, auxiliary captions are often used to enhance video understanding, bridging the gap between the modalities. While recent advances in multi-modal large language models (MLLMs) have enabled strong zero-shot caption generation, we observe that such captions tend to be generic and indistinguishable across visually similar videos, limiting their utility for fine-grained retrieval. Moreover, conventional captioning approaches are typically evaluated using language generation metrics, such as BLEU, which are not typically tailored for retrieval tasks that require making discriminative distinctions between candidates. To address this, we propose $\\textbf{CaRe-DPO}$, a retrieval framework that directly optimizes caption generation using retrieval relevance scores. At its core is Dual-Group Direct Preference Optimization (DG-DPO), a novel learning strategy that supervises captioning by modeling preferences across groups of distinct video and caption pairs. In addition, we present an MLLM-based retrieval model that incorporates role-embeddings to better distinguish between textual inputs with different functional roles, such as an auxiliary caption and a text query. Through extensive experiments, we demonstrate that CaRe-DPO significantly enhances retrieval performance by effectively leveraging auxiliary knowledge to generate fine-grained captions for retrieval. Code is available at https://github.com/mlvlab/CaReDPO.",
        "tags": [
            "DPO",
            "LLM"
        ]
    },
    {
        "id": "72",
        "title": "SalaMAnder: Shapley-based Mathematical Expression Attribution and Metric for Chain-of-Thought Reasoning",
        "author": [
            "Yue Xin",
            "Chen Shen",
            "Shaotian Yan",
            "Xiaosong Yuan",
            "Yaoming Wang",
            "Xiaofeng Zhang",
            "Chenxi Huang",
            "Jieping Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16561",
        "abstract": "Chain-of-Thought (CoT) prompting enhances the math reasoning capability of large language models (LLMs) to a large margin. However, the mechanism underlying such improvements remains unexplored. In this paper, we present \\textbf{SalaMAnder} (\\textbf{S}h\\textbf{a}p\\textbf{l}ey-b\\textbf{a}sed \\textbf{M}athematical Expression \\textbf{A}ttribution a\\textbf{nd} M\\textbf{e}t\\textbf{r}ic), a theoretically grounded methodology as well as a mathematically rigorous evaluation metric for quantifying component-level contributions in few-shot CoT reasoning. Concretely, we leverage the Shapley value for mathematical expression attribution and develop an efficient stratified sampling algorithm that significantly reduces the computational complexity. Besides, we develop the \\textbf{CoSP} (\\textbf{C}ardinality \\textbf{o}f \\textbf{S}hapley \\textbf{P}ositives) metric through covariance analysis. Comprehensive validation across popular LLM models and diverse mathematical benchmarks demonstrates that the CoSP metric within our SalaMAnder framework exhibits a robust monotonic correlation with model performance, not only providing theoretical explanations for the empirical success of existing few-shot CoT but also establishing mathematically rigorous principles for prompt construction optimization. Furthermore, we verify the reliability of the explanation, based on which we unify the insights of previous work.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "73",
        "title": "MPCG: Multi-Round Persona-Conditioned Generation for Modeling the Evolution of Misinformation with LLMs",
        "author": [
            "Jun Rong Brian Chong",
            "Yixuan Tang",
            "Anthony K.H. Tung"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16564",
        "abstract": "Misinformation evolves as it spreads, shifting in language, framing, and moral emphasis to adapt to new audiences. However, current misinformation detection approaches implicitly assume that misinformation is static. We introduce MPCG, a multi-round, persona-conditioned framework that simulates how claims are iteratively reinterpreted by agents with distinct ideological perspectives. Our approach uses an uncensored large language model (LLM) to generate persona-specific claims across multiple rounds, conditioning each generation on outputs from the previous round, enabling the study of misinformation evolution. We evaluate the generated claims through human and LLM-based annotations, cognitive effort metrics (readability, perplexity), emotion evocation metrics (sentiment analysis, morality), clustering, feasibility, and downstream classification. Results show strong agreement between human and GPT-4o-mini annotations, with higher divergence in fluency judgments. Generated claims require greater cognitive effort than the original claims and consistently reflect persona-aligned emotional and moral framing. Clustering and cosine similarity analyses confirm semantic drift across rounds while preserving topical coherence. Feasibility results show a 77% feasibility rate, confirming suitability for downstream tasks. Classification results reveal that commonly used misinformation detectors experience macro-F1 performance drops of up to 49.7%. The code is available at https://github.com/bcjr1997/MPCG",
        "tags": [
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "74",
        "title": "V-CECE: Visual Counterfactual Explanations via Conceptual Edits",
        "author": [
            "Nikolaos Spanos",
            "Maria Lymperaiou",
            "Giorgos Filandrianos",
            "Konstantinos Thomas",
            "Athanasios Voulodimos",
            "Giorgos Stamou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16567",
        "abstract": "Recent black-box counterfactual generation frameworks fail to take into account the semantic content of the proposed edits, while relying heavily on training to guide the generation process. We propose a novel, plug-and-play black-box counterfactual generation framework, which suggests step-by-step edits based on theoretical guarantees of optimal edits to produce human-level counterfactual explanations with zero training. Our framework utilizes a pre-trained image editing diffusion model, and operates without access to the internals of the classifier, leading to an explainable counterfactual generation process. Throughout our experimentation, we showcase the explanatory gap between human reasoning and neural model behavior by utilizing both Convolutional Neural Network (CNN), Vision Transformer (ViT) and Large Vision Language Model (LVLM) classifiers, substantiated through a comprehensive human evaluation.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "75",
        "title": "Zero-Shot Human Mobility Forecasting via Large Language Model with Hierarchical Reasoning",
        "author": [
            "Wenyao Li",
            "Ran Zhang",
            "Pengyang Wang",
            "Yuanchun Zhou",
            "Pengfei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16578",
        "abstract": "Human mobility forecasting is important for applications such as transportation planning, urban management, and personalized recommendations. However, existing methods often fail to generalize to unseen users or locations and struggle to capture dynamic intent due to limited labeled data and the complexity of mobility patterns. We propose ZHMF, a framework for zero-shot human mobility forecasting that combines a semantic enhanced retrieval and reflection mechanism with a hierarchical language model based reasoning system. The task is reformulated as a natural language question answering paradigm. Leveraging LLMs semantic understanding of user histories and context, our approach handles previously unseen prediction scenarios. We further introduce a hierarchical reflection mechanism for iterative reasoning and refinement by decomposing forecasting into an activity level planner and a location level selector, enabling collaborative modeling of long term user intentions and short term contextual preferences. Experiments on standard human mobility datasets show that our approach outperforms existing models. Ablation studies reveal the contribution of each module, and case studies illustrate how the method captures user intentions and adapts to diverse contextual scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "76",
        "title": "Benchmarking Contextual and Paralinguistic Reasoning in Speech-LLMs: A Case Study with In-the-Wild Data",
        "author": [
            "Qiongqiong Wang",
            "Hardik Bhupendra Sailor",
            "Tianchi Liu",
            "Wenyu Zhang",
            "Muhammad Huzaifah",
            "Nattadaporn Lertcheva",
            "Shuo Sun",
            "Nancy F. Chen",
            "Jinyang Wu",
            "AiTi Aw"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16589",
        "abstract": "Recent speech-LLMs have shown impressive performance in tasks like transcription and translation, yet they remain limited in understanding the paralinguistic aspects of speech crucial for social and emotional intelligence. We propose CP-Bench, a benchmark for evaluating speech-LLMs on contextual paralinguistic reasoning the integration of verbal content with non-verbal cues like emotion and prosody. The benchmark includes two curated question answering (QA) datasets requiring both linguistic and empathetic understanding. We evaluate state-of-the-art speech-LLMs from both open and closed-source models and perform a comprehensive analysis across different question types. The top two models were further analyzed under temperature tuning to understand its effect on this task. Our benchmark reveals a key gap in existing evaluations and offers insights into building more context-aware and emotionally intelligent speech-capable LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "77",
        "title": "Question Answering with LLMs and Learning from Answer Sets",
        "author": [
            "Manuel Borroto",
            "Katie Gallagher",
            "Antonio Ielo",
            "Irfan Kareem",
            "Francesco Ricca",
            "Alessandra Russo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16590",
        "abstract": "Large Language Models (LLMs) excel at understanding natural language but struggle with explicit commonsense reasoning. A recent trend of research suggests that the combination of LLM with robust symbolic reasoning systems can overcome this problem on story-based question answering tasks. In this setting, existing approaches typically depend on human expertise to manually craft the symbolic component. We argue, however, that this component can also be automatically learned from examples. In this work, we introduce LLM2LAS, a hybrid system that effectively combines the natural language understanding capabilities of LLMs, the rule induction power of the Learning from Answer Sets (LAS) system ILASP, and the formal reasoning strengths of Answer Set Programming (ASP). LLMs are used to extract semantic structures from text, which ILASP then transforms into interpretable logic rules. These rules allow an ASP solver to perform precise and consistent reasoning, enabling correct answers to previously unseen questions. Empirical results outline the strengths and weaknesses of our automatic approach for learning and reasoning in a story-based question answering benchmark.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "78",
        "title": "From Uniform to Heterogeneous: Tailoring Policy Optimization to Every Token's Nature",
        "author": [
            "Zheng Liu",
            "Mengjie Liu",
            "Siwei Wen",
            "Mengzhang Cai",
            "Bin Cui",
            "Conghui He",
            "Wentao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16591",
        "abstract": "Reinforcement Learning has emerged as the fundamental technique for enhancing reasoning in LLMs. However, existing algorithms apply uniform optimization to all tokens, ignoring their different roles in reasoning process. To address this limitation, we introduce Heterogeneous Adaptive Policy Optimization (HAPO), a comprehensive token-aware algorithm that dynamically adapts optimization based on token entropy. For rollout sampling, we propose Adaptive Temperature Sampling, which adjusts sampling temperature in real time, promoting exploration at high-entropy tokens while preserving coherence at low-entropy ones. For advantage calculation, we introduce Token Level Group Average that normalizes advantages at token level, jointly accounting for sequence-length as in token-mean loss while preserving non-biased treatment. We then develop Differential Advantage Redistribution that leverages entropy and importance ratios to modulate rewards-adjusting updates for tokens with clear signals. For clipping loss, we design Asymmetric Adaptive Clipping, allowing aggressive probability reduction for noisy low-entropy tokens while enabling exploration for high-entropy tokens. Through systematic investigation between entropy and training dynamics, we embedded token-level treatment into every stages to achieve fine-grained control. Extensive experiments demonstrate that HAPO consistently outperforms DAPO across multiple model scales. Our code can be found in https://github.com/starriver030515/HAPO.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "79",
        "title": "Analyzing the Effects of Supervised Fine-Tuning on Model Knowledge from Token and Parameter Levels",
        "author": [
            "Junjie Ye",
            "Yuming Yang",
            "Yang Nan",
            "Shuo Li",
            "Qi Zhang",
            "Tao Gui",
            "Xuanjing Huang",
            "Peng Wang",
            "Zhongchao Shi",
            "Jianping Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16596",
        "abstract": "Large language models (LLMs) acquire substantial world knowledge during pre-training, which is further shaped by post-training techniques such as supervised fine-tuning (SFT). However, the impact of SFT on a model's knowledge remains underexplored, limiting our ability to control knowledge change behavior in fine-tuned models. To address this gap, we evaluate closed-book question answering (CBQA) performance across five LLMs from the LLaMA-2 and LLaMA-3 families. Surprisingly, models fine-tuned on 1,920 samples perform up to 14% worse than those fine-tuned on only 240 samples. Furthermore, varying the level of knowledge mastery in the fine-tuning data leads to performance fluctuations of over 12%. To investigate these effects, we analyze model behavior at both the token and parameter levels. Our analysis reveals that up to 90% of parameter updates during SFT do not contribute to knowledge enhancement. Restoring these updates can improve performance on the CBQA task, depending on the characteristics of the fine-tuning data. These insights offer practical guidance for developing fine-tuning strategies that more effectively strengthen model knowledge.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "80",
        "title": "MCP: A Control-Theoretic Orchestration Framework for Synergistic Efficiency and Interpretability in Multimodal Large Language Models",
        "author": [
            "Luyan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16597",
        "abstract": "Aiming at the problems of computational inefficiency and insufficient interpretability faced by large models in complex tasks such as multi-round reasoning and multi-modal collaboration, this study proposes a three-layer collaboration framework based on model-controller-task adaptation (MCP). By decoupling large model functions into reasoning, generation and retrieval modules, and combining reinforcement learning-driven dynamic routing algorithms and task adaptation mechanisms, the systematic integration of control theory and large model dynamic reasoning is achieved for the first time. Experiments show that the MCP framework improves the performance of cross-modal benchmarking tasks, such as GLUE, COCO, ScienceQA, etc., by 15-30% compared with the baseline model, improves the reasoning efficiency by 40%, and generates the interpretable intermediate results through the Presenter layer, obtaining 90% of the manual interpretability scores, which provides a brand-new technological path to solve the bottleneck of the practical application of the large model.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "81",
        "title": "PruneCD: Contrasting Pruned Self Model to Improve Decoding Factuality",
        "author": [
            "Byeongho Yu",
            "Changhun Lee",
            "Jungyu Jin",
            "Eunhyeok Park"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16598",
        "abstract": "To mitigate the hallucination problem in large language models, DoLa exploits early exit logits from the same model as a contrastive prior. However, we found that these early exit logits tend to be flat, low in magnitude, and fail to reflect meaningful contrasts. To address this, we propose PruneCD, a novel contrastive decoding method that constructs the amateur model via layer pruning rather than early exit. This design leads to more informative and well-aligned logits, enabling more effective contrastive decoding. Through qualitative and quantitative analyses, we demonstrate that PruneCD consistently improves factuality with minimal inference overhead, offering a robust and practical approach to mitigating hallucinations in LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "82",
        "title": "FakeChain: Exposing Shallow Cues in Multi-Step Deepfake Detection",
        "author": [
            "Minji Heo",
            "Simon S. Woo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16602",
        "abstract": "Multi-step or hybrid deepfakes, created by sequentially applying different deepfake creation methods such as Face-Swapping, GAN-based generation, and Diffusion methods, can pose an emerging and unforseen technical challenge for detection models trained on single-step forgeries. While prior studies have mainly focused on detecting isolated single manipulation, little is known about the detection model behavior under such compositional, hybrid, and complex manipulation pipelines. In this work, we introduce \\textbf{FakeChain}, a large-scale benchmark comprising 1-, 2-, and 3-Step forgeries synthesized using five state-of-the-art representative generators. Using this approach, we analyze detection performance and spectral properties across hybrid manipulation at different step, along with varying generator combinations and quality settings. Surprisingly, our findings reveal that detection performance highly depends on the final manipulation type, with F1-score dropping by up to \\textbf{58.83\\%} when it differs from training distribution. This clearly demonstrates that detectors rely on last-stage artifacts rather than cumulative manipulation traces, limiting generalization. Such findings highlight the need for detection models to explicitly consider manipulation history and sequences. Our results highlight the importance of benchmarks such as FakeChain, reflecting growing synthesis complexity and diversity in real-world scenarios. Our sample code is available here\\footnote{https://github.com/minjihh/FakeChain}.",
        "tags": [
            "Detection",
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "83",
        "title": "Bayesian Ego-graph inference for Networked Multi-Agent Reinforcement Learning",
        "author": [
            "Wei Duan",
            "Jie Lu",
            "Junyu Xuan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16606",
        "abstract": "In networked multi-agent reinforcement learning (Networked-MARL), decentralized agents must act under local observability and constrained communication over fixed physical graphs. Existing methods often assume static neighborhoods, limiting adaptability to dynamic or heterogeneous environments. While centralized frameworks can learn dynamic graphs, their reliance on global state access and centralized infrastructure is impractical in real-world decentralized systems. We propose a stochastic graph-based policy for Networked-MARL, where each agent conditions its decision on a sampled subgraph over its local physical neighborhood. Building on this formulation, we introduce BayesG, a decentralized actor-framework that learns sparse, context-aware interaction structures via Bayesian variational inference. Each agent operates over an ego-graph and samples a latent communication mask to guide message passing and policy computation. The variational distribution is trained end-to-end alongside the policy using an evidence lower bound (ELBO) objective, enabling agents to jointly learn both interaction topology and decision-making strategies. BayesG outperforms strong MARL baselines on large-scale traffic control tasks with up to 167 agents, demonstrating superior scalability, efficiency, and performance.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "84",
        "title": "LLMsPark: A Benchmark for Evaluating Large Language Models in Strategic Gaming Contexts",
        "author": [
            "Junhao Chen",
            "Jingbo Sun",
            "Xiang Li",
            "Haidong Xin",
            "Yuhao Xue",
            "Yibin Xu",
            "Hao Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16610",
        "abstract": "As large language models (LLMs) advance across diverse tasks, the need for comprehensive evaluation beyond single metrics becomes increasingly important. To fully assess LLM intelligence, it is crucial to examine their interactive dynamics and strategic behaviors. We present LLMsPark, a game theory-based evaluation platform that measures LLMs' decision-making strategies and social behaviors in classic game-theoretic settings, providing a multi-agent environment to explore strategic depth. Our system cross-evaluates 15 leading LLMs (both commercial and open-source) using leaderboard rankings and scoring mechanisms. Higher scores reflect stronger reasoning and strategic capabilities, revealing distinct behavioral patterns and performance differences across models. This work introduces a novel perspective for evaluating LLMs' strategic intelligence, enriching existing benchmarks and broadening their assessment in interactive, game-theoretic scenarios. The benchmark and rankings are publicly available at https://llmsparks.github.io/.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "85",
        "title": "Video-to-BT: Generating Reactive Behavior Trees from Human Demonstration Videos for Robotic Assembly",
        "author": [
            "Xiwei Zhao",
            "Yiwei Wang",
            "Yansong Wu",
            "Fan Wu",
            "Teng Sun",
            "Zhonghua Miao",
            "Sami Haddadin",
            "Alois Knoll"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16611",
        "abstract": "Modern manufacturing demands robotic assembly systems with enhanced flexibility and reliability. However, traditional approaches often rely on programming tailored to each product by experts for fixed settings, which are inherently inflexible to product changes and lack the robustness to handle variations. As Behavior Trees (BTs) are increasingly used in robotics for their modularity and reactivity, we propose a novel hierarchical framework, Video-to-BT, that seamlessly integrates high-level cognitive planning with low-level reactive control, with BTs serving both as the structured output of planning and as the governing structure for execution. Our approach leverages a Vision-Language Model (VLM) to decompose human demonstration videos into subtasks, from which Behavior Trees are generated. During the execution, the planned BTs combined with real-time scene interpretation enable the system to operate reactively in the dynamic environment, while VLM-driven replanning is triggered upon execution failure. This closed-loop architecture ensures stability and adaptivity. We validate our framework on real-world assembly tasks through a series of experiments, demonstrating high planning reliability, robust performance in long-horizon assembly tasks, and strong generalization across diverse and perturbed conditions. Project website: https://video2bt.github.io/video2bt_page/",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "86",
        "title": "ORN-CBF: Learning Observation-conditioned Residual Neural Control Barrier Functions via Hypernetworks",
        "author": [
            "Bojan DerajiÄ",
            "Sebastian Bernhard",
            "Wolfgang HÃ¶nig"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16614",
        "abstract": "Control barrier functions (CBFs) have been demonstrated as an effective method for safety-critical control of autonomous systems. Although CBFs are simple to deploy, their design remains challenging, motivating the development of learning-based approaches. Yet, issues such as suboptimal safe sets, applicability in partially observable environments, and lack of rigorous safety guarantees persist. In this work, we propose observation-conditioned neural CBFs based on Hamilton-Jacobi (HJ) reachability analysis, which approximately recover the maximal safe sets. We exploit certain mathematical properties of the HJ value function, ensuring that the predicted safe set never intersects with the observed failure set. Moreover, we leverage a hypernetwork-based architecture that is particularly suitable for the design of observation-conditioned safety filters. The proposed method is examined both in simulation and hardware experiments for a ground robot and a quadcopter. The results show improved success rates and generalization to out-of-domain environments compared to the baselines.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "87",
        "title": "LLM-Guided Task- and Affordance-Level Exploration in Reinforcement Learning",
        "author": [
            "Jelle Luijkx",
            "Runyu Ma",
            "Zlatan AjanoviÄ",
            "Jens Kober"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16615",
        "abstract": "Reinforcement learning (RL) is a promising approach for robotic manipulation, but it can suffer from low sample efficiency and requires extensive exploration of large state-action spaces. Recent methods leverage the commonsense knowledge and reasoning abilities of large language models (LLMs) to guide exploration toward more meaningful states. However, LLMs can produce plans that are semantically plausible yet physically infeasible, yielding unreliable behavior. We introduce LLM-TALE, a framework that uses LLMs' planning to directly steer RL exploration. LLM-TALE integrates planning at both the task level and the affordance level, improving learning efficiency by directing agents toward semantically meaningful actions. Unlike prior approaches that assume optimal LLM-generated plans or rewards, LLM-TALE corrects suboptimality online and explores multimodal affordance-level plans without human supervision. We evaluate LLM-TALE on pick-and-place tasks in standard RL benchmarks, observing improvements in both sample efficiency and success rates over strong baselines. Real-robot experiments indicate promising zero-shot sim-to-real transfer. Code and supplementary material are available at https://llm-tale.github.io.",
        "tags": [
            "LLM",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "88",
        "title": "The Role of Vocabularies in Learning Sparse Representations for Ranking",
        "author": [
            "Hiun Kim",
            "Tae Kwan Lee",
            "Taeryun Won"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16621",
        "abstract": "Learned Sparse Retrieval (LSR) such as SPLADE has growing interest for effective semantic 1st stage matching while enjoying the efficiency of inverted indices. A recent work on learning SPLADE models with expanded vocabularies (ESPLADE) was proposed to represent queries and documents into a sparse space of custom vocabulary which have different levels of vocabularic granularity. Within this effort, however, there have not been many studies on the role of vocabulary in SPLADE models and their relationship to retrieval efficiency and effectiveness.\nTo study this, we construct BERT models with 100K-sized output vocabularies, one initialized with the ESPLADE pretraining method and one initialized randomly. After finetune on real-world search click logs, we applied logit score-based queries and documents pruning to max size for further balancing efficiency. The experimental result in our evaluation set shows that, when pruning is applied, the two models are effective compared to the 32K-sized normal SPLADE model in the computational budget under the BM25. And the ESPLADE models are more effective than the random vocab model, while having a similar retrieval cost.\nThe result indicates that the size and pretrained weight of output vocabularies play the role of configuring the representational specification for queries, documents, and their interactions in the retrieval engine, beyond their original meaning and purposes in NLP. These findings can provide a new room for improvement for LSR by identifying the importance of representational specification from vocabulary configuration for efficient and effective retrieval.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "89",
        "title": "CGTGait: Collaborative Graph and Transformer for Gait Emotion Recognition",
        "author": [
            "Junjie Zhou",
            "Haijun Xiong",
            "Junhao Lu",
            "Ziyu Lin",
            "Bin Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16623",
        "abstract": "Skeleton-based gait emotion recognition has received significant attention due to its wide-ranging applications. However, existing methods primarily focus on extracting spatial and local temporal motion information, failing to capture long-range temporal representations. In this paper, we propose \\textbf{CGTGait}, a novel framework that collaboratively integrates graph convolution and transformers to extract discriminative spatiotemporal features for gait emotion recognition. Specifically, CGTGait consists of multiple CGT blocks, where each block employs graph convolution to capture frame-level spatial topology and the transformer to model global temporal dependencies. Additionally, we introduce a Bidirectional Cross-Stream Fusion (BCSF) module to effectively aggregate posture and motion spatiotemporal features, facilitating the exchange of complementary information between the two streams. We evaluate our method on two widely used datasets, Emotion-Gait and ELMD, demonstrating that our CGTGait achieves state-of-the-art or at least competitive performance while reducing computational complexity by approximately \\textbf{82.2\\%} (only requiring 0.34G FLOPs) during testing. Code is available at \\small{https://github.com/githubzjj1/CGTGait.}",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "90",
        "title": "Enhancing Scientific Visual Question Answering via Vision-Caption aware Supervised Fine-Tuning",
        "author": [
            "Janak Kapuriya",
            "Anwar Shaikh",
            "Arnav Goel",
            "Medha Hira",
            "Apoorv Singh",
            "Jay Saraf",
            "Sanjana",
            "Vaibhav Nauriyal",
            "Avinash Anand",
            "Zhengkui Wang",
            "Rajiv Ratn Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16628",
        "abstract": "In this study, we introduce Vision-Caption aware Supervised FineTuning (VCASFT), a novel learning paradigm designed to enhance the performance of smaller Vision Language Models(VLMs) on scientific visual question answering(VQA) tasks. VCASFT leverages image captions as zero-shot prompts alongside question-answer pairs and instruction-tunes models to yield significant performance improvements. To comprehensively evaluate VCASFT, we benchmark it on ScienceQA, which consists of questions across diverse languages, subjects, and fields, demonstrating its adaptability and effectiveness in a variety of educational contexts. Additionally, to further demonstrate the effectiveness of this technique on lowresource languages, we developed HiSciVQA, a dataset comprising 2,245 high-quality, hand-annotated Hindi multimodal Q&A pairs. This dataset addresses the critical need for low-resource language Q&A datasets and serves as a foundation for testing VCASFT. Additionally, we introduce a novel LLM-based evaluation scheme to evaluate VLMs on HiSciVQA which offers deeper insights into model effectiveness surpassing traditional n-gram matching accuracy metrics. We are committed to advancing the field by open-sourcing all code files and the HiSciVQA dataset for the research community.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "91",
        "title": "Causality-Induced Positional Encoding for Transformer-Based Representation Learning of Non-Sequential Features",
        "author": [
            "Kaichen Xu",
            "Yihang Du",
            "Mianpeng Liu",
            "Zimu Yu",
            "Xiaobo Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16629",
        "abstract": "Positional encoding is essential for supplementing transformer with positional information of tokens. Existing positional encoding methods demand predefined token/feature order, rendering them unsuitable for real-world data with non-sequential yet causally-related features. To address this limitation, we propose CAPE, a novel method that identifies underlying causal structure over non-sequential features as a weighted directed acyclic graph (DAG) using generalized structural equation modeling. The DAG is then embedded in hyperbolic space where its geometric structure is well-preserved using a hyperboloid model-based approach that effectively captures two important causal graph properties (causal strength & causal specificity). This step yields causality-aware positional encodings for the features, which are converted into their rotary form for integrating with transformer's self-attention mechanism. Theoretical analysis reveals that CAPE-generated rotary positional encodings possess three valuable properties for enhanced self-attention, including causal distance-induced attenuation, causal generality-induced attenuation, and robustness to positional disturbances. We evaluate CAPE over both synthetic and real-word datasets, empirically demonstrating its theoretical properties and effectiveness in enhancing transformer for data with non-sequential features. Our code is available at https://github.com/Catchxu/CAPE.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "92",
        "title": "Follow-Your-Emoji-Faster: Towards Efficient, Fine-Controllable, and Expressive Freestyle Portrait Animation",
        "author": [
            "Yue Ma",
            "Zexuan Yan",
            "Hongyu Liu",
            "Hongfa Wang",
            "Heng Pan",
            "Yingqing He",
            "Junkun Yuan",
            "Ailing Zeng",
            "Chengfei Cai",
            "Heung-Yeung Shum",
            "Zhifeng Li",
            "Wei Liu",
            "Linfeng Zhang",
            "Qifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16630",
        "abstract": "We present Follow-Your-Emoji-Faster, an efficient diffusion-based framework for freestyle portrait animation driven by facial landmarks. The main challenges in this task are preserving the identity of the reference portrait, accurately transferring target expressions, and maintaining long-term temporal consistency while ensuring generation efficiency. To address identity preservation and accurate expression retargeting, we enhance Stable Diffusion with two key components: a expression-aware landmarks as explicit motion signals, which improve motion alignment, support exaggerated expressions, and reduce identity leakage; and a fine-grained facial loss that leverages both expression and facial masks to better capture subtle expressions and faithfully preserve the reference appearance. With these components, our model supports controllable and expressive animation across diverse portrait types, including real faces, cartoons, sculptures, and animals. However, diffusion-based frameworks typically struggle to efficiently generate long-term stable animation results, which remains a core challenge in this task. To address this, we propose a progressive generation strategy for stable long-term animation, and introduce a Taylor-interpolated cache, achieving a 2.6X lossless acceleration. These two strategies ensure that our method produces high-quality results efficiently, making it user-friendly and accessible. Finally, we introduce EmojiBench++, a more comprehensive benchmark comprising diverse portraits, driving videos, and landmark sequences. Extensive evaluations on EmojiBench++ demonstrate that Follow-Your-Emoji-Faster achieves superior performance in both animation quality and controllability. The code, training dataset and benchmark will be found in https://follow-your-emoji.github.io/.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "93",
        "title": "DA-Font: Few-Shot Font Generation via Dual-Attention Hybrid Integration",
        "author": [
            "Weiran Chen",
            "Guiqian Zhu",
            "Ying Li",
            "Yi Ji",
            "Chunping Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16632",
        "abstract": "Few-shot font generation aims to create new fonts with a limited number of glyph references. It can be used to significantly reduce the labor cost of manual font design. However, due to the variety and complexity of font styles, the results generated by existing methods often suffer from visible defects, such as stroke errors, artifacts and blurriness. To address these issues, we propose DA-Font, a novel framework which integrates a Dual-Attention Hybrid Module (DAHM). Specifically, we introduce two synergistic attention blocks: the component attention block that leverages component information from content images to guide the style transfer process, and the relation attention block that further refines spatial relationships through interacting the content feature with both original and stylized component-wise representations. These two blocks collaborate to preserve accurate character shapes and stylistic textures. Moreover, we also design a corner consistency loss and an elastic mesh feature loss to better improve geometric alignment. Extensive experiments show that our DA-Font outperforms the state-of-the-art methods across diverse font styles and characters, demonstrating its effectiveness in enhancing structural integrity and local fidelity. The source code can be found at \\href{https://github.com/wrchen2001/DA-Font}{\\textit{https://github.com/wrchen2001/DA-Font}}.",
        "tags": [
            "Style Transfer"
        ]
    },
    {
        "id": "94",
        "title": "When Big Models Train Small Ones: Label-Free Model Parity Alignment for Efficient Visual Question Answering using Small VLMs",
        "author": [
            "Abhirama Subramanyam Penamakuri",
            "Navlika Singh",
            "Piyush Arora",
            "Anand Mishra"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16633",
        "abstract": "Large Vision-Language Models (L-VLMs) have demonstrated remarkable performance in various vision and language tasks, including visual question answering (VQA). However, their high computational cost makes them impractical for resource-constrained settings and inference-heavy applications. In contrast, Small Vision-Language Models (S-VLMs) offer efficiency but suffer from a significant performance gap compared to their larger counterparts. In this work, we introduce the Model Parity Aligner (MPA), a novel framework designed to systematically improve S-VLMs by leveraging unlabeled images and effective knowledge transfer from L-VLMs. Instead of traditional knowledge distillation methods that rely on labeled training data, MPA employs a strategic parity-based approach that precisely identifies the knowledge disparities between S-VLMs and L-VLMs, and optimizes training by targeting only these disparities. We conduct extensive experiments on four diverse VQA benchmarks, namely TextVQA, ST-VQA, ChartQA, and OKVQA, each of which requires specialized reasoning capabilities such as text recognition, chart interpretation, and commonsense and factual understanding. Our results demonstrate that MPA consistently enhances the performance of S-VLMs on all benchmarks, reducing the performance gap while maintaining computational efficiency. We make our code publicly available.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "95",
        "title": "KungfuBot2: Learning Versatile Motion Skills for Humanoid Whole-Body Control",
        "author": [
            "Jinrui Han",
            "Weiji Xie",
            "Jiakun Zheng",
            "Jiyuan Shi",
            "Weinan Zhang",
            "Ting Xiao",
            "Chenjia Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16638",
        "abstract": "Learning versatile whole-body skills by tracking various human motions is a fundamental step toward general-purpose humanoid robots. This task is particularly challenging because a single policy must master a broad repertoire of motion skills while ensuring stability over long-horizon sequences. To this end, we present VMS, a unified whole-body controller that enables humanoid robots to learn diverse and dynamic behaviors within a single policy. Our framework integrates a hybrid tracking objective that balances local motion fidelity with global trajectory consistency, and an Orthogonal Mixture-of-Experts (OMoE) architecture that encourages skill specialization while enhancing generalization across motions. A segment-level tracking reward is further introduced to relax rigid step-wise matching, enhancing robustness when handling global displacements and transient inaccuracies. We validate VMS extensively in both simulation and real-world experiments, demonstrating accurate imitation of dynamic skills, stable performance over minute-long sequences, and strong generalization to unseen motions. These results highlight the potential of VMS as a scalable foundation for versatile humanoid whole-body control. The project page is available at https://kungfubot2-humanoid.github.io.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "96",
        "title": "FESTA: Functionally Equivalent Sampling for Trust Assessment of Multimodal LLMs",
        "author": [
            "Debarpan Bhattacharya",
            "Apoorva Kulkarni",
            "Sriram Ganapathy"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16648",
        "abstract": "The accurate trust assessment of multimodal large language models (MLLMs) generated predictions, which can enable selective prediction and improve user confidence, is challenging due to the diverse multi-modal input paradigms. We propose Functionally Equivalent Sampling for Trust Assessment (FESTA), a multimodal input sampling technique for MLLMs, that generates an uncertainty measure based on the equivalent and complementary input samplings. The proposed task-preserving sampling approach for uncertainty quantification expands the input space to probe the consistency (through equivalent samples) and sensitivity (through complementary samples) of the model. FESTA uses only input-output access of the model (black-box), and does not require ground truth (unsupervised). The experiments are conducted with various off-the-shelf multi-modal LLMs, on both visual and audio reasoning tasks. The proposed FESTA uncertainty estimate achieves significant improvement (33.3% relative improvement for vision-LLMs and 29.6% relative improvement for audio-LLMs) in selective prediction performance, based on area-under-receiver-operating-characteristic curve (AUROC) metric in detecting mispredictions. The code implementation is open-sourced.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "97",
        "title": "AISTAT lab system for DCASE2025 Task6: Language-based audio retrieval",
        "author": [
            "Hyun Jun Kim",
            "Hyeong Yong Choi",
            "Changwon Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16649",
        "abstract": "This report presents the AISTAT team's submission to the language-based audio retrieval task in DCASE 2025 Task 6. Our proposed system employs dual encoder architecture, where audio and text modalities are encoded separately, and their representations are aligned using contrastive learning. Drawing inspiration from methodologies of the previous year's challenge, we implemented a distillation approach and leveraged large language models (LLMs) for effective data augmentation techniques, including back-translation and LLM mix. Additionally, we incorporated clustering to introduce an auxiliary classification task for further finetuning. Our best single system achieved a mAP@16 of 46.62, while an ensemble of four systems reached a mAP@16 of 48.83 on the Clotho development test split.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "98",
        "title": "Safe Guaranteed Dynamics Exploration with Probabilistic Models",
        "author": [
            "Manish Prajapat",
            "Johannes KÃ¶hler",
            "Melanie N. Zeilinger",
            "Andreas Krause"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16650",
        "abstract": "Ensuring both optimality and safety is critical for the real-world deployment of agents, but becomes particularly challenging when the system dynamics are unknown. To address this problem, we introduce a notion of maximum safe dynamics learning via sufficient exploration in the space of safe policies. We propose a $\\textit{pessimistically}$ safe framework that $\\textit{optimistically}$ explores informative states and, despite not reaching them due to model uncertainty, ensures continuous online learning of dynamics. The framework achieves first-of-its-kind results: learning the dynamics model sufficiently $-$ up to an arbitrary small tolerance (subject to noise) $-$ in a finite time, while ensuring provably safe operation throughout with high probability and without requiring resets. Building on this, we propose an algorithm to maximize rewards while learning the dynamics $\\textit{only to the extent needed}$ to achieve close-to-optimal performance. Unlike typical reinforcement learning (RL) methods, our approach operates online in a non-episodic setting and ensures safety throughout the learning process. We demonstrate the effectiveness of our approach in challenging domains such as autonomous car racing and drone navigation under aerodynamic effects $-$ scenarios where safety is critical and accurate modeling is difficult.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "99",
        "title": "Are VLMs Ready for Lane Topology Awareness in Autonomous Driving?",
        "author": [
            "Xin Chen",
            "Jia He",
            "Maozheng Li",
            "Dongliang Xu",
            "Tianyu Wang",
            "Yixiao Chen",
            "Zhixin Lin",
            "Yue Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16654",
        "abstract": "Vision-Language Models (VLMs) have recently shown remarkable progress in multimodal reasoning, yet their applications in autonomous driving remain limited. In particular, the ability to understand road topology, a key requirement for safe navigation, has received relatively little attention. While some recent works have begun to explore VLMs in driving contexts, their performance on topology reasoning is far from satisfactory. In this work, we systematically evaluate VLMs' capabilities in road topology understanding. Specifically, multi-view images are projected into unified ground-plane coordinate system and fused into bird's-eye-view (BEV) lanes. Based on these BEV lanes, we formulate four topology-related diagnostic VQA tasks, which together capture essential components of spatial topology reasoning. Through extensive evaluation, we find that while frontier closed-source models (e.g., GPT-4o) achieve relatively high accuracy in some tasks, they still fail in some temporal questions that humans can answer (e.g., GPT-4o achieve only 67.8% in vector, a two-class classification problem). Furthermore, we find open-source VLMs, even at 30B scale, struggle significantly. These results indicate that spatial reasoning remains a fundamental bottleneck for current VLMs. We also find that the model's capability is positively correlated with model size, length of reasoning tokens and shots provided as examples, showing direction for future research.",
        "tags": [
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "100",
        "title": "NUMINA: A Natural Understanding Benchmark for Multi-dimensional Intelligence and Numerical Reasoning Abilities",
        "author": [
            "Changyu Zeng",
            "Yifan Wang",
            "Zimu Wang",
            "Wei Wang",
            "Zhengni Yang",
            "Muyi Bao",
            "Jiming Xiao",
            "Ahn Nguyen",
            "Yutao Yue"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16656",
        "abstract": "Recent advancements in 2D multimodal large language models (MLLMs) have significantly improved performance in vision-language tasks. However, extending these capabilities to 3D environments remains a distinct challenge due to the complexity of spatial reasoning. Nevertheless, existing 3D benchmarks often lack fine-grained numerical reasoning task annotations, limiting MLLMs' ability to perform precise spatial measurements and complex numerical reasoning. To address this gap, we introduce NUMINA, the first Natural Understanding benchmark for Multi-dimensional Intelligence and Numerical reasoning Abilities to enhance multimodal indoor perceptual understanding. NUMINA features multi-scale annotations and various question-answer pairs, generated using NUMINA-Flow, an automated annotation pipeline that integrates LLM rewriting and rule-based self-verification. We evaluate the performance of various state-of-the-art LLMs on NUMINA following the Chat-Scene framework, demonstrating that current LLMs struggle with multimodal numerical reasoning, particularly in performing precise computations such as distance and volume estimation, highlighting the need for further advancements in 3D models. The dataset and source codes can be obtained from https://github.com/fengshun124/NUMINA.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "101",
        "title": "Redefining Experts: Interpretable Decomposition of Language Models for Toxicity Mitigation",
        "author": [
            "Zuhair Hasan Shaik",
            "Abdullah Mazhar",
            "Aseem Srivastava",
            "Md Shad Akhtar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16660",
        "abstract": "Large Language Models have demonstrated impressive fluency across diverse tasks, yet their tendency to produce toxic content remains a critical challenge for AI safety and public trust. Existing toxicity mitigation approaches primarily manipulate individual neuron activations, but these methods suffer from instability, context dependence, and often compromise the model's core language abilities. To address these shortcomings, we investigate three key questions: the stability of neuron-level toxicity indicators, the advantages of structural (layer-wise) representations, and the interpretability of mechanisms driving toxic generation. Through extensive experiments on Jigsaw and ToxiCN datasets, we show that aggregated layer-wise features provide more robust signals than single neurons. Moreover, we observe conceptual limitations in prior works that conflate toxicity detection experts and generation experts within neuron-based interventions. To mitigate this, we propose a novel principled intervention technique, EigenShift, based on eigen-decomposition of the language model's final output layer. This method selectively targets generation-aligned components, enabling precise toxicity suppression without impairing linguistic competence. Our method requires no additional training or fine-tuning, incurs minimal computational cost, and is grounded in rigorous theoretical analysis.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "102",
        "title": "On the de-duplication of the Lakh MIDI dataset",
        "author": [
            "Eunjin Choi",
            "Hyerin Kim",
            "Jiwoo Ryu",
            "Juhan Nam",
            "Dasaem Jeong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16662",
        "abstract": "A large-scale dataset is essential for training a well-generalized deep-learning model. Most such datasets are collected via scraping from various internet sources, inevitably introducing duplicated data. In the symbolic music domain, these duplicates often come from multiple user arrangements and metadata changes after simple editing. However, despite critical issues such as unreliable training evaluation from data leakage during random splitting, dataset duplication has not been extensively addressed in the MIR community. This study investigates the dataset duplication issues regarding Lakh MIDI Dataset (LMD), one of the largest publicly available sources in the symbolic music domain. To find and evaluate the best retrieval method for duplicated data, we employed the Clean MIDI subset of the LMD as a benchmark test set, in which different versions of the same songs are grouped together. We first evaluated rule-based approaches and previous symbolic music retrieval models for de-duplication and also investigated with a contrastive learning-based BERT model with various augmentations to find duplicate files. As a result, we propose three different versions of the filtered list of LMD, which filters out at least 38,134 samples in the most conservative settings among 178,561 files.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "103",
        "title": "Robust Native Language Identification through Agentic Decomposition",
        "author": [
            "Ahmet Yavuz Uluslu",
            "Tannon Kew",
            "Tilia Ellendorff",
            "Gerold Schneider",
            "Rico Sennrich"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16666",
        "abstract": "Large language models (LLMs) often achieve high performance in native language identification (NLI) benchmarks by leveraging superficial contextual clues such as names, locations, and cultural stereotypes, rather than the underlying linguistic patterns indicative of native language (L1) influence. To improve robustness, previous work has instructed LLMs to disregard such clues. In this work, we demonstrate that such a strategy is unreliable and model predictions can be easily altered by misleading hints. To address this problem, we introduce an agentic NLI pipeline inspired by forensic linguistics, where specialized agents accumulate and categorize diverse linguistic evidence before an independent final overall assessment. In this final assessment, a goal-aware coordinating agent synthesizes all evidence to make the NLI prediction. On two benchmark datasets, our approach significantly enhances NLI robustness against misleading contextual clues and performance consistency compared to standard prompting methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "104",
        "title": "Speech-to-See: End-to-End Speech-Driven Open-Set Object Detection",
        "author": [
            "Wenhuan Lu",
            "Xinyue Song",
            "Wenjun Ke",
            "Zhizhi Yu",
            "Wenhao Yang",
            "Jianguo Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16670",
        "abstract": "Audio grounding, or speech-driven open-set object detection, aims to localize and identify objects directly from speech, enabling generalization beyond predefined categories. This task is crucial for applications like human-robot interaction where textual input is impractical. However, progress in this domain faces a fundamental bottleneck from the scarcity of large-scale, paired audio-image data, and is further constrained by previous methods that rely on indirect, text-mediated pipelines. In this paper, we introduce Speech-to-See (Speech2See), an end-to-end approach built on a pre-training and fine-tuning paradigm. Specifically, in the pre-training stage, we design a Query-Guided Semantic Aggregation module that employs learnable queries to condense redundant speech embeddings into compact semantic representations. During fine-tuning, we incorporate a parameter-efficient Mixture-of-LoRA-Experts (MoLE) architecture to achieve deeper and more nuanced cross-modal adaptation. Extensive experiments show that Speech2See achieves robust and adaptable performance across multiple benchmarks, demonstrating its strong generalization ability and broad applicability.",
        "tags": [
            "Detection",
            "LoRA",
            "Robotics"
        ]
    },
    {
        "id": "105",
        "title": "\"Digital Camouflage\": The LLVM Challenge in LLM-Based Malware Detection",
        "author": [
            "Ekin BÃ¶ke",
            "Simon Torka"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16671",
        "abstract": "Large Language Models (LLMs) have emerged as promising tools for malware detection by analyzing code semantics, identifying vulnerabilities, and adapting to evolving threats. However, their reliability under adversarial compiler-level obfuscation is yet to be discovered. In this study, we empirically evaluate the robustness of three state-of-the-art LLMs: ChatGPT-4o, Gemini Flash 2.5, and Claude Sonnet 4 against compiler-level obfuscation techniques implemented via the LLVM infrastructure. These include control flow flattening, bogus control flow injection, instruction substitution, and split basic blocks, which are widely used to evade detection while preserving malicious behavior. We perform a structured evaluation on 40~C functions (20 vulnerable, 20 secure) sourced from the Devign dataset and obfuscated using LLVM passes. Our results show that these models often fail to correctly classify obfuscated code, with precision, recall, and F1-score dropping significantly after transformation. This reveals a critical limitation: LLMs, despite their language understanding capabilities, can be easily misled by compiler-based obfuscation strategies. To promote reproducibility, we release all evaluation scripts, prompts, and obfuscated code samples in a public repository. We also discuss the implications of these findings for adversarial threat modeling, and outline future directions such as software watermarking, compiler-aware defenses, and obfuscation-resilient model design.",
        "tags": [
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "106",
        "title": "Reinforcement Learning Meets Large Language Models: A Survey of Advancements and Applications Across the LLM Lifecycle",
        "author": [
            "Keliang Liu",
            "Dingkang Yang",
            "Ziyun Qian",
            "Weijie Yin",
            "Yuchi Wang",
            "Hongsheng Li",
            "Jun Liu",
            "Peng Zhai",
            "Yang Liu",
            "Lihua Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16679",
        "abstract": "In recent years, training methods centered on Reinforcement Learning (RL) have markedly enhanced the reasoning and alignment performance of Large Language Models (LLMs), particularly in understanding human intents, following user instructions, and bolstering inferential strength. Although existing surveys offer overviews of RL augmented LLMs, their scope is often limited, failing to provide a comprehensive summary of how RL operates across the full lifecycle of LLMs. We systematically review the theoretical and practical advancements whereby RL empowers LLMs, especially Reinforcement Learning with Verifiable Rewards (RLVR). First, we briefly introduce the basic theory of RL. Second, we thoroughly detail application strategies for RL across various phases of the LLM lifecycle, including pre-training, alignment fine-tuning, and reinforced reasoning. In particular, we emphasize that RL methods in the reinforced reasoning phase serve as a pivotal driving force for advancing model reasoning to its limits. Next, we collate existing datasets and evaluation benchmarks currently used for RL fine-tuning, spanning human-annotated datasets, AI-assisted preference data, and program-verification-style corpora. Subsequently, we review the mainstream open-source tools and training frameworks available, providing clear practical references for subsequent research. Finally, we analyse the future challenges and trends in the field of RL-enhanced LLMs. This survey aims to present researchers and practitioners with the latest developments and frontier trends at the intersection of RL and LLMs, with the goal of fostering the evolution of LLMs that are more intelligent, generalizable, and secure.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "107",
        "title": "Design and Development of an Intelligent LLM-based LDAP Honeypot",
        "author": [
            "Javier JimÃ©nez-RomÃ¡n",
            "Florina Almenares-Mendoza",
            "Alfonso SÃ¡nchez-MaciÃ¡n"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16682",
        "abstract": "Cybersecurity threats continue to increase, with a growing number of previously unknown attacks each year targeting both large corporations and smaller entities. This scenario demands the implementation of advanced security measures, not only to mitigate damage but also to anticipate emerging attack trends. In this context, deception tools have become a key strategy, enabling the detection, deterrence, and deception of potential attackers while facilitating the collection of information about their tactics and methods. Among these tools, honeypots have proven their value, although they have traditionally been limited by rigidity and configuration complexity, hindering their adaptability to dynamic scenarios. The rise of artificial intelligence, and particularly general-purpose Large Language Models (LLMs), is driving the development of new deception solutions capable of offering greater adaptability and ease of use. This work proposes the design and implementation of an LLM-based honeypot to simulate an LDAP server, a critical protocol present in most organizations due to its central role in identity and access management. The proposed solution aims to provide a flexible and realistic tool capable of convincingly interacting with attackers, thereby contributing to early detection and threat analysis while enhancing the defensive capabilities of infrastructures against intrusions targeting this service.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "108",
        "title": "EG-MLA: Embedding-Gated Multi-head Latent Attention for Scalable and Efficient LLMs",
        "author": [
            "Zhengge Cai",
            "Haowen Hou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16686",
        "abstract": "Reducing the key-value (KV) cache size is a crucial step toward enabling efficient inference in large language models (LLMs), especially under latency and memory constraints. While Multi-Head Attention (MHA) offers strong representational power, it incurs significant memory overhead. Recent work on Multi-head Latent Attention (MLA) mitigates this by compressing KV representations into a shared latent space, achieving a better trade-off between performance and cache efficiency. While MLA already achieves significant KV cache reduction, the scope for further compression remains limited without performance loss. In this paper, we propose \\textbf{Embedding-Gated Multi-head Latent Attention (EG-MLA)}, a novel extension of MLA that further reduces KV cache size while enhancing representational expressiveness. EG-MLA introduces a token-specific embedding gating mechanism applied in the latent space, enabling fine-grained modulation of compressed KV vectors with minimal additional computation. Compared to MHA, EG-MLA achieves over 91.6\\% reduction in KV cache size with negligible performance degradation. Relative to MLA, EG-MLA consistently improves task accuracy across diverse reasoning benchmarks while achieving up to 59.9\\% additional memory savings. Our theoretical analysis highlights how embedding gating induces implicit high-order interactions, and empirical evaluations demonstrate robust generalization across model scales and compression regimes. Notably, we successfully scale EG-MLA to over 1 billion parameters, demonstrating its practical viability for large-scale LLM deployment. These results establish EG-MLA as a memory- and compute-efficient attention mechanism that enables scalable, high-performance inference in modern LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "109",
        "title": "Spectral Compressive Imaging via Chromaticity-Intensity Decomposition",
        "author": [
            "Xiaodong Wang",
            "Zijun He",
            "Ping Wang",
            "Lishun Wang",
            "Yanan Hu",
            "Xin Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16690",
        "abstract": "In coded aperture snapshot spectral imaging (CASSI), the captured measurement entangles spatial and spectral information, posing a severely ill-posed inverse problem for hyperspectral images (HSIs) reconstruction. Moreover, the captured radiance inherently depends on scene illumination, making it difficult to recover the intrinsic spectral reflectance that remains invariant to lighting conditions. To address these challenges, we propose a chromaticity-intensity decomposition framework, which disentangles an HSI into a spatially smooth intensity map and a spectrally variant chromaticity cube. The chromaticity encodes lighting-invariant reflectance, enriched with high-frequency spatial details and local spectral sparsity. Building on this decomposition, we develop CIDNet, a Chromaticity-Intensity Decomposition unfolding network within a dual-camera CASSI system. CIDNet integrates a hybrid spatial-spectral Transformer tailored to reconstruct fine-grained and sparse spectral chromaticity and a degradation-aware, spatially-adaptive noise estimation module that captures anisotropic noise across iterative stages. Extensive experiments on both synthetic and real-world CASSI datasets demonstrate that our method achieves superior performance in both spectral and chromaticity fidelity. Code and models will be publicly available.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "110",
        "title": "InstanceAssemble: Layout-Aware Image Generation via Instance Assembling Attention",
        "author": [
            "Qiang Xiang",
            "Shuang Sun",
            "Binglei Li",
            "Dejia Song",
            "Huaxia Li",
            "Nemo Chen",
            "Xu Tang",
            "Yao Hu",
            "Junping Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16691",
        "abstract": "Diffusion models have demonstrated remarkable capabilities in generating high-quality images. Recent advancements in Layout-to-Image (L2I) generation have leveraged positional conditions and textual descriptions to facilitate precise and controllable image synthesis. Despite overall progress, current L2I methods still exhibit suboptimal performance. Therefore, we propose InstanceAssemble, a novel architecture that incorporates layout conditions via instance-assembling attention, enabling position control with bounding boxes (bbox) and multimodal content control including texts and additional visual content. Our method achieves flexible adaption to existing DiT-based T2I models through light-weighted LoRA modules. Additionally, we propose a Layout-to-Image benchmark, Denselayout, a comprehensive benchmark for layout-to-image generation, containing 5k images with 90k instances in total. We further introduce Layout Grounding Score (LGS), an interpretable evaluation metric to more precisely assess the accuracy of L2I generation. Experiments demonstrate that our InstanceAssemble method achieves state-of-the-art performance under complex layout conditions, while exhibiting strong compatibility with diverse style LoRA modules.",
        "tags": [
            "DiT",
            "Diffusion",
            "LoRA"
        ]
    },
    {
        "id": "111",
        "title": "Decoding Uncertainty: The Impact of Decoding Strategies for Uncertainty Estimation in Large Language Models",
        "author": [
            "Wataru Hashimoto",
            "Hidetaka Kamigaito",
            "Taro Watanabe"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16696",
        "abstract": "Decoding strategies manipulate the probability distribution underlying the output of a language model and can therefore affect both generation quality and its uncertainty. In this study, we investigate the impact of decoding strategies on uncertainty estimation in Large Language Models (LLMs). Our experiments show that Contrastive Search, which mitigates repetition, yields better uncertainty estimates on average across a range of preference-aligned LLMs. In contrast, the benefits of these strategies sometimes diverge when the model is only post-trained with supervised fine-tuning, i.e. without explicit alignment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "112",
        "title": "RelRepair: Enhancing Automated Program Repair by Retrieving Relevant Code",
        "author": [
            "Shunyu Liu",
            "Guangdong Bai",
            "Mark Utting",
            "Guowei Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16701",
        "abstract": "Automated Program Repair (APR) has emerged as a promising paradigm for reducing debugging time and improving the overall efficiency of software development. Recent advances in Large Language Models (LLMs) have demonstrated their potential for automated bug fixing and other software engineering tasks. Nevertheless, the general-purpose nature of LLM pre-training means these models often lack the capacity to perform project-specific repairs, which require understanding of domain-specific identifiers, code structures, and contextual relationships within a particular codebase. As a result, LLMs may struggle to generate correct patches when the repair depends on project-specific information.\nTo address this limitation, we introduce RelRepair, a novel approach that retrieves relevant project-specific code to enhance automated program repair. RelRepair first identifies relevant function signatures by analyzing function names and code comments within the project. It then conducts deeper code analysis to retrieve code snippets relevant to the repair context. The retrieved relevant information is then incorporated into the LLM's input prompt, guiding the model to generate more accurate and informed patches. We evaluate RelRepair on two widely studied datasets, Defects4J V1.2 and ManySStuBs4J, and compare its performance against several state-of-the-art LLM-based APR approaches. RelRepair successfully repairs 101 bugs in Defects4J V1.2. Furthermore, RelRepair achieves a 17.1\\% improvement in the ManySStuBs4J dataset, increasing the overall fix rate to 48.3\\%. These results highlight the importance of providing relevant project-specific information to LLMs, shedding light on effective strategies for leveraging LLMs in APR tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "113",
        "title": "Animalbooth: multimodal feature enhancement for animal subject personalization",
        "author": [
            "Chen Liu",
            "Haitao Wu",
            "Kafeng Wang",
            "Xiaowang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16702",
        "abstract": "Personalized animal image generation is challenging due to rich appearance cues and large morphological variability. Existing approaches often exhibit feature misalignment across domains, which leads to identity drift. We present AnimalBooth, a framework that strengthens identity preservation with an Animal Net and an adaptive attention module, mitigating cross domain alignment errors. We further introduce a frequency controlled feature integration module that applies Discrete Cosine Transform filtering in the latent space to guide the diffusion process, enabling a coarse to fine progression from global structure to detailed texture. To advance research in this area, we curate AnimalBench, a high resolution dataset for animal personalization. Extensive experiments show that AnimalBooth consistently outperforms strong baselines on multiple benchmarks and improves both identity fidelity and perceptual quality.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "114",
        "title": "HypeMARL: Multi-Agent Reinforcement Learning For High-Dimensional, Parametric, and Distributed Systems",
        "author": [
            "NicolÃ² Botteghi",
            "Matteo Tomasetto",
            "Urban Fasel",
            "Francesco Braghin",
            "Andrea Manzoni"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16709",
        "abstract": "Deep reinforcement learning has recently emerged as a promising feedback control strategy for complex dynamical systems governed by partial differential equations (PDEs). When dealing with distributed, high-dimensional problems in state and control variables, multi-agent reinforcement learning (MARL) has been proposed as a scalable approach for breaking the curse of dimensionality. In particular, through decentralized training and execution, multiple agents cooperate to steer the system towards a target configuration, relying solely on local state and reward information. However, the principle of locality may become a limiting factor whenever a collective, nonlocal behavior of the agents is crucial to maximize the reward function, as typically happens in PDE-constrained optimal control problems. In this work, we propose HypeMARL: a decentralized MARL algorithm tailored to the control of high-dimensional, parametric, and distributed systems. HypeMARL employs hypernetworks to effectively parametrize the agents' policies and value functions with respect to the system parameters and the agents' relative positions, encoded by sinusoidal positional encoding. Through the application on challenging control problems, such as density and flow control, we show that HypeMARL (i) can effectively control systems through a collective behavior of the agents, outperforming state-of-the-art decentralized MARL, (ii) can efficiently deal with parametric dependencies, (iii) requires minimal hyperparameter tuning and (iv) can reduce the amount of expensive environment interactions by a factor of ~10 thanks to its model-based extension, MB-HypeMARL, which relies on computationally efficient deep learning-based surrogate models approximating the dynamics locally, with minimal deterioration of the policy performance.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "115",
        "title": "OPEN-THEATRE: An Open-Source Toolkit for LLM-based Interactive Drama",
        "author": [
            "Tianyang Xu",
            "Hongqiu Wu",
            "Weiqi Wu",
            "Hai Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16713",
        "abstract": "LLM-based Interactive Drama introduces a novel dialogue scenario in which the player immerses into a character and engages in a dramatic story by interacting with LLM agents. Despite the fact that this emerging area holds significant promise, it remains largely underexplored due to the lack of a well-designed playground to develop a complete drama. This makes a significant barrier for researchers to replicate, extend, and study such systems. Hence, we present Open-Theatre, the first open-source toolkit for experiencing and customizing LLM-based interactive drama. It refines prior work with an efficient multi-agent architecture and a hierarchical retrieval-based memory system, designed to enhance narrative coherence and realistic long-term behavior in complex interactions. In addition, we provide a highly configurable pipeline, making it easy for researchers to develop and optimize new approaches.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "116",
        "title": "Time to Revist Exact Match",
        "author": [
            "Auss Abbood",
            "Zaiqiao Meng",
            "Nigel Collier"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16720",
        "abstract": "Temporal question answering is an established method for evaluating temporal reasoning in large language models. Expected answers are often numeric (e.g., dates or durations), yet model responses are evaluated like regular text with exact match (EM), unable to distinguish small from large errors. In this investigative work, we frame temporal question answering as a numerical estimation task to assess the shortcomings of EM. We introduce TempAnswerQA, a benchmark distilled from Test of Time and TempTabQA, where all questions require a numerical, temporal answer, allowing us to evaluate models beyond EM. We use the forecasting metrics symmetric mean absolute percentage error (sMAPE) and mean absolute scaled error (MASE). With sMAPE, we find that error size and EM are decoupled. Models with low EM still have low sMAPE (both ~20%), and some models have high sMAPE despite high EM. Scaling errors by the deviation of the ground truth data with MASE reshuffles model rankings compared to EM, revealing gaps in models' understanding of temporal domain knowledge, especially when trained with synthetic data. Lastly, the models' most frequent error is to deviate by only $\\pm1$ from the ground truth. sMAPE and MASE, unlike EM, adequately weight these errors. Our findings underscore the need for specialised metrics for temporal QA tasks. Code and data are available on https://github.com/aauss/temporal-answer-qa.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "117",
        "title": "Text-Scene: A Scene-to-Language Parsing Framework for 3D Scene Understanding",
        "author": [
            "Haoyuan Li",
            "Rui Liu",
            "Hehe Fan",
            "Yi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16721",
        "abstract": "Enabling agents to understand and interact with complex 3D scenes is a fundamental challenge for embodied artificial intelligence systems. While Multimodal Large Language Models (MLLMs) have achieved significant progress in 2D image understanding, extending such capabilities to 3D scenes remains difficult: 1) 3D environment involves richer concepts such as spatial relationships, affordances, physics, layout, and so on, 2) the absence of large-scale 3D vision-language datasets has posed a significant obstacle. In this paper, we introduce Text-Scene, a framework that automatically parses 3D scenes into textual descriptions for scene understanding. Given a 3D scene, our model identifies object attributes and spatial relationships, and then generates a coherent summary of the whole scene, bridging the gap between 3D observation and language without requiring human-in-the-loop intervention. By leveraging both geometric analysis and MLLMs, Text-Scene produces descriptions that are accurate, detailed, and human-interpretable, capturing object-level details and global-level context. Experimental results on benchmarks demonstrate that our textual parses can faithfully represent 3D scenes and benefit downstream tasks. To evaluate the reasoning capability of MLLMs, we present InPlan3D, a comprehensive benchmark for 3D task planning, consisting of 3174 long-term planning tasks across 636 indoor scenes. We emphasize clarity and accessibility in our approach, aiming to make 3D scene content understandable through language. Code and datasets will be released.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "118",
        "title": "Pain in 3D: Generating Controllable Synthetic Faces for Automated Pain Assessment",
        "author": [
            "Xin Lei Lin",
            "Soroush Mehraban",
            "Abhishek Moturu",
            "Babak Taati"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16727",
        "abstract": "Automated pain assessment from facial expressions is crucial for non-communicative patients, such as those with dementia. Progress has been limited by two challenges: (i) existing datasets exhibit severe demographic and label imbalance due to ethical constraints, and (ii) current generative models cannot precisely control facial action units (AUs), facial structure, or clinically validated pain levels.\nWe present 3DPain, a large-scale synthetic dataset specifically designed for automated pain assessment, featuring unprecedented annotation richness and demographic diversity. Our three-stage framework generates diverse 3D meshes, textures them with diffusion models, and applies AU-driven face rigging to synthesize multi-view faces with paired neutral and pain images, AU configurations, PSPI scores, and the first dataset-level annotations of pain-region heatmaps. The dataset comprises 82,500 samples across 25,000 pain expression heatmaps and 2,500 synthetic identities balanced by age, gender, and ethnicity.\nWe further introduce ViTPain, a Vision Transformer based cross-modal distillation framework in which a heatmap-trained teacher guides a student trained on RGB images, enhancing accuracy, interpretability, and clinical reliability. Together, 3DPain and ViTPain establish a controllable, diverse, and clinically grounded foundation for generalizable automated pain assessment.",
        "tags": [
            "3D",
            "Diffusion",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "119",
        "title": "Towards Transparent and Incentive-Compatible Collaboration in Decentralized LLM Multi-Agent Systems: A Blockchain-Driven Approach",
        "author": [
            "Minfeng Qi",
            "Tianqing Zhu",
            "Lefeng Zhang",
            "Ningran Li",
            "Wanlei Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16736",
        "abstract": "Large Language Models (LLMs) have enabled the emergence of autonomous agents capable of complex reasoning, planning, and interaction. However, coordinating such agents at scale remains a fundamental challenge, particularly in decentralized environments where communication lacks transparency and agent behavior cannot be shaped through centralized incentives. We propose a blockchain-based framework that enables transparent agent registration, verifiable task allocation, and dynamic reputation tracking through smart contracts. The core of our design lies in two mechanisms: a matching score-based task allocation protocol that evaluates agents by reputation, capability match, and workload; and a behavior-shaping incentive mechanism that adjusts agent behavior via feedback on performance and reward. Our implementation integrates GPT-4 agents with Solidity contracts and demonstrates, through 50-round simulations, strong task success rates, stable utility distribution, and emergent agent specialization. The results underscore the potential for trustworthy, incentive-compatible multi-agent coordination in open environments.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "120",
        "title": "Sycophancy Mitigation Through Reinforcement Learning with Uncertainty-Aware Adaptive Reasoning Trajectories",
        "author": [
            "Mohammad Beigi",
            "Ying Shen",
            "Parshin Shojaee",
            "Qifan Wang",
            "Zichao Wang",
            "Chandan Reddy",
            "Ming Jin",
            "Lifu Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16742",
        "abstract": "Despite the remarkable capabilities of large language models, current training paradigms inadvertently foster \\textit{sycophancy}, i.e., the tendency of a model to agree with or reinforce user-provided information even when it's factually incorrect. To address this challenge, we introduce \\textbf{SMART} (Sycophancy Mitigation through Adaptive Reasoning Trajectories), which reframes sycophancy as a \\textit{reasoning optimization problem} rather than an output alignment issue. SMART is a two-stage framework comprising: (1) Uncertainty-Aware Adaptive Monte Carlo Tree Search (UA-MCTS), which dynamically adjusts model exploration based on state-level uncertainty to collect high-quality, diverse reasoning trajectories alongside both stepwise progress and final outcome rewards; and (2) progress-based reinforcement learning, which fine-tunes the model using the collected trajectories and reward signals to reinforce effective reasoning patterns. Through extensive experiments, we show that SMART significantly reduces sycophantic behavior while preserving strong performance on out-of-distribution inputs and maintaining general capabilities. These results underscore the importance of optimizing internal reasoning mechanisms to build more truthful and aligned AI assistants.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "121",
        "title": "A Hybrid PCA-PR-Seq2Seq-Adam-LSTM Framework for Time-Series Power Outage Prediction",
        "author": [
            "Subhabrata Das",
            "Bodruzzaman Khan",
            "Xiao-Yang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16743",
        "abstract": "Accurately forecasting power outages is a complex task influenced by diverse factors such as weather conditions [1], vegetation, wildlife, and load fluctuations. These factors introduce substantial variability and noise into outage data, making reliable prediction challenging. Long Short-Term Memory (LSTM) networks, a type of Recurrent Neural Network (RNN), are particularly effective for modeling nonlinear and dynamic time-series data, with proven applications in stock price forecasting [2], energy demand prediction, demand response [3], and traffic flow management [4]. This paper introduces a hybrid deep learning framework, termed PCA-PR-Seq2Seq-Adam-LSTM, that integrates Principal Component Analysis (PCA), Poisson Regression (PR), a Sequence-to-Sequence (Seq2Seq) architecture, and an Adam-optimized LSTM. PCA is employed to reduce dimensionality and stabilize data variance, while Poisson Regression effectively models discrete outage events. The Seq2Seq-Adam-LSTM component enhances temporal feature learning through efficient gradient optimization and long-term dependency capture. The framework is evaluated using real-world outage records from Michigan, and results indicate that the proposed approach significantly improves forecasting accuracy and robustness compared to existing methods.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "122",
        "title": "Evaluating LLM Generated Detection Rules in Cybersecurity",
        "author": [
            "Anna Bertiger",
            "Bobby Filar",
            "Aryan Luthra",
            "Stefano Meschiari",
            "Aiden Mitchell",
            "Sam Scholten",
            "Vivek Sharath"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16749",
        "abstract": "LLMs are increasingly pervasive in the security environment, with limited measures of their effectiveness, which limits trust and usefulness to security practitioners. Here, we present an open-source evaluation framework and benchmark metrics for evaluating LLM-generated cybersecurity rules. The benchmark employs a holdout set-based methodology to measure the effectiveness of LLM-generated security rules in comparison to a human-generated corpus of rules. It provides three key metrics inspired by the way experts evaluate security rules, offering a realistic, multifaceted evaluation of the effectiveness of an LLM-based security rule generator. This methodology is illustrated using rules from Sublime Security's detection team and those written by Sublime Security's Automated Detection Engineer (ADE), with a thorough analysis of ADE's skills presented in the results section.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "123",
        "title": "Discrete Diffusion Models: Novel Analysis and New Sampler Guarantees",
        "author": [
            "Yuchen Liang",
            "Yingbin Liang",
            "Lifeng Lai",
            "Ness Shroff"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16756",
        "abstract": "Discrete diffusion models have recently gained significant prominence in applications involving natural language and graph data. A key factor influencing their effectiveness is the efficiency of discretized samplers. Among these, $\\tau$-leaping samplers have become particularly popular due to their empirical success. However, existing theoretical analyses of $\\tau$-leaping often rely on somewhat restrictive and difficult-to-verify regularity assumptions, and their convergence bounds contain quadratic dependence on the vocabulary size. In this work, we introduce a new analytical approach for discrete diffusion models that removes the need for such assumptions. For the standard $\\tau$-leaping method, we establish convergence guarantees in KL divergence that scale linearly with vocabulary size, improving upon prior results with quadratic dependence. Our approach is also more broadly applicable: it provides the first convergence guarantees for other widely used samplers, including the Euler method and Tweedie $\\tau$-leaping. Central to our approach is a novel technique based on differential inequalities, offering a more flexible alternative to the traditional Girsanov change-of-measure methods. This technique may also be of independent interest for the analysis of other stochastic processes.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "124",
        "title": "HDMI: Learning Interactive Humanoid Whole-Body Control from Human Videos",
        "author": [
            "Haoyang Weng",
            "Yitang Li",
            "Nikhil Sobanbabu",
            "Zihan Wang",
            "Zhengyi Luo",
            "Tairan He",
            "Deva Ramanan",
            "Guanya Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16757",
        "abstract": "Enabling robust whole-body humanoid-object interaction (HOI) remains challenging due to motion data scarcity and the contact-rich nature. We present HDMI (HumanoiD iMitation for Interaction), a simple and general framework that learns whole-body humanoid-object interaction skills directly from monocular RGB videos. Our pipeline (i) extracts and retargets human and object trajectories from unconstrained videos to build structured motion datasets, (ii) trains a reinforcement learning (RL) policy to co-track robot and object states with three key designs: a unified object representation, a residual action space, and a general interaction reward, and (iii) zero-shot deploys the RL policies on real humanoid robots. Extensive sim-to-real experiments on a Unitree G1 humanoid demonstrate the robustness and generality of our approach: HDMI achieves 67 consecutive door traversals and successfully performs 6 distinct loco-manipulation tasks in the real world and 14 tasks in simulation. Our results establish HDMI as a simple and general framework for acquiring interactive humanoid skills from human videos.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "125",
        "title": "The Sound of Syntax: Finetuning and Comprehensive Evaluation of Language Models for Speech Pathology",
        "author": [
            "Fagun Patel",
            "Duc Q. Nguyen",
            "Sang T. Truong",
            "Jody Vaynshtok",
            "Sanmi Koyejo",
            "Nick Haber"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16765",
        "abstract": "According to the U.S. National Institutes of Health, more than 3.4 million children experience speech disorders that require clinical intervention. The number of speech-language pathologists (SLPs) is roughly 20 times fewer than the number of affected children, highlighting a significant gap in children's care and a pressing need for technological support that improves the productivity of SLPs. State-of-the-art multimodal language models (MLMs) show promise for supporting SLPs, but their use remains underexplored largely due to a limited understanding of their performance in high-stakes clinical settings. To address this gap, we collaborate with domain experts to develop a taxonomy of real-world use cases of MLMs in speech-language pathologies. Building on this taxonomy, we introduce the first comprehensive benchmark for evaluating MLM across five core use cases, each containing 1,000 manually annotated data points. This benchmark includes robustness and sensitivity tests under various settings, including background noise, speaker gender, and accent. Our evaluation of 15 state-of-the-art MLMs reveals that no single model consistently outperforms others across all tasks. Notably, we find systematic disparities, with models performing better on male speakers, and observe that chain-of-thought prompting can degrade performance on classification tasks with large label spaces and narrow decision boundaries. Furthermore, we study fine-tuning MLMs on domain-specific data, achieving improvements of over 30% compared to base models. These findings highlight both the potential and limitations of current MLMs for speech-language pathology applications, underscoring the need for further research and targeted development.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "126",
        "title": "DiffEye: Diffusion-Based Continuous Eye-Tracking Data Generation Conditioned on Natural Images",
        "author": [
            "Ozgur Kara",
            "Harris Nisar",
            "James M. Rehg"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16767",
        "abstract": "Numerous models have been developed for scanpath and saliency prediction, which are typically trained on scanpaths, which model eye movement as a sequence of discrete fixation points connected by saccades, while the rich information contained in the raw trajectories is often discarded. Moreover, most existing approaches fail to capture the variability observed among human subjects viewing the same image. They generally predict a single scanpath of fixed, pre-defined length, which conflicts with the inherent diversity and stochastic nature of real-world visual attention. To address these challenges, we propose DiffEye, a diffusion-based training framework designed to model continuous and diverse eye movement trajectories during free viewing of natural images. Our method builds on a diffusion model conditioned on visual stimuli and introduces a novel component, namely Corresponding Positional Embedding (CPE), which aligns spatial gaze information with the patch-based semantic features of the visual input. By leveraging raw eye-tracking trajectories rather than relying on scanpaths, DiffEye captures the inherent variability in human gaze behavior and generates high-quality, realistic eye movement patterns, despite being trained on a comparatively small dataset. The generated trajectories can also be converted into scanpaths and saliency maps, resulting in outputs that more accurately reflect the distribution of human visual attention. DiffEye is the first method to tackle this task on natural images using a diffusion model while fully leveraging the richness of raw eye-tracking data. Our extensive evaluation shows that DiffEye not only achieves state-of-the-art performance in scanpath generation but also enables, for the first time, the generation of continuous eye movement trajectories. Project webpage: https://diff-eye.github.io/",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "127",
        "title": "MMPart: Harnessing Multi-Modal Large Language Models for Part-Aware 3D Generation",
        "author": [
            "Omid Bonakdar",
            "Nasser Mozayani"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16768",
        "abstract": "Generative 3D modeling has advanced rapidly, driven by applications in VR/AR, metaverse, and robotics. However, most methods represent the target object as a closed mesh devoid of any structural information, limiting editing, animation, and semantic understanding. Part-aware 3D generation addresses this problem by decomposing objects into meaningful components, but existing pipelines face challenges: in existing methods, the user has no control over which objects are separated and how model imagine the occluded parts in isolation phase. In this paper, we introduce MMPart, an innovative framework for generating part-aware 3D models from a single image. We first use a VLM to generate a set of prompts based on the input image and user descriptions. In the next step, a generative model generates isolated images of each object based on the initial image and the previous step's prompts as supervisor (which control the pose and guide model how imagine previously occluded areas). Each of those images then enters the multi-view generation stage, where a number of consistent images from different views are generated. Finally, a reconstruction model converts each of these multi-view images into a 3D model.",
        "tags": [
            "3D",
            "LLM",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "128",
        "title": "AI Knows Best? The Paradox of Expertise, AI-Reliance, and Performance in Educational Tutoring Decision-Making Tasks",
        "author": [
            "Eason Chen",
            "Jeffrey Li",
            "Scarlett Huang",
            "Xinyi Tang",
            "Jionghao Lin",
            "Paulo Carvalho",
            "Kenneth Koedinger"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16772",
        "abstract": "We present an empirical study of how both experienced tutors and non-tutors judge the correctness of tutor praise responses under different Artificial Intelligence (AI)-assisted interfaces, types of explanation (textual explanations vs. inline highlighting). We first fine-tuned several Large Language Models (LLMs) to produce binary correctness labels and explanations, achieving up to 88% accuracy and 0.92 F1 score with GPT-4. We then let the GPT-4 models assist 95 participants in tutoring decision-making tasks by offering different types of explanations. Our findings show that although human-AI collaboration outperforms humans alone in evaluating tutor responses, it remains less accurate than AI alone. Moreover, we find that non-tutors tend to follow the AI's advice more consistently, which boosts their overall accuracy on the task: especially when the AI is correct. In contrast, experienced tutors often override the AI's correct suggestions and thus miss out on potential gains from the AI's generally high baseline accuracy. Further analysis reveals that explanations in text reasoning will increase over-reliance and reduce underreliance, while inline highlighting does not. Moreover, neither explanation style actually has a significant effect on performance and costs participants more time to complete the task, instead of saving time. Our findings reveal a tension between expertise, explanation design, and efficiency in AI-assisted decision-making, highlighting the need for balanced approaches that foster more effective human-AI collaboration.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "129",
        "title": "Generative AI alone may not be enough: Evaluating AI Support for Learning Mathematical Proof",
        "author": [
            "Eason Chen",
            "Sophia Judicke",
            "Kayla Beigh",
            "Xinyi Tang",
            "Zimo Xiao",
            "Chuangji Li",
            "Shizhuo Li",
            "Reed Luttmer",
            "Shreya Singh",
            "Maria Yampolsky",
            "Naman Parikh",
            "Yi Zhao",
            "Meiyi Chen",
            "Scarlett Huang",
            "Anishka Mohanty",
            "Gregory Johnson",
            "John Mackey",
            "Jionghao Lin",
            "Ken Koedinger"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16778",
        "abstract": "We evaluate the effectiveness of LLM-Tutor, a large language model (LLM)-powered tutoring system that combines an AI-based proof-review tutor for real-time feedback on proof-writing and a chatbot for mathematics-related queries. Our experiment, involving 148 students, demonstrated that the use of LLM-Tutor significantly improved homework performance compared to a control group without access to the system. However, its impact on exam performance and time spent on tasks was found to be insignificant. Mediation analysis revealed that students with lower self-efficacy tended to use the chatbot more frequently, which partially contributed to lower midterm scores. Furthermore, students with lower self-efficacy were more likely to engage frequently with the proof-review-AI-tutor, a usage pattern that positively contributed to higher final exam scores. Interviews with 19 students highlighted the accessibility of LLM-Tutor and its effectiveness in addressing learning needs, while also revealing limitations and concerns regarding potential over-reliance on the tool. Our results suggest that generative AI alone like chatbot may not suffice for comprehensive learning support, underscoring the need for iterative design improvements with learning sciences principles with generative AI educational tools like LLM-Tutor.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "130",
        "title": "Improving User Interface Generation Models from Designer Feedback",
        "author": [
            "Jason Wu",
            "Amanda Swearngin",
            "Arun Krishna Vajjala",
            "Alan Leung",
            "Jeffrey Nichols",
            "Titus Barik"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16779",
        "abstract": "Despite being trained on vast amounts of data, most LLMs are unable to reliably generate well-designed UIs. Designer feedback is essential to improving performance on UI generation; however, we find that existing RLHF methods based on ratings or rankings are not well-aligned with designers' workflows and ignore the rich rationale used to critique and improve UI designs. In this paper, we investigate several approaches for designers to give feedback to UI generation models, using familiar interactions such as commenting, sketching and direct manipulation. We first perform a study with 21 designers where they gave feedback using these interactions, which resulted in ~1500 design annotations. We then use this data to finetune a series of LLMs to generate higher quality UIs. Finally, we evaluate these models with human judges, and we find that our designer-aligned approaches outperform models trained with traditional ranking feedback and all tested baselines, including GPT-5.",
        "tags": [
            "GPT",
            "LLM",
            "RLHF"
        ]
    },
    {
        "id": "131",
        "title": "Comparing RAG and GraphRAG for Page-Level Retrieval Question Answering on Math Textbook",
        "author": [
            "Eason Chen",
            "Chuangji Li",
            "Shizhuo Li",
            "Conrad Borchers",
            "Zimo Xiao",
            "Chloe Qianhui Zhao",
            "Jionghao Lin",
            "Kenneth R. Koedinger"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16780",
        "abstract": "Technology-enhanced learning environments often help students retrieve relevant learning content for questions arising during self-paced study. Large language models (LLMs) have emerged as novel aids for information retrieval during learning. While LLMs are effective for general-purpose question-answering, they typically lack alignment with the domain knowledge of specific course materials such as textbooks and slides. We investigate Retrieval-Augmented Generation (RAG) and GraphRAG, a knowledge graph-enhanced RAG approach, for page-level question answering in an undergraduate mathematics textbook. While RAG has been effective for retrieving discrete, contextually relevant passages, GraphRAG may excel in modeling interconnected concepts and hierarchical knowledge structures. We curate a dataset of 477 question-answer pairs, each tied to a distinct textbook page. We then compare the standard embedding-based RAG methods to GraphRAG for evaluating both retrieval accuracy-whether the correct page is retrieved-and generated answer quality via F1 scores. Our findings show that embedding-based RAG achieves higher retrieval accuracy and better F1 scores compared to GraphRAG, which tends to retrieve excessive and sometimes irrelevant content due to its entity-based structure. We also explored re-ranking the retrieved pages with LLM and observed mixed results, including performance drop and hallucinations when dealing with larger context windows. Overall, this study highlights both the promises and challenges of page-level retrieval systems in educational contexts, emphasizing the need for more refined retrieval methods to build reliable AI tutoring solutions in providing reference page numbers.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "132",
        "title": "Controlled Yet Natural: A Hybrid BDI-LLM Conversational Agent for Child Helpline Training",
        "author": [
            "Mohammed Al Owayyed",
            "Adarsh Denga",
            "Willem-Paul Brinkman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16784",
        "abstract": "Child helpline training often relies on human-led roleplay, which is both time- and resource-consuming. To address this, rule-based interactive agent simulations have been proposed to provide a structured training experience for new counsellors. However, these agents might suffer from limited language understanding and response variety. To overcome these limitations, we present a hybrid interactive agent that integrates Large Language Models (LLMs) into a rule-based Belief-Desire-Intention (BDI) framework, simulating more realistic virtual child chat conversations. This hybrid solution incorporates LLMs into three components: intent recognition, response generation, and a bypass mechanism. We evaluated the system through two studies: a script-based assessment comparing LLM-generated responses to human-crafted responses, and a within-subject experiment (N=37) comparing the LLM-integrated agent with a rule-based version. The first study provided evidence that the three LLM components were non-inferior to human-crafted responses. In the second study, we found credible support for two hypotheses: participants perceived the LLM-integrated agent as more believable and reported more positive attitudes toward it than the rule-based agent. Additionally, although weaker, there was some support for increased engagement (posterior probability = 0.845, 95% HDI [-0.149, 0.465]). Our findings demonstrate the potential of integrating LLMs into rule-based systems, offering a promising direction for more flexible but controlled training systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "133",
        "title": "Domain-Adaptive Pre-Training for Arabic Aspect-Based Sentiment Analysis: A Comparative Study of Domain Adaptation and Fine-Tuning Strategies",
        "author": [
            "Salha Alyami",
            "Amani Jamal",
            "Areej Alhothali"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16788",
        "abstract": "Aspect-based sentiment analysis (ABSA) in natural language processing enables organizations to understand customer opinions on specific product aspects. While deep learning models are widely used for English ABSA, their application in Arabic is limited due to the scarcity of labeled data. Researchers have attempted to tackle this issue by using pre-trained contextualized language models such as BERT. However, these models are often based on fact-based data, which can introduce bias in domain-specific tasks like ABSA. To our knowledge, no studies have applied adaptive pre-training with Arabic contextualized models for ABSA. This research proposes a novel approach using domain-adaptive pre-training for aspect-sentiment classification (ASC) and opinion target expression (OTE) extraction. We examine fine-tuning strategies - feature extraction, full fine-tuning, and adapter-based methods - to enhance performance and efficiency, utilizing multiple adaptation corpora and contextualized models. Our results show that in-domain adaptive pre-training yields modest improvements. Adapter-based fine-tuning is a computationally efficient method that achieves competitive results. However, error analyses reveal issues with model predictions and dataset labeling. In ASC, common problems include incorrect sentiment labeling, misinterpretation of contrastive markers, positivity bias for early terms, and challenges with conflicting opinions and subword tokenization. For OTE, issues involve mislabeling targets, confusion over syntactic roles, difficulty with multi-word expressions, and reliance on shallow heuristics. These findings underscore the need for syntax- and semantics-aware models, such as graph convolutional networks, to more effectively capture long-distance relations and complex aspect-based opinion alignments.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "134",
        "title": "The Even Sheen of AI: Kitsch, LLMs, and Homogeneity",
        "author": [
            "Gyburg Uhlmann"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16794",
        "abstract": "The exploding use and impact of Chatbots such as ChatGPT that are based on Large Language Models urgently call for a language which is fit to clearly describe functions and problems of the production process and qualities of the Chatbots' textual and image output. Recently, the discussion about appropriate and illuminating metaphors to describe LLMs has gained momentum. As an alternative to well-established metaphors such as \"hallucinating\" and \"bullshit\", we propose \"kitsch\" as a new metaphor. As an internationally widespread term from literary and cultural studies, we argue that \"kitsch\" is particularly suitable for analytically illuminating a previously neglected feature of LLM-based images and texts: their tendency to produce homogeneous and average content, which is becoming increasingly dominant as the proportion of AI-generated content on the internet grows. This is leading to the equalisation of language, style and argument. In view of the potential negative consequences of this averaging, including for human content producers on the internet, we advocate combining methods and insights from kitsch studies with AI research, philosophy, and communication studies in order to better understand the phenomenon and develop countermeasures.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "135",
        "title": "KuBERT: Central Kurdish BERT Model and Its Application for Sentiment Analysis",
        "author": [
            "Kozhin muhealddin Awlla",
            "Hadi Veisi",
            "Abdulhady Abas Abdullah"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16804",
        "abstract": "This paper enhances the study of sentiment analysis for the Central Kurdish language by integrating the Bidirectional Encoder Representations from Transformers (BERT) into Natural Language Processing techniques. Kurdish is a low-resourced language, having a high level of linguistic diversity with minimal computational resources, making sentiment analysis somewhat challenging. Earlier, this was done using a traditional word embedding model, such as Word2Vec, but with the emergence of new language models, specifically BERT, there is hope for improvements. The better word embedding capabilities of BERT lend to this study, aiding in the capturing of the nuanced semantic pool and the contextual intricacies of the language under study, the Kurdish language, thus setting a new benchmark for sentiment analysis in low-resource languages.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "136",
        "title": "Benchmarking and Mitigating MCQA Selection Bias of Large Vision-Language Models",
        "author": [
            "Md. Atabuzzaman",
            "Ali Asgarov",
            "Chris Thomas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16805",
        "abstract": "Large Vision-Language Models (LVLMs) have achieved strong performance on vision-language tasks, particularly Visual Question Answering (VQA). While prior work has explored unimodal biases in VQA, the problem of selection bias in Multiple-Choice Question Answering (MCQA), where models may favor specific option tokens (e.g., \"A\") or positions, remains underexplored. In this paper, we investigate both the presence and nature of selection bias in LVLMs through fine-grained MCQA benchmarks spanning easy, medium, and hard difficulty levels, defined by the semantic similarity of the options. We further propose an inference-time logit-level debiasing method that estimates an ensemble bias vector from general and contextual prompts and applies confidence-adaptive corrections to the model's output. Our method mitigates bias without retraining and is compatible with frozen LVLMs. Extensive experiments across several state-of-the-art models reveal consistent selection biases that intensify with task difficulty, and show that our mitigation approach significantly reduces bias while improving accuracy in challenging settings. This work offers new insights into the limitations of LVLMs in MCQA and presents a practical approach to improve their robustness in fine-grained visual reasoning. Datasets and code are available at: https://github.com/Atabuzzaman/Selection-Bias-of-LVLMs",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "137",
        "title": "Automated Procedural Analysis via Video-Language Models for AI-assisted Nursing Skills Assessment",
        "author": [
            "Shen Chang",
            "Dennis Liu",
            "Renran Tian",
            "Kristen L. Swartzell",
            "Stacie L. Klingler",
            "Amy M. Nagle",
            "Nan Kong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16810",
        "abstract": "Consistent high-quality nursing care is essential for patient safety, yet current nursing education depends on subjective, time-intensive instructor feedback in training future nurses, which limits scalability and efficiency in their training, and thus hampers nursing competency when they enter the workforce. In this paper, we introduce a video-language model (VLM) based framework to develop the AI capability of automated procedural assessment and feedback for nursing skills training, with the potential of being integrated into existing training programs. Mimicking human skill acquisition, the framework follows a curriculum-inspired progression, advancing from high-level action recognition, fine-grained subaction decomposition, and ultimately to procedural reasoning. This design supports scalable evaluation by reducing instructor workload while preserving assessment quality. The system provides three core capabilities: 1) diagnosing errors by identifying missing or incorrect subactions in nursing skill instruction videos, 2) generating explainable feedback by clarifying why a step is out of order or omitted, and 3) enabling objective, consistent formative evaluation of procedures. Validation on synthesized videos demonstrates reliable error detection and temporal localization, confirming its potential to handle real-world training variability. By addressing workflow bottlenecks and supporting large-scale, standardized evaluation, this work advances AI applications in nursing education, contributing to stronger workforce development and ultimately safer patient care.",
        "tags": [
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "138",
        "title": "Prompt-Driven Agentic Video Editing System: Autonomous Comprehension of Long-Form, Story-Driven Media",
        "author": [
            "Zihan Ding",
            "Junlong Chen",
            "Per Ola Kristensson",
            "Junxiao Shen",
            "Xinyi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16811",
        "abstract": "Creators struggle to edit long-form, narrative-rich videos not because of UI complexity, but due to the cognitive demands of searching, storyboarding, and sequencing hours of footage. Existing transcript- or embedding-based methods fall short for creative workflows, as models struggle to track characters, infer motivations, and connect dispersed events. We present a prompt-driven, modular editing system that helps creators restructure multi-hour content through free-form prompts rather than timelines. At its core is a semantic indexing pipeline that builds a global narrative via temporal segmentation, guided memory compression, and cross-granularity fusion, producing interpretable traces of plot, dialogue, emotion, and context. Users receive cinematic edits while optionally refining transparent intermediate outputs. Evaluated on 400+ videos with expert ratings, QA, and preference studies, our system scales prompt-driven editing, preserves narrative coherence, and balances automation with creator control.",
        "tags": [
            "Segmentation",
            "Video Editing"
        ]
    },
    {
        "id": "139",
        "title": "Cognitive Linguistic Identity Fusion Score (CLIFS): A Scalable Cognition-Informed Approach to Quantifying Identity Fusion from Text",
        "author": [
            "Devin R. Wright",
            "Jisun An",
            "Yong-Yeol Ahn"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16813",
        "abstract": "Quantifying identity fusion -- the psychological merging of self with another entity or abstract target (e.g., a religious group, political party, ideology, value, brand, belief, etc.) -- is vital for understanding a wide range of group-based human behaviors. We introduce the Cognitive Linguistic Identity Fusion Score (CLIFS), a novel metric that integrates cognitive linguistics with large language models (LLMs), which builds on implicit metaphor detection. Unlike traditional pictorial and verbal scales, which require controlled surveys or direct field contact, CLIFS delivers fully automated, scalable assessments while maintaining strong alignment with the established verbal measure. In benchmarks, CLIFS outperforms both existing automated approaches and human annotation. As a proof of concept, we apply CLIFS to violence risk assessment to demonstrate that it can improve violence risk assessment by more than 240%. Building on our identification of a new NLP task and early success, we underscore the need to develop larger, more diverse datasets that encompass additional fusion-target domains and cultural backgrounds to enhance generalizability and further advance this emerging area. CLIFS models and code are public at https://github.com/DevinW-sudo/CLIFS.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "140",
        "title": "DISCO: Disentangled Communication Steering for Large Language Models",
        "author": [
            "Max Torop",
            "Aria Masoomi",
            "Masih Eskandar",
            "Jennifer Dy"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16820",
        "abstract": "A variety of recent methods guide large language model outputs via the inference-time addition of steering vectors to residual-stream or attention-head representations. In contrast, we propose to inject steering vectors directly into the query and value representation spaces within attention heads. We provide evidence that a greater portion of these spaces exhibit high linear discriminability of concepts --a key property motivating the use of steering vectors-- than attention head outputs. We analytically characterize the effect of our method, which we term DISentangled COmmunication (DISCO) Steering, on attention head outputs. Our analysis reveals that DISCO disentangles a strong but underutilized baseline, steering attention inputs, which implicitly modifies queries and values in a rigid manner. In contrast, DISCO's direct modulation of these components enables more granular control. We find that DISCO achieves superior performance over a number of steering vector baselines across multiple datasets on LLaMA 3.1 8B and Gemma 2 9B, with steering efficacy scoring up to 19.1% higher than the runner-up. Our results support the conclusion that the query and value spaces are powerful building blocks for steering vector methods.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "141",
        "title": "Factorizing Diffusion Policies for Observation Modality Prioritization",
        "author": [
            "Omkar Patil",
            "Prabin Rath",
            "Kartikay Pangaonkar",
            "Eric Rosen",
            "Nakul Gopalan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16830",
        "abstract": "Diffusion models have been extensively leveraged for learning robot skills from demonstrations. These policies are conditioned on several observational modalities such as proprioception, vision and tactile. However, observational modalities have varying levels of influence for different tasks that diffusion polices fail to capture. In this work, we propose 'Factorized Diffusion Policies' abbreviated as FDP, a novel policy formulation that enables observational modalities to have differing influence on the action diffusion process by design. This results in learning policies where certain observations modalities can be prioritized over the others such as $\\texttt{vision>tactile}$ or $\\texttt{proprioception>vision}$. FDP achieves modality prioritization by factorizing the observational conditioning for diffusion process, resulting in more performant and robust policies. Our factored approach shows strong performance improvements in low-data regimes with $15\\%$ absolute improvement in success rate on several simulated benchmarks when compared to a standard diffusion policy that jointly conditions on all input modalities. Moreover, our benchmark and real-world experiments show that factored policies are naturally more robust with $40\\%$ higher absolute success rate across several visuomotor tasks under distribution shifts such as visual distractors or camera occlusions, where existing diffusion policies fail catastrophically. FDP thus offers a safer and more robust alternative to standard diffusion policies for real-world deployment. Videos are available at https://fdp-policy.github.io/fdp-policy/ .",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "142",
        "title": "Semantic-Driven Topic Modeling for Analyzing Creativity in Virtual Brainstorming",
        "author": [
            "Melkamu Abay Mersha",
            "Jugal Kalita"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16835",
        "abstract": "Virtual brainstorming sessions have become a central component of collaborative problem solving, yet the large volume and uneven distribution of ideas often make it difficult to extract valuable insights efficiently. Manual coding of ideas is time-consuming and subjective, underscoring the need for automated approaches to support the evaluation of group creativity. In this study, we propose a semantic-driven topic modeling framework that integrates four modular components: transformer-based embeddings (Sentence-BERT), dimensionality reduction (UMAP), clustering (HDBSCAN), and topic extraction with refinement. The framework captures semantic similarity at the sentence level, enabling the discovery of coherent themes from brainstorming transcripts while filtering noise and identifying outliers. We evaluate our approach on structured Zoom brainstorming sessions involving student groups tasked with improving their university. Results demonstrate that our model achieves higher topic coherence compared to established methods such as LDA, ETM, and BERTopic, with an average coherence score of 0.687 (CV), outperforming baselines by a significant margin. Beyond improved performance, the model provides interpretable insights into the depth and diversity of topics explored, supporting both convergent and divergent dimensions of group creativity. This work highlights the potential of embedding-based topic modeling for analyzing collaborative ideation and contributes an efficient and scalable framework for studying creativity in synchronous virtual meetings.",
        "tags": [
            "BERT",
            "Transformer"
        ]
    },
    {
        "id": "143",
        "title": "Roundtable Policy: Improving Scientific Reasoning and Narratives through Confidence-Weighted Consensus of LLMs",
        "author": [
            "Yu Yao",
            "Jiayi Dong",
            "Ju Li",
            "Yang Yang",
            "Yilun Du"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16839",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities not only in language generation but also in advancing scientific discovery. A growing body of work has explored ways to improve their reasoning, from self-consistency and chain-of-thought to multi-agent debate. Inspired by the dynamics of scientific committees and the \"Society of Mind,\" we introduce Roundtable Policy, a complementary inference-time reasoning framework that performs inference through the weighted consensus of multiple LLMs. Our findings indicate that this approach significantly enhances reasoning in complex heterogeneous scientific tasks and improves scientific narratives in terms of creativity, rigor, and logical coherence, while reducing hallucinations that single models are prone to. Our approach emphasizes structured and interpretable consensus rather than opaque convergence, while requiring only black-box access and uniform procedures, making it broadly applicable to multi-LLM reasoning.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "144",
        "title": "ISCS: Parameter-Guided Channel Ordering and Grouping for Learned Image Compression",
        "author": [
            "Jinhao Wang",
            "Cihan Ruan",
            "Nam Ling",
            "Wei Wang",
            "Wei Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16853",
        "abstract": "Prior studies in learned image compression (LIC) consistently show that only a small subset of latent channels is critical for reconstruction, while many others carry limited information. Exploiting this imbalance could improve both coding and computational efficiency, yet existing approaches often rely on costly, dataset-specific ablation tests and typically analyze channels in isolation, ignoring their interdependencies.\nWe propose a generalizable, dataset-agnostic method to identify and organize important channels in pretrained VAE-based LIC models. Instead of brute-force empirical evaluations, our approach leverages intrinsic parameter statistics-weight variances, bias magnitudes, and pairwise correlations-to estimate channel importance. This analysis reveals a consistent organizational structure, termed the Invariant Salient Channel Space (ISCS), where Salient-Core channels capture dominant structures and Salient-Auxiliary channels provide complementary details. Building on ISCS, we introduce a deterministic channel ordering and grouping strategy that enables slice-parallel decoding, reduces redundancy, and improves bitrate efficiency.\nExperiments across multiple LIC architectures demonstrate that our method effectively reduces bitrate and computation while maintaining reconstruction quality, providing a practical and modular enhancement to existing learned compression frameworks.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "145",
        "title": "ShadowServe: Interference-Free KV Cache Fetching for Distributed Prefix Caching",
        "author": [
            "Xingyu Xiang",
            "Raj Joshi",
            "Yuhan Liu",
            "Jiayi Yao",
            "Chenxingyu Zhao",
            "Junchen Jiang",
            "Yang Zhou",
            "Eddie Kohler",
            "Minlan Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16857",
        "abstract": "Distributed prefix caching accelerates long-context LLM serving by reusing KV cache entries for common context prefixes. However, KV cache fetches can become a bottleneck when network bandwidth is limited. Compression mitigates the bandwidth issue, but can degrade overall performance when decompression interferes with model computation.\nWe present ShadowServe, the first SmartNIC-accelerated, interference-free prefix caching system for LLM serving. ShadowServe separates a control plane on the host and a data plane fully offloaded to the SmartNIC, which eliminates interference to both host GPU and CPU. To overcome the SmartNIC's limited compute and memory resources, we design a chunked pipeline that parallelizes data plane operations across the SmartNIC's compute resources, and a minimal-copy memory management scheme that reduces memory pressure on the SmartNIC. Compared to state-of-the-art solutions, ShadowServe achieves up to 2.2x lower loaded time-per-output-token (TPOT), and reduces time-to-first-token (TTFT) by up to 1.38x in low-bandwidth scenarios (<= 20 Gbps), translating to up to 1.35x higher throughput.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "146",
        "title": "Benchmarking Offline Reinforcement Learning for Emotion-Adaptive Social Robotics",
        "author": [
            "Soon Jynn Chu",
            "Raju Gottumukkala",
            "Alan Barhorst"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16858",
        "abstract": "The ability of social robots to respond to human emotions is crucial for building trust and acceptance in human-robot collaborative environments. However, developing such capabilities through online reinforcement learning is sometimes impractical due to the prohibitive cost of data collection and the risk of generating unsafe behaviors. In this paper, we study the use of offline reinforcement learning as a practical and efficient alternative. This technique uses pre-collected data to enable emotion-adaptive social robots. We present a system architecture that integrates multimodal sensing and recognition, decision-making, and adaptive responses. Using a limited dataset from a human-robot game-playing scenario, we establish a benchmark for comparing offline reinforcement learning algorithms that do not require an online environment. Our results show that BCQ and CQL are more robust to data sparsity, achieving higher state-action values compared to NFQ, DQN, and DDQN. This work establishes a foundation for benchmarking offline RL in emotion-adaptive robotics and informs future deployment in real-world HRI. Our findings provide empirical insight into the performance of offline reinforcement learning algorithms in data-constrained HRI. This work establishes a foundation for benchmarking offline RL in emotion-adaptive robotics and informs its future deployment in real-world HRI, such as in conversational agents, educational partners, and personal assistants, require reliable emotional responsiveness.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "147",
        "title": "The Principles of Human-like Conscious Machine",
        "author": [
            "Fangfang Li",
            "Xiaojie Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16859",
        "abstract": "Determining whether another system, biological or artificial, possesses phenomenal consciousness has long been a central challenge in consciousness studies. This attribution problem has become especially pressing with the rise of large language models and other advanced AI systems, where debates about \"AI consciousness\" implicitly rely on some criterion for deciding whether a given system is conscious. In this paper, we propose a substrate-independent, logically rigorous, and counterfeit-resistant sufficiency criterion for phenomenal consciousness. We argue that any machine satisfying this criterion should be regarded as conscious with at least the same level of confidence with which we attribute consciousness to other humans. Building on this criterion, we develop a formal framework and specify a set of operational principles that guide the design of systems capable of meeting the sufficiency condition. We further argue that machines engineered according to this framework can, in principle, realize phenomenal consciousness. As an initial validation, we show that humans themselves can be viewed as machines that satisfy this framework and its principles. If correct, this proposal carries significant implications for philosophy, cognitive science, and artificial intelligence. It offers an explanation for why certain qualia, such as the experience of red, are in principle irreducible to physical description, while simultaneously providing a general reinterpretation of human information processing. Moreover, it suggests a path toward a new paradigm of AI beyond current statistics-based approaches, potentially guiding the construction of genuinely human-like AI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "148",
        "title": "AdaptiveGuard: Towards Adaptive Runtime Safety for LLM-Powered Software",
        "author": [
            "Rui Yang",
            "Michael Fu",
            "Chakkrit Tantithamthavorn",
            "Chetan Arora",
            "Gunel Gulmammadova",
            "Joey Chua"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16861",
        "abstract": "Guardrails are critical for the safe deployment of Large Language Models (LLMs)-powered software. Unlike traditional rule-based systems with limited, predefined input-output spaces that inherently constrain unsafe behavior, LLMs enable open-ended, intelligent interactions--opening the door to jailbreak attacks through user inputs. Guardrails serve as a protective layer, filtering unsafe prompts before they reach the LLM. However, prior research shows that jailbreak attacks can still succeed over 70% of the time, even against advanced models like GPT-4o. While guardrails such as LlamaGuard report up to 95% accuracy, our preliminary analysis shows their performance can drop sharply--to as low as 12%--when confronted with unseen attacks. This highlights a growing software engineering challenge: how to build a post-deployment guardrail that adapts dynamically to emerging threats? To address this, we propose AdaptiveGuard, an adaptive guardrail that detects novel jailbreak attacks as out-of-distribution (OOD) inputs and learns to defend against them through a continual learning framework. Through empirical evaluation, AdaptiveGuard achieves 96% OOD detection accuracy, adapts to new attacks in just two update steps, and retains over 85% F1-score on in-distribution data post-adaptation, outperforming other baselines. These results demonstrate that AdaptiveGuard is a guardrail capable of evolving in response to emerging jailbreak strategies post deployment. We release our AdaptiveGuard and studied datasets at https://github.com/awsm-research/AdaptiveGuard to support further research.",
        "tags": [
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "149",
        "title": "Drum-to-Vocal Percussion Sound Conversion and Its Evaluation Methodology",
        "author": [
            "Rinka Nobukawa",
            "Makito Kitamura",
            "Tomohiko Nakamura",
            "Shinnosuke Takamichi",
            "Hiroshi Saruwatari"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16862",
        "abstract": "This paper defines the novel task of drum-to-vocal percussion (VP) sound conversion. VP imitates percussion instruments through human vocalization and is frequently employed in contemporary a cappella music. It exhibits acoustic properties distinct from speech and singing (e.g., aperiodicity, noisy transients, and the absence of linguistic structure), making conventional speech or singing synthesis methods unsuitable. We thus formulate VP synthesis as a timbre transfer problem from drum sounds, leveraging their rhythmic and timbral correspondence. To support this formulation, we define three requirements for successful conversion: rhythmic fidelity, timbral consistency, and naturalness as VP. We also propose corresponding subjective evaluation criteria. We implement two baseline conversion methods using a neural audio synthesizer, the real-time audio variational autoencoder (RAVE), with and without vector quantization (VQ). Subjective experiments show that both methods produce plausible VP outputs, with the VQ-based RAVE model yielding more consistent conversion.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "150",
        "title": "ConfidentSplat: Confidence-Weighted Depth Fusion for Accurate 3D Gaussian Splatting SLAM",
        "author": [
            "Amanuel T. Dufera",
            "Yuan-Li Cai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16863",
        "abstract": "We introduce ConfidentSplat, a novel 3D Gaussian Splatting (3DGS)-based SLAM system for robust, highfidelity RGB-only reconstruction. Addressing geometric inaccuracies in existing RGB-only 3DGS SLAM methods that stem from unreliable depth estimation, ConfidentSplat incorporates a core innovation: a confidence-weighted fusion mechanism. This mechanism adaptively integrates depth cues from multiview geometry with learned monocular priors (Omnidata ViT), dynamically weighting their contributions based on explicit reliability estimates-derived predominantly from multi-view geometric consistency-to generate high-fidelity proxy depth for map supervision. The resulting proxy depth guides the optimization of a deformable 3DGS map, which efficiently adapts online to maintain global consistency following pose updates from a DROID-SLAM-inspired frontend and backend optimizations (loop closure, global bundle adjustment). Extensive validation on standard benchmarks (TUM-RGBD, ScanNet) and diverse custom mobile datasets demonstrates significant improvements in reconstruction accuracy (L1 depth error) and novel view synthesis fidelity (PSNR, SSIM, LPIPS) over baselines, particularly in challenging conditions. ConfidentSplat underscores the efficacy of principled, confidence-aware sensor fusion for advancing state-of-the-art dense visual SLAM.",
        "tags": [
            "3D",
            "Depth Estimation",
            "Gaussian Splatting",
            "SLAM",
            "ViT"
        ]
    },
    {
        "id": "151",
        "title": "Large Language Models as End-to-end Combinatorial Optimization Solvers",
        "author": [
            "Xia Jiang",
            "Yaoxin Wu",
            "Minshuo Li",
            "Zhiguang Cao",
            "Yingqian Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16865",
        "abstract": "Combinatorial optimization (CO) problems, central to decision-making scenarios like logistics and manufacturing, are traditionally solved using problem-specific algorithms requiring significant domain expertise. While large language models (LLMs) have shown promise in automating CO problem solving, existing approaches rely on intermediate steps such as code generation or solver invocation, limiting their generality and accessibility. This paper introduces a novel framework that empowers LLMs to serve as end-to-end CO solvers by directly mapping natural language problem descriptions to solutions. We propose a two-stage training strategy: supervised fine-tuning (SFT) imparts LLMs with solution generation patterns from domain-specific solvers, while a feasibility-and-optimality-aware reinforcement learning (FOARL) process explicitly mitigates constraint violations and refines solution quality. Evaluation across seven NP-hard CO problems shows that our method achieves a high feasibility rate and reduces the average optimality gap to 1.03-8.20% by tuning a 7B-parameter LLM, surpassing both general-purpose LLMs (e.g., GPT-4o), reasoning models (e.g., DeepSeek-R1), and domain-specific heuristics. Our method establishes a unified language-based pipeline for CO without extensive code execution or manual architectural adjustments for different problems, offering a general and language-driven alternative to traditional solver design while maintaining relative feasibility guarantees.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "152",
        "title": "seqBench: A Tunable Benchmark to Quantify Sequential Reasoning Limits of LLMs",
        "author": [
            "Mohammad Ramezanali",
            "Mo Vazifeh",
            "Paolo Santi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16866",
        "abstract": "We introduce seqBench, a parametrized benchmark for probing sequential reasoning limits in Large Language Models (LLMs) through precise, multi-dimensional control over several key complexity dimensions. seqBench allows systematic variation of (1) the logical depth, defined as the number of sequential actions required to solve the task; (2) the number of backtracking steps along the optimal path, quantifying how often the agent must revisit prior states to satisfy deferred preconditions (e.g., retrieving a key after encountering a locked door); and (3) the noise ratio, defined as the ratio between supporting and distracting facts about the environment. Our evaluations on state-of-the-art LLMs reveal a universal failure pattern: accuracy collapses exponentially beyond a model-specific logical depth. Unlike existing benchmarks, seqBench's fine-grained control facilitates targeted analyses of these reasoning failures, illuminating universal scaling laws and statistical limits, as detailed in this paper alongside its generation methodology and evaluation metrics. We find that even top-performing models systematically fail on seqBench's structured reasoning tasks despite minimal search complexity, underscoring key limitations in their commonsense reasoning capabilities. Designed for future evolution to keep pace with advancing models, the seqBench datasets are publicly released to spur deeper scientific inquiry into LLM reasoning, aiming to establish a clearer understanding of their true potential and current boundaries for robust real-world application.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "153",
        "title": "PhysHDR: When Lighting Meets Materials and Scene Geometry in HDR Reconstruction",
        "author": [
            "Hrishav Bakul Barua",
            "Kalin Stefanov",
            "Ganesh Krishnasamy",
            "KokSheik Wong",
            "Abhinav Dhall"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16869",
        "abstract": "Low Dynamic Range (LDR) to High Dynamic Range (HDR) image translation is a fundamental task in many computational vision problems. Numerous data-driven methods have been proposed to address this problem; however, they lack explicit modeling of illumination, lighting, and scene geometry in images. This limits the quality of the reconstructed HDR images. Since lighting and shadows interact differently with different materials, (e.g., specular surfaces such as glass and metal, and lambertian or diffuse surfaces such as wood and stone), modeling material-specific properties (e.g., specular and diffuse reflectance) has the potential to improve the quality of HDR image reconstruction. This paper presents PhysHDR, a simple yet powerful latent diffusion-based generative model for HDR image reconstruction. The denoising process is conditioned on lighting and depth information and guided by a novel loss to incorporate material properties of surfaces in the scene. The experimental results establish the efficacy of PhysHDR in comparison to a number of recent state-of-the-art methods.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "154",
        "title": "DecipherGuard: Understanding and Deciphering Jailbreak Prompts for a Safer Deployment of Intelligent Software Systems",
        "author": [
            "Rui Yang",
            "Michael Fu",
            "Chakkrit Tantithamthavorn",
            "Chetan Arora",
            "Gunel Gulmammadova",
            "Joey Chua"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16870",
        "abstract": "Intelligent software systems powered by Large Language Models (LLMs) are increasingly deployed in critical sectors, raising concerns about their safety during runtime. Through an industry-academic collaboration when deploying an LLM-powered virtual customer assistant, a critical software engineering challenge emerged: how to enhance a safer deployment of LLM-powered software systems at runtime? While LlamaGuard, the current state-of-the-art runtime guardrail, offers protection against unsafe inputs, our study reveals a Defense Success Rate (DSR) drop of 24% under obfuscation- and template-based jailbreak attacks. In this paper, we propose DecipherGuard, a novel framework that integrates a deciphering layer to counter obfuscation-based prompts and a low-rank adaptation mechanism to enhance guardrail effectiveness against template-based attacks. Empirical evaluation on over 22,000 prompts demonstrates that DecipherGuard improves DSR by 36% to 65% and Overall Guardrail Performance (OGP) by 20% to 50% compared to LlamaGuard and two other runtime guardrails. These results highlight the effectiveness of DecipherGuard in defending LLM-powered software systems against jailbreak attacks during runtime.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "155",
        "title": "HOGraspFlow: Exploring Vision-based Generative Grasp Synthesis with Hand-Object Priors and Taxonomy Awareness",
        "author": [
            "Yitian Shi",
            "Zicheng Guo",
            "Rosa Wolf",
            "Edgar Welte",
            "Rania Rayyes"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16871",
        "abstract": "We propose Hand-Object\\emph{(HO)GraspFlow}, an affordance-centric approach that retargets a single RGB with hand-object interaction (HOI) into multi-modal executable parallel jaw grasps without explicit geometric priors on target objects. Building on foundation models for hand reconstruction and vision, we synthesize $SE(3)$ grasp poses with denoising flow matching (FM), conditioned on the following three complementary cues: RGB foundation features as visual semantics, HOI contact reconstruction, and taxonomy-aware prior on grasp types. Our approach demonstrates high fidelity in grasp synthesis without explicit HOI contact input or object geometry, while maintaining strong contact and taxonomy recognition. Another controlled comparison shows that \\emph{HOGraspFlow} consistently outperforms diffusion-based variants (\\emph{HOGraspDiff}), achieving high distributional fidelity and more stable optimization in $SE(3)$. We demonstrate a reliable, object-agnostic grasp synthesis from human demonstrations in real-world experiments, where an average success rate of over $83\\%$ is achieved.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "156",
        "title": "$\\mathtt{M^3VIR}$: A Large-Scale Multi-Modality Multi-View Synthesized Benchmark Dataset for Image Restoration and Content Creation",
        "author": [
            "Yuanzhi Li",
            "Lebin Zhou",
            "Nam Ling",
            "Zhenghao Chen",
            "Wei Wang",
            "Wei Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16873",
        "abstract": "The gaming and entertainment industry is rapidly evolving, driven by immersive experiences and the integration of generative AI (GAI) technologies. Training such models effectively requires large-scale datasets that capture the diversity and context of gaming environments. However, existing datasets are often limited to specific domains or rely on artificial degradations, which do not accurately capture the unique characteristics of gaming content. Moreover, benchmarks for controllable video generation remain absent.\nTo address these limitations, we introduce $\\mathtt{M^3VIR}$, a large-scale, multi-modal, multi-view dataset specifically designed to overcome the shortcomings of current resources. Unlike existing datasets, $\\mathtt{M^3VIR}$ provides diverse, high-fidelity gaming content rendered with Unreal Engine 5, offering authentic ground-truth LR-HR paired and multi-view frames across 80 scenes in 8 categories. It includes $\\mathtt{M^3VIR\\_MR}$ for super-resolution (SR), novel view synthesis (NVS), and combined NVS+SR tasks, and $\\mathtt{M^3VIR\\_{MS}}$, the first multi-style, object-level ground-truth set enabling research on controlled video generation. Additionally, we benchmark several state-of-the-art SR and NVS methods to establish performance baselines. While no existing approaches directly handle controlled video generation, $\\mathtt{M^3VIR}$ provides a benchmark for advancing this area. By releasing the dataset, we aim to facilitate research in AI-powered restoration, compression, and controllable content generation for next-generation cloud gaming and entertainment.",
        "tags": [
            "Super Resolution",
            "Video Generation"
        ]
    },
    {
        "id": "157",
        "title": "Dynamic Expert Specialization: Towards Catastrophic Forgetting-Free Multi-Domain MoE Adaptation",
        "author": [
            "Junzhuo Li",
            "Bo Wang",
            "Xiuze Zhou",
            "Xuming Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16882",
        "abstract": "Mixture-of-Experts (MoE) models offer immense capacity via sparsely gated expert subnetworks, yet adapting them to multiple domains without catastrophic forgetting remains an open challenge. Existing approaches either incur prohibitive computation, suffer cross-domain interference, or require separate runs per domain. We propose DES-MoE, a dynamic expert specialization framework for multi-domain adaptation of Mixture-of-Experts models. DES-MoE addresses catastrophic forgetting through three innovations: (1) an adaptive router balancing pre-trained knowledge retention and task-specific updates via distillation, (2) real-time expert-domain correlation mapping to isolate domain-specific gradients, and (3) a three-phase adaptive fine-tuning schedule that progressively freezes non-specialized parameters. Evaluated on six domains (math, code, law, etc.), DES-MoE matches single-domain ESFT performance while training one unified model, reduces forgetting by 89% compared to full fine-tuning as domains scale from 2 to 6, and achieves 68% faster convergence than conventional methods. Our work establishes dynamic expert isolation as a scalable paradigm for multi-task MoE adaptation.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "158",
        "title": "LLMs as Layout Designers: A Spatial Reasoning Perspective",
        "author": [
            "Sha Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16891",
        "abstract": "While Large Language Models (LLMs) have demonstrated impressive reasoning and planning abilities in textual domains and can effectively follow instructions for complex tasks, their capacity for spatial understanding and reasoning remains limited. Such capabilities, however, are critical for applications like content-aware graphic layout design, which demands precise placement, alignment, and structural organization of multiple elements within constrained visual spaces. To address this gap, we propose LaySPA, a reinforcement learning-based framework that augments LLM agents with explicit spatial reasoning capabilities. LaySPA leverages hybrid reward signals that capture geometric validity, structural fidelity, and visual quality, enabling agents to model inter-element relationships, navigate the canvas, and optimize spatial arrangements. Through iterative self-exploration and adaptive policy optimization, LaySPA produces both interpretable reasoning traces and structured layouts. Experimental results demonstrate that LaySPA generates structurally sound and visually appealing layouts, outperforming larger general-purpose LLMs and achieving results on par with state-of-the-art specialized layout models.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "159",
        "title": "Temporal-Aware User Behaviour Simulation with Large Language Models for Recommender Systems",
        "author": [
            "Xinye Wanyan",
            "Danula Hettiachchi",
            "Chenglong Ma",
            "Ziqi Xu",
            "Jeffrey Chan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16895",
        "abstract": "Large Language Models (LLMs) demonstrate human-like capabilities in language understanding, reasoning, and generation, driving interest in using LLM-based agents to simulate human feedback in recommender systems. However, most existing approaches rely on static user profiling, neglecting the temporal and dynamic nature of user interests. This limitation stems from a disconnect between language modelling and behaviour modelling, which constrains the capacity of agents to represent sequential patterns. To address this challenge, we propose a Dynamic Temporal-aware Agent-based simulator for Recommender Systems, DyTA4Rec, which enables agents to model and utilise evolving user behaviour based on historical interactions. DyTA4Rec features a dynamic updater for real-time profile refinement, temporal-enhanced prompting for sequential context, and self-adaptive aggregation for coherent feedback. Experimental results at group and individual levels show that DyTA4Rec significantly improves the alignment between simulated and actual user behaviour by modelling dynamic characteristics and enhancing temporal awareness in LLM-based agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "160",
        "title": "PRISM: Precision-Recall Informed Data-Free Knowledge Distillation via Generative Diffusion",
        "author": [
            "Xuewan He",
            "Jielei Wang",
            "Zihan Cheng",
            "Yuchen Su",
            "Shiyue Huang",
            "Guoming Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16897",
        "abstract": "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access to the real in-distribution (ID) data. While existing methods perform well on small-scale images, they suffer from mode collapse when synthesizing large-scale images, resulting in limited knowledge transfer. Recently, leveraging advanced generative models to synthesize photorealistic images has emerged as a promising alternative. Nevertheless, directly using off-the-shelf diffusion to generate datasets faces the precision-recall challenges: 1) ensuring synthetic data aligns with the real distribution, and 2) ensuring coverage of the real ID manifold. In response, we propose PRISM, a precision-recall informed synthesis method. Specifically, we introduce Energy-guided Distribution Alignment to avoid the generation of out-of-distribution samples, and design the Diversified Prompt Engineering to enhance coverage of the real ID manifold. Extensive experiments on various large-scale image datasets demonstrate the superiority of PRISM. Moreover, we demonstrate that models trained with PRISM exhibit strong domain generalization.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "161",
        "title": "CLaC at DISRPT 2025: Hierarchical Adapters for Cross-Framework Multi-lingual Discourse Relation Classification",
        "author": [
            "Nawar Turk",
            "Daniele Comitogianni",
            "Leila Kosseim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16903",
        "abstract": "We present our submission to Task 3 (Discourse Relation Classification) of the DISRPT 2025 shared task. Task 3 introduces a unified set of 17 discourse relation labels across 39 corpora in 16 languages and six discourse frameworks, posing significant multilingual and cross-formalism challenges. We first benchmark the task by fine-tuning multilingual BERT-based models (mBERT, XLM-RoBERTa-Base, and XLM-RoBERTa-Large) with two argument-ordering strategies and progressive unfreezing ratios to establish strong baselines. We then evaluate prompt-based large language models (namely Claude Opus 4.0) in zero-shot and few-shot settings to understand how LLMs respond to the newly proposed unified labels. Finally, we introduce HiDAC, a Hierarchical Dual-Adapter Contrastive learning model. Results show that while larger transformer models achieve higher accuracy, the improvements are modest, and that unfreezing the top 75% of encoder layers yields performance comparable to full fine-tuning while training far fewer parameters. Prompt-based models lag significantly behind fine-tuned transformers, and HiDAC achieves the highest overall accuracy (67.5%) while remaining more parameter-efficient than full fine-tuning.",
        "tags": [
            "BERT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "162",
        "title": "SLAM-Former: Putting SLAM into One Transformer",
        "author": [
            "Yijun Yuan",
            "Zhuoguang Chen",
            "Kenan Li",
            "Weibang Wang",
            "Hang Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16909",
        "abstract": "We present SLAM-Former, a novel neural approach that integrates full SLAM capabilities into a single transformer. Similar to traditional SLAM systems, SLAM-Former comprises both a frontend and a backend that operate in tandem. The frontend processes sequential monocular images in real-time for incremental mapping and tracking, while the backend performs global refinement to ensure a geometrically consistent result. This alternating execution allows the frontend and backend to mutually promote one another, enhancing overall system performance. Comprehensive experimental results demonstrate that SLAM-Former achieves superior or highly competitive performance compared to state-of-the-art dense SLAM methods.",
        "tags": [
            "SLAM",
            "Transformer"
        ]
    },
    {
        "id": "163",
        "title": "CUTE: A Multilingual Dataset for Enhancing Cross-Lingual Knowledge Transfer in Low-Resource Languages",
        "author": [
            "Wenhao Zhuang",
            "Yuan Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16914",
        "abstract": "Large Language Models (LLMs) demonstrate exceptional zero-shot capabilities in various NLP tasks, significantly enhancing user experience and efficiency. However, this advantage is primarily limited to resource-rich languages. For the diverse array of low-resource languages, support remains inadequate, with the scarcity of training corpora considered the primary cause. We construct and open-source CUTE Chinese, Uyghur, Tibetan,English dataset, consisting of two 25GB sets of four-language corpora (one parallel and one non-parallel), obtained through machine translation. CUTE encompasses two resource-rich languages (Chinese and English) and two low-resource languages (Uyghur and Tibetan). Prior to constructing CUTE, human assessment validates that the machine translation quality between Chinese-Uyghur and Chinese-Tibetan approaches that of Chinese-English translation. CUTE represents the largest open-source corpus for Uyghur and Tibetan languages to date, and we demonstrate its effectiveness in enhancing LLMs' ability to process low-resource languages while investigating the role of corpus parallelism in cross-lingual transfer learning. The CUTE corpus and related models are made publicly available to the research community.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "164",
        "title": "SwarmChat: An LLM-Based, Context-Aware Multimodal Interaction System for Robotic Swarms",
        "author": [
            "Ettilla Mohiuddin Eumi",
            "Hussein Abbass",
            "Nadine Marcus"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16920",
        "abstract": "Traditional Human-Swarm Interaction (HSI) methods often lack intuitive real-time adaptive interfaces, making decision making slower and increasing cognitive load while limiting command flexibility. To solve this, we present SwarmChat, a context-aware, multimodal interaction system powered by Large Language Models (LLMs). SwarmChat enables users to issue natural language commands to robotic swarms using multiple modalities, such as text, voice, or teleoperation. The system integrates four LLM-based modules: Context Generator, Intent Recognition, Task Planner, and Modality Selector. These modules collaboratively generate context from keywords, detect user intent, adapt commands based on real-time robot state, and suggest optimal communication modalities. Its three-layer architecture offers a dynamic interface with both fixed and customizable command options, supporting flexible control while optimizing cognitive effort. The preliminary evaluation also shows that the SwarmChat's LLM modules provide accurate context interpretation, relevant intent recognition, and effective command delivery, achieving high user satisfaction.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "165",
        "title": "PGSTalker: Real-Time Audio-Driven Talking Head Generation via 3D Gaussian Splatting with Pixel-Aware Density Control",
        "author": [
            "Tianheng Zhu",
            "Yinfeng Yu",
            "Liejun Wang",
            "Fuchun Sun",
            "Wendong Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16922",
        "abstract": "Audio-driven talking head generation is crucial for applications in virtual reality, digital avatars, and film production. While NeRF-based methods enable high-fidelity reconstruction, they suffer from low rendering efficiency and suboptimal audio-visual synchronization. This work presents PGSTalker, a real-time audio-driven talking head synthesis framework based on 3D Gaussian Splatting (3DGS). To improve rendering performance, we propose a pixel-aware density control strategy that adaptively allocates point density, enhancing detail in dynamic facial regions while reducing redundancy elsewhere. Additionally, we introduce a lightweight Multimodal Gated Fusion Module to effectively fuse audio and spatial features, thereby improving the accuracy of Gaussian deformation prediction. Extensive experiments on public datasets demonstrate that PGSTalker outperforms existing NeRF- and 3DGS-based approaches in rendering quality, lip-sync precision, and inference speed. Our method exhibits strong generalization capabilities and practical potential for real-world deployment.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF",
            "Talking Head"
        ]
    },
    {
        "id": "166",
        "title": "Audio-Guided Dynamic Modality Fusion with Stereo-Aware Attention for Audio-Visual Navigation",
        "author": [
            "Jia Li",
            "Yinfeng Yu",
            "Liejun Wang",
            "Fuchun Sun",
            "Wendong Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16924",
        "abstract": "In audio-visual navigation (AVN) tasks, an embodied agent must autonomously localize a sound source in unknown and complex 3D environments based on audio-visual signals. Existing methods often rely on static modality fusion strategies and neglect the spatial cues embedded in stereo audio, leading to performance degradation in cluttered or occluded scenes. To address these issues, we propose an end-to-end reinforcement learning-based AVN framework with two key innovations: (1) a \\textbf{S}tereo-Aware \\textbf{A}ttention \\textbf{M}odule (\\textbf{SAM}), which learns and exploits the spatial disparity between left and right audio channels to enhance directional sound perception; and (2) an \\textbf{A}udio-\\textbf{G}uided \\textbf{D}ynamic \\textbf{F}usion Module (\\textbf{AGDF}), which dynamically adjusts the fusion ratio between visual and auditory features based on audio cues, thereby improving robustness to environmental changes. Extensive experiments are conducted on two realistic 3D scene datasets, Replica and Matterport3D, demonstrating that our method significantly outperforms existing approaches in terms of navigation success rate and path efficiency. Notably, our model achieves over 40\\% improvement under audio-only conditions compared to the best-performing baselines. These results highlight the importance of explicitly modeling spatial cues from stereo channels and performing deep multi-modal fusion for robust and efficient audio-visual navigation.",
        "tags": [
            "3D",
            "RL",
            "SAM"
        ]
    },
    {
        "id": "167",
        "title": "K-DeCore: Facilitating Knowledge Transfer in Continual Structured Knowledge Reasoning via Knowledge Decoupling",
        "author": [
            "Yongrui Chen",
            "Yi Huang",
            "Yunchang Liu",
            "Shenyu Zhang",
            "Junhao He",
            "Tongtong Wu",
            "Guilin Qi",
            "Tianxing Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16929",
        "abstract": "Continual Structured Knowledge Reasoning (CSKR) focuses on training models to handle sequential tasks, where each task involves translating natural language questions into structured queries grounded in structured knowledge. Existing general continual learning approaches face significant challenges when applied to this task, including poor generalization to heterogeneous structured knowledge and inefficient reasoning due to parameter growth as tasks increase. To address these limitations, we propose a novel CSKR framework, \\textsc{K-DeCore}, which operates with a fixed number of tunable parameters. Unlike prior methods, \\textsc{K-DeCore} introduces a knowledge decoupling mechanism that disentangles the reasoning process into task-specific and task-agnostic stages, effectively bridging the gaps across diverse tasks. Building on this foundation, \\textsc{K-DeCore} integrates a dual-perspective memory consolidation mechanism for distinct stages and introduces a structure-guided pseudo-data synthesis strategy to further enhance the model's generalization capabilities. Extensive experiments on four benchmark datasets demonstrate the superiority of \\textsc{K-DeCore} over existing continual learning methods across multiple metrics, leveraging various backbone large language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "168",
        "title": "NeuFACO: Neural Focused Ant Colony Optimization for Traveling Salesman Problem",
        "author": [
            "Tran Thanh Dat",
            "Tran Quang Khai",
            "Pham Anh Khoi",
            "Vu Van Khu",
            "Do Duc Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16938",
        "abstract": "This study presents Neural Focused Ant Colony Optimization (NeuFACO), a non-autoregressive framework for the Traveling Salesman Problem (TSP) that combines advanced reinforcement learning with enhanced Ant Colony Optimization (ACO). NeuFACO employs Proximal Policy Optimization (PPO) with entropy regularization to train a graph neural network for instance-specific heuristic guidance, which is integrated into an optimized ACO framework featuring candidate lists, restricted tour refinement, and scalable local search. By leveraging amortized inference alongside ACO stochastic exploration, NeuFACO efficiently produces high-quality solutions across diverse TSP instances.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "169",
        "title": "SWE-Bench Pro: Can AI Agents Solve Long-Horizon Software Engineering Tasks?",
        "author": [
            "Xiang Deng",
            "Jeff Da",
            "Edwin Pan",
            "Yannis Yiming He",
            "Charles Ide",
            "Kanak Garg",
            "Niklas Lauffer",
            "Andrew Park",
            "Nitin Pasari",
            "Chetan Rane",
            "Karmini Sampath",
            "Maya Krishnan",
            "Srivatsa Kundurthy",
            "Sean Hendryx",
            "Zifan Wang",
            "Chen Bo Calvin Zhang",
            "Noah Jacobson",
            "Bing Liu",
            "Brad Kenstler"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16941",
        "abstract": "We introduce SWE-Bench Pro, a substantially more challenging benchmark that builds upon the best practices of SWE-BENCH [25], but is explicitly designed to capture realistic, complex, enterprise-level problems beyond the scope of SWE-BENCH. SWE-BENCH PRO contains 1,865 problems sourced from a diverse set of 41 actively maintained repositories spanning business applications, B2B services, and developer tools. The benchmark is partitioned into a public set with open access to problems sourced from 11 repositories, a held-out set of 12 repositories and a commercial set of 18 proprietary repositories where we have formal partnership agreements with early-stage startups. Problems in the held-out and the commercial set are not publicly accessible, but we release results on the commercial set. Our benchmark features long-horizon tasks that may require hours to days for a professional software engineer to complete, often involving patches across multiple files and substantial code modifications. All tasks are human-verified and augmented with sufficient context to ensure resolvability. In our evaluation of widely used coding models, under a unified scaffold, we observe that their performance on SWE-Bench PRO remains below 25% (Pass@1), with GPT-5 achieving the highest score to date at 23.3%. To better understand these limitations, we cluster the failure modes observed in the collected agent trajectories for a clearer characterization of the error patterns exhibited by current models. Overall, SWE-BENCH PRO provides a contamination-resistant testbed that more faithfully captures the complexity and diversity of real-world software development, advancing the pursuit of truly autonomous software engineering agents at a professional level.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "170",
        "title": "Catching the Details: Self-Distilled RoI Predictors for Fine-Grained MLLM Perception",
        "author": [
            "Yuheng Shi",
            "Xiaohuan Pei",
            "Minjing Dong",
            "Chang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16944",
        "abstract": "Multimodal Large Language Models (MLLMs) require high-resolution visual information to perform fine-grained perception, yet processing entire high-resolution images is computationally prohibitive. While recent methods leverage a Region-of-Interest (RoI) mechanism to focus on salient areas, they typically present a difficult trade-off: training-based approaches depend on large-scale annotated datasets, while training-free methods that utilize the model's internal attention are computationally inefficient and less accurate, requiring either multi-pass prefill stages or reliance on the slow auto-regressive decoding process. In this paper, we propose an efficient, annotation-free Self-Distilled Region Proposal Network (SD-RPN) that resolves this trade-off. The SD-RPN is built around a pipeline that transforms the noisy attention maps from the MLLM's middle layers into high-quality pseudo-RoI labels by explicitly denoising the signal and resolving ambiguity. We use these labels to train a lightweight Region Proposal Network (RPN) that learns a more precise localization. This RPN is also highly efficient, predicting the RoI in a single forward pass using features from the MLLM's middle layers, decoupling RoI identification from the auto-regressive generation and avoiding costly multi-pass http://operations.To validate our approach, we integrate the framework into the LLaVA-1.5 architecture. Despite being trained on only a few (e.g. 10K) question-answer pairs, our method demonstrates exceptional data efficiency and generalization, achieving over a 10% absolute accuracy improvement on unseen benchmarks, including TextVQA, DocVQA, and V-Star. Our work presents a practical and scalable solution for enhancing the fine-grained perception of MLLMs without requiring costly supervision or full model fine-tuning. Code is available at https://github.com/YuHengsss/SD-RPN.",
        "tags": [
            "LLM",
            "LLaVA"
        ]
    },
    {
        "id": "171",
        "title": "AirQA: A Comprehensive QA Dataset for AI Research with Instance-Level Evaluation",
        "author": [
            "Tiancheng Huang",
            "Ruisheng Cao",
            "Yuxin Zhang",
            "Zhangyi Kang",
            "Zijian Wang",
            "Chenrun Wang",
            "Yijie Luo",
            "Hang Zheng",
            "Lirong Qian",
            "Lu Chen",
            "Kai Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16952",
        "abstract": "The growing volume of academic papers has made it increasingly difficult for researchers to efficiently extract key information. While large language models (LLMs) based agents are capable of automating question answering (QA) workflows for scientific papers, there still lacks a comprehensive and realistic benchmark to evaluate their capabilities. Moreover, training an interactive agent for this specific task is hindered by the shortage of high-quality interaction trajectories. In this work, we propose AirQA, a human-annotated comprehensive paper QA dataset in the field of artificial intelligence (AI), with 13,948 papers and 1,246 questions, that encompasses multi-task, multi-modal and instance-level evaluation. Furthermore, we propose ExTrActor, an automated framework for instruction data synthesis. With three LLM-based agents, ExTrActor can perform example generation and trajectory collection without human intervention. Evaluations of multiple open-source and proprietary models show that most models underperform on AirQA, demonstrating the quality of our dataset. Extensive experiments confirm that ExTrActor consistently improves the multi-turn tool-use capability of small models, enabling them to achieve performance comparable to larger ones.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "172",
        "title": "VidCLearn: A Continual Learning Approach for Text-to-Video Generation",
        "author": [
            "Luca Zanchetta",
            "Lorenzo Papa",
            "Luca Maiano",
            "Irene Amerini"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16956",
        "abstract": "Text-to-video generation is an emerging field in generative AI, enabling the creation of realistic, semantically accurate videos from text prompts. While current models achieve impressive visual quality and alignment with input text, they typically rely on static knowledge, making it difficult to incorporate new data without retraining from scratch. To address this limitation, we propose VidCLearn, a continual learning framework for diffusion-based text-to-video generation. VidCLearn features a student-teacher architecture where the student model is incrementally updated with new text-video pairs, and the teacher model helps preserve previously learned knowledge through generative replay. Additionally, we introduce a novel temporal consistency loss to enhance motion smoothness and a video retrieval module to provide structural guidance at inference. Our architecture is also designed to be more computationally efficient than existing models while retaining satisfactory generation performance. Experimental results show VidCLearn's superiority over baseline methods in terms of visual quality, semantic alignment, and temporal coherence.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "173",
        "title": "A Reliable Robot Motion Planner in Complex Real-world Environments via Action Imagination",
        "author": [
            "Chengjin Wang",
            "Yanmin Zhou",
            "Zhipeng Wang",
            "Zheng Yan",
            "Feng Luan",
            "Shuo Jiang",
            "Runjie Shen",
            "Hongrui Sang",
            "Bin He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16963",
        "abstract": "Humans and animals can make real-time adjustments to movements by imagining their action outcomes to prevent unanticipated or even catastrophic motion failures in unknown unstructured environments. Action imagination, as a refined sensorimotor strategy, leverages perception-action loops to handle physical interaction-induced uncertainties in perception and system modeling within complex systems. Inspired by the action-awareness capability of animal intelligence, this study proposes an imagination-inspired motion planner (I-MP) framework that specifically enhances robots' action reliability by imagining plausible spatial states for approaching. After topologizing the workspace, I-MP build perception-action loop enabling robots autonomously build contact models. Leveraging fixed-point theory and Hausdorff distance, the planner computes convergent spatial states under interaction characteristics and mission constraints. By homogenously representing multi-dimensional environmental characteristics through work, the robot can approach the imagined spatial states via real-time computation of energy gradients. Consequently, experimental results demonstrate the practicality and robustness of I-MP in complex cluttered environments.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "174",
        "title": "Preference Distillation via Value based Reinforcement Learning",
        "author": [
            "Minchan Kwon",
            "Junwon Ko",
            "Kangil Kim",
            "Junmo Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16965",
        "abstract": "Direct Preference Optimization (DPO) is a powerful paradigm to align language models with human preferences using pairwise comparisons. However, its binary win-or-loss supervision often proves insufficient for training small models with limited capacity. Prior works attempt to distill information from large teacher models using behavior cloning or KL divergence. These methods often focus on mimicking current behavior and overlook distilling reward modeling. To address this issue, we propose \\textit{Teacher Value-based Knowledge Distillation} (TVKD), which introduces an auxiliary reward from the value function of the teacher model to provide a soft guide. This auxiliary reward is formulated to satisfy potential-based reward shaping, ensuring that the global reward structure and optimal policy of DPO are preserved. TVKD can be integrated into the standard DPO training framework and does not require additional rollouts. Our experimental results show that TVKD consistently improves performance across various benchmarks and model sizes.",
        "tags": [
            "DPO",
            "RL"
        ]
    },
    {
        "id": "175",
        "title": "Penalizing Boundary Activation for Object Completeness in Diffusion Models",
        "author": [
            "Haoyang Xu",
            "Tianhao Zhao",
            "Sibei Yang",
            "Yutian Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16968",
        "abstract": "Diffusion models have emerged as a powerful technique for text-to-image (T2I) generation, creating high-quality, diverse images across various domains. However, a common limitation in these models is the incomplete display of objects, where fragments or missing parts undermine the model's performance in downstream applications. In this study, we conduct an in-depth analysis of the incompleteness issue and reveal that the primary factor behind incomplete object generation is the usage of RandomCrop during model training. This widely used data augmentation method, though enhances model generalization ability, disrupts object continuity during training. To address this, we propose a training-free solution that penalizes activation values at image boundaries during the early denoising steps. Our method is easily applicable to pre-trained Stable Diffusion models with minimal modifications and negligible computational overhead. Extensive experiments demonstrate the effectiveness of our method, showing substantial improvements in object integrity and image quality.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "176",
        "title": "AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for Coarse-to-Fine Audio Deep Reasoning",
        "author": [
            "Yan Rong",
            "Chenxing Li",
            "Dong Yu",
            "Li Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16971",
        "abstract": "Audio deep reasoning is a challenging task that requires expert-level perception, multi-step logical inference, and the integration of contextual knowledge. However, existing models suffer from a gap between audio perception and reasoning abilities due to the lack of training data with explicit reasoning chains and the absence of mechanisms for active exploration and iterative refinement. To address these challenges, we propose AudioGenie-Reasoner (AGR), the first unified training-free multi-agent system that coordinates perception and reasoning over an evolving chain of textual evidence. Our key idea is a paradigm shift that transforms audio deep reasoning into complex text understanding task from a new perspective, thereby unlocking the full potential of large language models. Specifically, the design of AGR mimics the human coarse-to-fine cognitive process. It first transforms the input audio into a coarse text-based document. Then, we design a novel proactive iterative document refinement loop, featuring tool-augmented routes and specialized agents, to continuously search for missing information and augment the evidence chain in a coarse-to-fine manner until sufficient question-related information is gathered for making final predictions. Experimental results show that AGR achieves state-of-the-art (SOTA) performance over existing open-source audio deep reasoning models across various benchmarks. The code will be made publicly available.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "177",
        "title": "Interpretable Audio Editing Evaluation via Chain-of-Thought Difference-Commonality Reasoning with Multimodal LLMs",
        "author": [
            "Yuhang Jia",
            "Xu Zhang",
            "Yang Chen",
            "Hui Wang",
            "Enzhi Wang",
            "Yong Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16975",
        "abstract": "Automatic mean opinion score (MOS) prediction provides a more perceptual alternative to objective metrics, offering deeper insights into the evaluated models. With the rapid progress of multimodal large language models (MLLMs), their enhanced perceptual and reasoning abilities enable more comprehensive and interpretable audio quality assessment. In this work, we tackle the challenging task of audio editing evaluation and propose the first natural language-based automated evaluation framework built on MLLMs. Our approach introduces two fine-tuning tasks to boost multi-audio understanding, combined with Chain-of-Thought prompting, and lightweight instruction tuning, to enhance step-by-step reasoning. Experiment demonstrate that our framework delivers accurate, interpretable, and text-based editing evaluation, closely aligning with human judgments and objective metrics while substantially improving over baselines. The code and demo are available at https://github.com/NKU-HLT/Eval_Reasoning.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "178",
        "title": "VCE: Safe Autoregressive Image Generation via Visual Contrast Exploitation",
        "author": [
            "Feng Han",
            "Chao Gong",
            "Zhipeng Wei",
            "Jingjing Chen",
            "Yu-Gang Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16986",
        "abstract": "Recently, autoregressive image generation models have wowed audiences with their remarkable capability in creating surprisingly realistic images. Models such as GPT-4o and LlamaGen can not only produce images that faithfully mimic renowned artistic styles like Ghibli, Van Gogh, or Picasso, but also potentially generate Not-Safe-For-Work (NSFW) content, raising significant concerns regarding copyright infringement and ethical use. Despite these concerns, methods to safeguard autoregressive text-to-image models remain underexplored. Previous concept erasure methods, primarily designed for diffusion models that operate in denoising latent space, are not directly applicable to autoregressive models that generate images token by token. To address this critical gap, we propose Visual Contrast Exploitation (VCE), a novel framework comprising: (1) an innovative contrastive image pair construction paradigm that precisely decouples unsafe concepts from their associated content semantics, and (2) a sophisticated DPO-based training approach that enhances the model's ability to identify and leverage visual contrastive features from image pairs, enabling precise concept erasure. Our comprehensive experiments across three challenging tasks-artist style erasure, explicit content erasure, and object removal-demonstrate that our method effectively secures the model, achieving state-of-the-art results while erasing unsafe concepts and maintaining the integrity of unrelated safe concepts. The code and models are available at https://github.com/Maplebb/VCE.",
        "tags": [
            "DPO",
            "Diffusion",
            "GPT",
            "Text-to-Image"
        ]
    },
    {
        "id": "179",
        "title": "PTQTP: Post-Training Quantization to Trit-Planes for Large Language Models",
        "author": [
            "He Xiao",
            "Runming Yang",
            "Qingyao Yang",
            "Wendong Xu",
            "Zheng Li",
            "Yupeng Su",
            "Zhengwu Liu",
            "Hongxia Yang",
            "Ngai Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16989",
        "abstract": "Post-training quantization (PTQ) of large language models (LLMs) to extremely low bit-widths remains challenging due to the fundamental trade-off between computational efficiency and model expressiveness. While existing ultra-low-bit PTQ methods rely on binary approximations or complex compensation mechanisms, they suffer from either limited representational capacity or computational overhead that undermines their efficiency gains. We introduce PTQ to Trit-Planes (PTQTP), the first ternary-weight PTQ framework that decomposes weight matrices into structured ternary {-1, 0, 1} trit-planes using 2x1.58-bit representation. PTQTP achieves multiplication-free inference, identical to 1-bit quantization, while maintaining superior expressiveness through its novel structured decomposition. Our approach provides: (1) a theoretically grounded progressive approximation algorithm ensuring global weight consistency; (2) model-agnostic deployment across diverse modern LLMs without architectural modifications; and (3) uniform ternary operations that eliminate the need for mixed-precision or compensation schemes. Comprehensive experiments across LLaMA3.x and Qwen3 model families (0.6B-70B parameters) demonstrate that PTQTP significantly outperforms existing low-bit PTQ methods, achieving 82.4% mathematical reasoning retention versus 0% for competing approaches. PTQTP approaches and sometimes surpasses 1.58-bit quantization-aware training performance while requiring only single-hour quantization compared to 10-14 GPU days for training-based methods. These results establish PTQTP as a practical solution for efficient LLM deployment in resource-constrained environments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "180",
        "title": "Advancing Speech Understanding in Speech-Aware Language Models with GRPO",
        "author": [
            "Avishai Elmakies",
            "Hagai Aronowitz",
            "Nimrod Shabtay",
            "Eli Schwartz",
            "Ron Hoory",
            "Avihu Dekel"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16990",
        "abstract": "In this paper, we introduce a Group Relative Policy Optimization (GRPO)-based method for training Speech-Aware Large Language Models (SALLMs) on open-format speech understanding tasks, such as Spoken Question Answering and Automatic Speech Translation. SALLMs have proven highly effective for speech understanding tasks. GRPO has recently gained traction for its efficiency in training LLMs, and prior work has explored its application to SALLMs, primarily in multiple-choice tasks. Building on this, we focus on open-format tasks that better reflect the generative abilities of the models. Our approach leverages GRPO with BLEU as the reward signal to optimize SALLMs, and we demonstrate empirically that it surpasses standard SFT across several key metrics. Finally, we explore the potential of incorporating off-policy samples within GRPO for these tasks, highlighting avenues for further improvement and further research.",
        "tags": [
            "GRPO",
            "LLM"
        ]
    },
    {
        "id": "181",
        "title": "MoA-Off: Adaptive Heterogeneous Modality-Aware Offloading with Edge-Cloud Collaboration for Efficient Multimodal LLM Inference",
        "author": [
            "Zheming Yang",
            "Qi Guo",
            "Yunqing Hu",
            "Chang Zhao",
            "Chang Zhang",
            "Jian Zhao",
            "Wen Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16995",
        "abstract": "Multimodal large language models (MLLMs) enable powerful cross-modal inference but impose significant computational and latency burdens, posing severe challenges for deployment in resource-constrained environments. In this paper, we propose MoA-Off, an adaptive heterogeneous modality-aware offloading framework with edge-cloud collaboration for efficient MLLM inference. MoA-Off introduces a lightweight heterogeneous modality-aware module that estimates the complexity of heterogeneous inputs through multi-dimensional feature analysis. Then, an adaptive edge-cloud collaborative offloading strategy is proposed that dynamically schedules workloads between edge and cloud based on modality-aware complexity scores and real-time system states. The experimental results demonstrate that MoA-Off can achieve over 30% reduction in latency and 30%-65% decrease in resource overhead while maintaining competitive accuracy compared to traditional approaches.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "182",
        "title": "IDfRA: Self-Verification for Iterative Design in Robotic Assembly",
        "author": [
            "Nishka Khendry",
            "Christos Margadji",
            "Sebastian W. Pattinson"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16998",
        "abstract": "As robots proliferate in manufacturing, Design for Robotic Assembly (DfRA), which is designing products for efficient automated assembly, is increasingly important. Traditional approaches to DfRA rely on manual planning, which is time-consuming, expensive and potentially impractical for complex objects. Large language models (LLM) have exhibited proficiency in semantic interpretation and robotic task planning, stimulating interest in their application to the automation of DfRA. But existing methodologies typically rely on heuristic strategies and rigid, hard-coded physics simulators that may not translate into real-world assembly contexts. In this work, we present Iterative Design for Robotic Assembly (IDfRA), a framework using iterative cycles of planning, execution, verification, and re-planning, each informed by self-assessment, to progressively enhance design quality within a fixed yet initially under-specified environment, thereby eliminating the physics simulation with the real world itself. The framework accepts as input a target structure together with a partial environmental representation. Through successive refinement, it converges toward solutions that reconcile semantic fidelity with physical feasibility. Empirical evaluation demonstrates that IDfRA attains 73.3\\% top-1 accuracy in semantic recognisability, surpassing the baseline on this metric. Moreover, the resulting assembly plans exhibit robust physical feasibility, achieving an overall 86.9\\% construction success rate, with design quality improving across iterations, albeit not always monotonically. Pairwise human evaluation further corroborates the advantages of IDfRA relative to alternative approaches. By integrating self-verification with context-aware adaptation, the framework evidences strong potential for deployment in unstructured manufacturing scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "183",
        "title": "MBCodec:Thorough disentangle for high-fidelity audio compression",
        "author": [
            "Ruonan Zhang",
            "Xiaoyang Hao",
            "Yichen Han",
            "Junjie Cao",
            "Yue Liu",
            "Kai Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17006",
        "abstract": "High-fidelity neural audio codecs in Text-to-speech (TTS) aim to compress speech signals into discrete representations for faithful reconstruction. However, prior approaches faced challenges in effectively disentangling acoustic and semantic information within tokens, leading to a lack of fine-grained details in synthesized speech. In this study, we propose MBCodec, a novel multi-codebook audio codec based on Residual Vector Quantization (RVQ) that learns a hierarchically structured representation. MBCodec leverages self-supervised semantic tokenization and audio subband features from the raw signals to construct a functionally-disentangled latent space. In order to encourage comprehensive learning across various layers of the codec embedding space, we introduce adaptive dropout depths to differentially train codebooks across layers, and employ a multi-channel pseudo-quadrature mirror filter (PQMF) during training. By thoroughly decoupling semantic and acoustic features, our method not only achieves near-lossless speech reconstruction but also enables a remarkable 170x compression of 24 kHz audio, resulting in a low bit rate of just 2.2 kbps. Experimental evaluations confirm its consistent and substantial outperformance of baselines across all evaluations.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "184",
        "title": "Multiscale solution decomposition of nonlocal-in-time problems with application in numerical computation",
        "author": [
            "Mengmeng Liu",
            "Jie Ma",
            "Wenlin Qiu",
            "Xiangcheng Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17020",
        "abstract": "This work develops a multiscale solution decomposition (MSD) method for nonlocal-in-time problems to separate a series of known terms with multiscale singularity from the original singular solution such that the remaining unknown part becomes smoother. We demonstrate that the MSD provides a scenario where the smoothness assumption for solutions of weakly singular nonlocal-in-time problems, a commonly encountered assumption in numerous literature of numerical methods that is in general not true for original solutions, becomes appropriate such that abundant numerical analysis results therein become applicable. From computational aspect, instead of handling solution singularity, the MSD significantly reduces the numerical difficulties by separating and thus circumventing the solution singularity. We consider typical problems, including the fractional relaxation equation, Volterra integral equation, subdiffusion, integrodifferential equation and diffusion-wave equation, to demonstrate the universality of MSD and its effectiveness in improving the numerical accuracy or stability in comparison with classical methods.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "185",
        "title": "VAInpaint: Zero-Shot Video-Audio inpainting framework with LLMs-driven Module",
        "author": [
            "Kam Man Wu",
            "Zeyue Tian",
            "Liya Ji",
            "Qifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17022",
        "abstract": "Video and audio inpainting for mixed audio-visual content has become a crucial task in multimedia editing recently. However, precisely removing an object and its corresponding audio from a video without affecting the rest of the scene remains a significant challenge. To address this, we propose VAInpaint, a novel pipeline that first utilizes a segmentation model to generate masks and guide a video inpainting model in removing objects. At the same time, an LLM then analyzes the scene globally, while a region-specific model provides localized descriptions. Both the overall and regional descriptions will be inputted into an LLM, which will refine the content and turn it into text queries for our text-driven audio separation model. Our audio separation model is fine-tuned on a customized dataset comprising segmented MUSIC instrument images and VGGSound backgrounds to enhance its generalization performance. Experiments show that our method achieves performance comparable to current benchmarks in both audio and video inpainting.",
        "tags": [
            "Inpainting",
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "186",
        "title": "When Color-Space Decoupling Meets Diffusion for Adverse-Weather Image Restoration",
        "author": [
            "Wenxuan Fang",
            "Jili Fan",
            "Chao Wang",
            "Xiantao Hu",
            "Jiangwei Weng",
            "Ying Tai",
            "Jian Yang",
            "Jun Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17024",
        "abstract": "Adverse Weather Image Restoration (AWIR) is a highly challenging task due to the unpredictable and dynamic nature of weather-related degradations. Traditional task-specific methods often fail to generalize to unseen or complex degradation types, while recent prompt-learning approaches depend heavily on the degradation estimation capabilities of vision-language models, resulting in inconsistent restorations. In this paper, we propose \\textbf{LCDiff}, a novel framework comprising two key components: \\textit{Lumina-Chroma Decomposition Network} (LCDN) and \\textit{Lumina-Guided Diffusion Model} (LGDM). LCDN processes degraded images in the YCbCr color space, separately handling degradation-related luminance and degradation-invariant chrominance components. This decomposition effectively mitigates weather-induced degradation while preserving color fidelity. To further enhance restoration quality, LGDM leverages degradation-related luminance information as a guiding condition, eliminating the need for explicit degradation prompts. Additionally, LGDM incorporates a \\textit{Dynamic Time Step Loss} to optimize the denoising network, ensuring a balanced recovery of both low- and high-frequency features in the image. Finally, we present DriveWeather, a comprehensive all-weather driving dataset designed to enable robust evaluation. Extensive experiments demonstrate that our approach surpasses state-of-the-art methods, setting a new benchmark in AWIR. The dataset and code are available at: https://github.com/fiwy0527/LCDiff.",
        "tags": [
            "Diffusion",
            "VLM"
        ]
    },
    {
        "id": "187",
        "title": "The Transfer Neurons Hypothesis: An Underlying Mechanism for Language Latent Space Transitions in Multilingual LLMs",
        "author": [
            "Hinata Tezuka",
            "Naoya Inoue"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17030",
        "abstract": "Recent studies have suggested a processing framework for multilingual inputs in decoder-based LLMs: early layers convert inputs into English-centric and language-agnostic representations; middle layers perform reasoning within an English-centric latent space; and final layers generate outputs by transforming these representations back into language-specific latent spaces. However, the internal dynamics of such transformation and the underlying mechanism remain underexplored. Towards a deeper understanding of this framework, we propose and empirically validate The Transfer Neurons Hypothesis: certain neurons in the MLP module are responsible for transferring representations between language-specific latent spaces and a shared semantic latent space. Furthermore, we show that one function of language-specific neurons, as identified in recent studies, is to facilitate movement between latent spaces. Finally, we show that transfer neurons are critical for reasoning in multilingual LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "188",
        "title": "From Easy to Hard: The MIR Benchmark for Progressive Interleaved Multi-Image Reasoning",
        "author": [
            "Hang Du",
            "Jiayang Zhang",
            "Guoshun Nan",
            "Wendi Deng",
            "Zhenyan Chen",
            "Chenyang Zhang",
            "Wang Xiao",
            "Shan Huang",
            "Yuqi Pan",
            "Tao Qi",
            "Sicong Leng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17040",
        "abstract": "Multi-image Interleaved Reasoning aims to improve Multi-modal Large Language Models (MLLMs) ability to jointly comprehend and reason across multiple images and their associated textual contexts, introducing unique challenges beyond single-image or non-interleaved multi-image tasks. While current multi-image benchmarks overlook interleaved textual contexts and neglect distinct relationships between individual images and their associated texts, enabling models to reason over multi-image interleaved data may significantly enhance their comprehension of complex scenes and better capture cross-modal correlations. To bridge this gap, we introduce a novel benchmark MIR, requiring joint reasoning over multiple images accompanied by interleaved textual contexts to accurately associate image regions with corresponding texts and logically connect information across images. To enhance MLLMs ability to comprehend multi-image interleaved data, we introduce reasoning steps for each instance within the benchmark and propose a stage-wise curriculum learning strategy. This strategy follows an \"easy to hard\" approach, progressively guiding models from simple to complex scenarios, thereby enhancing their ability to handle challenging tasks. Extensive experiments benchmarking multiple MLLMs demonstrate that our method significantly enhances models reasoning performance on MIR and other established benchmarks. We believe that MIR will encourage further research into multi-image interleaved reasoning, facilitating advancements in MLLMs capability to handle complex inter-modal http://tasks.Our code and dataset are available at https://github.com/Shelly-coder239/MIRBench.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "189",
        "title": "Orchestrate, Generate, Reflect: A VLM-Based Multi-Agent Collaboration Framework for Automated Driving Policy Learning",
        "author": [
            "Zengqi Peng",
            "Yusen Xie",
            "Yubin Wang",
            "Rui Yang",
            "Qifeng Chen",
            "Jun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17042",
        "abstract": "The advancement of foundation models fosters new initiatives for policy learning in achieving safe and efficient autonomous driving. However, a critical bottleneck lies in the manual engineering of reward functions and training curricula for complex and dynamic driving tasks, which is a labor-intensive and time-consuming process. To address this problem, we propose OGR (Orchestrate, Generate, Reflect), a novel automated driving policy learning framework that leverages vision-language model (VLM)-based multi-agent collaboration. Our framework capitalizes on advanced reasoning and multimodal understanding capabilities of VLMs to construct a hierarchical agent system. Specifically, a centralized orchestrator plans high-level training objectives, while a generation module employs a two-step analyze-then-generate process for efficient generation of reward-curriculum pairs. A reflection module then facilitates iterative optimization based on the online evaluation. Furthermore, a dedicated memory module endows the VLM agents with the capabilities of long-term memory. To enhance robustness and diversity of the generation process, we introduce a parallel generation scheme and a human-in-the-loop technique for augmentation of the reward observation space. Through efficient multi-agent cooperation and leveraging rich multimodal information, OGR enables the online evolution of reinforcement learning policies to acquire interaction-aware driving skills. Extensive experiments in the CARLA simulator demonstrate the superior performance, robust generalizability across distinct urban scenarios, and strong compatibility with various RL algorithms. Further real-world experiments highlight the practical viability and effectiveness of our framework. The source code will be available upon acceptance of the paper.",
        "tags": [
            "RL",
            "VLM"
        ]
    },
    {
        "id": "190",
        "title": "Geodesic Prototype Matching via Diffusion Maps for Interpretable Fine-Grained Recognition",
        "author": [
            "Junhao Jia",
            "Yunyou Liu",
            "Yifei Sun",
            "Huangwei Chen",
            "Feiwei Qin",
            "Changmiao Wang",
            "Yong Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17050",
        "abstract": "Nonlinear manifolds are widespread in deep visual features, where Euclidean distances often fail to capture true similarity. This limitation becomes particularly severe in prototype-based interpretable fine-grained recognition, where subtle semantic distinctions are essential. To address this challenge, we propose a novel paradigm for prototype-based recognition that anchors similarity within the intrinsic geometry of deep features. Specifically, we distill the latent manifold structure of each class into a diffusion space and introduce a differentiable NystrÃ¶m interpolation, making the geometry accessible to both unseen samples and learnable prototypes. To ensure efficiency, we employ compact per-class landmark sets with periodic updates. This design keeps the embedding aligned with the evolving backbone, enabling fast and scalable inference. Extensive experiments on the CUB-200-2011 and Stanford Cars datasets show that our GeoProto framework produces prototypes focusing on semantically aligned parts, significantly outperforming Euclidean prototype networks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "191",
        "title": "Sidon: Fast and Robust Open-Source Multilingual Speech Restoration for Large-scale Dataset Cleansing",
        "author": [
            "Wataru Nakata",
            "Yuki Saito",
            "Yota Ueda",
            "Hiroshi Saruwatari"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17052",
        "abstract": "Large-scale text-to-speech (TTS) systems are limited by the scarcity of clean, multilingual recordings. We introduce Sidon, a fast, open-source speech restoration model that converts noisy in-the-wild speech into studio-quality speech and scales to dozens of languages. Sidon consists of two models: w2v-BERT 2.0 finetuned feature predictor to cleanse features from noisy speech and vocoder trained to synthesize restored speech from the cleansed features. Sidon achieves restoration performance comparable to Miipher: Google's internal speech restoration model with the aim of dataset cleansing for speech synthesis. Sidon is also computationally efficient, running up to 3,390 times faster than real time on a single GPU. We further show that training a TTS model using a Sidon-cleansed automatic speech recognition corpus improves the quality of synthetic speech in a zero-shot setting. Code and model are released to facilitate reproducible dataset cleansing for the research community.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "192",
        "title": "FILIC: Dual-Loop Force-Guided Imitation Learning with Impedance Torque Control for Contact-Rich Manipulation Tasks",
        "author": [
            "Haizhou Ge",
            "Yufei Jia",
            "Zheng Li",
            "Yue Li",
            "Zhixing Chen",
            "Ruqi Huang",
            "Guyue Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17053",
        "abstract": "Contact-rich manipulation is crucial for robots to perform tasks requiring precise force control, such as insertion, assembly, and in-hand manipulation. However, most imitation learning (IL) policies remain position-centric and lack explicit force awareness, and adding force/torque sensors to collaborative robot arms is often costly and requires additional hardware design. To overcome these issues, we propose FILIC, a Force-guided Imitation Learning framework with impedance torque control. FILIC integrates a Transformer-based IL policy with an impedance controller in a dual-loop structure, enabling compliant force-informed, force-executed manipulation. For robots without force/torque sensors, we introduce a cost-effective end-effector force estimator using joint torque measurements through analytical Jacobian-based inversion while compensating with model-predicted torques from a digital twin. We also design complementary force feedback frameworks via handheld haptics and VR visualization to improve demonstration quality. Experiments show that FILIC significantly outperforms vision-only and joint-torque-based methods, achieving safer, more compliant, and adaptable contact-rich manipulation. Our code can be found in https://github.com/TATP-233/FILIC.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "193",
        "title": "TactfulToM: Do LLMs Have the Theory of Mind Ability to Understand White Lies?",
        "author": [
            "Yiwei Liu",
            "Emma Jane Pretty",
            "Jiahao Huang",
            "Saku Sugawara"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17054",
        "abstract": "While recent studies explore Large Language Models' (LLMs) performance on Theory of Mind (ToM) reasoning tasks, research on ToM abilities that require more nuanced social context is limited, such as white lies. We introduce TactfulToM, a novel English benchmark designed to evaluate LLMs' ability to understand white lies within real-life conversations and reason about prosocial motivations behind them, particularly when they are used to spare others' feelings and maintain social harmony. Our benchmark is generated through a multi-stage human-in-the-loop pipeline where LLMs expand manually designed seed stories into conversations to maintain the information asymmetry between participants necessary for authentic white lies. We show that TactfulToM is challenging for state-of-the-art models, which perform substantially below humans, revealing shortcomings in their ability to fully comprehend the ToM reasoning that enables true understanding of white lies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "194",
        "title": "RoboManipBaselines: A Unified Framework for Imitation Learning in Robotic Manipulation across Real and Simulated Environments",
        "author": [
            "Masaki Murooka",
            "Tomohiro Motoda",
            "Ryoichi Nakajo",
            "Hanbit Oh",
            "Koshi Makihara",
            "Keisuke Shirai",
            "Yukiyasu Domae"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17057",
        "abstract": "RoboManipBaselines is an open framework for robot imitation learning that unifies data collection, training, and evaluation across simulation and real robots. We introduce it as a platform enabling systematic benchmarking of diverse tasks, robots, and multimodal policies with emphasis on integration, generality, extensibility, and reproducibility.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "195",
        "title": "TSGym: Design Choices for Deep Multivariate Time-Series Forecasting",
        "author": [
            "Shuang Liang",
            "Chaochuan Hou",
            "Xu Yao",
            "Shiping Wang",
            "Minqi Jiang",
            "Songqiao Han",
            "Hailiang Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17063",
        "abstract": "Recently, deep learning has driven significant advancements in multivariate time series forecasting (MTSF) tasks. However, much of the current research in MTSF tends to evaluate models from a holistic perspective, which obscures the individual contributions and leaves critical issues unaddressed. Adhering to the current modeling paradigms, this work bridges these gaps by systematically decomposing deep MTSF methods into their core, fine-grained components like series-patching tokenization, channel-independent strategy, attention modules, or even Large Language Models and Time-series Foundation Models. Through extensive experiments and component-level analysis, our work offers more profound insights than previous benchmarks that typically discuss models as a whole.\nFurthermore, we propose a novel automated solution called TSGym for MTSF tasks. Unlike traditional hyperparameter tuning, neural architecture searching or fixed model selection, TSGym performs fine-grained component selection and automated model construction, which enables the creation of more effective solutions tailored to diverse time series data, therefore enhancing model transferability across different data sources and robustness against distribution shifts. Extensive experiments indicate that TSGym significantly outperforms existing state-of-the-art MTSF and AutoML methods. All code is publicly available on https://github.com/SUFE-AILAB/TSGym.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "196",
        "title": "CardiacCLIP: Video-based CLIP Adaptation for LVEF Prediction in a Few-shot Manner",
        "author": [
            "Yao Du",
            "Jiarong Guo",
            "Xiaomeng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17065",
        "abstract": "Echocardiography is a vital non-invasive modality for cardiac assessment, with left ventricular ejection fraction (LVEF) serving as a key indicator of heart function. Existing LVEF estimation methods depend on large-scale annotated video datasets, which are costly and limit adaptability across various clinical settings. Recent vision-language models for echocardiography, such as EchoCLIP, apply image-to-text pretraining but fail to capture crucial temporal dynamics and localized cardiac structures essential for accurate diagnosis. To address these challenges, we propose CardiacCLIP, a video-based framework that enhances LVEF prediction through attention-based frame aggregation and multi-resolution input scaling. Specifically, we introduce MFL (Multi Frame Learning), a novel attention-based mechanism for selectively fusing informative frames, and EchoZoom, a multi-scale feature extraction strategy that refines spatial representations of cardiac structures. As a novel adaptation of CLIP models for few-shot echocardiogram video analysis, our approach significantly improves diagnostic accuracy, reducing MAE by 2.07 on the EchoNet-Dynamic dataset under 1-shot setting. The code is available at https://github.com/xmed-lab/CardiacCLIP.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "197",
        "title": "SnipSnap: A Joint Compression Format and Dataflow Co-Optimization Framework for Efficient Sparse LLM Accelerator Design",
        "author": [
            "Junyi Wu",
            "Chao Fang",
            "Zhongfeng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17072",
        "abstract": "The growing scale of large language models (LLMs) has intensified demands on computation and memory, making efficient inference a key challenge. While sparsity can reduce these costs, existing design space exploration (DSE) frameworks often overlook compression formats, a key factor for leveraging sparsity on accelerators. This paper proposes SnipSnap, a joint compression format and dataflow co-optimization framework for efficient sparse LLM accelerator design. SnipSnap introduces: (1) a hierarchical compression format encoding to expand the design space; (2) an adaptive compression engine for selecting formats under diverse sparsity; and (3) a progressive co-search workflow that jointly optimizes dataflow and compression formats. SnipSnap achieves 18.24\\% average memory energy savings via format optimization, along with 2248.3$\\times$ and 21.0$\\times$ speedups over Sparseloop and DiMO-Sparse frameworks, respectively.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "198",
        "title": "A Dual-Modulation Framework for RGB-T Crowd Counting via Spatially Modulated Attention and Adaptive Fusion",
        "author": [
            "Yuhong Feng",
            "Hongtao Chen",
            "Qi Zhang",
            "Jie Chen",
            "Zhaoxi He",
            "Mingzhe Liu",
            "Jianghai Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17079",
        "abstract": "Accurate RGB-Thermal (RGB-T) crowd counting is crucial for public safety in challenging conditions. While recent Transformer-based methods excel at capturing global context, their inherent lack of spatial inductive bias causes attention to spread to irrelevant background regions, compromising crowd localization precision. Furthermore, effectively bridging the gap between these distinct modalities remains a major hurdle. To tackle this, we propose the Dual Modulation Framework, comprising two modules: Spatially Modulated Attention (SMA), which improves crowd localization by using a learnable Spatial Decay Mask to penalize attention between distant tokens and prevent focus from spreading to the background; and Adaptive Fusion Modulation (AFM), which implements a dynamic gating mechanism to prioritize the most reliable modality for adaptive cross-modal fusion. Extensive experiments on RGB-T crowd counting datasets demonstrate the superior performance of our method compared to previous works. Code available at https://github.com/Cht2924/RGBT-Crowd-Counting.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "199",
        "title": "CoPlanner: An Interactive Motion Planner with Contingency-Aware Diffusion for Autonomous Driving",
        "author": [
            "Ruiguo Zhong",
            "Ruoyu Yao",
            "Pei Liu",
            "Xiaolong Chen",
            "Rui Yang",
            "Jun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17080",
        "abstract": "Accurate trajectory prediction and motion planning are crucial for autonomous driving systems to navigate safely in complex, interactive environments characterized by multimodal uncertainties. However, current generation-then-evaluation frameworks typically construct multiple plausible trajectory hypotheses but ultimately adopt a single most likely outcome, leading to overconfident decisions and a lack of fallback strategies that are vital for safety in rare but critical scenarios. Moreover, the usual decoupling of prediction and planning modules could result in socially inconsistent or unrealistic joint trajectories, especially in highly interactive traffic. To address these challenges, we propose a contingency-aware diffusion planner (CoPlanner), a unified framework that jointly models multi-agent interactive trajectory generation and contingency-aware motion planning. Specifically, the pivot-conditioned diffusion mechanism anchors trajectory sampling on a validated, shared short-term segment to preserve temporal consistency, while stochastically generating diverse long-horizon branches that capture multimodal motion evolutions. In parallel, we design a contingency-aware multi-scenario scoring strategy that evaluates candidate ego trajectories across multiple plausible long-horizon evolution scenarios, balancing safety, progress, and comfort. This integrated design preserves feasible fallback options and enhances robustness under uncertainty, leading to more realistic interaction-aware planning. Extensive closed-loop experiments on the nuPlan benchmark demonstrate that CoPlanner consistently surpasses state-of-the-art methods on both Val14 and Test14 datasets, achieving significant improvements in safety and comfort under both reactive and non-reactive settings. Code and model will be made publicly available upon acceptance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "200",
        "title": "HyRF: Hybrid Radiance Fields for Memory-efficient and High-quality Novel View Synthesis",
        "author": [
            "Zipeng Wang",
            "Dan Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17083",
        "abstract": "Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful alternative to NeRF-based approaches, enabling real-time, high-quality novel view synthesis through explicit, optimizable 3D Gaussians. However, 3DGS suffers from significant memory overhead due to its reliance on per-Gaussian parameters to model view-dependent effects and anisotropic shapes. While recent works propose compressing 3DGS with neural fields, these methods struggle to capture high-frequency spatial variations in Gaussian properties, leading to degraded reconstruction of fine details. We present Hybrid Radiance Fields (HyRF), a novel scene representation that combines the strengths of explicit Gaussians and neural fields. HyRF decomposes the scene into (1) a compact set of explicit Gaussians storing only critical high-frequency parameters and (2) grid-based neural fields that predict remaining properties. To enhance representational capacity, we introduce a decoupled neural field architecture, separately modeling geometry (scale, opacity, rotation) and view-dependent color. Additionally, we propose a hybrid rendering scheme that composites Gaussian splatting with a neural field-predicted background, addressing limitations in distant scene representation. Experiments demonstrate that HyRF achieves state-of-the-art rendering quality while reducing model size by over 20 times compared to 3DGS and maintaining real-time performance. Our project page is available at https://wzpscott.github.io/hyrf/.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "201",
        "title": "MoCLIP-Lite: Efficient Video Recognition by Fusing CLIP with Motion Vectors",
        "author": [
            "Binhua Huang",
            "Nan Wang",
            "Arjun Parakash",
            "Soumyabrata Dev"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17084",
        "abstract": "Video action recognition is a fundamental task in computer vision, but state-of-the-art models are often computationally expensive and rely on extensive video pre-training. In parallel, large-scale vision-language models like Contrastive Language-Image Pre-training (CLIP) offer powerful zero-shot capabilities on static images, while motion vectors (MV) provide highly efficient temporal information directly from compressed video streams. To synergize the strengths of these paradigms, we propose MoCLIP-Lite, a simple yet powerful two-stream late fusion framework for efficient video recognition. Our approach combines features from a frozen CLIP image encoder with features from a lightweight, supervised network trained on raw MV. During fusion, both backbones are frozen, and only a tiny Multi-Layer Perceptron (MLP) head is trained, ensuring extreme efficiency. Through comprehensive experiments on the UCF101 dataset, our method achieves a remarkable 89.2% Top-1 accuracy, significantly outperforming strong zero-shot (65.0%) and MV-only (66.5%) baselines. Our work provides a new, highly efficient baseline for video understanding that effectively bridges the gap between large static models and dynamic, low-cost motion cues. Our code and models are available at https://github.com/microa/MoCLIP-Lite.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "202",
        "title": "AlignedGen: Aligning Style Across Generated Images",
        "author": [
            "Jiexuan Zhang",
            "Yiheng Du",
            "Qian Wang",
            "Weiqi Li",
            "Yu Gu",
            "Jian Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17088",
        "abstract": "Despite their generative power, diffusion models struggle to maintain style consistency across images conditioned on the same style prompt, hindering their practical deployment in creative workflows. While several training-free methods attempt to solve this, they are constrained to the U-Net architecture, which not only leads to low-quality results and artifacts like object repetition but also renders them incompatible with superior Diffusion Transformer (DiT). To address these issues, we introduce AlignedGen, a novel training-free framework that enhances style consistency across images generated by DiT models. Our work first reveals a critical insight: naive attention sharing fails in DiT due to conflicting positional signals from improper position embeddings. We introduce Shifted Position Embedding (ShiftPE), an effective solution that resolves this conflict by allocating a non-overlapping set of positional indices to each image. Building on this foundation, we develop Advanced Attention Sharing (AAS), a suite of three techniques meticulously designed to fully unleash the potential of attention sharing within the DiT. Furthermore, to broaden the applicability of our method, we present an efficient query, key, and value feature extraction algorithm, enabling our method to seamlessly incorporate external images as style references. Extensive experimental results validate that our method effectively enhances style consistency across generated images while maintaining precise text-to-image alignment.",
        "tags": [
            "DiT",
            "Diffusion",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "203",
        "title": "Prompt-with-Me: in-IDE Structured Prompt Management for LLM-Driven Software Engineering",
        "author": [
            "Ziyou Li",
            "Agnia Sergeyuk",
            "Maliheh Izadi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17096",
        "abstract": "Large Language Models are transforming software engineering, yet prompt management in practice remains ad hoc, hindering reliability, reuse, and integration into industrial workflows. We present Prompt-with-Me, a practical solution for structured prompt management embedded directly in the development environment. The system automatically classifies prompts using a four-dimensional taxonomy encompassing intent, author role, software development lifecycle stage, and prompt type. To enhance prompt reuse and quality, Prompt-with-Me suggests language refinements, masks sensitive information, and extracts reusable templates from a developer's prompt library. Our taxonomy study of 1108 real-world prompts demonstrates that modern LLMs can accurately classify software engineering prompts. Furthermore, our user study with 11 participants shows strong developer acceptance, with high usability (Mean SUS=73), low cognitive load (Mean NASA-TLX=21), and reported gains in prompt quality and efficiency through reduced repetitive effort. Lastly, we offer actionable insights for building the next generation of prompt management and maintenance tools for software engineering workflows.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "204",
        "title": "GRPOformer: Advancing Hyperparameter Optimization via Group Relative Policy Optimization",
        "author": [
            "Haoxin Guo",
            "Jiawen Pan",
            "Weixin Zhai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17105",
        "abstract": "Hyperparameter optimization (HPO) plays a critical role in improving model performance. Transformer-based HPO methods have shown great potential; however, existing approaches rely heavily on large-scale historical optimization trajectories and lack effective reinforcement learning (RL) techniques, thereby limiting their efficiency and performance improvements. Inspired by the success of Group Relative Policy Optimization (GRPO) in large language models (LLMs), we propose GRPOformer -- a novel hyperparameter optimization framework that integrates reinforcement learning (RL) with Transformers. In GRPOformer, Transformers are employed to generate new hyperparameter configurations from historical optimization trajectories, while GRPO enables rapid trajectory construction and optimization strategy learning from scratch. Moreover, we introduce Policy Churn Regularization (PCR) to enhance the stability of GRPO training. Experimental results on OpenML demonstrate that GRPOformer consistently outperforms baseline methods across diverse tasks, offering new insights into the application of RL for HPO.",
        "tags": [
            "GRPO",
            "LLM",
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "205",
        "title": "CoBEVMoE: Heterogeneity-aware Feature Fusion with Dynamic Mixture-of-Experts for Collaborative Perception",
        "author": [
            "Lingzhao Kong",
            "Jiacheng Lin",
            "Siyu Li",
            "Kai Luo",
            "Zhiyong Li",
            "Kailun Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17107",
        "abstract": "Collaborative perception aims to extend sensing coverage and improve perception accuracy by sharing information among multiple agents. However, due to differences in viewpoints and spatial positions, agents often acquire heterogeneous observations. Existing intermediate fusion methods primarily focus on aligning similar features, often overlooking the perceptual diversity among agents. To address this limitation, we propose CoBEVMoE, a novel collaborative perception framework that operates in the Bird's Eye View (BEV) space and incorporates a Dynamic Mixture-of-Experts (DMoE) architecture. In DMoE, each expert is dynamically generated based on the input features of a specific agent, enabling it to extract distinctive and reliable cues while attending to shared semantics. This design allows the fusion process to explicitly model both feature similarity and heterogeneity across agents. Furthermore, we introduce a Dynamic Expert Metric Loss (DEML) to enhance inter-expert diversity and improve the discriminability of the fused representation. Extensive experiments on the OPV2V and DAIR-V2X-C datasets demonstrate that CoBEVMoE achieves state-of-the-art performance. Specifically, it improves the IoU for Camera-based BEV segmentation by +1.5% on OPV2V and the AP@50 for LiDAR-based 3D object detection by +3.0% on DAIR-V2X-C, verifying the effectiveness of expert-based heterogeneous feature modeling in multi-agent collaborative perception. The source code will be made publicly available at https://github.com/godk0509/CoBEVMoE.",
        "tags": [
            "3D",
            "Detection",
            "MoE",
            "Segmentation"
        ]
    },
    {
        "id": "206",
        "title": "MCTS-EP: Empowering Embodied Planning with Online Preference Optimization",
        "author": [
            "Hang Xu",
            "Zang Yu",
            "Yehui Tang",
            "Pengbo Hu",
            "Yuhao Tang",
            "Hao Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17116",
        "abstract": "This paper introduces MCTS-EP, an online learning framework that combines large language models (LLM) with Monte Carlo Tree Search (MCTS) for training embodied agents. MCTS-EP integrates three key components: MCTS-guided exploration for preference data collection, efficient multi-modal reasoning mechanism, and iterative training pipeline based on preference optimization. We theoretically prove that MCTS-EP achieves better performance bounds than conventional on-policy algorithms when the loss function is strongly convex, and demonstrate that it can be formulated as a search-enhanced variant of GAIL. MCTS-EP achieves state-of-the-art performace across serval benchmarks. In ALFWorld, it achieves 92% and 87% success rates for textual and visual tasks. In WebShop, it reaches an average reward of 0.81. MTCS-EP also reduces average interaction steps from from 18.7/19.5 to 10.2/9.9 steps in visual http://ALFWorld.Code available at: https://github.com/xuhang-2/Embodied-Agent-Planning",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "207",
        "title": "Stencil: Subject-Driven Generation with Context Guidance",
        "author": [
            "Gordon Chen",
            "Ziqi Huang",
            "Cheston Tan",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17120",
        "abstract": "Recent text-to-image diffusion models can generate striking visuals from text prompts, but they often fail to maintain subject consistency across generations and contexts. One major limitation of current fine-tuning approaches is the inherent trade-off between quality and efficiency. Fine-tuning large models improves fidelity but is computationally expensive, while fine-tuning lightweight models improves efficiency but compromises image fidelity. Moreover, fine-tuning pre-trained models on a small set of images of the subject can damage the existing priors, resulting in suboptimal results. To this end, we present Stencil, a novel framework that jointly employs two diffusion models during inference. Stencil efficiently fine-tunes a lightweight model on images of the subject, while a large frozen pre-trained model provides contextual guidance during inference, injecting rich priors to enhance generation with minimal overhead. Stencil excels at generating high-fidelity, novel renditions of the subject in less than a minute, delivering state-of-the-art performance and setting a new benchmark in subject-driven generation.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "208",
        "title": "Imagine2Act: Leveraging Object-Action Motion Consistency from Imagined Goals for Robotic Manipulation",
        "author": [
            "Liang Heng",
            "Jiadong Xu",
            "Yiwen Wang",
            "Xiaoqi Li",
            "Muhe Cai",
            "Yan Shen",
            "Juan Zhu",
            "Guanghui Ren",
            "Hao Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17125",
        "abstract": "Relational object rearrangement (ROR) tasks (e.g., insert flower to vase) require a robot to manipulate objects with precise semantic and geometric reasoning. Existing approaches either rely on pre-collected demonstrations that struggle to capture complex geometric constraints or generate goal-state observations to capture semantic and geometric knowledge, but fail to explicitly couple object transformation with action prediction, resulting in errors due to generative noise. To address these limitations, we propose Imagine2Act, a 3D imitation-learning framework that incorporates semantic and geometric constraints of objects into policy learning to tackle high-precision manipulation tasks. We first generate imagined goal images conditioned on language instructions and reconstruct corresponding 3D point clouds to provide robust semantic and geometric priors. These imagined goal point clouds serve as additional inputs to the policy model, while an object-action consistency strategy with soft pose supervision explicitly aligns predicted end-effector motion with generated object transformation. This design enables Imagine2Act to reason about semantic and geometric relationships between objects and predict accurate actions across diverse tasks. Experiments in both simulation and the real world demonstrate that Imagine2Act outperforms previous state-of-the-art policies. More visualizations can be found at https://sites.google.com/view/imagine2act.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "209",
        "title": "Delay compensation of multi-input distinct delay nonlinear systems via neural operators",
        "author": [
            "Filip Bajraktari",
            "Luke Bhan",
            "Miroslav Krstic",
            "Yuanyuan Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17131",
        "abstract": "In this work, we present the first stability results for approximate predictors in multi-input non-linear systems with distinct actuation delays. We show that if the predictor approximation satisfies a uniform (in time) error bound, semi-global practical stability is correspondingly achieved. For such approximators, the required uniform error bound depends on the desired region of attraction and the number of control inputs in the system. The result is achieved through transforming the delay into a transport PDE and conducting analysis on the coupled ODE-PDE cascade. To highlight the viability of such error bounds, we demonstrate our results on a class of approximators - neural operators - showcasing sufficiency for satisfying such a universal bound both theoretically and in simulation on a mobile robot experiment.",
        "tags": [
            "ODE",
            "Robotics"
        ]
    },
    {
        "id": "210",
        "title": "SAEC: Scene-Aware Enhanced Edge-Cloud Collaborative Industrial Vision Inspection with Multimodal LLM",
        "author": [
            "Yuhao Tian",
            "Zheming Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17136",
        "abstract": "Industrial vision inspection requires high accuracy under stringent resource constraints, yet existing approaches face a fundamental trade-off. Multimodal LLMs (MLLMs) deliver strong reasoning capabilities but incur prohibitive computational costs, while lightweight edge models often fail on complex cases. In this paper, we present SAEC, a scene-aware enhanced edge-cloud collaborative industrial vision inspection framework with MLLM. The framework is composed of three synergistic components: (1) Efficient MLLM Fine-Tuning for Complex Defect Inspection, (2) Lightweight Multiscale Scene-Complexity Estimation, and (3) Adaptive Edge-Cloud Scheduler. Together, these modules enable robust defect detection by tailoring multimodal reasoning to scene complexity and dynamically balancing computation between edge and cloud resources. Experimental results on MVTec AD and KSDD2 datasets demonstrate that SAEC attains 85.11% and 82.72% accuracy, surpassing Qwen by 22.1% and 20.8%, and LLaVA by 33.3% and 31.6%. It also reduces runtime by up to 22.4% and cuts energy per correct decision by 40%-74%. The code is available at https://github.com/YuHao-Tian/SAEC.",
        "tags": [
            "Detection",
            "LLM",
            "LLaVA",
            "Qwen"
        ]
    },
    {
        "id": "211",
        "title": "On the Simplification of Neural Network Architectures for Predictive Process Monitoring",
        "author": [
            "Amaan Ansari",
            "Lukas Kirchdorfer",
            "Raheleh Hadian"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17145",
        "abstract": "Predictive Process Monitoring (PPM) aims to forecast the future behavior of ongoing process instances using historical event data, enabling proactive decision-making. While recent advances rely heavily on deep learning models such as LSTMs and Transformers, their high computational cost hinders practical adoption. Prior work has explored data reduction techniques and alternative feature encodings, but the effect of simplifying model architectures themselves remains underexplored. In this paper, we analyze how reducing model complexity, both in terms of parameter count and architectural depth, impacts predictive performance, using two established PPM approaches. Across five diverse event logs, we show that shrinking the Transformer model by 85% results in only a 2-3% drop in performance across various PPM tasks, while the LSTM proves slightly more sensitive, particularly for waiting time prediction. Overall, our findings suggest that substantial model simplification can preserve predictive accuracy, paving the way for more efficient and scalable PPM solutions.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "212",
        "title": "Time Series Forecasting Using a Hybrid Deep Learning Method: A Bi-LSTM Embedding Denoising Auto Encoder Transformer",
        "author": [
            "Sahar Koohfar",
            "Wubeshet Woldemariam"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17165",
        "abstract": "Time series data is a prevalent form of data found in various fields. It consists of a series of measurements taken over time. Forecasting is a crucial application of time series models, where future values are predicted based on historical data. Accurate forecasting is essential for making well-informed decisions across industries. When it comes to electric vehicles (EVs), precise predictions play a key role in planning infrastructure development, load balancing, and energy management. This study introduces a BI-LSTM embedding denoising autoencoder model (BDM) designed to address time series problems, focusing on short-term EV charging load prediction. The performance of the proposed model is evaluated by comparing it with benchmark models like Transformer, CNN, RNN, LSTM, and GRU. Based on the results of the study, the proposed model outperforms the benchmark models in four of the five-time steps, demonstrating its effectiveness for time series forecasting. This research makes a significant contribution to enhancing time series forecasting, thereby improving decision-making processes.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "213",
        "title": "SFT-TA: Supervised Fine-Tuned Agents in Multi-Agent LLMs for Automated Inductive Thematic Analysis",
        "author": [
            "Seungjun Yi",
            "Joakim Nguyen",
            "Huimin Xu",
            "Terence Lim",
            "Joseph Skrovan",
            "Mehak Beri",
            "Hitakshi Modi",
            "Andrew Well",
            "Liu Leqi",
            "Mia Markey",
            "Ying Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17167",
        "abstract": "Thematic Analysis (TA) is a widely used qualitative method that provides a structured yet flexible framework for identifying and reporting patterns in clinical interview transcripts. However, manual thematic analysis is time-consuming and limits scalability. Recent advances in LLMs offer a pathway to automate thematic analysis, but alignment with human results remains limited. To address these limitations, we propose SFT-TA, an automated thematic analysis framework that embeds supervised fine-tuned (SFT) agents within a multi-agent system. Our framework outperforms existing frameworks and the gpt-4o baseline in alignment with human reference themes. We observed that SFT agents alone may underperform, but achieve better results than the baseline when embedded within a multi-agent system. Our results highlight that embedding SFT agents in specific roles within a multi-agent system is a promising pathway to improve alignment with desired outputs for thematic analysis.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "214",
        "title": "SynergyNet: Fusing Generative Priors and State-Space Models for Facial Beauty Prediction",
        "author": [
            "Djamel Eddine Boukhari"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17172",
        "abstract": "The automated prediction of facial beauty is a benchmark task in affective computing that requires a sophisticated understanding of both local aesthetic details (e.g., skin texture) and global facial harmony (e.g., symmetry, proportions). Existing models, based on either Convolutional Neural Networks (CNNs) or Vision Transformers (ViTs), exhibit inherent architectural biases that limit their performance; CNNs excel at local feature extraction but struggle with long-range dependencies, while ViTs model global relationships at a significant computational cost. This paper introduces the \\textbf{Mamba-Diffusion Network (MD-Net)}, a novel dual-stream architecture that resolves this trade-off by delegating specialized roles to state-of-the-art models. The first stream leverages a frozen U-Net encoder from a pre-trained latent diffusion model, providing a powerful generative prior for fine-grained aesthetic qualities. The second stream employs a Vision Mamba (Vim), a modern state-space model, to efficiently capture global facial structure with linear-time complexity. By synergistically integrating these complementary representations through a cross-attention mechanism, MD-Net creates a holistic and nuanced feature space for prediction. Evaluated on the SCUT-FBP5500 benchmark, MD-Net sets a new state-of-the-art, achieving a Pearson Correlation of \\textbf{0.9235} and demonstrating the significant potential of hybrid architectures that fuse generative and sequential modeling paradigms for complex visual assessment tasks.",
        "tags": [
            "Diffusion",
            "Mamba",
            "SSMs"
        ]
    },
    {
        "id": "215",
        "title": "FlagEval Findings Report: A Preliminary Evaluation of Large Reasoning Models on Automatically Verifiable Textual and Visual Questions",
        "author": [
            "Bowen Qin",
            "Chen Yue",
            "Fang Yin",
            "Hui Wang",
            "JG Yao",
            "Jiakang Liu",
            "Jing-Shu Zheng",
            "Miguel Hu Chen",
            "Richeng Xuan",
            "Shibei Meng",
            "Shiqi Zhou",
            "Teng Dai",
            "Tong-Shuai Ren",
            "Wei Cui",
            "Xi Yang",
            "Xialin Du",
            "Xiaojing Xu",
            "Xue Sun",
            "Xuejing Li",
            "Yaming Liu",
            "Yesheng Liu",
            "Ying Liu",
            "Yonghua Lin",
            "Yu Zhao",
            "Yunduo Zhang",
            "Yuwen Luo",
            "Zheqi He",
            "Zhiyuan He",
            "Zhongyuan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17177",
        "abstract": "We conduct a moderate-scale contamination-free (to some extent) evaluation of current large reasoning models (LRMs) with some preliminary findings. We also release ROME, our evaluation benchmark for vision language models intended to test reasoning from visual clues. We attach links to the benchmark, evaluation data, and other updates on this website: https://flageval-baai.github.io/LRM-Eval/",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "216",
        "title": "Attention Consistency for LLMs Explanation",
        "author": [
            "Tian Lan",
            "Jinyuan Xu",
            "Xue He",
            "Jenq-Neng Hwang",
            "Lei Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17178",
        "abstract": "Understanding the decision-making processes of large language models (LLMs) is essential for their trustworthy development and deployment. However, current interpretability methods often face challenges such as low resolution and high computational cost. To address these limitations, we propose the \\textbf{Multi-Layer Attention Consistency Score (MACS)}, a novel, lightweight, and easily deployable heuristic for estimating the importance of input tokens in decoder-based models. MACS measures contributions of input tokens based on the consistency of maximal attention. Empirical evaluations demonstrate that MACS achieves a favorable trade-off between interpretability quality and computational efficiency, showing faithfulness comparable to complex techniques with a 22\\% decrease in VRAM usage and 30\\% reduction in latency.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "217",
        "title": "LifeAlign: Lifelong Alignment for Large Language Models with Memory-Augmented Focalized Preference Optimization",
        "author": [
            "Junsong Li",
            "Jie Zhou",
            "Bihao Zhan",
            "Yutao Yang",
            "Qianjun Pan",
            "Shilian Chen",
            "Tianyu Huai",
            "Xin Li",
            "Qin Chen",
            "Liang He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17183",
        "abstract": "Alignment plays a crucial role in Large Language Models (LLMs) in aligning with human preferences on a specific task/domain. Traditional alignment methods suffer from catastrophic forgetting, where models lose previously acquired knowledge when adapting to new preferences or domains. We introduce LifeAlign, a novel framework for lifelong alignment that enables LLMs to maintain consistent human preference alignment across sequential learning tasks without forgetting previously learned knowledge. Our approach consists of two key innovations. First, we propose a focalized preference optimization strategy that aligns LLMs with new preferences while preventing the erosion of knowledge acquired from previous tasks. Second, we develop a short-to-long memory consolidation mechanism that merges denoised short-term preference representations into stable long-term memory using intrinsic dimensionality reduction, enabling efficient storage and retrieval of alignment patterns across diverse domains. We evaluate LifeAlign across multiple sequential alignment tasks spanning different domains and preference types. Experimental results demonstrate that our method achieves superior performance in maintaining both preference alignment quality and knowledge retention compared to existing lifelong learning approaches. The codes and datasets will be released on GitHub.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "218",
        "title": "VaseVQA: Multimodal Agent and Benchmark for Ancient Greek Pottery",
        "author": [
            "Jinchao Ge",
            "Tengfei Cheng",
            "Biao Wu",
            "Zeyu Zhang",
            "Shiya Huang",
            "Judith Bishop",
            "Gillian Shepherd",
            "Meng Fang",
            "Ling Chen",
            "Yang Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17191",
        "abstract": "Analyzing cultural-heritage artifacts remains challenging for MLLMs: general models lack domain expertise, and SFT often overfits superficial patterns, yielding brittle reasoning for authentication and historical attribution. This raises the question of how to equip MLLMs with robust, expert-level reasoning for ancient Greek pottery. We present VaseVL, an SFT-then-RL system that turns evaluation into supervision: we construct a taxonomy of question types, probe the SFT model to localize type-specific performance gaps, and optimize with type-conditioned, compositionality-oriented rewards targeting those gaps. We also release VaseVQA, a comprehensive benchmark of 31,773 images designed to probe deep understanding. Experiments show state-of-the-art results on style classification and historical attribution with marked gains in compositional robustness over SFT-only baselines, validating diagnosis-guided, taxonomy-conditioned reward engineering and providing a reusable resource for future research. Code and dataset will be available at https://github.com/AIGeeksGroup/VaseVQA.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "219",
        "title": "MAST: Multi-Agent Spatial Transformer for Learning to Collaborate",
        "author": [
            "Damian Owerko",
            "Frederic Vatnsdal",
            "Saurav Agarwal",
            "Vijay Kumar",
            "Alejandro Ribeiro"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17195",
        "abstract": "This article presents a novel multi-agent spatial transformer (MAST) for learning communication policies in large-scale decentralized and collaborative multi-robot systems (DC-MRS). Challenges in collaboration in DC-MRS arise from: (i) partial observable states as robots make only localized perception, (ii) limited communication range with no central server, and (iii) independent execution of actions. The robots need to optimize a common task-specific objective, which, under the restricted setting, must be done using a communication policy that exhibits the desired collaborative behavior. The proposed MAST is a decentralized transformer architecture that learns communication policies to compute abstract information to be shared with other agents and processes the received information with the robot's own observations. The MAST extends the standard transformer with new positional encoding strategies and attention operations that employ windowing to limit the receptive field for MRS. These are designed for local computation, shift-equivariance, and permutation equivariance, making it a promising approach for DC-MRS. We demonstrate the efficacy of MAST on decentralized assignment and navigation (DAN) and decentralized coverage control. Efficiently trained using imitation learning in a centralized setting, the decentralized MAST policy is robust to communication delays, scales to large teams, and performs better than the baselines and other learning-based approaches.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "220",
        "title": "Evolution of Concepts in Language Model Pre-Training",
        "author": [
            "Xuyang Ge",
            "Wentao Shu",
            "Jiaxing Wu",
            "Yunhua Zhou",
            "Zhengfu He",
            "Xipeng Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17196",
        "abstract": "Language models obtain extensive capabilities through pre-training. However, the pre-training process remains a black box. In this work, we track linear interpretable feature evolution across pre-training snapshots using a sparse dictionary learning method called crosscoders. We find that most features begin to form around a specific point, while more complex patterns emerge in later training stages. Feature attribution analyses reveal causal connections between feature evolution and downstream performance. Our feature-level observations are highly consistent with previous findings on Transformer's two-stage learning process, which we term a statistical learning phase and a feature learning phase. Our work opens up the possibility to track fine-grained representation progress during language model learning dynamics.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "221",
        "title": "SignalLLM: A General-Purpose LLM Agent Framework for Automated Signal Processing",
        "author": [
            "Junlong Ke",
            "Qiying Hu",
            "Shenghai Yuan",
            "Yuecong Xu",
            "Jianfei Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17197",
        "abstract": "Modern signal processing (SP) pipelines, whether model-based or data-driven, often constrained by complex and fragmented workflow, rely heavily on expert knowledge and manual engineering, and struggle with adaptability and generalization under limited data. In contrast, Large Language Models (LLMs) offer strong reasoning capabilities, broad general-purpose knowledge, in-context learning, and cross-modal transfer abilities, positioning them as powerful tools for automating and generalizing SP workflows. Motivated by these potentials, we introduce SignalLLM, the first general-purpose LLM-based agent framework for general SP tasks. Unlike prior LLM-based SP approaches that are limited to narrow applications or tricky prompting, SignalLLM introduces a principled, modular architecture. It decomposes high-level SP goals into structured subtasks via in-context learning and domain-specific retrieval, followed by hierarchical planning through adaptive retrieval-augmented generation (RAG) and refinement; these subtasks are then executed through prompt-based reasoning, cross-modal reasoning, code synthesis, model invocation, or data-driven LLM-assisted modeling. Its generalizable design enables the flexible selection of problem solving strategies across different signal modalities, task types, and data conditions. We demonstrate the versatility and effectiveness of SignalLLM through five representative tasks in communication and sensing, such as radar target detection, human activity recognition, and text compression. Experimental results show superior performance over traditional and existing LLM-based methods, particularly in few-shot and zero-shot settings.",
        "tags": [
            "Detection",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "222",
        "title": "Ratatouille: Imitation Learning Ingredients for Real-world Social Robot Navigation",
        "author": [
            "James R. Han",
            "Mithun Vanniasinghe",
            "Hshmat Sahak",
            "Nicholas Rhinehart",
            "Timothy D. Barfoot"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17204",
        "abstract": "Scaling Reinforcement Learning to in-the-wild social robot navigation is both data-intensive and unsafe, since policies must learn through direct interaction and inevitably encounter collisions. Offline Imitation learning (IL) avoids these risks by collecting expert demonstrations safely, training entirely offline, and deploying policies zero-shot. However, we find that naively applying Behaviour Cloning (BC) to social navigation is insufficient; achieving strong performance requires careful architectural and training choices. We present Ratatouille, a pipeline and model architecture that, without changing the data, reduces collisions per meter by 6 times and improves success rate by 3 times compared to naive BC. We validate our approach in both simulation and the real world, where we collected over 11 hours of data on a dense university campus. We further demonstrate qualitative results in a public food court. Our findings highlight that thoughtful IL design, rather than additional data, can substantially improve safety and reliability in real-world social navigation. Video: https://youtu.be/tOdLTXsaYLQ. Code will be released after acceptance.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "223",
        "title": "Conditional Policy Generator for Dynamic Constraint Satisfaction and Optimization",
        "author": [
            "Wook Lee",
            "Frans A. Oliehoek"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17205",
        "abstract": "Leveraging machine learning methods to solve constraint satisfaction problems has shown promising, but they are mostly limited to a static situation where the problem description is completely known and fixed from the beginning. In this work we present a new approach to constraint satisfaction and optimization in dynamically changing environments, particularly when variables in the problem are statistically independent. We frame it as a reinforcement learning problem and introduce a conditional policy generator by borrowing the idea of class conditional generative adversarial networks (GANs). Assuming that the problem includes both static and dynamic constraints, the former are used in a reward formulation to guide the policy training such that it learns to map to a probabilistic distribution of solutions satisfying static constraints from a noise prior, which is similar to a generator in GANs. On the other hand, dynamic constraints in the problem are encoded to different class labels and fed with the input noise. The policy is then simultaneously updated for maximum likelihood of correctly classifying given the dynamic conditions in a supervised manner. We empirically demonstrate a proof-of-principle experiment with a multi-modal constraint satisfaction problem and compare between unconditional and conditional cases.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "224",
        "title": "Point-RTD: Replaced Token Denoising for Pretraining Transformer Models on Point Clouds",
        "author": [
            "Gunner Stone",
            "Youngsook Choi",
            "Alireza Tavakkoli",
            "Ankita Shukla"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17207",
        "abstract": "Pre-training strategies play a critical role in advancing the performance of transformer-based models for 3D point cloud tasks. In this paper, we introduce Point-RTD (Replaced Token Denoising), a novel pretraining strategy designed to improve token robustness through a corruption-reconstruction framework. Unlike traditional mask-based reconstruction tasks that hide data segments for later prediction, Point-RTD corrupts point cloud tokens and leverages a discriminator-generator architecture for denoising. This shift enables more effective learning of structural priors and significantly enhances model performance and efficiency. On the ShapeNet dataset, Point-RTD reduces reconstruction error by over 93% compared to PointMAE, and achieves more than 14x lower Chamfer Distance on the test set. Our method also converges faster and yields higher classification accuracy on ShapeNet, ModelNet10, and ModelNet40 benchmarks, clearly outperforming the baseline Point-MAE framework in every case.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "225",
        "title": "Prompt-Based Simplification for Plain Language using Spanish Language Models",
        "author": [
            "Lourdes Moreno",
            "Jesus M. Sanchez-Gomez",
            "Marco Antonio Sanchez-Escudero",
            "Paloma MartÃ­nez"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17209",
        "abstract": "This paper describes the participation of HULAT-UC3M in CLEARS 2025 Subtask 1: Adaptation of Text to Plain Language (PL) in Spanish. We explored strategies based on models trained on Spanish texts, including a zero-shot configuration using prompt engineering and a fine-tuned version with Low-Rank Adaptation (LoRA). Different strategies were evaluated on representative internal subsets of the training data, using the official task metrics, cosine similarity (SIM) and the FernÃ¡ndez-Huerta readability index (FH) to guide the selection of the optimal model and prompt combination. The final system was selected for its balanced and consistent performance, combining normalization steps, the RigoChat-7B-v2 model, and a dedicated PL-oriented prompt. It ranked first in semantic similarity (SIM = 0.75), however, fourth in readability (FH = 69.72). We also discuss key challenges related to training data heterogeneity and the limitations of current evaluation metrics in capturing both linguistic clarity and content preservation.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "226",
        "title": "Combining Performance and Passivity in Linear Control of Series Elastic Actuators",
        "author": [
            "Shaunak A. Mehta",
            "Dylan P. Losey"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17210",
        "abstract": "When humans physically interact with robots, we need the robots to be both safe and performant. Series elastic actuators (SEAs) fundamentally advance safety by introducing compliant actuation. On the one hand, adding a spring mitigates the impact of accidental collisions between human and robot; but on the other hand, this spring introduces oscillations and fundamentally decreases the robot's ability to perform precise, accurate motions. So how should we trade off between physical safety and performance? In this paper, we enumerate the different linear control and mechanical configurations for series elastic actuators, and explore how each choice affects the rendered compliance, passivity, and tracking performance. While prior works focus on load side control, we find that actuator side control has significant benefits. Indeed, simple PD controllers on the actuator side allow for a much wider range of control gains that maintain safety, and combining these with a damper in the elastic transmission yields high performance. Our simulations and real world experiments suggest that, by designing a system with low physical stiffness and high controller gains, this solution enables accurate performance while also ensuring user safety during collisions.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "227",
        "title": "Virtual Consistency for Audio Editing",
        "author": [
            "Matthieu Cervera",
            "Francesco Paissan",
            "Mirco Ravanelli",
            "Cem Subakan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17219",
        "abstract": "Free-form, text-based audio editing remains a persistent challenge, despite progress in inversion-based neural methods. Current approaches rely on slow inversion procedures, limiting their practicality. We present a virtual-consistency based audio editing system that bypasses inversion by adapting the sampling process of diffusion models. Our pipeline is model-agnostic, requiring no fine-tuning or architectural changes, and achieves substantial speed-ups over recent neural editing baselines. Crucially, it achieves this efficiency without compromising quality, as demonstrated by quantitative benchmarks and a user study involving 16 participants.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "228",
        "title": "MirrorSAM2: Segment Mirror in Videos with Depth Perception",
        "author": [
            "Mingchen Xu",
            "Yukun Lai",
            "Ze Ji",
            "Jing Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17220",
        "abstract": "This paper presents MirrorSAM2, the first framework that adapts Segment Anything Model 2 (SAM2) to the task of RGB-D video mirror segmentation. MirrorSAM2 addresses key challenges in mirror detection, such as reflection ambiguity and texture confusion, by introducing four tailored modules: a Depth Warping Module for RGB and depth alignment, a Depth-guided Multi-Scale Point Prompt Generator for automatic prompt generation, a Frequency Detail Attention Fusion Module to enhance structural boundaries, and a Mirror Mask Decoder with a learnable mirror token for refined segmentation. By fully leveraging the complementarity between RGB and depth, MirrorSAM2 extends SAM2's capabilities to the prompt-free setting. To our knowledge, this is the first work to enable SAM2 for automatic video mirror segmentation. Experiments on the VMD and DVMD benchmark demonstrate that MirrorSAM2 achieves SOTA performance, even under challenging conditions such as small mirrors, weak boundaries, and strong reflections.",
        "tags": [
            "Detection",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "229",
        "title": "Causal Representation Learning from Multimodal Clinical Records under Non-Random Modality Missingness",
        "author": [
            "Zihan Liang",
            "Ziwen Pan",
            "Ruoxuan Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17228",
        "abstract": "Clinical notes contain rich patient information, such as diagnoses or medications, making them valuable for patient representation learning. Recent advances in large language models have further improved the ability to extract meaningful representations from clinical texts. However, clinical notes are often missing. For example, in our analysis of the MIMIC-IV dataset, 24.5% of patients have no available discharge summaries. In such cases, representations can be learned from other modalities such as structured data, chest X-rays, or radiology reports. Yet the availability of these modalities is influenced by clinical decision-making and varies across patients, resulting in modality missing-not-at-random (MMNAR) patterns. We propose a causal representation learning framework that leverages observed data and informative missingness in multimodal clinical records. It consists of: (1) an MMNAR-aware modality fusion component that integrates structured data, imaging, and text while conditioning on missingness patterns to capture patient health and clinician-driven assignment; (2) a modality reconstruction component with contrastive learning to ensure semantic sufficiency in representation learning; and (3) a multitask outcome prediction model with a rectifier that corrects for residual bias from specific modality observation patterns. Comprehensive evaluations across MIMIC-IV and eICU show consistent gains over the strongest baselines, achieving up to 13.8% AUC improvement for hospital readmission and 13.1% for ICU admission.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "230",
        "title": "DT-NeRF: A Diffusion and Transformer-Based Optimization Approach for Neural Radiance Fields in 3D Reconstruction",
        "author": [
            "Bo Liu",
            "Runlong Li",
            "Li Zhou",
            "Yan Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17232",
        "abstract": "This paper proposes a Diffusion Model-Optimized Neural Radiance Field (DT-NeRF) method, aimed at enhancing detail recovery and multi-view consistency in 3D scene reconstruction. By combining diffusion models with Transformers, DT-NeRF effectively restores details under sparse viewpoints and maintains high accuracy in complex geometric scenes. Experimental results demonstrate that DT-NeRF significantly outperforms traditional NeRF and other state-of-the-art methods on the Matterport3D and ShapeNet datasets, particularly in metrics such as PSNR, SSIM, Chamfer Distance, and Fidelity. Ablation experiments further confirm the critical role of the diffusion and Transformer modules in the model's performance, with the removal of either module leading to a decline in performance. The design of DT-NeRF showcases the synergistic effect between modules, providing an efficient and accurate solution for 3D scene reconstruction. Future research may focus on further optimizing the model, exploring more advanced generative models and network architectures to enhance its performance in large-scale dynamic scenes.",
        "tags": [
            "3D",
            "Diffusion",
            "NeRF",
            "Transformer"
        ]
    },
    {
        "id": "231",
        "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with RoE",
        "author": [
            "Soheil Zibakhsh",
            "Mohammad Samragh",
            "Kumari Nishu",
            "Lauren Hannah",
            "Arnav Kundu",
            "Minsik Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17238",
        "abstract": "The generation quality of large language models (LLMs) is often improved by utilizing inference-time sequence-level scaling methods (e.g., Chain-of-Thought). We introduce hyper-parallel scaling, a complementary framework that improves prediction quality at the token level. Hyper-parallel scaling computes and aggregates multiple output proposals for a single token from the model. We implement this concept in Mixture-of-Experts (MoE) models, which we refer to as Roster of Experts (RoE). RoE is a training-free inference algorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects controlled stochasticity into the expert routing mechanism, enabling it to sample multiple diverse experts for each token and aggregate their outputs for a more accurate final http://prediction.To overcome the computational cost, we introduce an efficient batching strategy and a specialized KV-caching mechanism that minimizes compute and memory overhead. For example, RoE enables a 7B MoE model to match the performance of a 10.5B MoE model while using 30% less compute for inference. These gains are achieved without any fine-tuning of model parameters.",
        "tags": [
            "CoT",
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "232",
        "title": "Can Agents Judge Systematic Reviews Like Humans? Evaluating SLRs with LLM-based Multi-Agent System",
        "author": [
            "Abdullah Mushtaq",
            "Muhammad Rafay Naeem",
            "Ibrahim Ghaznavi",
            "Alaa Abd-alrazaq",
            "Aliya Tabassum",
            "Junaid Qadir"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17240",
        "abstract": "Systematic Literature Reviews (SLRs) are foundational to evidence-based research but remain labor-intensive and prone to inconsistency across disciplines. We present an LLM-based SLR evaluation copilot built on a Multi-Agent System (MAS) architecture to assist researchers in assessing the overall quality of the systematic literature reviews. The system automates protocol validation, methodological assessment, and topic relevance checks using a scholarly database. Unlike conventional single-agent methods, our design integrates a specialized agentic approach aligned with PRISMA guidelines to support more structured and interpretable evaluations. We conducted an initial study on five published SLRs from diverse domains, comparing system outputs to expert-annotated PRISMA scores, and observed 84% agreement. While early results are promising, this work represents a first step toward scalable and accurate NLP-driven systems for interdisciplinary workflows and reveals their capacity for rigorous, domain-agnostic knowledge aggregation to streamline the review process.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "233",
        "title": "TraceHiding: Scalable Machine Unlearning for Mobility Data",
        "author": [
            "Ali Faraji",
            "Manos Papagelis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17241",
        "abstract": "This work introduces TraceHiding, a scalable, importance-aware machine unlearning framework for mobility trajectory data. Motivated by privacy regulations such as GDPR and CCPA granting users \"the right to be forgotten,\" TraceHiding removes specified user trajectories from trained deep models without full retraining. It combines a hierarchical data-driven importance scoring scheme with teacher-student distillation. Importance scores--computed at token, trajectory, and user levels from statistical properties (coverage diversity, entropy, length)--quantify each training sample's impact, enabling targeted forgetting of high-impact data while preserving common patterns. The student model retains knowledge on remaining data and unlearns targeted trajectories through an importance-weighted loss that amplifies forgetting signals for unique samples and attenuates them for frequent ones. We validate on Trajectory--User Linking (TUL) tasks across three real-world higher-order mobility datasets (HO-Rome, HO-Geolife, HO-NYC) and multiple architectures (GRU, LSTM, BERT, ModernBERT, GCN-TULHOR), against strong unlearning baselines including SCRUB, NegGrad, NegGrad+, Bad-T, and Finetuning. Experiments under uniform and targeted user deletion show TraceHiding, especially its entropy-based variant, achieves superior unlearning accuracy, competitive membership inference attack (MIA) resilience, and up to 40\\times speedup over retraining with minimal test accuracy loss. Results highlight robustness to adversarial deletion of high-information users and consistent performance across models. To our knowledge, this is the first systematic study of machine unlearning for trajectory data, providing a reproducible pipeline with public code and preprocessing tools.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "234",
        "title": "Scalable Multi Agent Diffusion Policies for Coverage Control",
        "author": [
            "Frederic Vatnsdal",
            "Romina Garcia Camargo",
            "Saurav Agarwal",
            "Alejandro Ribeiro"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17244",
        "abstract": "We propose MADP, a novel diffusion-model-based approach for collaboration in decentralized robot swarms. MADP leverages diffusion models to generate samples from complex and high-dimensional action distributions that capture the interdependencies between agents' actions. Each robot conditions policy sampling on a fused representation of its own observations and perceptual embeddings received from peers. To evaluate this approach, we task a team of holonomic robots piloted by MADP to address coverage control-a canonical multi agent navigation problem. The policy is trained via imitation learning from a clairvoyant expert on the coverage control problem, with the diffusion process parameterized by a spatial transformer architecture to enable decentralized inference. We evaluate the system under varying numbers, locations, and variances of importance density functions, capturing the robustness demands of real-world coverage tasks. Experiments demonstrate that our model inherits valuable properties from diffusion models, generalizing across agent densities and environments, and consistently outperforming state-of-the-art baselines.",
        "tags": [
            "Diffusion",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "235",
        "title": "SPFSplatV2: Efficient Self-Supervised Pose-Free 3D Gaussian Splatting from Sparse Views",
        "author": [
            "Ranran Huang",
            "Krystian Mikolajczyk"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17246",
        "abstract": "We introduce SPFSplatV2, an efficient feed-forward framework for 3D Gaussian splatting from sparse multi-view images, requiring no ground-truth poses during training and inference. It employs a shared feature extraction backbone, enabling simultaneous prediction of 3D Gaussian primitives and camera poses in a canonical space from unposed inputs. A masked attention mechanism is introduced to efficiently estimate target poses during training, while a reprojection loss enforces pixel-aligned Gaussian primitives, providing stronger geometric constraints. We further demonstrate the compatibility of our training framework with different reconstruction architectures, resulting in two model variants. Remarkably, despite the absence of pose supervision, our method achieves state-of-the-art performance in both in-domain and out-of-domain novel view synthesis, even under extreme viewpoint changes and limited image overlap, and surpasses recent methods that rely on geometric supervision for relative pose estimation. By eliminating dependence on ground-truth poses, our method offers the scalability to leverage larger and more diverse datasets. Code and pretrained models will be available on our project page: https://ranrhuang.github.io/spfsplatv2/.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Pose Estimation"
        ]
    },
    {
        "id": "236",
        "title": "Extending Automatic Machine Translation Evaluation to Book-Length Documents",
        "author": [
            "Kuang-Da Wang",
            "Shuoyang Ding",
            "Chao-Han Huck Yang",
            "Ping-Chun Hsieh",
            "Wen-Chih Peng",
            "Vitaly Lavrukhin",
            "Boris Ginsburg"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17249",
        "abstract": "Despite Large Language Models (LLMs) demonstrating superior translation performance and long-context capabilities, evaluation methodologies remain constrained to sentence-level assessment due to dataset limitations, token number restrictions in metrics, and rigid sentence boundary requirements. We introduce SEGALE, an evaluation scheme that extends existing automatic metrics to long-document translation by treating documents as continuous text and applying sentence segmentation and alignment methods. Our approach enables previously unattainable document-level evaluation, handling translations of arbitrary length generated with document-level prompts while accounting for under-/over-translations and varied sentence boundaries. Experiments show our scheme significantly outperforms existing long-form document evaluation schemes, while being comparable to evaluations performed with groundtruth sentence alignments. Additionally, we apply our scheme to book-length texts and newly demonstrate that many open-weight LLMs fail to effectively translate documents at their reported maximum context lengths.",
        "tags": [
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "237",
        "title": "Graph Signal Generative Diffusion Models",
        "author": [
            "Yigit Berkay Uslu",
            "Samar Hadou",
            "Sergio Rozada",
            "Shirin Saeedi Bidokhti",
            "Alejandro Ribeiro"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17250",
        "abstract": "We introduce U-shaped encoder-decoder graph neural networks (U-GNNs) for stochastic graph signal generation using denoising diffusion processes. The architecture learns node features at different resolutions with skip connections between the encoder and decoder paths, analogous to the convolutional U-Net for image generation. The U-GNN is prominent for a pooling operation that leverages zero-padding and avoids arbitrary graph coarsening, with graph convolutions layered on top to capture local dependencies. This technique permits learning feature embeddings for sampled nodes at deeper levels of the architecture that remain convolutional with respect to the original graph. Applied to stock price prediction -- where deterministic forecasts struggle to capture uncertainties and tail events that are paramount -- we demonstrate the effectiveness of the diffusion model in probabilistic forecasting of stock prices.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "238",
        "title": "Mind the Gap: Comparing Model- vs Agentic-Level Red Teaming with Action-Graph Observability on GPT-OSS-20B",
        "author": [
            "Ilham Wicaksono",
            "Zekun Wu",
            "Rahul Patel",
            "Theo King",
            "Adriano Koshiyama",
            "Philip Treleaven"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17259",
        "abstract": "As the industry increasingly adopts agentic AI systems, understanding their unique vulnerabilities becomes critical. Prior research suggests that security flaws at the model level do not fully capture the risks present in agentic deployments, where models interact with tools and external environments. This paper investigates this gap by conducting a comparative red teaming analysis of GPT-OSS-20B, a 20-billion parameter open-source model. Using our observability framework AgentSeer to deconstruct agentic systems into granular actions and components, we apply iterative red teaming attacks with harmful objectives from HarmBench at two distinct levels: the standalone model and the model operating within an agentic loop. Our evaluation reveals fundamental differences between model level and agentic level vulnerability profiles. Critically, we discover the existence of agentic-only vulnerabilities, attack vectors that emerge exclusively within agentic execution contexts while remaining inert against standalone models. Agentic level iterative attacks successfully compromise objectives that completely failed at the model level, with tool-calling contexts showing 24\\% higher vulnerability than non-tool contexts. Conversely, certain model-specific exploits work exclusively at the model level and fail when transferred to agentic contexts, demonstrating that standalone model vulnerabilities do not always generalize to deployed systems.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "239",
        "title": "Learning and Optimization with 3D Orientations",
        "author": [
            "Alexandros Ntagkas",
            "Constantinos Tsakonas",
            "Chairi Kiourt",
            "Konstantinos Chatzilygeroudis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17274",
        "abstract": "There exist numerous ways of representing 3D orientations. Each representation has both limitations and unique features. Choosing the best representation for one task is often a difficult chore, and there exist conflicting opinions on which representation is better suited for a set of family of tasks. Even worse, when dealing with scenarios where we need to learn or optimize functions with orientations as inputs and/or outputs, the set of possibilities (representations, loss functions, etc.) is even larger and it is not easy to decide what is best for each scenario. In this paper, we attempt to a) present clearly, concisely and with unified notation all available representations, and \"tricks\" related to 3D orientations (including Lie Group algebra), and b) benchmark them in representative scenarios. The first part feels like it is missing from the robotics literature as one has to read many different textbooks and papers in order have a concise and clear understanding of all possibilities, while the benchmark is necessary in order to come up with recommendations based on empirical evidence. More precisely, we experiment with the following settings that attempt to cover most widely used scenarios in robotics: 1) direct optimization, 2) imitation/supervised learning with a neural network controller, 3) reinforcement learning, and 4) trajectory optimization using differential dynamic programming. We finally provide guidelines depending on the scenario, and make available a reference implementation of all the orientation math described.",
        "tags": [
            "3D",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "240",
        "title": "Probabilistic Token Alignment for Large Language Model Fusion",
        "author": [
            "Runjia Zeng",
            "James Chenhao Liang",
            "Cheng Han",
            "Zhiwen Cao",
            "Jiahao Liu",
            "Xiaojun Quan",
            "Yingjie Victor Chen",
            "Lifu Huang",
            "Tong Geng",
            "Qifan Wang",
            "Dongfang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17276",
        "abstract": "Training large language models (LLMs) from scratch can yield models with unique functionalities and strengths, but it is costly and often leads to redundant capabilities. A more cost-effective alternative is to fuse existing pre-trained LLMs with different architectures into a more powerful model. However, a key challenge in existing model fusion is their dependence on manually predefined vocabulary alignment, which may not generalize well across diverse contexts, leading to performance degradation in several evaluation. To solve this, we draw inspiration from distribution learning and propose the probabilistic token alignment method as a general and soft mapping for alignment, named as PTA-LLM. Our approach innovatively reformulates token alignment into a classic mathematical problem: optimal transport, seamlessly leveraging distribution-aware learning to facilitate more coherent model fusion. Apart from its inherent generality, PTA-LLM exhibits interpretability from a distributional perspective, offering insights into the essence of the token alignment. Empirical results demonstrate that probabilistic token alignment enhances the target model's performance across multiple capabilities. Our code is avaliable at https://runjia.tech/neurips_pta-llm/.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "241",
        "title": "Task-Oriented Communications for 3D Scene Representation: Balancing Timeliness and Fidelity",
        "author": [
            "Xiangmin Xu",
            "Zhen Meng",
            "Kan Chen",
            "Jiaming Yang",
            "Emma Li",
            "Philip G. Zhao",
            "David Flynn"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17282",
        "abstract": "Real-time Three-dimensional (3D) scene representation is a foundational element that supports a broad spectrum of cutting-edge applications, including digital manufacturing, Virtual, Augmented, and Mixed Reality (VR/AR/MR), and the emerging metaverse. Despite advancements in real-time communication and computing, achieving a balance between timeliness and fidelity in 3D scene representation remains a challenge. This work investigates a wireless network where multiple homogeneous mobile robots, equipped with cameras, capture an environment and transmit images to an edge server over channels for 3D representation. We propose a contextual-bandit Proximal Policy Optimization (PPO) framework incorporating both Age of Information (AoI) and semantic information to optimize image selection for representation, balancing data freshness and representation quality. Two policies -- the $\\omega$-threshold and $\\omega$-wait policies -- together with two benchmark methods are evaluated, timeliness embedding and weighted sum, on standard datasets and baseline 3D scene representation models. Experimental results demonstrate improved representation fidelity while maintaining low latency, offering insight into the model's decision-making process. This work advances real-time 3D scene representation by optimizing the trade-off between timeliness and fidelity in dynamic environments.",
        "tags": [
            "3D",
            "PPO"
        ]
    },
    {
        "id": "242",
        "title": "Automated Facility Enumeration for Building Compliance Checking using Door Detection and Large Language Models",
        "author": [
            "Licheng Zhan",
            "Bach Le",
            "Naveed Akhtar",
            "Tuan Ngo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17283",
        "abstract": "Building compliance checking (BCC) is a critical process for ensuring that constructed facilities meet regulatory standards. A core component of BCC is the accurate enumeration of facility types and their spatial distribution. Despite its importance, this problem has been largely overlooked in the literature, posing a significant challenge for BCC and leaving a critical gap in existing workflows. Performing this task manually is time-consuming and labor-intensive. Recent advances in large language models (LLMs) offer new opportunities to enhance automation by combining visual recognition with reasoning capabilities. In this paper, we introduce a new task for BCC: automated facility enumeration, which involves validating the quantity of each facility type against statutory requirements. To address it, we propose a novel method that integrates door detection with LLM-based reasoning. We are the first to apply LLMs to this task and further enhance their performance through a Chain-of-Thought (CoT) pipeline. Our approach generalizes well across diverse datasets and facility types. Experiments on both real-world and synthetic floor plan data demonstrate the effectiveness and robustness of our method.",
        "tags": [
            "CoT",
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "243",
        "title": "Event-Based Visual Teach-and-Repeat via Fast Fourier-Domain Cross-Correlation",
        "author": [
            "Gokul B. Nair",
            "Alejandro Fontan",
            "Michael Milford",
            "Tobias Fischer"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17287",
        "abstract": "Visual teach-and-repeat navigation enables robots to autonomously traverse previously demonstrated paths by comparing current sensory input with recorded trajectories. However, conventional frame-based cameras fundamentally limit system responsiveness: their fixed frame rates (typically 30-60 Hz) create inherent latency between environmental changes and control responses. Here we present the first event-camera-based visual teach-and-repeat system. To achieve this, we develop a frequency-domain cross-correlation framework that transforms the event stream matching problem into computationally efficient Fourier space multiplications, capable of exceeding 300Hz processing rates, an order of magnitude faster than frame-based approaches. By exploiting the binary nature of event frames and applying image compression techniques, we further enhance the computational speed of the cross-correlation process without sacrificing localization accuracy. Extensive experiments using a Prophesee EVK4 HD event camera mounted on an AgileX Scout Mini robot demonstrate successful autonomous navigation across 4000+ meters of indoor and outdoor trajectories. Our system achieves ATEs below 24 cm while maintaining consistent high-frequency control updates. Our evaluations show that our approach achieves substantially higher update rates compared to conventional frame-based systems, underscoring the practical viability of event-based perception for real-time robotic navigation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "244",
        "title": "GraphWeave: Interpretable and Robust Graph Generation via Random Walk Trajectories",
        "author": [
            "Rahul Nandakumar",
            "Deepayan Chakrabarti"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17291",
        "abstract": "Given a set of graphs from some unknown family, we want to generate new graphs from that family. Recent methods use diffusion on either graph embeddings or the discrete space of nodes and edges. However, simple changes to embeddings (say, adding noise) can mean uninterpretable changes in the graph. In discrete-space diffusion, each step may add or remove many nodes/edges. It is hard to predict what graph patterns we will observe after many diffusion steps. Our proposed method, called GraphWeave, takes a different approach. We separate pattern generation and graph construction. To find patterns in the training graphs, we see how they transform vectors during random walks. We then generate new graphs in two steps. First, we generate realistic random walk \"trajectories\" which match the learned patterns. Then, we find the optimal graph that fits these trajectories. The optimization infers all edges jointly, which improves robustness to errors. On four simulated and five real-world benchmark datasets, GraphWeave outperforms existing methods. The most significant differences are on large-scale graph structures such as PageRank, cuts, communities, degree distributions, and flows. GraphWeave is also 10x faster than its closest competitor. Finally, GraphWeave is simple, needing only a transformer and standard optimizers.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "245",
        "title": "TextCrafter: Optimization-Calibrated Noise for Defending Against Text Embedding Inversion",
        "author": [
            "Duoxun Tang",
            "Xinhang Jiang",
            "Jiajun Niu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17302",
        "abstract": "Text embedding inversion attacks reconstruct original sentences from latent representations, posing severe privacy threats in collaborative inference and edge computing. We propose TextCrafter, an optimization-based adversarial perturbation mechanism that combines RL learned, geometry aware noise injection orthogonal to user embeddings with cluster priors and PII signal guidance to suppress inversion while preserving task utility. Unlike prior defenses either non learnable or agnostic to perturbation direction, TextCrafter provides a directional protective policy that balances privacy and utility. Under strong privacy setting, TextCrafter maintains 70 percentage classification accuracy on four datasets and consistently outperforms Gaussian/LDP baselines across lower privacy budgets, demonstrating a superior privacy utility trade off.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "246",
        "title": "$i$MIND: Insightful Multi-subject Invariant Neural Decoding",
        "author": [
            "Zixiang Yin",
            "Jiarui Li",
            "Zhengming Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17313",
        "abstract": "Decoding visual signals holds the tantalizing potential to unravel the complexities of cognition and perception. While recent studies have focused on reconstructing visual stimuli from neural recordings to bridge brain activity with visual imagery, existing methods offer limited insights into the underlying mechanisms of visual processing in the brain. To mitigate this gap, we present an \\textit{i}nsightful \\textbf{M}ulti-subject \\textbf{I}nvariant \\textbf{N}eural \\textbf{D}ecoding ($i$MIND) model, which employs a novel dual-decoding framework--both biometric and semantic decoding--to offer neural interpretability in a data-driven manner and deepen our understanding of brain-based visual functionalities. Our $i$MIND model operates through three core steps: establishing a shared neural representation space across subjects using a ViT-based masked autoencoder, disentangling neural features into complementary subject-specific and object-specific components, and performing dual decoding to support both biometric and semantic classification tasks. Experimental results demonstrate that $i$MIND achieves state-of-the-art decoding performance with minimal scalability limitations. Furthermore, $i$MIND empirically generates voxel-object activation fingerprints that reveal object-specific neural patterns and enable investigation of subject-specific variations in attention to identical stimuli. These findings provide a foundation for more interpretable and generalizable subject-invariant neural decoding, advancing our understanding of the voxel semantic selectivity as well as the neural vision processing dynamics.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "247",
        "title": "Clotho: Measuring Task-Specific Pre-Generation Test Adequacy for LLM Inputs",
        "author": [
            "Juyeon Yoon",
            "Somin Kim",
            "Robert Feldt",
            "Shin Yoo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17314",
        "abstract": "Software increasingly relies on the emergent capabilities of Large Language Models (LLMs), from natural language understanding to program analysis and generation. Yet testing them on specific tasks remains difficult and costly: many prompts lack ground truth, forcing reliance on human judgment, while existing uncertainty and adequacy measures typically require full inference. A key challenge is to assess input adequacy in a way that reflects the demands of the task, ideally before even generating any output. We introduce CLOTHO, a task-specific, pre-generation adequacy measure that estimates input difficulty directly from hidden LLM states. Given a large pool of unlabelled inputs for a specific task, CLOTHO uses a Gaussian Mixture Model (GMM) to adaptively sample the most informative cases for human labelling. Based on this reference set the GMM can then rank unseen inputs by their likelihood of failure. In our empirical evaluation across eight benchmark tasks and three open-weight LLMs, CLOTHO can predict failures with a ROC-AUC of 0.716, after labelling reference sets that are on average only 5.4% of inputs. It does so without generating any outputs, thereby reducing costs compared to existing uncertainty measures. Comparison of CLOTHO and post-generation uncertainty measures shows that the two approaches complement each other. Crucially, we show that adequacy scores learnt from open-weight LLMs transfer effectively to proprietary models, extending the applicability of the approach. When prioritising test inputs for proprietary models, CLOTHO increases the average number of failing inputs from 18.7 to 42.5 out of 100, compared to random prioritisation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "248",
        "title": "Scaling, Simplification, and Adaptation: Lessons from Pretraining on Machine-Translated Text",
        "author": [
            "Dan John Velasco",
            "Matthew Theodore Roque"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17317",
        "abstract": "Most languages lack sufficient data for large-scale monolingual pretraining, creating a \"data wall.\" Multilingual pretraining helps but is limited by language imbalance and the \"curse of multilinguality.\" An alternative is to translate high-resource text with machine translation (MT), which raises three questions: (1) How does MT-derived data scale with model capacity? (2) Can source-side transformations (e.g., simplifying English with an LLM) improve generalization to native text? (3) How well do models pretrained on MT-derived data adapt when continually trained on limited native text? We investigate these questions by translating English into Indonesian and Tamil--two typologically distant, lower-resource languages--and pretraining GPT-2 models (124M-774M) on native or MT-derived corpora from raw and LLM-simplified English. We evaluate cross-entropy loss on native text, along with accuracy on syntactic probes and downstream tasks. Our results show that (1) MT-pretrained models benefit from scaling; (2) source-side simplification harms generalization to native text; and (3) adapting MT-pretrained models on native text often yields better performance than native-only models, even with less native data. However, tasks requiring cultural nuance (e.g., toxicity detection) demand more exposure to native data.",
        "tags": [
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "249",
        "title": "CogAtom: From Cognitive Atoms to Olympiad-level Mathematical Reasoning in Large Language Models",
        "author": [
            "Zhuofan Chen",
            "Jiyuan He",
            "Yichi Zhang",
            "Xing Hu",
            "Haoxing Wen",
            "Jun Bai",
            "Wenge Rong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17318",
        "abstract": "Mathematical reasoning poses significant challenges for Large Language Models (LLMs) due to its demand for multi-step reasoning and abstract conceptual integration. While recent test-time scaling techniques rely heavily on high-quality, challenging problems, the scarcity of Olympiad-level math problems remains a bottleneck. We introduce CogAtom, a novel cognitive atom-based framework for synthesizing mathematically rigorous and cognitively diverse problems. Unlike prior approaches, CogAtom models problem construction as a process of selecting and recombining fundamental reasoning units, cognitive atoms, extracted from human-authored solutions. A diversity-promoting random walk algorithm enables exploration of the cognitive atom space, while a constraint-based recombination mechanism ensures logical soundness and structural validity. The combinatorial nature of the graph structure provides a near-infinite space of reasoning paths, and the walk algorithm systematically explores this space to achieve large-scale synthesis of high-quality problems; meanwhile, by controlling the number of cognitive atoms, we can precisely adjust problem difficulty, ensuring diversity, scalability, and controllability of the generated problems. Experimental results demonstrate that CogAtom outperforms existing methods in accuracy, reasoning depth, and diversity, generating problems that closely match the difficulty of AIME while exceeding it in structural variation. Our work offers a cognitively grounded pathway toward scalable, high-quality math problem http://generation.Our code is publicly available at https://github.com/Icarus-1111/CogAtom.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "250",
        "title": "OpenGVL - Benchmarking Visual Temporal Progress for Data Curation",
        "author": [
            "PaweÅ Budzianowski",
            "Emilia WiÅnios",
            "Gracjan GÃ³ral",
            "Igor Kulakov",
            "Viktor Petrenko",
            "Krzysztof Walas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17321",
        "abstract": "Data scarcity remains one of the most limiting factors in driving progress in robotics. However, the amount of available robotics data in the wild is growing exponentially, creating new opportunities for large-scale data utilization. Reliable temporal task completion prediction could help automatically annotate and curate this data at scale. The Generative Value Learning (GVL) approach was recently proposed, leveraging the knowledge embedded in vision-language models (VLMs) to predict task progress from visual observations. Building upon GVL, we propose OpenGVL, a comprehensive benchmark for estimating task progress across diverse challenging manipulation tasks involving both robotic and human embodiments. We evaluate the capabilities of publicly available open-source foundation models, showing that open-source model families significantly underperform closed-source counterparts, achieving only approximately $70\\%$ of their performance on temporal progress prediction tasks. Furthermore, we demonstrate how OpenGVL can serve as a practical tool for automated data curation and filtering, enabling efficient quality assessment of large-scale robotics datasets. We release the benchmark along with the complete codebase at \\href{http://github.com/budzianowski/opengvl}{OpenGVL}.",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "251",
        "title": "DiffQ: Unified Parameter Initialization for Variational Quantum Algorithms via Diffusion Models",
        "author": [
            "Chi Zhang",
            "Mengxin Zheng",
            "Qian Lou",
            "Fan Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17324",
        "abstract": "Variational Quantum Algorithms (VQAs) are widely used in the noisy intermediate-scale quantum (NISQ) era, but their trainability and performance depend critically on initialization parameters that shape the optimization landscape. Existing machine learning-based initializers achieve state-of-the-art results yet remain constrained to single-task domains and small datasets of only hundreds of samples. We address these limitations by reformulating VQA parameter initialization as a generative modeling problem and introducing DiffQ, a parameter initializer based on the Denoising Diffusion Probabilistic Model (DDPM). To support robust training and evaluation, we construct a dataset of 15,085 instances spanning three domains and five representative tasks. Experiments demonstrate that DiffQ surpasses baselines, reducing initial loss by up to 8.95 and convergence steps by up to 23.4%.",
        "tags": [
            "DDPM",
            "Diffusion"
        ]
    },
    {
        "id": "252",
        "title": "Generalizable End-to-End Tool-Use RL with Synthetic CodeGym",
        "author": [
            "Weihua Du",
            "Hailei Gong",
            "Zhan Ling",
            "Kang Liu",
            "Lingfeng Shen",
            "Xuesong Yao",
            "Yufei Xu",
            "Dingyuan Shi",
            "Yiming Yang",
            "Jiecao Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17325",
        "abstract": "Tool-augmented large language models (LLMs), hereafter LLM agents, leverage external tools to solve diverse tasks and interface with the real world. However, current training practices largely rely on supervised fine-tuning (SFT) over static trajectories or reinforcement learning (RL) on narrow tasks, and generalize poorly beyond development settings, leading to brittleness with new tools and unseen workflows. Because code execution reflects many structures of real-world workflows, coding problems provide a natural basis for building agent training environments. Motivated by this, we introduce CodeGym, a scalable framework that synthesizes diverse, verifiable, and controllable multi-turn tool-use environments for agent RL, enabling LLM agents to explore and master various workflows actively. CodeGym rewrites static coding problems into interactive environments by extracting atomic functions or logic into callable tools, yielding verifiable tasks that span various tool-execution workflows. Models of varying sizes and chain-of-thought configurations, trained in CodeGym, exhibit consistent out-of-distribution generalizability; for example, Qwen2.5-32B-Instruct achieves an absolute accuracy gain of 8.7 points on the OOD benchmark $\\tau$-Bench. These results highlight CodeGym as a step toward scalable general-purpose RL environments that align with real-world agent workflows.",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "253",
        "title": "UIPro: Unleashing Superior Interaction Capability For GUI Agents",
        "author": [
            "Hongxin Li",
            "Jingran Su",
            "Jingfan Chen",
            "Zheng Ju",
            "Yuntao Chen",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17328",
        "abstract": "Building autonomous agents that perceive and operate graphical user interfaces (GUIs) like humans has long been a vision in the field of artificial intelligence. Central to these agents is the capability for GUI interaction, which involves GUI understanding and planning capabilities. Existing methods have tried developing GUI agents based on the multi-modal comprehension ability of vision-language models (VLMs). However, the limited scenario, insufficient size, and heterogeneous action spaces hinder the progress of building generalist GUI agents. To resolve these issues, this paper proposes \\textbf{UIPro}, a novel generalist GUI agent trained with extensive multi-platform and multi-task GUI interaction data, coupled with a unified action space. We first curate a comprehensive dataset encompassing 20.6 million GUI understanding tasks to pre-train UIPro, granting it a strong GUI grounding capability, which is key to downstream GUI agent tasks. Subsequently, we establish a unified action space to harmonize heterogeneous GUI agent task datasets and produce a merged dataset to foster the action prediction ability of UIPro via continued fine-tuning. Experimental results demonstrate UIPro's superior performance across multiple GUI task benchmarks on various platforms, highlighting the effectiveness of our approach.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "254",
        "title": "SmokeSeer: 3D Gaussian Splatting for Smoke Removal and Scene Reconstruction",
        "author": [
            "Neham Jain",
            "Andrew Jong",
            "Sebastian Scherer",
            "Ioannis Gkioulekas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17329",
        "abstract": "Smoke in real-world scenes can severely degrade the quality of images and hamper visibility. Recent methods for image restoration either rely on data-driven priors that are susceptible to hallucinations, or are limited to static low-density smoke. We introduce SmokeSeer, a method for simultaneous 3D scene reconstruction and smoke removal from a video capturing multiple views of a scene. Our method uses thermal and RGB images, leveraging the fact that the reduced scattering in thermal images enables us to see through the smoke. We build upon 3D Gaussian splatting to fuse information from the two image modalities, and decompose the scene explicitly into smoke and non-smoke components. Unlike prior approaches, SmokeSeer handles a broad range of smoke densities and can adapt to temporally varying smoke. We validate our approach on synthetic data and introduce a real-world multi-view smoke dataset with RGB and thermal images. We provide open-source code and data at the project website.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "255",
        "title": "BASFuzz: Towards Robustness Evaluation of LLM-based NLP Software via Automated Fuzz Testing",
        "author": [
            "Mingxuan Xiao",
            "Yan Xiao",
            "Shunhui Ji",
            "Jiahe Tu",
            "Pengcheng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17335",
        "abstract": "Fuzzing has shown great success in evaluating the robustness of intelligent natural language processing (NLP) software. As large language model (LLM)-based NLP software is widely deployed in critical industries, existing methods still face two main challenges: 1 testing methods are insufficiently coupled with the behavioral patterns of LLM-based NLP software; 2 fuzzing capability for the testing scenario of natural language generation (NLG) generally degrades. To address these issues, we propose BASFuzz, an efficient Fuzz testing method tailored for LLM-based NLP software. BASFuzz targets complete test inputs composed of prompts and examples, and uses a text consistency metric to guide mutations of the fuzzing loop, aligning with the behavioral patterns of LLM-based NLP software. A Beam-Annealing Search algorithm, which integrates beam search and simulated annealing, is employed to design an efficient fuzzing loop. In addition, information entropy-based adaptive adjustment and an elitism strategy further enhance fuzzing capability. We evaluate BASFuzz on six datasets in representative scenarios of NLG and natural language understanding (NLU). Experimental results demonstrate that BASFuzz achieves a testing effectiveness of 90.335% while reducing the average time overhead by 2,163.852 seconds compared to the current best baseline, enabling more effective robustness evaluation prior to software deployment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "256",
        "title": "Mano Report",
        "author": [
            "Tianyu Fu",
            "Anyang Su",
            "Chenxu Zhao",
            "Hanning Wang",
            "Minghui Wu",
            "Zhe Yu",
            "Fei Hu",
            "Mingjia Shi",
            "Wei Dong",
            "Jiayao Wang",
            "Yuyang Chen",
            "Ruiyang Yu",
            "Siran Peng",
            "Menglin Li",
            "Nan Huang",
            "Haitian Wei",
            "Jiawei Yu",
            "Yi Xin",
            "Xilin Zhao",
            "Kai Gu",
            "Ping Jiang",
            "Sifan Zhou",
            "Shuo Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17336",
        "abstract": "Graphical user interfaces (GUIs) are the primary medium for human-computer interaction, yet automating GUI interactions remains challenging due to the complexity of visual elements, dynamic environments, and the need for multi-step reasoning. Existing methods based on vision-language models (VLMs) often suffer from limited resolution, domain mismatch, and insufficient sequential decisionmaking capability. To address these issues, we propose Mano, a robust GUI agent built upon a multi-modal foundation model pre-trained on extensive web and computer system data. Our approach integrates a novel simulated environment for high-fidelity data generation, a three-stage training pipeline (supervised fine-tuning, offline reinforcement learning, and online reinforcement learning), and a verification module for error recovery. Mano demonstrates state-of-the-art performance on multiple GUI benchmarks, including Mind2Web and OSWorld, achieving significant improvements in success rate and operational accuracy. Our work provides new insights into the effective integration of reinforcement learning with VLMs for practical GUI agent deployment, highlighting the importance of domain-specific data, iterative training, and holistic reward design.",
        "tags": [
            "RL",
            "VLM"
        ]
    },
    {
        "id": "257",
        "title": "LLaVul: A Multimodal LLM for Interpretable Vulnerability Reasoning about Source Code",
        "author": [
            "Ala Jararweh",
            "Michael Adams",
            "Avinash Sahu",
            "Abdullah Mueen",
            "Afsah Anwar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17337",
        "abstract": "Increasing complexity in software systems places a growing demand on reasoning tools that unlock vulnerabilities manifest in source code. Many current approaches focus on vulnerability analysis as a classifying task, oversimplifying the nuanced and context-dependent real-world scenarios. Even though current code large language models (LLMs) excel in code understanding, they often pay little attention to security-specific reasoning. We propose LLaVul, a multimodal LLM tailored to provide fine-grained reasoning about code through question-answering (QA). Our model is trained to integrate paired code and natural queries into a unified space, enhancing reasoning and context-dependent insights about code vulnerability. To evaluate our model performance, we construct a curated dataset of real-world vulnerabilities paired with security-focused questions and answers. Our model outperforms state-of-the-art general-purpose and code LLMs in the QA and detection tasks. We further explain decision-making by conducting qualitative analysis to highlight capabilities and limitations. By integrating code and QA, LLaVul enables more interpretable and security-focused code understanding.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "258",
        "title": "AIMMerging: Adaptive Iterative Model Merging Using Training Trajectories for Language Model Continual Learning",
        "author": [
            "Yujie Feng",
            "Jian Li",
            "Xiaoyu Dong",
            "Pengfei Xu",
            "Xiaohui Zhou",
            "Yujia Zhang",
            "Zexin LU",
            "Yasha Wang",
            "Alan Zhao",
            "Xu Chu",
            "Xiao-Ming Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17348",
        "abstract": "Continual learning (CL) is essential for deploying large language models (LLMs) in dynamic real-world environments without the need for costly retraining. Recent model merging-based methods have attracted significant attention, but they still struggle to effectively manage the trade-off between learning new knowledge and preventing forgetting, a challenge largely stemming from suboptimal number of merges and merging frequency. In this paper, we introduce Adaptive Iterative Model Merging (AimMerging), a novel CL framework that utilizes learning and forgetting signals from the training trajectory to dynamically monitor the model's training status. Guided by dynamic monitoring, the training trajectory-guided merge controller adaptively determines the timing and frequency of iterative fusion, while the rehearsal-based knowledge fusion module computes the merging weights and executes the fusion. Comprehensive experiments on three CL benchmarks with various model sizes (from 770M to 13B) demonstrate that AimMerging achieves significant performance improvements over existing state-of-the-art methods, with an average relative improvement of 80% and 59% on FWT and BWT, respectively. The source code is provided for reproducibility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "259",
        "title": "DyDexHandover: Human-like Bimanual Dynamic Dexterous Handover using RGB-only Perception",
        "author": [
            "Haoran Zhou",
            "Yangwei You",
            "Shuaijun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17350",
        "abstract": "Dynamic in air handover is a fundamental challenge for dual-arm robots, requiring accurate perception, precise coordination, and natural motion. Prior methods often rely on dynamics models, strong priors, or depth sensing, limiting generalization and naturalness. We present DyDexHandover, a novel framework that employs multi-agent reinforcement learning to train an end to end RGB based policy for bimanual object throwing and catching. To achieve more human-like behavior, the throwing policy is guided by a human policy regularization scheme, encouraging fluid and natural motion, and enhancing the generalization capability of the policy. A dual arm simulation environment was built in Isaac Sim for experimental evaluation. DyDexHandover achieves nearly 99 percent success on training objects and 75 percent on unseen objects, while generating human-like throwing and catching behaviors. To our knowledge, it is the first method to realize dual-arm in-air handover using only raw RGB perception.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "260",
        "title": "CMOS Implementation of Field Programmable Spiking Neural Network for Hardware Reservoir Computing",
        "author": [
            "Ckristian Duran",
            "Nanako Kimura",
            "Zolboo Byambadorj",
            "Tetsuya Iizuka"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17355",
        "abstract": "The increasing complexity and energy demands of large-scale neural networks, such as Deep Neural Networks (DNNs) and Large Language Models (LLMs), challenge their practical deployment in edge applications due to high power consumption, area requirements, and privacy concerns. Spiking Neural Networks (SNNs), particularly in analog implementations, offer a promising low-power alternative but suffer from noise sensitivity and connectivity limitations. This work presents a novel CMOS-implemented field-programmable neural network architecture for hardware reservoir computing. We propose a Leaky Integrate-and-Fire (LIF) neuron circuit with integrated voltage-controlled oscillators (VCOs) and programmable weighted interconnections via an on-chip FPGA framework, enabling arbitrary reservoir configurations. The system demonstrates effective implementation of the FORCE algorithm learning, linear and non-linear memory capacity benchmarks, and NARMA10 tasks, both in simulation and actual chip measurements. The neuron design achieves compact area utilization (around 540 NAND2-equivalent units) and low energy consumption (21.7 pJ/pulse) without requiring ADCs for information readout, making it ideal for system-on-chip integration of reservoir computing. This architecture paves the way for scalable, energy-efficient neuromorphic systems capable of performing real-time learning and inference with high configurability and digital interfacing.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "261",
        "title": "Cronus: Efficient LLM inference on Heterogeneous GPU Clusters via Partially Disaggregated Prefill",
        "author": [
            "Yunzhao Liu",
            "Qiang Xu",
            "Y. Charlie Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17357",
        "abstract": "Efficient LLM inference is critical for real-world applications, especially within heterogeneous GPU clusters commonly found in organizations and on-premise datacenters as GPU architecture rapidly evolves. Current disaggregated prefill strategies, which separate the prefill and decode stages of LLM inference across different GPUs, often suffer from suboptimal performance due to imbalances between GPU capabilities and workload demands. On the other hand, extending conventional data parallelism and pipeline parallelism to heterogeneous setups incurs high inference latencies. To address these challenges, we introduce Cronus, a novel LLM inference system designed to dynamically balance workloads across heterogeneous GPUs using partially disaggregated prefill. Cronus partitions each prefill stage and executes its initial portion on the low-end GPU, while overlapping the remaining prefill and decode stages of earlier requests on the high-end GPU. Extensive evaluations across various high-end and low-end GPU combinations demonstrate that Cronus significantly improves the throughput over disaggregated prefill. It also reduces TTFT P99 and TBT P99 significantly over DP and PP while maintaining similar or better throughput.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "262",
        "title": "MLLM-Driven Semantic Identifier Generation for Generative Cross-Modal Retrieval",
        "author": [
            "Tianyuan Li",
            "Lei Wang",
            "Ahtamjan Ahmat",
            "Yating Yang",
            "Bo Ma",
            "Rui Dong",
            "Bangju Han"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17359",
        "abstract": "Generative cross-modal retrieval, which treats retrieval as a generation task, has emerged as a promising direction with the rise of Multimodal Large Language Models (MLLMs). In this setting, the model responds to a text query by generating an identifier corresponding to the target image. However, existing methods typically rely on manually crafted string IDs, clustering-based labels, or atomic identifiers requiring vocabulary expansion, all of which face challenges in semantic alignment or http://scalability.To address these limitations, we propose a vocabulary-efficient identifier generation framework that prompts MLLMs to generate Structured Semantic Identifiers from image-caption pairs. These identifiers are composed of concept-level tokens such as objects and actions, naturally aligning with the model's generation space without modifying the tokenizer. Additionally, we introduce a Rationale-Guided Supervision Strategy, prompting the model to produce a one-sentence explanation alongside each identifier serves as an auxiliary supervision signal that improves semantic grounding and reduces hallucinations during training.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "263",
        "title": "Asteria: Semantic-Aware Cross-Region Caching for Agentic LLM Tool Access",
        "author": [
            "Chaoyi Ruan",
            "Chao Bi",
            "Kaiwen Zheng",
            "Ziji Shi",
            "Xinyi Wan",
            "Jialin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17360",
        "abstract": "Large Language Model (LLM) agents tackle data-intensive tasks such as deep research and code generation. However, their effectiveness depends on frequent interactions with knowledge sources across remote clouds or regions. Such interactions can create non-trivial latency and cost bottlenecks. Existing caching solutions focus on exact-match queries, limiting their effectiveness for semantic knowledge reuse.\nTo address this challenge, we introduce Asteria, a novel cross-region knowledge caching architecture for LLM agents. At its core are two abstractions: Semantic Element (SE) and Semantic Retrieval Index (Sine). A semantic element captures the semantic embedding representation of an LLM query together with performance-aware metadata such as latency, cost, and staticity. Sine then provides two-stage retrieval: a vector similar index with semantic embedding for fast candidate selection and a lightweight LLM-powered semantic judger for precise validation. Atop these primitives, Asteria builds a new cache interface that includes a new semantic-aware cache hit definition, a cost-efficient eviction policy, and proactive prefetching. To reduce overhead, Asteria co-locates the small LLM judger with the main LLM using adaptive scheduling and resource sharing. Our evaluation demonstrates that Asteria delivers substantial performance improvements without compromising correctness. On representative search workloads, Asteria achieves up to a 3.6$\\times$ increase in throughput by maintaining cache hit rates of over 85%, while preserving accuracy virtually identical to non-cached baselines. Asteria also improves throughput for complex coding tasks by 20%, showcasing its versatility across diverse agentic workloads.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "264",
        "title": "Pre-Trained CNN Architecture for Transformer-Based Image Caption Generation Model",
        "author": [
            "Amanuel Tafese Dufera"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17365",
        "abstract": "Automatic image captioning, a multifaceted task bridging computer vision and natural lan- guage processing, aims to generate descriptive textual content from visual input. While Convolutional Neural Networks (CNNs) and Long Short-Term Memory (LSTM) networks have achieved significant advancements, they present limitations. The inherent sequential nature of RNNs leads to sluggish training and inference times. LSTMs further struggle with retaining information from earlier sequence elements when dealing with very long se- quences. This project presents a comprehensive guide to constructing and comprehending transformer models for image captioning. Transformers employ self-attention mechanisms, capturing both short- and long-range dependencies within the data. This facilitates efficient parallelization during both training and inference phases. We leverage the well-established Transformer architecture, recognized for its effectiveness in managing sequential data, and present a meticulous methodology. Utilizing the Flickr30k dataset, we conduct data pre- processing, construct a model architecture that integrates an EfficientNetB0 CNN for fea- ture extraction, and train the model with attention mechanisms incorporated. Our approach exemplifies the utilization of parallelization for efficient training and inference. You can find the project on GitHub.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "265",
        "title": "Scale-free Characteristics of Multilingual Legal Texts and the Limitations of LLMs",
        "author": [
            "Haoyang Chen",
            "Kumiko Tanaka-Ishii"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17367",
        "abstract": "We present a comparative analysis of text complexity across domains using scale-free metrics. We quantify linguistic complexity via Heaps' exponent $\\beta$ (vocabulary growth), Taylor's exponent $\\alpha$ (word-frequency fluctuation scaling), compression rate $r$ (redundancy), and entropy. Our corpora span three domains: legal documents (statutes, cases, deeds) as a specialized domain, general natural language texts (literature, Wikipedia), and AI-generated (GPT) text. We find that legal texts exhibit slower vocabulary growth (lower $\\beta$) and higher term consistency (higher $\\alpha$) than general texts. Within legal domain, statutory codes have the lowest $\\beta$ and highest $\\alpha$, reflecting strict drafting conventions, while cases and deeds show higher $\\beta$ and lower $\\alpha$. In contrast, GPT-generated text shows the statistics more aligning with general language patterns. These results demonstrate that legal texts exhibit domain-specific structures and complexities, which current generative models do not fully replicate.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "266",
        "title": "SilentStriker:Toward Stealthy Bit-Flip Attacks on Large Language Models",
        "author": [
            "Haotian Xu",
            "Qingsong Peng",
            "Jie Shi",
            "Huadi Zheng",
            "Yu Li",
            "Cheng Zhuo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17371",
        "abstract": "The rapid adoption of large language models (LLMs) in critical domains has spurred extensive research into their security issues. While input manipulation attacks (e.g., prompt injection) have been well studied, Bit-Flip Attacks (BFAs) -- which exploit hardware vulnerabilities to corrupt model parameters and cause severe performance degradation -- have received far less attention. Existing BFA methods suffer from key limitations: they fail to balance performance degradation and output naturalness, making them prone to discovery. In this paper, we introduce SilentStriker, the first stealthy bit-flip attack against LLMs that effectively degrades task performance while maintaining output naturalness. Our core contribution lies in addressing the challenge of designing effective loss functions for LLMs with variable output length and the vast output space. Unlike prior approaches that rely on output perplexity for attack loss formulation, which inevitably degrade output naturalness, we reformulate the attack objective by leveraging key output tokens as targets for suppression, enabling effective joint optimization of attack effectiveness and stealthiness. Additionally, we employ an iterative, progressive search strategy to maximize attack efficacy. Experiments show that SilentStriker significantly outperforms existing baselines, achieving successful attacks without compromising the naturalness of generated text.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "267",
        "title": "Revisiting Vision Language Foundations for No-Reference Image Quality Assessment",
        "author": [
            "Ankit Yadav",
            "Ta Duc Huy",
            "Lingqiao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17374",
        "abstract": "Large-scale vision language pre-training has recently shown promise for no-reference image-quality assessment (NR-IQA), yet the relative merits of modern Vision Transformer foundations remain poorly understood. In this work, we present the first systematic evaluation of six prominent pretrained backbones, CLIP, SigLIP2, DINOv2, DINOv3, Perception, and ResNet, for the task of No-Reference Image Quality Assessment (NR-IQA), each finetuned using an identical lightweight MLP head. Our study uncovers two previously overlooked factors: (1) SigLIP2 consistently achieves strong performance; and (2) the choice of activation function plays a surprisingly crucial role, particularly for enhancing the generalization ability of image quality assessment models. Notably, we find that simple sigmoid activations outperform commonly used ReLU and GELU on several benchmarks. Motivated by this finding, we introduce a learnable activation selection mechanism that adaptively determines the nonlinearity for each channel, eliminating the need for manual activation design, and achieving new state-of-the-art SRCC on CLIVE, KADID10K, and AGIQA3K. Extensive ablations confirm the benefits across architectures and regimes, establishing strong, resource-efficient NR-IQA baselines.",
        "tags": [
            "CLIP",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "268",
        "title": "Robustness of Neurosymbolic Reasoners on First-Order Logic Problems",
        "author": [
            "Hannah Bansal",
            "Kemal Kurniawan",
            "Lea Frermann"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17377",
        "abstract": "Recent trends in NLP aim to improve reasoning capabilities in Large Language Models (LLMs), with key focus on generalization and robustness to variations in tasks. Counterfactual task variants introduce minimal but semantically meaningful changes to otherwise valid first-order logic (FOL) problem instances altering a single predicate or swapping roles of constants to probe whether a reasoning system can maintain logical consistency under perturbation. Previous studies showed that LLMs becomes brittle on counterfactual variations, suggesting that they often rely on spurious surface patterns to generate responses. In this work, we explore if a neurosymbolic (NS) approach that integrates an LLM and a symbolic logical solver could mitigate this problem. Experiments across LLMs of varying sizes show that NS methods are more robust but perform worse overall that purely neural methods. We then propose NSCoT that combines an NS method and Chain-of-Thought (CoT) prompting and demonstrate that while it improves performance, NSCoT still lags behind standard CoT. Our analysis opens research directions for future work.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "269",
        "title": "Correlation or Causation: Analyzing the Causal Structures of LLM and LRM Reasoning Process",
        "author": [
            "Zhizhang FU",
            "Guangsheng Bao",
            "Hongbo Zhang",
            "Chenkai Hu",
            "Yue Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17380",
        "abstract": "LLMs suffer from critical reasoning issues such as unfaithfulness, bias, and inconsistency, since they lack robust causal underpinnings and may rely on superficial correlations rather than genuine understanding. Successive LRMs have emerged as a promising alternative, leveraging advanced training techniques such as reinforcement learning (RL) and distillation to improve task accuracy. However, the impact of these training methods on causality remains largely unexplored. In this study, we conduct a systematic causal analysis on LLMs and LRMs, examining structural causal models (SCMs) of four key variables: problem instruction (Z), thinking process (T), reasoning steps (X), and answer (Y). Our findings reveal that RLVR-trained LRMs exhibit enhanced causal reasoning capabilities, aligning more closely with ideal causal structures, while LLMs and distilled LRMs fail to address causality-related deficiencies. Our further investigation indicates that RLVR reduces spurious correlations and strengthens genuine causal patterns, thereby mitigating unfaithfulness and bias. In addition, our inspection on the dynamics of the RLVR training process observes a high correlation between reduced spurious features and improved causal structures, where the causal relationships consistently improve in the training process. This study contributes to the understanding of causality in reasoning models, highlights the critical role of RLVR in enhancing causal reasoning, and provides insights for designing future AI systems with stronger causal foundations. We release our code and data at https://github.com/Harryking1999/CoT_Causal_Analysis.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "270",
        "title": "Fast Trajectory Planner with a Reinforcement Learning-based Controller for Robotic Manipulators",
        "author": [
            "Yongliang Wang",
            "Hamidreza Kasaei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17381",
        "abstract": "Generating obstacle-free trajectories for robotic manipulators in unstructured and cluttered environments remains a significant challenge. Existing motion planning methods often require additional computational effort to generate the final trajectory by solving kinematic or dynamic equations. This paper highlights the strong potential of model-free reinforcement learning methods over model-based approaches for obstacle-free trajectory planning in joint space. We propose a fast trajectory planning system for manipulators that combines vision-based path planning in task space with reinforcement learning-based obstacle avoidance in joint space. We divide the framework into two key components. The first introduces an innovative vision-based trajectory planner in task space, leveraging the large-scale fast segment anything (FSA) model in conjunction with basis spline (B-spline)-optimized kinodynamic path searching. The second component enhances the proximal policy optimization (PPO) algorithm by integrating action ensembles (AE) and policy feedback (PF), which greatly improve precision and stability in goal-reaching and obstacle avoidance within the joint space. These PPO enhancements increase the algorithm's adaptability across diverse robotic tasks, ensuring consistent execution of commands from the first component by the manipulator, while also enhancing both obstacle avoidance efficiency and reaching accuracy. The experimental results demonstrate the effectiveness of PPO enhancements, as well as simulation-to-simulation (Sim-to-Sim) and simulation-to-reality (Sim-to-Real) transfer, in improving model robustness and planner efficiency in complex scenarios. These enhancements allow the robot to perform obstacle avoidance and real-time trajectory planning in obstructed environments. Project page available at: https://sites.google.com/view/ftp4rm/home",
        "tags": [
            "PPO",
            "RL",
            "Robotics",
            "SAM"
        ]
    },
    {
        "id": "271",
        "title": "FGGS-LiDAR: Ultra-Fast, GPU-Accelerated Simulation from General 3DGS Models to LiDAR",
        "author": [
            "Junzhe Wu",
            "Yufei Jia",
            "Yiyi Yan",
            "Zhixing Chen",
            "Tiao Tan",
            "Zifan Wang",
            "Guangyu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17390",
        "abstract": "While 3D Gaussian Splatting (3DGS) has revolutionized photorealistic rendering, its vast ecosystem of assets remains incompatible with high-performance LiDAR simulation, a critical tool for robotics and autonomous driving. We present \\textbf{FGGS-LiDAR}, a framework that bridges this gap with a truly plug-and-play approach. Our method converts \\textit{any} pretrained 3DGS model into a high-fidelity, watertight mesh without requiring LiDAR-specific supervision or architectural alterations. This conversion is achieved through a general pipeline of volumetric discretization and Truncated Signed Distance Field (TSDF) extraction. We pair this with a highly optimized, GPU-accelerated ray-casting module that simulates LiDAR returns at over 500 FPS. We validate our approach on indoor and outdoor scenes, demonstrating exceptional geometric fidelity; By enabling the direct reuse of 3DGS assets for geometrically accurate depth sensing, our framework extends their utility beyond visualization and unlocks new capabilities for scalable, multimodal simulation. Our open-source implementation is available at https://github.com/TATP-233/FGGS-LiDAR.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Robotics"
        ]
    },
    {
        "id": "272",
        "title": "Program Synthesis via Test-Time Transduction",
        "author": [
            "Kang-il Lee",
            "Jahyun Koo",
            "Seunghyun Yoon",
            "Minbeom Kim",
            "Hyukhun Koh",
            "Dongryeol Lee",
            "Kyomin Jung"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17393",
        "abstract": "We introduce transductive program synthesis, a new formulation of the program synthesis task that explicitly leverages test inputs during synthesis. While prior approaches to program synthesis--whether based on natural language descriptions or input-output examples--typically aim to generalize from training examples, they often struggle with robustness, especially in real-world settings where training examples are limited and test inputs involve various edge cases. To address this, we propose a novel framework that improves robustness by treating synthesis as an active learning over a finite hypothesis class defined by programs' outputs. We use an LLM to predict outputs for selected test inputs and eliminate inconsistent hypotheses, where the inputs are chosen via a greedy maximin algorithm to minimize the number of LLM queries required. We evaluate our approach on two real-world datasets: Playgol, a string transformation benchmark, and MBPP+, a Python code generation benchmark. We demonstrate that our method significantly improves program synthesis in both accuracy and efficiency. We release our code at https://github.com/klee972/SYNTRA.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "273",
        "title": "FinDebate: Multi-Agent Collaborative Intelligence for Financial Analysis",
        "author": [
            "Tianshi Cai",
            "Guanxu Li",
            "Nijia Han",
            "Ce Huang",
            "Zimu Wang",
            "Changyu Zeng",
            "Yuqi Wang",
            "Jingshi Zhou",
            "Haiyang Zhang",
            "Qi Chen",
            "Yushan Pan",
            "Shuihua Wang",
            "Wei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17395",
        "abstract": "We introduce FinDebate, a multi-agent framework for financial analysis, integrating collaborative debate with domain-specific Retrieval-Augmented Generation (RAG). Five specialized agents, covering earnings, market, sentiment, valuation, and risk, run in parallel to synthesize evidence into multi-dimensional insights. To mitigate overconfidence and improve reliability, we introduce a safe debate protocol that enables agents to challenge and refine initial conclusions while preserving coherent recommendations. Experimental results, based on both LLM-based and human evaluations, demonstrate the framework's efficacy in producing high-quality analysis with calibrated confidence levels and actionable investment strategies across multiple time horizons.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "274",
        "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question Answering",
        "author": [
            "Minsoo Kim",
            "Arnav Kundu",
            "Han-Byul Kim",
            "Richa Dixit",
            "Minsik Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17396",
        "abstract": "Recent advances in large language models (LLMs) have extended context lengths, enabling assistants to sustain long histories for coherent, personalized responses. This ability, however, hinges on Key-Value (KV) caching, whose memory grows linearly with dialogue length and quickly dominates under strict resource constraints. An active line of research for reducing this overhead is KV cache compression, which seeks to limit cache size while preserving accuracy. Yet existing methods face two major limitations: (i) evicting entries after full-context prefill causes unbounded peak memory, and (ii) query-dependent eviction narrows the cache to a single query, leading to degraded accuracy in multi-turn conversations. We introduce EpiCache, a training-free KV cache management framework for long conversational question answering (LongConvQA) under fixed memory budgets. EpiCache bounds cache growth through block-wise prefill and preserves topic-relevant context via episodic KV compression, which clusters conversation history into coherent episodes and applies episode-specific KV cache eviction. We further design an adaptive layer-wise budget allocation strategy that measures each layer's sensitivity to eviction and distributes the memory budget across layers accordingly. Across three LongConvQA benchmarks, EpiCache improves accuracy by up to 40% over recent baselines, sustains near-full KV accuracy under 4-6x compression, and reduces latency and memory by up to 2.4x and 3.5x, thereby enabling efficient multi-turn interaction under strict resource constraints.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "275",
        "title": "Diff-GNSS: Diffusion-based Pseudorange Error Estimation",
        "author": [
            "Jiaqi Zhu",
            "Shouyi Lu",
            "Ziyao Li",
            "Guirong Zhuo",
            "Lu Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17397",
        "abstract": "Global Navigation Satellite Systems (GNSS) are vital for reliable urban positioning. However, multipath and non-line-of-sight reception often introduce large measurement errors that degrade accuracy. Learning-based methods for predicting and compensating pseudorange errors have gained traction, but their performance is limited by complex error distributions. To address this challenge, we propose Diff-GNSS, a coarse-to-fine GNSS measurement (pseudorange) error estimation framework that leverages a conditional diffusion model to capture such complex distributions. Firstly, a Mamba-based module performs coarse estimation to provide an initial prediction with appropriate scale and trend. Then, a conditional denoising diffusion layer refines the estimate, enabling fine-grained modeling of pseudorange errors. To suppress uncontrolled generative diversity and achieve controllable synthesis, three key features related to GNSS measurement quality are used as conditions to precisely guide the reverse denoising process. We further incorporate per-satellite uncertainty modeling within the diffusion stage to assess the reliability of the predicted errors. We have collected and publicly released a real-world dataset covering various scenes. Experiments on public and self-collected datasets show that DiffGNSS consistently outperforms state-of-the-art baselines across multiple metrics. To the best of our knowledge, this is the first application of diffusion models to pseudorange error estimation. The proposed diffusion-based refinement module is plug-and-play and can be readily integrated into existing networks to markedly improve estimation accuracy.",
        "tags": [
            "Diffusion",
            "Mamba"
        ]
    },
    {
        "id": "276",
        "title": "DIWALI - Diversity and Inclusivity aWare cuLture specific Items for India: Dataset and Assessment of LLMs for Cultural Text Adaptation in Indian Context",
        "author": [
            "Pramit Sahoo",
            "Maharaj Brahma",
            "Maunendra Sankar Desarkar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17399",
        "abstract": "Large language models (LLMs) are widely used in various tasks and applications. However, despite their wide capabilities, they are shown to lack cultural alignment \\citep{ryan-etal-2024-unintended, alkhamissi-etal-2024-investigating} and produce biased generations \\cite{naous-etal-2024-beer} due to a lack of cultural knowledge and competence. Evaluation of LLMs for cultural awareness and alignment is particularly challenging due to the lack of proper evaluation metrics and unavailability of culturally grounded datasets representing the vast complexity of cultures at the regional and sub-regional levels. Existing datasets for culture specific items (CSIs) focus primarily on concepts at the regional level and may contain false positives. To address this issue, we introduce a novel CSI dataset for Indian culture, belonging to 17 cultural facets. The dataset comprises $\\sim$8k cultural concepts from 36 sub-regions. To measure the cultural competence of LLMs on a cultural text adaptation task, we evaluate the adaptations using the CSIs created, LLM as Judge, and human evaluations from diverse socio-demographic region. Furthermore, we perform quantitative analysis demonstrating selective sub-regional coverage and surface-level adaptations across all considered LLMs. Our dataset is available here: \\href{https://huggingface.co/datasets/nlip/DIWALI}{https://huggingface.co/datasets/nlip/DIWALI}, project webpage\\footnote{\\href{https://nlip-lab.github.io/nlip/publications/diwali/}{https://nlip-lab.github.io/nlip/publications/diwali/}}, and our codebase with model outputs can be found here: \\href{https://github.com/pramitsahoo/culture-evaluation}{https://github.com/pramitsahoo/culture-evaluation}.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "277",
        "title": "Interpreting vision transformers via residual replacement model",
        "author": [
            "Jinyeong Kim",
            "Junhyeok Kim",
            "Yumin Shim",
            "Joohyeok Kim",
            "Sunyoung Jung",
            "Seong Jae Hwang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17401",
        "abstract": "How do vision transformers (ViTs) represent and process the world? This paper addresses this long-standing question through the first systematic analysis of 6.6K features across all layers, extracted via sparse autoencoders, and by introducing the residual replacement model, which replaces ViT computations with interpretable features in the residual stream. Our analysis reveals not only a feature evolution from low-level patterns to high-level semantics, but also how ViTs encode curves and spatial positions through specialized feature types. The residual replacement model scalably produces a faithful yet parsimonious circuit for human-scale interpretability by significantly simplifying the original computations. As a result, this framework enables intuitive understanding of ViT mechanisms. Finally, we demonstrate the utility of our framework in debiasing spurious correlations.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "278",
        "title": "Vision Language Models Are Not (Yet) Spelling Correctors",
        "author": [
            "Junhong Liang",
            "Bojun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17418",
        "abstract": "Spelling correction from visual input poses unique challenges for vision language models (VLMs), as it requires not only detecting but also correcting textual errors directly within images. We present ReViCo (Real Visual Correction), the first benchmark that systematically evaluates VLMs on real-world visual spelling correction across Chinese and English. ReViCo contains naturally occurring errors collected from real-world image data and supports fine-grained evaluation at both image and token levels. Through comprehensive experiments on representative cascaded (Qwen) and native (InternVL) open-source models, as well as closed-source systems (GPT-4o, Claude), we show that current VLMs fall significantly short of human performance, particularly in correction. To address these limitations, we explore two solution paradigms: a Joint OCR-Correction pipeline and a Background Information enhanced approach, both of which yield consistent performance gains. Our analysis highlights fundamental limitations of existing architectures and provides actionable insights for advancing multimodal spelling correction.",
        "tags": [
            "GPT",
            "Qwen",
            "VLM"
        ]
    },
    {
        "id": "279",
        "title": "RealBench: A Chinese Multi-image Understanding Benchmark Close to Real-world Scenarios",
        "author": [
            "Fei Zhao",
            "Chengqiang Lu",
            "Yufan Shen",
            "Qimeng Wang",
            "Yicheng Qian",
            "Haoxin Zhang",
            "Yan Gao",
            "Yi Wu",
            "Yao Hu",
            "Zhen Wu",
            "Shangyu Xing",
            "Xinyu Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17421",
        "abstract": "While various multimodal multi-image evaluation datasets have been emerged, but these datasets are primarily based on English, and there has yet to be a Chinese multi-image dataset. To fill this gap, we introduce RealBench, the first Chinese multimodal multi-image dataset, which contains 9393 samples and 69910 images. RealBench distinguishes itself by incorporating real user-generated content, ensuring high relevance to real-world applications. Additionally, the dataset covers a wide variety of scenes, image resolutions, and image structures, further increasing the difficulty of multi-image understanding. Ultimately, we conduct a comprehensive evaluation of RealBench using 21 multimodal LLMs of different sizes, including closed-source models that support multi-image inputs as well as open-source visual and video models. The experimental results indicate that even the most powerful closed-source models still face challenges when handling multi-image Chinese scenarios. Moreover, there remains a noticeable performance gap of around 71.8\\% on average between open-source visual/video models and closed-source models. These results show that RealBench provides an important research foundation for further exploring multi-image understanding capabilities in the Chinese context.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "280",
        "title": "Methods for Multi-objective Optimization PID Controller for quadrotor UAVs",
        "author": [
            "Andrea Vaiuso",
            "Gabriele Immordino",
            "Ludovica Onofri",
            "Giuliano Coppotelli",
            "Marcello Righi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17423",
        "abstract": "Integrating unmanned aerial vehicles into daily use requires controllers that ensure stable flight, efficient energy use, and reduced noise. Proportional integral derivative controllers remain standard but are highly sensitive to gain selection, with manual tuning often yielding suboptimal trade-offs. This paper studies different optimization techniques for the automated tuning of quadrotor proportional integral derivative gains under a unified simulation that couples a blade element momentum based aerodynamic model with a fast deep neural network surrogate, six degrees of freedom rigid body dynamics, turbulence, and a data driven acoustic surrogate model that predicts third octave spectra and propagates them to ground receivers. We compare three families of gradient-free optimizers: metaheuristics, Bayesian optimization, and deep reinforcement learning. Candidate controllers are evaluated using a composite cost function that incorporates multiple metrics, such as noise footprint and power consumption, simultaneously. Metaheuristics improve performance consistently, with Grey Wolf Optimization producing optimal results. Bayesian optimization is sample efficient but carries higher per iteration overhead and depends on the design domain. The reinforcement learning agents do not surpass the baseline in the current setup, suggesting the problem formulation requires further refinement. On unseen missions the best tuned controller maintains accurate tracking while reducing oscillations, power demand, and acoustic emissions. These results show that noise aware proportional integral derivative tuning through black box search can deliver quieter and more efficient flight without hardware changes.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "281",
        "title": "Evaluating Multimodal Large Language Models with Daily Composite Tasks in Home Environments",
        "author": [
            "Zhenliang Zhang",
            "Yuxi Wang",
            "Hongzhao Xie",
            "Shiyun Zhao",
            "Mingyuan Liu",
            "Yujie Lu",
            "Xinyi He",
            "Zhenku Cheng",
            "Yujia Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17425",
        "abstract": "A key feature differentiating artificial general intelligence (AGI) from traditional AI is that AGI can perform composite tasks that require a wide range of capabilities. Although embodied agents powered by multimodal large language models (MLLMs) offer rich perceptual and interactive capabilities, it remains largely unexplored whether they can solve composite tasks. In the current work, we designed a set of composite tasks inspired by common daily activities observed in early childhood development. Within a dynamic and simulated home environment, these tasks span three core domains: object understanding, spatial intelligence, and social activity. We evaluated 17 leading proprietary and open-source MLLMs on these tasks. The results consistently showed poor performance across all three domains, indicating a substantial gap between current capabilities and general intelligence requirements. Together, our tasks offer a preliminary framework for evaluating the general capabilities of embodied agents, marking an early but significant step toward the development of embodied MLLMs and their real-world deployment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "282",
        "title": "Single-Image Depth from Defocus with Coded Aperture and Diffusion Posterior Sampling",
        "author": [
            "Hodaka Kawachi",
            "Jose Reinaldo Cunha Santos A. V. Silva Neto",
            "Yasushi Yagi",
            "Hajime Nagahara",
            "Tomoya Nakamura"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17427",
        "abstract": "We propose a single-snapshot depth-from-defocus (DFD) reconstruction method for coded-aperture imaging that replaces hand-crafted priors with a learned diffusion prior used purely as regularization. Our optimization framework enforces measurement consistency via a differentiable forward model while guiding solutions with the diffusion prior in the denoised image domain, yielding higher accuracy and stability than clas- sical optimization. Unlike U-Net-style regressors, our approach requires no paired defocus-RGBD training data and does not tie training to a specific camera configuration. Experiments on comprehensive simulations and a prototype camera demonstrate consistently strong RGBD reconstructions across noise levels, outperforming both U-Net baselines and a classical coded- aperture DFD method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "283",
        "title": "QWHA: Quantization-Aware Walsh-Hadamard Adaptation for Parameter-Efficient Fine-Tuning on Large Language Models",
        "author": [
            "Hyesung Jeon",
            "Seojune Lee",
            "Beomseok Kang",
            "Yulhwa Kim",
            "Jae-Joon Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17428",
        "abstract": "The demand for efficient deployment of large language models (LLMs) has driven interest in quantization, which reduces inference cost, and parameter-efficient fine-tuning (PEFT), which lowers training overhead. This motivated the development of quantization-aware PEFT to produce accurate yet efficient quantized models. In this setting, reducing quantization error prior to fine-tuning is crucial for achieving high model accuracy. However, existing methods that rely on low-rank adaptation suffer from limited representational capacity. Recent Fourier-related transform (FT)-based adapters offer greater representational power than low-rank adapters, but their direct integration into quantized models often results in ineffective error reduction and increased computational overhead. To overcome these limitations, we propose QWHA, a method that integrates FT-based adapters into quantized models by employing the Walsh-Hadamard Transform (WHT) as the transform kernel, together with a novel adapter initialization scheme incorporating adaptive parameter selection and value refinement. We demonstrate that QWHA effectively mitigates quantization errors while facilitating fine-tuning, and that its design substantially reduces computational cost. Experimental results show that QWHA consistently outperforms baselines in low-bit quantization accuracy and achieves significant training speedups over existing FT-based adapters. The code is available at https://github.com/vantaa89/qwha.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "284",
        "title": "EmbodiedSplat: Personalized Real-to-Sim-to-Real Navigation with Gaussian Splats from a Mobile Device",
        "author": [
            "Gunjan Chhablani",
            "Xiaomeng Ye",
            "Muhammad Zubair Irshad",
            "Zsolt Kira"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17430",
        "abstract": "The field of Embodied AI predominantly relies on simulation for training and evaluation, often using either fully synthetic environments that lack photorealism or high-fidelity real-world reconstructions captured with expensive hardware. As a result, sim-to-real transfer remains a major challenge. In this paper, we introduce EmbodiedSplat, a novel approach that personalizes policy training by efficiently capturing the deployment environment and fine-tuning policies within the reconstructed scenes. Our method leverages 3D Gaussian Splatting (GS) and the Habitat-Sim simulator to bridge the gap between realistic scene capture and effective training environments. Using iPhone-captured deployment scenes, we reconstruct meshes via GS, enabling training in settings that closely approximate real-world conditions. We conduct a comprehensive analysis of training strategies, pre-training datasets, and mesh reconstruction techniques, evaluating their impact on sim-to-real predictivity in real-world scenarios. Experimental results demonstrate that agents fine-tuned with EmbodiedSplat outperform both zero-shot baselines pre-trained on large-scale real-world datasets (HM3D) and synthetically generated datasets (HSSD), achieving absolute success rate improvements of 20\\% and 40\\% on real-world Image Navigation task. Moreover, our approach yields a high sim-vs-real correlation (0.87--0.97) for the reconstructed meshes, underscoring its effectiveness in adapting policies to diverse environments with minimal effort. Project page: https://gchhablani.github.io/embodied-splat",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "285",
        "title": "GeoPQA: Bridging the Visual Perception Gap in MLLMs for Geometric Reasoning",
        "author": [
            "Guizhen Chen",
            "Weiwen Xu",
            "Hao Zhang",
            "Hou Pong Chan",
            "Deli Zhao",
            "Anh Tuan Luu",
            "Yu Rong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17437",
        "abstract": "Recent advancements in reinforcement learning (RL) have enhanced the reasoning abilities of large language models (LLMs), yet the impact on multimodal LLMs (MLLMs) is limited. Particularly in vision-intensive tasks like geometric reasoning, MLLMs hallucinate frequently, leading to inaccurate reasoning. We attribute this to the perceptual bottleneck in MLLMs, which caps the benefits of reasoning training. To quantify this, we design a Geo-Perception Question-Answering (GeoPQA) benchmark, targeting basic geometric concepts and spatial relationships. Experiments on GeoPQA reveal significant shortcomings of MLLMs in visual perception, which constrain RL reward signals for effective training. To address this bottleneck, we propose a two-stage RL training framework by first enhancing the visual perception of geometric structures, then fostering reasoning capabilities. Applied to Qwen2.5-VL-3B-Instruct, our two-stage training improves geometric reasoning by 9.7% and geometric problem solving by 9.1%, compared to the direct reasoning training approach. Our method also generalizes to other vision-intensive domains like figure understanding, highlighting the importance of perceptual grounding in effective MLLM reasoning.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "286",
        "title": "WildClaims: Information Access Conversations in the Wild(Chat)",
        "author": [
            "Hideaki Joko",
            "Shakiba Amirshahi",
            "Charles L. A. Clarke",
            "Faegheh Hasibi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17442",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has transformed conversational systems into practical tools used by millions. However, the nature and necessity of information retrieval in real-world conversations remain largely unexplored, as research has focused predominantly on traditional, explicit information access conversations. The central question is: What do real-world information access conversations look like? To this end, we first conduct an observational study on the WildChat dataset, large-scale user-ChatGPT conversations, finding that users' access to information occurs implicitly as check-worthy factual assertions made by the system, even when the conversation's primary intent is non-informational, such as creative writing. To enable the systematic study of this phenomenon, we release the WildClaims dataset, a novel resource consisting of 121,905 extracted factual claims from 7,587 utterances in 3,000 WildChat conversations, each annotated for check-worthiness. Our preliminary analysis of this resource reveals that conservatively 18% to 51% of conversations contain check-worthy assertions, depending on the methods employed, and less conservatively, as many as 76% may contain such assertions. This high prevalence underscores the importance of moving beyond the traditional understanding of explicit information access, to address the implicit information access that arises in real-world user-system conversations.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "287",
        "title": "Semantic Reformulation Entropy for Robust Hallucination Detection in QA Tasks",
        "author": [
            "Chaodong Tong",
            "Qi Zhang",
            "Lei Jiang",
            "Yanbing Liu",
            "Nannan Sun",
            "Wei Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17445",
        "abstract": "Reliable question answering with large language models (LLMs) is challenged by hallucinations, fluent but factually incorrect outputs arising from epistemic uncertainty. Existing entropy-based semantic-level uncertainty estimation methods are limited by sampling noise and unstable clustering of variable-length answers. We propose Semantic Reformulation Entropy (SRE), which improves uncertainty estimation in two ways. First, input-side semantic reformulations produce faithful paraphrases, expand the estimation space, and reduce biases from superficial decoder tendencies. Second, progressive, energy-based hybrid clustering stabilizes semantic grouping. Experiments on SQuAD and TriviaQA show that SRE outperforms strong baselines, providing more robust and generalizable hallucination detection. These results demonstrate that combining input diversification with multi-signal clustering substantially enhances semantic-level uncertainty estimation.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "288",
        "title": "SLAyiNG: Towards Queer Language Processing",
        "author": [
            "Leonor Veloso",
            "Lea Hirlimann",
            "Philipp Wicke",
            "Hinrich SchÃ¼tze"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17449",
        "abstract": "Knowledge of slang is a desirable feature of LLMs in the context of user interaction, as slang often reflects an individual's social identity. Several works on informal language processing have defined and curated benchmarks for tasks such as detection and identification of slang. In this paper, we focus on queer slang. Queer slang can be mistakenly flagged as hate speech or can evoke negative responses from LLMs during user interaction. Research efforts so far have not focused explicitly on queer slang. In particular, detection and processing of queer slang have not been thoroughly evaluated due to the lack of a high-quality annotated benchmark. To address this gap, we curate SLAyiNG, the first dataset containing annotated queer slang derived from subtitles, social media posts, and podcasts, reflecting real-world usage. We describe our data curation process, including the collection of slang terms and definitions, scraping sources for examples that reflect usage of these terms, and our ongoing annotation process. As preliminary results, we calculate inter-annotator agreement for human annotators and OpenAI's model o3-mini, evaluating performance on the task of sense disambiguation. Reaching an average Krippendorff's alpha of 0.746, we argue that state-of-the-art reasoning models can serve as tools for pre-filtering, but the complex and often sensitive nature of queer language data requires expert and community-driven annotation efforts.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "289",
        "title": "Training-Free Label Space Alignment for Universal Domain Adaptation",
        "author": [
            "Dujin Lee",
            "Sojung An",
            "Jungmyung Wi",
            "Kuniaki Saito",
            "Donghyun Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17452",
        "abstract": "Universal domain adaptation (UniDA) transfers knowledge from a labeled source domain to an unlabeled target domain, where label spaces may differ and the target domain may contain private classes. Previous UniDA methods primarily focused on visual space alignment but often struggled with visual ambiguities due to content differences, which limited their robustness and generalizability. To overcome this, we introduce a novel approach that leverages the strong \\textit{zero-shot capabilities} of recent vision-language foundation models (VLMs) like CLIP, concentrating solely on label space alignment to enhance adaptation stability. CLIP can generate task-specific classifiers based only on label names. However, adapting CLIP to UniDA is challenging because the label space is not fully known in advance. In this study, we first utilize generative vision-language models to identify unknown categories in the target domain. Noise and semantic ambiguities in the discovered labels -- such as those similar to source labels (e.g., synonyms, hypernyms, hyponyms) -- complicate label alignment. To address this, we propose a training-free label-space alignment method for UniDA (\\ours). Our method aligns label spaces instead of visual spaces by filtering and refining noisy labels between the domains. We then construct a \\textit{universal classifier} that integrates both shared knowledge and target-private class information, thereby improving generalizability under domain shifts. The results reveal that the proposed method considerably outperforms existing UniDA techniques across key DomainBed benchmarks, delivering an average improvement of \\textcolor{blue}{+7.9\\%}in H-score and \\textcolor{blue}{+6.1\\%} in H$^3$-score. Furthermore, incorporating self-training further enhances performance and achieves an additional (\\textcolor{blue}{+1.6\\%}) increment in both H- and H$^3$-scores.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "290",
        "title": "CARINOX: Inference-time Scaling with Category-Aware Reward-based Initial Noise Optimization and Exploration",
        "author": [
            "Seyed Amir Kasaei",
            "Ali Aghayari",
            "Arash Marioriyad",
            "Niki Sepasian",
            "Shayan Baghayi Nejad",
            "MohammadAmin Fazli",
            "Mahdieh Soleymani Baghshah",
            "Mohammad Hossein Rohban"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17458",
        "abstract": "Text-to-image diffusion models, such as Stable Diffusion, can produce high-quality and diverse images but often fail to achieve compositional alignment, particularly when prompts describe complex object relationships, attributes, or spatial arrangements. Recent inference-time approaches address this by optimizing or exploring the initial noise under the guidance of reward functions that score text-image alignment without requiring model fine-tuning. While promising, each strategy has intrinsic limitations when used alone: optimization can stall due to poor initialization or unfavorable search trajectories, whereas exploration may require a prohibitively large number of samples to locate a satisfactory output. Our analysis further shows that neither single reward metrics nor ad-hoc combinations reliably capture all aspects of compositionality, leading to weak or inconsistent guidance. To overcome these challenges, we present Category-Aware Reward-based Initial Noise Optimization and Exploration (CARINOX), a unified framework that combines noise optimization and exploration with a principled reward selection procedure grounded in correlation with human judgments. Evaluations on two complementary benchmarks covering diverse compositional challenges show that CARINOX raises average alignment scores by +16% on T2I-CompBench++ and +11% on the HRS benchmark, consistently outperforming state-of-the-art optimization and exploration-based methods across all major categories, while preserving image quality and diversity. The project page is available at https://amirkasaei.com/carinox/{this URL}.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "291",
        "title": "PRINCIPLES: Synthetic Strategy Memory for Proactive Dialogue Agents",
        "author": [
            "Namyoung Kim",
            "Kai Tzu-iunn Ong",
            "Yeonjun Hwang",
            "Minseok Kang",
            "Iiseo Jihn",
            "Gayoung Kim",
            "Minju Kim",
            "Jinyoung Yeo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17459",
        "abstract": "Dialogue agents based on large language models (LLMs) have shown promising performance in proactive dialogue, which requires effective strategy planning. However, existing approaches to strategy planning for proactive dialogue face several limitations: limited strategy coverage, preference bias in planning, and reliance on costly additional training. To address these, we propose PRINCIPLES: a synthetic strategy memory for proactive dialogue agents. PRINCIPLES is derived through offline self-play simulations and serves as reusable knowledge that guides strategy planning during inference, eliminating the need for additional training and data annotation. We evaluate PRINCIPLES in both emotional support and persuasion domains, demonstrating consistent improvements over strong baselines. Furthermore, PRINCIPLES maintains its robustness across extended and more diverse evaluation settings. See our project page at https://huggingface.co/spaces/kimnamssya/Principles.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "292",
        "title": "CSDformer: A Conversion Method for Fully Spike-Driven Transformer",
        "author": [
            "Yuhao Zhang",
            "Chengjun Zhang",
            "Di Wu",
            "Jie Yang",
            "Mohamad Sawan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17461",
        "abstract": "Spike-based transformer is a novel architecture aiming to enhance the performance of spiking neural networks while mitigating the energy overhead inherent to transformers. However, methods for generating these models suffer from critical limitations: excessive training costs introduced by direct training methods, or unavoidably hardware-unfriendly operations in existing conversion methods. In this paper, we propose CSDformer, a novel conversion method for fully spike-driven transformers. We tailor a conversion-oriented transformer-based architecture and propose a new function NReLU to replace softmax in self-attention. Subsequently, this model is quantized and trained, and converted into a fully spike-driven model with temporal decomposition technique. Also, we propose delayed Integrate-andFire neurons to reduce conversion errors and improve the performance of spiking models. We evaluate CSDformer on ImageNet, CIFAR-10 and CIFAR-100 datasets and achieve 76.36% top-1 accuracy under 7 time-steps on ImageNet, demonstrating superiority over state-of-the-art models. Furthermore, CSDformer eliminates the need for training SNNs, thereby reducing training costs (reducing computational resource by 75% and accelerating training speed by 2-3$\\times$). To the best of our knowledge, this is the first fully spike-driven transformer-based model developed via conversion method, achieving high performance under ultra-low latency, while dramatically reducing both computational complexity and training overhead.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "293",
        "title": "Transformer-Gather, Fuzzy-Reconsider: A Scalable Hybrid Framework for Entity Resolution",
        "author": [
            "Mohammadreza Sharifi",
            "Danial Ahmadzadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17470",
        "abstract": "Entity resolution plays a significant role in enterprise systems where data integrity must be rigorously maintained. Traditional methods often struggle with handling noisy data or semantic understanding, while modern methods suffer from computational costs or the excessive need for parallel computation. In this study, we introduce a scalable hybrid framework, which is designed to address several important problems, including scalability, noise robustness, and reliable results. We utilized a pre-trained language model to encode each structured data into corresponding semantic embedding vectors. Subsequently, after retrieving a semantically relevant subset of candidates, we apply a syntactic verification stage using fuzzy string matching techniques to refine classification on the unlabeled data. This approach was applied to a real-world entity resolution task, which exposed a linkage between a central user management database and numerous shared hosting server records. Compared to other methods, this approach exhibits an outstanding performance in terms of both processing time and robustness, making it a reliable solution for a server-side product. Crucially, this efficiency does not compromise results, as the system maintains a high retrieval recall of approximately 0.97. The scalability of the framework makes it deployable on standard CPU-based infrastructure, offering a practical and effective solution for enterprise-level data integrity auditing.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "294",
        "title": "Stable Video-Driven Portraits",
        "author": [
            "Mallikarjun B. R.",
            "Fei Yin",
            "Vikram Voleti",
            "Nikita Drobyshev",
            "Maksim Lapin",
            "Aaryaman Vasishta",
            "Varun Jampani"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17476",
        "abstract": "Portrait animation aims to generate photo-realistic videos from a single source image by reenacting the expression and pose from a driving video. While early methods relied on 3D morphable models or feature warping techniques, they often suffered from limited expressivity, temporal inconsistency, and poor generalization to unseen identities or large pose variations. Recent advances using diffusion models have demonstrated improved quality but remain constrained by weak control signals and architectural limitations. In this work, we propose a novel diffusion based framework that leverages masked facial regions specifically the eyes, nose, and mouth from the driving video as strong motion control cues. To enable robust training without appearance leakage, we adopt cross identity supervision. To leverage the strong prior from the pretrained diffusion model, our novel architecture introduces minimal new parameters that converge faster and help in better generalization. We introduce spatial temporal attention mechanisms that allow inter frame and intra frame interactions, effectively capturing subtle motions and reducing temporal artifacts. Our model uses history frames to ensure continuity across segments. At inference, we propose a novel signal fusion strategy that balances motion fidelity with identity preservation. Our approach achieves superior temporal consistency and accurate expression control, enabling high-quality, controllable portrait animation suitable for real-world applications.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "295",
        "title": "LingoQ: Bridging the Gap between ESL Learning and Work through AI-Generated Work-Related Quizzes",
        "author": [
            "Yeonsun Yang",
            "Sang Won Lee",
            "Jean Y. Song",
            "Sangdoo Yun",
            "Young-Ho Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17477",
        "abstract": "Non-native English speakers performing English-related tasks at work struggle to sustain ESL learning, despite their motivation. Often, study materials are disconnected from their work context. Although workers rely on LLM assistants to address their immediate needs, these interactions may not directly contribute to their English skills. We present LingoQ, an AI-mediated system that allows workers to practice English using quizzes generated from their LLM queries during work. LingoQ leverages these queries using AI to generate personalized quizzes that workers can review and practice on their smartphones. We conducted a three-week deployment study with 28 ESL workers to evaluate LingoQ. Participants valued the relevance of quizzes that reflect their own context, constantly engaging with the app during the study. This active engagement improved self-efficacy and led to learning gains for beginners and, potentially, for intermediate learners. We discuss opportunities of leveraging users' reliance on LLMs to situate their learning in the user context for improved learning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "296",
        "title": "ChartHal: A Fine-grained Framework Evaluating Hallucination of Large Vision Language Models in Chart Understanding",
        "author": [
            "Xingqi Wang",
            "Yiming Cui",
            "Xin Yao",
            "Shijin Wang",
            "Guoping Hu",
            "Xiaoyu Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17481",
        "abstract": "Large Vision-Language Models (LVLMs) have recently demonstrated remarkable progress, yet hallucination remains a critical barrier, particularly in chart understanding, which requires sophisticated perceptual and cognitive abilities as well as rigorous factual accuracy. While prior work has investigated hallucinations and chart comprehension independently, their intersection remains largely unexplored. To address this gap, we present ChartHal, a benchmark that features a fine-grained taxonomy of hallucination scenarios in chart understanding, along with a human-validated dataset of 1,062 samples. Our evaluation shows that state-of-the-art LVLMs suffer from severe hallucinations on ChartHal, including proprietary models such as GPT-5 and o4-mini, which achieve only 34.46% and 22.79% accuracy, respectively. Further analysis reveals that questions involving information absent from or contradictory to charts are especially likely to trigger hallucinations, underscoring the urgent need for more robust mitigation strategies. Code and data are available at https://github.com/ymcui/ChartHal .",
        "tags": [
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "297",
        "title": "AttnComp: Attention-Guided Adaptive Context Compression for Retrieval-Augmented Generation",
        "author": [
            "Lvzhou Luo",
            "Yixuan Cao",
            "Ping Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17486",
        "abstract": "Retrieval-augmented generation improves the factual accuracy of Large Language Models (LLMs) by incorporating external context, but often suffers from irrelevant retrieved content that hinders effectiveness. Context compression addresses this issue by filtering out irrelevant information from context before LLM generation. However, existing methods struggle to adaptively adjust compression rates for different context, maintain low latency and integrate information across multiple documents. To overcome these limitations, We introduce AttnComp, an adaptive, efficient and context-aware compression framework. By leveraging the attention mechanism of LLMs to identify relevant information, AttnComp employs a Top-P compression algorithm to retain the minimal set of documents whose cumulative attention weights exceeds a predefined threshold. In addition to compression, AttnComp estimates response confidence by assessing the overall relevance of the retrieved content, enabling users to gauge response reliability. Experiments demonstrate that AttnComp outperforms existing compression methods and uncompressed baselines, achieving higher accuracy with substantial compression rates and lower latency.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "298",
        "title": "Privacy in Action: Towards Realistic Privacy Mitigation and Evaluation for LLM-Powered Agents",
        "author": [
            "Shouju Wang",
            "Fenglin Yu",
            "Xirui Liu",
            "Xiaoting Qin",
            "Jue Zhang",
            "Qingwei Lin",
            "Dongmei Zhang",
            "Saravan Rajmohan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17488",
        "abstract": "The increasing autonomy of LLM agents in handling sensitive communications, accelerated by Model Context Protocol (MCP) and Agent-to-Agent (A2A) frameworks, creates urgent privacy challenges. While recent work reveals significant gaps between LLMs' privacy Q&A performance and their agent behavior, existing benchmarks remain limited to static, simplified scenarios. We present PrivacyChecker, a model-agnostic, contextual integrity based mitigation approach that effectively reduces privacy leakage from 36.08% to 7.30% on DeepSeek-R1 and from 33.06% to 8.32% on GPT-4o, all while preserving task helpfulness. We also introduce PrivacyLens-Live, transforming static benchmarks into dynamic MCP and A2A environments that reveal substantially higher privacy risks in practical. Our modular mitigation approach integrates seamlessly into agent protocols through three deployment strategies, providing practical privacy protection for the emerging agentic ecosystem. Our data and code will be made available at https://aka.ms/privacy_in_action.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "299",
        "title": "MapCoder-Lite: Squeezing Multi-Agent Coding into a Single Small LLM",
        "author": [
            "Woongkyu Lee",
            "Junhee Cho",
            "Jungwook Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17489",
        "abstract": "Large language models (LLMs) have advanced code generation from single-function tasks to competitive-programming problems, but existing multi-agent solutions either rely on costly large-scale ($>$ 30B) models or collapse when downsized to small open-source models. We present MapCoder-Lite, which upgrades a single 7B model into four role-specialised agents-retriever, planner, coder, and debugger-using only rank-32, role-specific LoRA adapters ($<3\\%$ extra parameters). Three lightweight techniques make this possible: (i) trajectory distillation from strong LLMs fixes format fragility in retrieval and debugging, (ii) supervisor-guided correction strengthens planning and coding agents, and (iii) agent-wise LoRA fine-tuning delivers memory-efficient specialisation. Comprehensive evaluation on xCodeEval, APPS, and CodeContests shows that MapCoder-Lite more than doubles xCodeEval accuracy (from $13.2\\%$ to $28.3\\%$), eliminates all format failures, and closes to within six points of a 32B baseline while cutting GPU memory and token-generation time by $4\\times$. These results demonstrate that careful agent-wise fine-tuning unleashes high-quality multi-agent coding on a small language model.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "300",
        "title": "Enhancing Cross-Lingual Transfer through Reversible Transliteration: A Huffman-Based Approach for Low-Resource Languages",
        "author": [
            "Wenhao Zhuang",
            "Yuan Sun",
            "Xiaobing Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17493",
        "abstract": "As large language models (LLMs) are trained on increasingly diverse and extensive multilingual corpora, they demonstrate cross-lingual transfer capabilities. However, these capabilities often fail to effectively extend to low-resource languages, particularly those utilizing non-Latin scripts. While transliterating low-resource languages into Latin script presents a natural solution, there currently lacks a comprehensive framework for integrating transliteration into LLMs training and deployment. Taking a pragmatic approach, this paper innovatively combines character transliteration with Huffman coding to design a complete transliteration framework. Our proposed framework offers the following advantages: 1) Compression: Reduces storage requirements for low-resource language content, achieving up to 50% reduction in file size and 50-80% reduction in token count. 2) Accuracy: Guarantees 100% lossless conversion from transliterated text back to the source language. 3) Efficiency: Eliminates the need for vocabulary expansion for low-resource languages, improving training and inference efficiency. 4) Scalability: The framework can be extended to other low-resource languages. We validate the effectiveness of our framework across multiple downstream tasks, including text classification, machine reading comprehension, and machine translation. Experimental results demonstrate that our method significantly enhances the model's capability to process low-resource languages while maintaining performance on high-resource languages. Our data and code are publicly available at https://github.com/CMLI-NLP/HuffmanTranslit.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "301",
        "title": "SAMSON: 3rd Place Solution of LSVOS 2025 VOS Challenge",
        "author": [
            "Yujie Xie",
            "Hongyang Zhang",
            "Zhihui Liu",
            "Shihai Ruan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17500",
        "abstract": "Large-scale Video Object Segmentation (LSVOS) addresses the challenge of accurately tracking and segmenting objects in long video sequences, where difficulties stem from object reappearance, small-scale targets, heavy occlusions, and crowded scenes. Existing approaches predominantly adopt SAM2-based frameworks with various memory mechanisms for complex video mask generation. In this report, we proposed Segment Anything with Memory Strengthened Object Navigation (SAMSON), the 3rd place solution in the MOSE track of ICCV 2025, which integrates the strengths of stateof-the-art VOS models into an effective paradigm. To handle visually similar instances and long-term object disappearance in MOSE, we incorporate a long-term memorymodule for reliable object re-identification. Additionly, we adopt SAM2Long as a post-processing strategy to reduce error accumulation and enhance segmentation stability in long video sequences. Our method achieved a final performance of 0.8427 in terms of J &F in the test-set leaderboard.",
        "tags": [
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "302",
        "title": "CorefInst: Leveraging LLMs for Multilingual Coreference Resolution",
        "author": [
            "TuÄba Pamay Arslan",
            "Emircan Erol",
            "GÃ¼lÅen EryiÄit"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17505",
        "abstract": "Coreference Resolution (CR) is a crucial yet challenging task in natural language understanding, often constrained by task-specific architectures and encoder-based language models that demand extensive training and lack adaptability. This study introduces the first multilingual CR methodology which leverages decoder-only LLMs to handle both overt and zero mentions. The article explores how to model the CR task for LLMs via five different instruction sets using a controlled inference method. The approach is evaluated across three LLMs; Llama 3.1, Gemma 2, and Mistral 0.3. The results indicate that LLMs, when instruction-tuned with a suitable instruction set, can surpass state-of-the-art task-specific architectures. Specifically, our best model, a fully fine-tuned Llama 3.1 for multilingual CR, outperforms the leading multilingual CR model (i.e., Corpipe 24 single stage variant) by 2 pp on average across all languages in the CorefUD v1.2 dataset collection.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "303",
        "title": "Achilles' Heel of Mamba: Essential difficulties of the Mamba architecture demonstrated by synthetic data",
        "author": [
            "Tianyi Chen",
            "Pengxiao Lin",
            "Zhiwei Wang",
            "Zhi-Qin John Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17514",
        "abstract": "State Space Models (SSMs) have emerged as promising alternatives to attention mechanisms, with the Mamba architecture demonstrating impressive performance and linear complexity for processing long sequences. However, the fundamental differences between Mamba and Transformer architectures remain incompletely understood. In this work, we use carefully designed synthetic tasks to reveal Mamba's inherent limitations. Through experiments, we identify that Mamba's nonlinear convolution introduces an asymmetry bias that significantly impairs its ability to recognize symmetrical patterns and relationships. Using composite function and inverse sequence matching tasks, we demonstrate that Mamba strongly favors compositional solutions over symmetrical ones and struggles with tasks requiring the matching of reversed sequences. We show these limitations stem not from the SSM module itself but from the nonlinear convolution preceding it, which fuses token information asymmetrically. These insights provide a new understanding of Mamba's constraints and suggest concrete architectural improvements for future sequence models.",
        "tags": [
            "Mamba",
            "SSMs",
            "Transformer"
        ]
    },
    {
        "id": "304",
        "title": "Chat-CBM: Towards Interactive Concept Bottleneck Models with Frozen Large Language Models",
        "author": [
            "Hangzhou He",
            "Lei Zhu",
            "Kaiwen Li",
            "Xinliang Zhang",
            "Jiakui Hu",
            "Ourui Fu",
            "Zhengjian Yao",
            "Yanye Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17522",
        "abstract": "Concept Bottleneck Models (CBMs) provide inherent interpretability by first predicting a set of human-understandable concepts and then mapping them to labels through a simple classifier. While users can intervene in the concept space to improve predictions, traditional CBMs typically employ a fixed linear classifier over concept scores, which restricts interventions to manual value adjustments and prevents the incorporation of new concepts or domain knowledge at test time. These limitations are particularly severe in unsupervised CBMs, where concept activations are often noisy and densely activated, making user interventions ineffective. We introduce Chat-CBM, which replaces score-based classifiers with a language-based classifier that reasons directly over concept semantics. By grounding prediction in the semantic space of concepts, Chat-CBM preserves the interpretability of CBMs while enabling richer and more intuitive interventions, such as concept correction, addition or removal of concepts, incorporation of external knowledge, and high-level reasoning guidance. Leveraging the language understanding and few-shot capabilities of frozen large language models, Chat-CBM extends the intervention interface of CBMs beyond numerical editing and remains effective even in unsupervised settings. Experiments on nine datasets demonstrate that Chat-CBM achieves higher predictive performance and substantially improves user interactivity while maintaining the concept-based interpretability of CBMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "305",
        "title": "Robust spectral preconditioning for high-PÃ©clet number convection-diffusion",
        "author": [
            "Lukas Holbach",
            "Peter Bastian",
            "Robert Scheichl"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17531",
        "abstract": "We introduce a two-level hybrid restricted additive Schwarz (RAS) preconditioner for heterogeneous steady-state convection-diffusion equations at high PÃ©clet numbers. Our construction builds on the multiscale spectral generalized finite element method (MS-GFEM), wherein the coarse space is spanned by locally optimal basis functions obtained from local generalized eigenproblems on operator-harmonic spaces. Extending the theory of Ma (2025) to convection-diffusion problems in conservation form, we establish exponential convergence of the MS-GFEM approximation. Rewriting MS-GFEM as a RAS-type iteration, we show for coercive problems that this exponential convergence property is inherited by the RAS-type iterative method (at least in the continuous setting). Employed as a preconditioner within the generalized minimal residual method (GMRES), the resulting method requires only a few iterations for high accuracy even with low-dimensional coarse spaces.\nThrough extensive numerical experiments on problems with high-contrast diffusion and non-divergence-free, rotating velocity fields, we demonstrate robustness with respect to the grid PÃ©clet number and the number of subdomains (tested up to $10^5$ subdomains), while coarse-space dimensions remain small as grid PÃ©clet numbers increase. By adapting the coarse space and oversampling size, we are able to achieve arbitrarily fast convergence of preconditioned GMRES. As an extension, for which we do not have theory yet, we show effectiveness of the method even for indefinite problems and in the vanishing-diffusion limit.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "306",
        "title": "SimToken: A Simple Baseline for Referring Audio-Visual Segmentation",
        "author": [
            "Dian Jin",
            "Yanghao Zhou",
            "Jinxing Zhou",
            "Jiaqi Ma",
            "Ruohao Guo",
            "Dan Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17537",
        "abstract": "Referring Audio-Visual Segmentation (Ref-AVS) aims to segment specific objects in videos based on natural language expressions involving audio, vision, and text information. This task poses significant challenges in cross-modal reasoning and fine-grained object localization. In this paper, we propose a simple framework, SimToken, that integrates a multimodal large language model (MLLM) with the Segment Anything Model (SAM). The MLLM is guided to generate a special semantic token representing the referred object. This compact token, enriched with contextual information from all modalities, acts as a prompt to guide SAM to segment objectsacross video frames. To further improve semantic learning, we introduce a novel target-consistent semantic alignment loss that aligns token embeddings from different expressions but referring to the same object. Experiments on the Ref-AVS benchmark demonstrate that our approach achieves superior performance compared to existing http://methods.Code will be available at https://github.com/DianJin-HFUT/SimToken",
        "tags": [
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "307",
        "title": "Disaggregated Prefill and Decoding Inference System for Large Language Model Serving on Multi-Vendor GPUs",
        "author": [
            "Xing Chen",
            "Rong Shi",
            "Lu Zhao",
            "Lingbin Wang",
            "Xiao Jin",
            "Yueqiang Chen",
            "Hongfeng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17542",
        "abstract": "LLM-based applications have been widely used in various industries, but with the increasing of models size, an efficient large language model (LLM) inference system is an urgent problem to be solved for service providers. Since the inference system is divided into two stage with different characteristics: Prefill and Decode, the two stage will interfere with each other during the inference process. Toward this end, a P-D disaggregated inference framework is proposed by some researchers. Current research is done on homogeneous GPUs, and lacks deployment solutions based on business scenarios. Compared with homogeneous GPUs, using heterogeneous GPUs to construct inference systems can better improve resource utilization and reduce costs. Even if GPUs from different vendors are used to build inference systems, on the basis of reducing costs, the resource utilization rate can be improved and the dependence on a single vendor can be reduced. Therefore, a P-D disaggreagetd inference system based on heterogeneous GPUs is designed, and the heterogeneous compatible transmission module in the system is designed to address heterogeneous GPU data compatibility issues. Then, a joint optimization algorithm of parallel strategy and instance number allocation is proposed to obtain the deployment solutions. Finally, the experimental results show that the P-D disaggregated inference system can well solve the hybrid inference problem of heterogeneous GPUs from different vendors, and the joint optimization algorithm can obtain the optimal deployment solution.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "308",
        "title": "A Multimodal Conversational Assistant for the Characterization of Agricultural Plots from Geospatial Open Data",
        "author": [
            "Juan CaÃ±ada",
            "RaÃºl Alonso",
            "Julio Molleda",
            "Fidel DÃ­ez"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17544",
        "abstract": "The increasing availability of open Earth Observation (EO) and agricultural datasets holds great potential for supporting sustainable land management. However, their high technical entry barrier limits accessibility for non-expert users. This study presents an open-source conversational assistant that integrates multimodal retrieval and large language models (LLMs) to enable natural language interaction with heterogeneous agricultural and geospatial data. The proposed architecture combines orthophotos, Sentinel-2 vegetation indices, and user-provided documents through retrieval-augmented generation (RAG), allowing the system to flexibly determine whether to rely on multimodal evidence, textual knowledge, or both in formulating an answer. To assess response quality, we adopt an LLM-as-a-judge methodology using Qwen3-32B in a zero-shot, unsupervised setting, applying direct scoring in a multi-dimensional quantitative evaluation framework. Preliminary results show that the system is capable of generating clear, relevant, and context-aware responses to agricultural queries, while remaining reproducible and scalable across geographic regions. The primary contributions of this work include an architecture for fusing multimodal EO and textual knowledge sources, a demonstration of lowering the barrier to access specialized agricultural information through natural language interaction, and an open and reproducible design.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "309",
        "title": "Prompts as Software Engineering Artifacts: A Research Agenda and Preliminary Findings",
        "author": [
            "Hugo Villamizar",
            "Jannik Fischbach",
            "Alexander Korn",
            "Andreas Vogelsang",
            "Daniel Mendez"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17548",
        "abstract": "Developers now routinely interact with large language models (LLMs) to support a range of software engineering (SE) tasks. This prominent role positions prompts as potential SE artifacts that, like other artifacts, may require systematic development, documentation, and maintenance. However, little is known about how prompts are actually used and managed in LLM-integrated workflows, what challenges practitioners face, and whether the benefits of systematic prompt management outweigh the associated effort. To address this gap, we propose a research programme that (a) characterizes current prompt practices, challenges, and influencing factors in SE; (b) analyzes prompts as software artifacts, examining their evolution, traceability, reuse, and the trade-offs of systematic management; and (c) develops and empirically evaluates evidence-based guidelines for managing prompts in LLM-integrated workflows. As a first step, we conducted an exploratory survey with 74 software professionals from six countries to investigate current prompt practices and challenges. The findings reveal that prompt usage in SE is largely ad-hoc: prompts are often refined through trial-and-error, rarely reused, and shaped more by individual heuristics than standardized practices. These insights not only highlight the need for more systematic approaches to prompt management but also provide the empirical foundation for the subsequent stages of our research programme.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "310",
        "title": "MontePrep: Monte-Carlo-Driven Automatic Data Preparation without Target Data Instances",
        "author": [
            "Congcong Ge",
            "Yachuan Liu",
            "Yixuan Tang",
            "Yifan Zhu",
            "Yaofeng Tu",
            "Yunjun Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17553",
        "abstract": "In commercial systems, a pervasive requirement for automatic data preparation (ADP) is to transfer relational data from disparate sources to targets with standardized schema specifications. Previous methods rely on labor-intensive supervision signals or target table data access permissions, limiting their usage in real-world scenarios. To tackle these challenges, we propose an effective end-to-end ADP framework MontePrep, which enables training-free pipeline synthesis with zero target-instance requirements. MontePrep is formulated as an open-source large language model (LLM) powered tree-structured search problem. It consists of three pivot components, i.e., a data preparation action sandbox (DPAS), a fundamental pipeline generator (FPG), and an execution-aware pipeline optimizer (EPO). We first introduce DPAS, a lightweight action sandbox, to navigate the search-based pipeline generation. The design of DPAS circumvents exploration of infeasible pipelines. Then, we present FPG to build executable DP pipelines incrementally, which explores the predefined action sandbox by the LLM-powered Monte Carlo Tree Search. Furthermore, we propose EPO, which invokes pipeline execution results from sources to targets to evaluate the reliability of the generated pipelines in FPG. In this way, unreasonable pipelines are eliminated, thus facilitating the search process from both efficiency and effectiveness perspectives. Extensive experimental results demonstrate the superiority of MontePrep with significant improvement against five state-of-the-art competitors.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "311",
        "title": "Specification-Aware Machine Translation and Evaluation for Purpose Alignment",
        "author": [
            "Yoko Kayano",
            "Saku Sugawara"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17559",
        "abstract": "In professional settings, translation is guided by communicative goals and client needs, often formalized as specifications. While existing evaluation frameworks acknowledge the importance of such specifications, these specifications are often treated only implicitly in machine translation (MT) research. Drawing on translation studies, we provide a theoretical rationale for why specifications matter in professional translation, as well as a practical guide to implementing specification-aware MT and evaluation. Building on this foundation, we apply our framework to the translation of investor relations texts from 33 publicly listed companies. In our experiment, we compare five translation types, including official human translations and prompt-based outputs from large language models (LLMs), using expert error analysis, user preference rankings, and an automatic metric. The results show that LLM translations guided by specifications consistently outperformed official human translations in human evaluations, highlighting a gap between perceived and expected quality. These findings demonstrate that integrating specifications into MT workflows, with human oversight, can improve translation quality in ways aligned with professional practice.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "312",
        "title": "LIMI: Less is More for Agency",
        "author": [
            "Yang Xiao",
            "Mohan Jiang",
            "Jie Sun",
            "Keyu Li",
            "Jifan Lin",
            "Yumin Zhuang",
            "Ji Zeng",
            "Shijie Xia",
            "Qishuo Hua",
            "Xuefeng Li",
            "Xiaojie Cai",
            "Tongyu Wang",
            "Yue Zhang",
            "Liming Liu",
            "Xia Wu",
            "Jinlong Hou",
            "Yuan Cheng",
            "Wenjie Li",
            "Xiang Wang",
            "Dequan Wang",
            "Pengfei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17567",
        "abstract": "We define Agency as the emergent capacity of AI systems to function as autonomous agents actively discovering problems, formulating hypotheses, and executing solutions through self-directed engagement with environments and tools. This fundamental capability marks the dawn of the Age of AI Agency, driven by a critical industry shift: the urgent need for AI systems that don't just think, but work. While current AI excels at reasoning and generating responses, industries demand autonomous agents that can execute tasks, operate tools, and drive real-world outcomes. As agentic intelligence becomes the defining characteristic separating cognitive systems from productive workers, efficiently cultivating machine autonomy becomes paramount. Current approaches assume that more data yields better agency, following traditional scaling laws from language modeling. We fundamentally challenge this paradigm. LIMI (Less Is More for Intelligent Agency) demonstrates that agency follows radically different development principles. Through strategic focus on collaborative software development and scientific research workflows, we show that sophisticated agentic intelligence can emerge from minimal but strategically curated demonstrations of autonomous behavior. Using only 78 carefully designed training samples, LIMI achieves 73.5% on comprehensive agency benchmarks, dramatically outperforming state-of-the-art models: Kimi-K2-Instruct (24.1%), DeepSeek-V3.1 (11.9%), Qwen3-235B-A22B-Instruct (27.5%), and GLM-4.5 (45.1%). Most strikingly, LIMI demonstrates 53.7% improvement over models trained on 10,000 samples-achieving superior agentic intelligence with 128 times fewer samples. Our findings establish the Agency Efficiency Principle: machine autonomy emerges not from data abundance but from strategic curation of high-quality agentic demonstrations.",
        "tags": [
            "DeepSeek",
            "GLM"
        ]
    },
    {
        "id": "313",
        "title": "Asking a Language Model for Diverse Responses",
        "author": [
            "Sergey Troshin",
            "Irina Saparina",
            "Antske Fokkens",
            "Vlad Niculae"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17570",
        "abstract": "Large language models increasingly rely on explicit reasoning chains and can produce multiple plausible responses for a given context. We study the candidate sampler that produces the set of plausible responses contrasting the ancestral (parallel) sampling against two alternatives: enumeration, which asks the model to produce $n$ candidates in one pass, and iterative sampling, which proposes candidates sequentially while conditioning on the currently generated response set. Under matched budgets, we compare these samplers on quality, lexical and computation flow diversity, and efficiency. Our empirical results demonstrate that enumeration and iterative strategies result in higher diversity at comparable quality. Our findings highlight the potential of simple non-independent sampling strategies to improve response diversity without sacrificing generation quality.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "314",
        "title": "Morphologies of a sagging elastica with intrinsic sensing and actuation",
        "author": [
            "Vishnu Deo Mishra",
            "S Ganga Prasath"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17572",
        "abstract": "The morphology of a slender soft-robot can be modified by sensing its shape via sensors and exerting moments via actuators embedded along its body. The actuating moments required to morph these soft-robots to a desired shape are often difficult to compute due to the geometric non-linearity associated with the structure, the errors in modeling the experimental system, and the limitations in sensing and feedback/actuation capabilities. In this article, we explore the effect of a simple feedback strategy (actuation being proportional to the sensed curvature) on the shape of a soft-robot, modeled as an elastica. The finite number of sensors and actuators, often seen in experiments, is captured in the model via filters of specified widths. Using proportional feedback, we study the simple task of straightening the device by compensating for the sagging introduced by its self-weight. The device undergoes a hierarchy of morphological instabilities defined in the phase-space given by the gravito-bending number, non-dimensional sensing/feedback gain, and the scaled width of the filter. For complex shape-morphing tasks, given a perfect model of the device with limited sensing and actuating capabilities, we find that a trade-off arises (set by the sensor spacing & actuator size) between capturing the long and short wavelength features. We show that the error in shape-morphing is minimal for a fixed filter width when we choose an appropriate actuating gain (whose magnitude goes as a square of the filter width). Our model provides a quantitative lens to study and design slender soft devices with limited sensing and actuating capabilities for complex maneuvering applications.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "315",
        "title": "GeCCo - a Generalist Contact-Conditioned Policy for Loco-Manipulation Skills on Legged Robots",
        "author": [
            "Vassil Atanassov",
            "Wanming Yu",
            "Siddhant Gangapurwala",
            "James Wilson",
            "Ioannis Havoutis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17582",
        "abstract": "Most modern approaches to quadruped locomotion focus on using Deep Reinforcement Learning (DRL) to learn policies from scratch, in an end-to-end manner. Such methods often fail to scale, as every new problem or application requires time-consuming and iterative reward definition and tuning. We present Generalist Contact-Conditioned Policy (GeCCo) -- a low-level policy trained with Deep Reinforcement Learning that is capable of tracking arbitrary contact points on a quadruped robot. The strength of our approach is that it provides a general and modular low-level controller that can be reused for a wider range of high-level tasks, without the need to re-train new controllers from scratch. We demonstrate the scalability and robustness of our method by evaluating on a wide range of locomotion and manipulation tasks in a common framework and under a single generalist policy. These include a variety of gaits, traversing complex terrains (eg. stairs and slopes) as well as previously unseen stepping-stones and narrow beams, and interacting with objects (eg. pushing buttons, tracking trajectories). Our framework acquires new behaviors more efficiently, simply by combining a task-specific high-level contact planner and the pre-trained generalist policy. A supplementary video can be found at https://youtu.be/o8Dd44MkG2E.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "316",
        "title": "Attention-based Mixture of Experts for Robust Speech Deepfake Detection",
        "author": [
            "Viola Negroni",
            "Davide Salvi",
            "Alessandro Ilic Mezza",
            "Paolo Bestagini",
            "Stefano Tubaro"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17585",
        "abstract": "AI-generated speech is becoming increasingly used in everyday life, powering virtual assistants, accessibility tools, and other applications. However, it is also being exploited for malicious purposes such as impersonation, misinformation, and biometric spoofing. As speech deepfakes become nearly indistinguishable from real human speech, the need for robust detection methods and effective countermeasures has become critically urgent. In this paper, we present the ISPL's submission to the SAFE challenge at IH&MMSec 2025, where our system ranked first across all tasks. Our solution introduces a novel approach to audio deepfake detection based on a Mixture of Experts architecture. The proposed system leverages multiple state-of-the-art detectors, combining their outputs through an attention-based gating network that dynamically weights each expert based on the input speech signal. In this design, each expert develops a specialized understanding of the shared training data by learning to capture different complementary aspects of the same input through inductive biases. Experimental results indicate that our method outperforms existing approaches across multiple datasets. We further evaluate and analyze the performance of our system in the SAFE challenge.",
        "tags": [
            "Detection",
            "MoE"
        ]
    },
    {
        "id": "317",
        "title": "Interpreting Attention Heads for Image-to-Text Information Flow in Large Vision-Language Models",
        "author": [
            "Jinyeong Kim",
            "Seil Kang",
            "Jiwoo Park",
            "Junhyeok Kim",
            "Seong Jae Hwang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17588",
        "abstract": "Large Vision-Language Models (LVLMs) answer visual questions by transferring information from images to text through a series of attention heads. While this image-to-text information flow is central to visual question answering, its underlying mechanism remains difficult to interpret due to the simultaneous operation of numerous attention heads. To address this challenge, we propose head attribution, a technique inspired by component attribution methods, to identify consistent patterns among attention heads that play a key role in information transfer. Using head attribution, we investigate how LVLMs rely on specific attention heads to identify and answer questions about the main object in an image. Our analysis reveals that a distinct subset of attention heads facilitates the image-to-text information flow. Remarkably, we find that the selection of these heads is governed by the semantic content of the input image rather than its visual appearance. We further examine the flow of information at the token level and discover that (1) text information first propagates to role-related tokens and the final token before receiving image information, and (2) image information is embedded in both object-related and background tokens. Our work provides evidence that image-to-text information flow follows a structured process, and that analysis at the attention-head level offers a promising direction toward understanding the mechanisms of LVLMs.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "318",
        "title": "COLA: Context-aware Language-driven Test-time Adaptation",
        "author": [
            "Aiming Zhang",
            "Tianyuan Yu",
            "Liang Bai",
            "Jun Tang",
            "Yanming Guo",
            "Yirun Ruan",
            "Yun Zhou",
            "Zhihe Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17598",
        "abstract": "Test-time adaptation (TTA) has gained increasing popularity due to its efficacy in addressing ``distribution shift'' issue while simultaneously protecting data privacy.\nHowever, most prior methods assume that a paired source domain model and target domain sharing the same label space coexist, heavily limiting their applicability.\nIn this paper, we investigate a more general source model capable of adaptation to multiple target domains without needing shared labels.\nThis is achieved by using a pre-trained vision-language model (VLM), \\egno, CLIP, that can recognize images through matching with class descriptions.\nWhile the zero-shot performance of VLMs is impressive, they struggle to effectively capture the distinctive attributes of a target domain.\nTo that end, we propose a novel method -- Context-aware Language-driven TTA (COLA).\nThe proposed method incorporates a lightweight context-aware module that consists of three key components: a task-aware adapter, a context-aware unit, and a residual connection unit for exploring task-specific knowledge, domain-specific knowledge from the VLM and prior knowledge of the VLM, respectively.\nIt is worth noting that the context-aware module can be seamlessly integrated into a frozen VLM, ensuring both minimal effort and parameter efficiency.\nAdditionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy to mitigate the adverse effects caused by class imbalance.\nWe demonstrate the effectiveness of our method not only in TTA scenarios but also in class generalisation tasks.\nThe source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "319",
        "title": "Overview of PlantCLEF 2025: Multi-Species Plant Identification in Vegetation Quadrat Images",
        "author": [
            "Giulio Martellucci",
            "Herve Goeau",
            "Pierre Bonnet",
            "Fabrice Vinatier",
            "Alexis Joly"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17602",
        "abstract": "Quadrat images are essential for ecological studies, as they enable standardized sampling, the assessment of plant biodiversity, long-term monitoring, and large-scale field campaigns. These images typically cover an area of fifty centimetres or one square meter, and botanists carefully identify all the species present. Integrating AI could help specialists accelerate their inventories and expand the spatial coverage of ecological studies. To assess progress in this area, the PlantCLEF 2025 challenge relies on a new test set of 2,105 high-resolution multi-label images annotated by experts and covering around 400 species. It also provides a large training set of 1.4 million individual plant images, along with vision transformer models pre-trained on this data. The task is formulated as a (weakly labelled) multi-label classification problem, where the goal is to predict all species present in a quadrat image using single-label training data. This paper provides a detailed description of the data, the evaluation methodology, the methods and models used by participants, and the results achieved.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "320",
        "title": "Audio Super-Resolution with Latent Bridge Models",
        "author": [
            "Chang Li",
            "Zehua Chen",
            "Liyuan Wang",
            "Jun Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17609",
        "abstract": "Audio super-resolution (SR), i.e., upsampling the low-resolution (LR) waveform to the high-resolution (HR) version, has recently been explored with diffusion and bridge models, while previous methods often suffer from sub-optimal upsampling quality due to their uninformative generation prior. Towards high-quality audio super-resolution, we present a new system with latent bridge models (LBMs), where we compress the audio waveform into a continuous latent space and design an LBM to enable a latent-to-latent generation process that naturally matches the LR-toHR upsampling process, thereby fully exploiting the instructive prior information contained in the LR waveform. To further enhance the training results despite the limited availability of HR samples, we introduce frequency-aware LBMs, where the prior and target frequency are taken as model input, enabling LBMs to explicitly learn an any-to-any upsampling process at the training stage. Furthermore, we design cascaded LBMs and present two prior augmentation strategies, where we make the first attempt to unlock the audio upsampling beyond 48 kHz and empower a seamless cascaded SR process, providing higher flexibility for audio post-production. Comprehensive experimental results evaluated on the VCTK, ESC-50, Song-Describer benchmark datasets and two internal testsets demonstrate that we achieve state-of-the-art objective and perceptual quality for any-to-48kHz SR across speech, audio, and music signals, as well as setting the first record for any-to-192kHz audio SR. Demo at https://AudioLBM.github.io/.",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "321",
        "title": "Human vs. Agent in Task-Oriented Conversations",
        "author": [
            "Zhefan Wang",
            "Ning Geng",
            "Zhiqiang Guo",
            "Weizhi Ma",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17619",
        "abstract": "Task-oriented conversational systems are essential for efficiently addressing diverse user needs, yet their development requires substantial amounts of high-quality conversational data that is challenging and costly to obtain. While large language models (LLMs) have demonstrated potential in generating synthetic conversations, the extent to which these agent-generated interactions can effectively substitute real human conversations remains unclear. This work presents the first systematic comparison between LLM-simulated users and human users in personalized task-oriented conversations. We propose a comprehensive analytical framework encompassing three key aspects (conversation strategy, interaction style, and conversation evaluation) and ten distinct dimensions for evaluating user behaviors, and collect parallel conversational datasets from both human users and LLM agent users across four representative scenarios under identical conditions. Our analysis reveals significant behavioral differences between the two user types in problem-solving approaches, question broadness, user engagement, context dependency, feedback polarity and promise, language style, and hallucination awareness. We found consistency in the agent users and human users across the depth-first or breadth-first dimensions, as well as the usefulness dimensions. These findings provide critical insights for advancing LLM-based user simulation. Our multi-dimensional taxonomy constructed a generalizable framework for analyzing user behavior patterns, offering insights from LLM agent users and human users. By this work, we provide perspectives on rethinking how to use user simulation in conversational systems in the future.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "322",
        "title": "OmniInsert: Mask-Free Video Insertion of Any Reference via Diffusion Transformer Models",
        "author": [
            "Jinshu Chen",
            "Xinghui Li",
            "Xu Bai",
            "Tianxiang Ma",
            "Pengze Zhang",
            "Zhuowei Chen",
            "Gen Li",
            "Lijie Liu",
            "Songtao Zhao",
            "Bingchuan Li",
            "Qian He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17627",
        "abstract": "Recent advances in video insertion based on diffusion models are impressive. However, existing methods rely on complex control signals but struggle with subject consistency, limiting their practical applicability. In this paper, we focus on the task of Mask-free Video Insertion and aim to resolve three key challenges: data scarcity, subject-scene equilibrium, and insertion harmonization. To address the data scarcity, we propose a new data pipeline InsertPipe, constructing diverse cross-pair data automatically. Building upon our data pipeline, we develop OmniInsert, a novel unified framework for mask-free video insertion from both single and multiple subject references. Specifically, to maintain subject-scene equilibrium, we introduce a simple yet effective Condition-Specific Feature Injection mechanism to distinctly inject multi-source conditions and propose a novel Progressive Training strategy that enables the model to balance feature injection from subjects and source video. Meanwhile, we design the Subject-Focused Loss to improve the detailed appearance of the subjects. To further enhance insertion harmonization, we propose an Insertive Preference Optimization methodology to optimize the model by simulating human preferences, and incorporate a Context-Aware Rephraser module during reference to seamlessly integrate the subject into the original scenes. To address the lack of a benchmark for the field, we introduce InsertBench, a comprehensive benchmark comprising diverse scenes with meticulously selected subjects. Evaluation on InsertBench indicates OmniInsert outperforms state-of-the-art closed-source commercial solutions. The code will be released.",
        "tags": [
            "DiT",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "323",
        "title": "MSCoRe: A Benchmark for Multi-Stage Collaborative Reasoning in LLM Agents",
        "author": [
            "Yuzhen Lei",
            "Hongbin Xie",
            "Jiaxing Zhao",
            "Shuangxue Liu",
            "Xuan Song"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17628",
        "abstract": "Large Language Models (LLMs) have excelled in question-answering (QA) tasks within single domains. However, their reasoning and coordination capabilities in complex, multi-stage scenarios remain underexplored. Existing benchmarks typically focus on isolated tasks or narrow domains, overlooking models' abilities for multi-stage collaboration and optimization without explicit external guidance. To bridge this gap, we propose \\textbf{MSCoRe}, a novel benchmark comprising 126696 domain-specific QA instances spanning scenarios in automotive, pharmaceutical, electronics, and energy sectors. The dataset is created using a structured three-phase pipeline: dynamic sampling, iterative question-answer generation, and a multi-level quality assessment to ensure data quality. Tasks are further categorized into three difficulty levels according to stage coverage and complexity. With MSCoRe, we have conducted a comprehensive evaluation of various state-of-the-art LLM agents. The commercial models performed best across all tasks and scenarios, but a notable gap in ROUGE scores remains between simple and complex tasks. We also tested the models' robustness and found that their performance is negatively affected by noisy data. MSCoRe provides a valuable new resource for the community to evaluate and improve multi-stage reasoning in LLM agents. The code and data are available at https://github.com/D3E0-source/MSCoRE.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "324",
        "title": "AuditoryBench++: Can Language Models Understand Auditory Knowledge without Hearing?",
        "author": [
            "Hyunjong Ok",
            "Suho Yoo",
            "Hyeonjun Kim",
            "Jaeho Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17641",
        "abstract": "Even without directly hearing sounds, humans can effortlessly reason about auditory properties, such as pitch, loudness, or sound-source associations, drawing on auditory commonsense. In contrast, language models often lack this capability, limiting their effectiveness in multimodal interactions. As an initial step to address this gap, we present AuditoryBench++, a comprehensive benchmark for evaluating auditory knowledge and reasoning in text-only settings. The benchmark encompasses tasks that range from basic auditory comparisons to contextually grounded reasoning, enabling fine-grained analysis of how models process and integrate auditory concepts. In addition, we introduce AIR-CoT, a novel auditory imagination reasoning method that generates and integrates auditory information during inference through span detection with special tokens and knowledge injection. Extensive experiments with recent LLMs and Multimodal LLMs demonstrate that AIR-CoT generally outperforms both the off-the-shelf models and those augmented with auditory knowledge. The project page is available at https://auditorybenchpp.github.io.",
        "tags": [
            "CoT",
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "325",
        "title": "SISMA: Semantic Face Image Synthesis with Mamba",
        "author": [
            "Filippo Botti",
            "Alex Ergasti",
            "Tomaso Fontanini",
            "Claudio Ferrari",
            "Massimo Bertozzi",
            "Andrea Prati"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17651",
        "abstract": "Diffusion Models have become very popular for Semantic Image Synthesis (SIS) of human faces. Nevertheless, their training and inference is computationally expensive and their computational requirements are high due to the quadratic complexity of attention layers. In this paper, we propose a novel architecture called SISMA, based on the recently proposed Mamba. SISMA generates high quality samples by controlling their shape using a semantic mask at a reduced computational demand. We validated our approach through comprehensive experiments with CelebAMask-HQ, revealing that our architecture not only achieves a better FID score yet also operates at three times the speed of state-of-the-art architectures. This indicates that the proposed design is a viable, lightweight substitute to transformer-based models.",
        "tags": [
            "Diffusion",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "326",
        "title": "Clothing agnostic Pre-inpainting Virtual Try-ON",
        "author": [
            "Sehyun Kim",
            "Hye Jun Lee",
            "Jiwoo Lee",
            "Taemin Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17654",
        "abstract": "With the development of deep learning technology, virtual try-on technology has become an important application value in the fields of e-commerce, fashion, and entertainment. The recently proposed Leffa has improved the texture distortion problem of diffu-sion-based models, but there are limitations in that the bottom detection inaccuracy and the existing clothing silhouette remain in the synthesis results. To solve this problem, this study proposes CaP-VTON (Clothing agnostic Pre-inpainting Virtual Try-ON). CaP-VTON has improved the naturalness and consistency of whole-body clothing syn-thesis by integrating multi-category masking based on Dress Code and skin inpainting based on Stable Diffusion. In particular, a generate skin module was introduced to solve the skin restoration problem that occurs when long-sleeved images are converted into short-sleeved or sleeveless ones, and high-quality restoration was implemented consider-ing the human body posture and color. As a result, CaP-VTON recorded 92.5\\%, which is 15.4\\% better than Leffa in short-sleeved synthesis accuracy, and showed the performance of consistently reproducing the style and shape of reference clothing in visual evaluation. These structures maintain model-agnostic properties and are applicable to various diffu-sion-based virtual inspection systems, and can contribute to applications that require high-precision virtual wearing, such as e-commerce, custom styling, and avatar creation.",
        "tags": [
            "Detection",
            "Diffusion",
            "Inpainting",
            "Virtual Try-On"
        ]
    },
    {
        "id": "327",
        "title": "SD-VLM: Spatial Measuring and Understanding with Depth-Encoded Vision-Language Models",
        "author": [
            "Pingyi Chen",
            "Yujing Lou",
            "Shen Cao",
            "Jinhui Guo",
            "Lubin Fan",
            "Yue Wu",
            "Lin Yang",
            "Lizhuang Ma",
            "Jieping Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17664",
        "abstract": "While vision language models (VLMs) excel in 2D semantic visual understanding, their ability to quantitatively reason about 3D spatial relationships remains under-explored, due to the deficiency of 2D images' spatial representation ability. In this paper, we analyze the problem hindering VLMs' spatial understanding abilities and propose SD-VLM, a novel framework that significantly enhances fundamental spatial perception abilities of VLMs through two key contributions: (1) propose Massive Spatial Measuring and Understanding (MSMU) dataset with precise spatial annotations, and (2) introduce a simple depth positional encoding method strengthening VLMs' spatial awareness. MSMU dataset covers massive quantitative spatial tasks with 700K QA pairs, 2.5M physical numerical annotations, and 10K chain-of-thought augmented samples. We have trained SD-VLM, a strong generalist VLM which shows superior quantitative spatial measuring and understanding capability. SD-VLM not only achieves state-of-the-art performance on our proposed MSMU-Bench, but also shows spatial generalization abilities on other spatial understanding benchmarks including Q-Spatial and SpatialRGPT-Bench. Extensive experiments demonstrate that SD-VLM outperforms GPT-4o and Intern-VL3-78B by 26.91% and 25.56% respectively on MSMU-Bench. Code and models are released at https://github.com/cpystan/SD-VLM.",
        "tags": [
            "3D",
            "CoT",
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "328",
        "title": "Mechanistic Interpretability with SAEs: Probing Religion, Violence, and Geography in Large Language Models",
        "author": [
            "Katharina Simbeck",
            "Mariam Mahran"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17665",
        "abstract": "Despite growing research on bias in large language models (LLMs), most work has focused on gender and race, with little attention to religious identity. This paper explores how religion is internally represented in LLMs and how it intersects with concepts of violence and geography. Using mechanistic interpretability and Sparse Autoencoders (SAEs) via the Neuronpedia API, we analyze latent feature activations across five models. We measure overlap between religion- and violence-related prompts and probe semantic patterns in activation contexts. While all five religions show comparable internal cohesion, Islam is more frequently linked to features associated with violent language. In contrast, geographic associations largely reflect real-world religious demographics, revealing how models embed both factual distributions and cultural stereotypes. These findings highlight the value of structural analysis in auditing not just outputs but also internal representations that shape model behavior.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "329",
        "title": "Robust and Resilient Soft Robotic Object Insertion with Compliance-Enabled Contact Formation and Failure Recovery",
        "author": [
            "Mimo Shirasaka",
            "Cristian C. Beltran-Hernandez",
            "Masashi Hamaya",
            "Yoshitaka Ushiku"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17666",
        "abstract": "Object insertion tasks are prone to failures under pose uncertainties and environmental variations, traditionally requiring manual finetuning or controller retraining. We present a novel approach for robust and resilient object insertion using a passively compliant soft wrist that enables safe contact absorption through large deformations, without high-frequency control or force sensing. Our method structures insertion as compliance-enabled contact formations, sequential contact states that progressively constrain degrees of freedom, and integrates automated failure recovery strategies. Our key insight is that wrist compliance permits safe, repeated recovery attempts; hence, we refer to it as compliance-enabled failure recovery. We employ a pre-trained vision-language model (VLM) that assesses each skill execution from terminal poses and images, identifies failure modes, and proposes recovery actions by selecting skills and updating goals. In simulation, our method achieved an 83% success rate, recovering from failures induced by randomized conditions--including grasp misalignments up to 5 degrees, hole-pose errors up to 20mm, fivefold increases in friction, and previously unseen square/rectangular pegs--and we further validate the approach on a real robot.",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "330",
        "title": "PG-CE: A Progressive Generation Dataset with Constraint Enhancement for Controllable Text Generation",
        "author": [
            "Yan Zhuang",
            "Yuan Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17669",
        "abstract": "With the rapid development of Large Language Models (LLMs), Controllable Text Generation (CTG) has become a critical technology for enhancing system reliability and user experience. Addressing the limitations of traditional methods, this paper proposes the PG-CE (Progressive Generation with Constraint Enhancement) approach, which decomposes CTG tasks into three steps: type prediction, constraint construction, and guided generation. This method employs constraint generation models to dynamically build multi-dimensional constraints including tone, expression style, and thematic focus to guide output. Experiments demonstrate that PG-CE significantly improves generation quality across multiple scenarios while maintaining text controllability, thematic relevance, and response practicality. The research developed a dataset containing 90,000 constraint-text pairs (with an 8:2 ratio between daily and other topics), effectively reflecting real-world application requirements.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "331",
        "title": "Turk-LettuceDetect: A Hallucination Detection Models for Turkish RAG Applications",
        "author": [
            "Selva TaÅ",
            "Mahmut El Huseyni",
            "Ãzay Ezerceli",
            "Reyhan Bayraktar",
            "Fatma BetÃ¼l TerzioÄlu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17671",
        "abstract": "The widespread adoption of Large Language Models (LLMs) has been hindered by their tendency to hallucinate, generating plausible but factually incorrect information. While Retrieval-Augmented Generation (RAG) systems attempt to address this issue by grounding responses in external knowledge, hallucination remains a persistent challenge, particularly for morphologically complex, low-resource languages like Turkish. This paper introduces Turk-LettuceDetect, the first suite of hallucination detection models specifically designed for Turkish RAG applications. Building on the LettuceDetect framework, we formulate hallucination detection as a token-level classification task and fine-tune three distinct encoder architectures: a Turkish-specific ModernBERT, TurkEmbed4STS, and multilingual EuroBERT. These models were trained on a machine-translated version of the RAGTruth benchmark dataset containing 17,790 instances across question answering, data-to-text generation, and summarization tasks. Our experimental results show that the ModernBERT-based model achieves an F1-score of 0.7266 on the complete test set, with particularly strong performance on structured tasks. The models maintain computational efficiency while supporting long contexts up to 8,192 tokens, making them suitable for real-time deployment. Comparative analysis reveals that while state-of-the-art LLMs demonstrate high recall, they suffer from low precision due to over-generation of hallucinated content, underscoring the necessity of specialized detection mechanisms. By releasing our models and translated dataset, this work addresses a critical gap in multilingual NLP and establishes a foundation for developing more reliable and trustworthy AI applications for Turkish and other languages.",
        "tags": [
            "Detection",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "332",
        "title": "GLo-MAPPO: A Multi-Agent Proximal Policy Optimization for Energy Efficiency in UAV-Assisted LoRa Networks",
        "author": [
            "Abdullahi Isa Ahmed",
            "Jamal Bentahar",
            "El Mehdi Amhoud"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17676",
        "abstract": "Long Range (LoRa) based low-power wide area networks (LPWANs) are crucial for enabling next-generation IoT (NG-IoT) applications in 5G/6G ecosystems due to their long-range, low-power, and low-cost characteristics. However, achieving high energy efficiency in such networks remains a critical challenge, particularly in large-scale or dynamically changing environments. Traditional terrestrial LoRa deployments often suffer from coverage gaps and non-line-of-sight (NLoS) propagation losses, while satellite-based IoT solutions consume excessive energy and introduce high latency, limiting their suitability for energy-constrained and delay-sensitive applications. To address these limitations, we propose a novel architecture using multiple unmanned aerial vehicles (UAVs) as flying LoRa gateways to dynamically collect data from ground-based LoRa end devices. Our approach aims to maximize the system's weighted global energy efficiency by jointly optimizing spreading factors, transmission powers, UAV trajectories, and end-device associations. Additionally, we formulate this complex optimization problem as a partially observable Markov decision process (POMDP) and propose green LoRa multi-agent proximal policy optimization (GLo-MAPPO), a multi-agent reinforcement learning (MARL) framework based on centralized training with decentralized execution (CTDE). Simulation results show that GLo-MAPPO significantly outperforms benchmark algorithms, achieving energy efficiency improvements of 71.25%, 18.56%, 67.00%, 59.73%, and 49.95% for networks with 10, 20, 30, 40, and 50 LoRa end devices, respectively.",
        "tags": [
            "LoRA",
            "RL"
        ]
    },
    {
        "id": "333",
        "title": "EngiBench: A Benchmark for Evaluating Large Language Models on Engineering Problem Solving",
        "author": [
            "Xiyuan Zhou",
            "Xinlei Wang",
            "Yirui He",
            "Yang Wu",
            "Ruixi Zou",
            "Yuheng Cheng",
            "Yulu Xie",
            "Wenxuan Liu",
            "Huan Zhao",
            "Yan Xu",
            "Jinjin Gu",
            "Junhua Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17677",
        "abstract": "Large language models (LLMs) have shown strong performance on mathematical reasoning under well-posed conditions. However, real-world engineering problems require more than mathematical symbolic computation -- they need to deal with uncertainty, context, and open-ended scenarios. Existing benchmarks fail to capture these complexities. We introduce EngiBench, a hierarchical benchmark designed to evaluate LLMs on solving engineering problems. It spans three levels of increasing difficulty (foundational knowledge retrieval, multi-step contextual reasoning, and open-ended modeling) and covers diverse engineering subfields. To facilitate a deeper understanding of model performance, we systematically rewrite each problem into three controlled variants (perturbed, knowledge-enhanced, and math abstraction), enabling us to separately evaluate the model's robustness, domain-specific knowledge, and mathematical reasoning abilities. Experiment results reveal a clear performance gap across levels: models struggle more as tasks get harder, perform worse when problems are slightly changed, and fall far behind human experts on the high-level engineering tasks. These findings reveal that current LLMs still lack the high-level reasoning needed for real-world engineering, highlighting the need for future models with deeper and more reliable problem-solving capabilities. Our source code and data are available at https://github.com/EngiBench/EngiBench.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "334",
        "title": "When TableQA Meets Noise: A Dual Denoising Framework for Complex Questions and Large-scale Tables",
        "author": [
            "Shenghao Ye",
            "Yu Guo",
            "Dong Jin",
            "Yikai Shen",
            "Yunpeng Hou",
            "Shuangwu Chen",
            "Jian Yang",
            "Xiaofeng Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17680",
        "abstract": "Table question answering (TableQA) is a fundamental task in natural language processing (NLP). The strong reasoning capabilities of large language models (LLMs) have brought significant advances in this field. However, as real-world applications involve increasingly complex questions and larger tables, substantial noisy data is introduced, which severely degrades reasoning performance. To address this challenge, we focus on improving two core capabilities: Relevance Filtering, which identifies and retains information truly relevant to reasoning, and Table Pruning, which reduces table size while preserving essential content. Based on these principles, we propose EnoTab, a dual denoising framework for complex questions and large-scale tables. Specifically, we first perform Evidence-based Question Denoising by decomposing the question into minimal semantic units and filtering out those irrelevant to answer reasoning based on consistency and usability criteria. Then, we propose Evidence Tree-guided Table Denoising, which constructs an explicit and transparent table pruning path to remove irrelevant data step by step. At each pruning step, we observe the intermediate state of the table and apply a post-order node rollback mechanism to handle abnormal table states, ultimately producing a highly reliable sub-table for final answer reasoning. Finally, extensive experiments show that EnoTab achieves outstanding performance on TableQA tasks with complex questions and large-scale tables, confirming its effectiveness.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "335",
        "title": "Towards Learning Boulder Excavation with Hydraulic Excavators",
        "author": [
            "Jonas Gruetter",
            "Lorenzo Terenzi",
            "Pascal Egli",
            "Marco Hutter"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17683",
        "abstract": "Construction sites frequently require removing large rocks before excavation or grading can proceed. Human operators typically extract these boulders using only standard digging buckets, avoiding time-consuming tool changes to specialized grippers. This task demands manipulating irregular objects with unknown geometries in harsh outdoor environments where dust, variable lighting, and occlusions hinder perception. The excavator must adapt to varying soil resistance--dragging along hard-packed surfaces or penetrating soft ground--while coordinating multiple hydraulic joints to secure rocks using a shovel. Current autonomous excavation focuses on continuous media (soil, gravel) or uses specialized grippers with detailed geometric planning for discrete objects. These approaches either cannot handle large irregular rocks or require impractical tool changes that interrupt workflow. We train a reinforcement learning policy in simulation using rigid-body dynamics and analytical soil models. The policy processes sparse LiDAR points (just 20 per rock) from vision-based segmentation and proprioceptive feedback to control standard excavator buckets. The learned agent discovers different strategies based on soil resistance: dragging along the surface in hard soil and penetrating directly in soft conditions. Field tests on a 12-ton excavator achieved 70% success across varied rocks (0.4-0.7m) and soil types, compared to 83% for human operators. This demonstrates that standard construction equipment can learn complex manipulation despite sparse perception and challenging outdoor conditions.",
        "tags": [
            "RL",
            "Segmentation"
        ]
    },
    {
        "id": "336",
        "title": "DINOv3-Diffusion Policy: Self-Supervised Large Visual Model for Visuomotor Diffusion Policy Learning",
        "author": [
            "ThankGod Egbe",
            "Peng Wang",
            "Zhihao Guo",
            "Zidong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17684",
        "abstract": "This paper evaluates DINOv3, a recent large-scale self-supervised vision backbone, for visuomotor diffusion policy learning in robotic manipulation. We investigate whether a purely self-supervised encoder can match or surpass conventional supervised ImageNet-pretrained backbones (e.g., ResNet-18) under three regimes: training from scratch, frozen, and finetuned. Across four benchmark tasks (Push-T, Lift, Can, Square) using a unified FiLM-conditioned diffusion policy, we find that (i) finetuned DINOv3 matches or exceeds ResNet-18 on several tasks, (ii) frozen DINOv3 remains competitive, indicating strong transferable priors, and (iii) self-supervised features improve sample efficiency and robustness. These results support self-supervised large visual models as effective, generalizable perceptual front-ends for action diffusion policies, motivating further exploration of scalable label-free pretraining in robotic manipulation. Compared to using ResNet18 as a backbone, our approach with DINOv3 achieves up to a 10% absolute increase in test-time success rates on challenging tasks such as Can, and on-the-par performance in tasks like Lift, PushT, and Square.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "337",
        "title": "TASO: Task-Aligned Sparse Optimization for Parameter-Efficient Model Adaptation",
        "author": [
            "Daiye Miao",
            "Yufang Liu",
            "Jie Wang",
            "Changzhi Sun",
            "Yunke Zhang",
            "Demei Yan",
            "Shaokang Dong",
            "Qi Zhang",
            "Yuanbin Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17688",
        "abstract": "LoRA has become one of the most widely used parameter-efficient fine-tuning methods due to its simplicity and effectiveness. However, numerous studies have shown that LoRA often introduces substantial parameter redundancy, which not only increases the number of trainable parameters but also hinders the effectiveness of fine-tuning. Since identifying redundant parameters in LoRA is inherently difficult, how to eliminate them efficiently and accurately remains a challenging problem. In this paper, we propose TASO, a redundancy reduction method that leverages importance information from the pretrained model's weights to mitigate LoRA redundancy. Specifically, we estimate parameter importance on downstream tasks and identify task-specific core regions based on the distribution of importance scores. The location information of these core regions is then used to determine the sparse structure of LoRA modules, enabling redundancy removal before fine-tuning. Our approach significantly reduces the number of trainable parameters required for task adaptation, while providing a novel task-aligned perspective for LoRA redundancy reduction. Experimental results demonstrate that, with a parameter budget comparable to LoRA with rank $r = 1$, TASO consistently outperforms standard LoRA across multiple tasks, achieving strong fine-tuning performance while effectively eliminating redundant parameters.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "338",
        "title": "RSU-Assisted Resource Allocation for Collaborative Perception",
        "author": [
            "Guowei Liu",
            "Le Liang",
            "Chongtao Guo",
            "Hao Ye",
            "Shi Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17691",
        "abstract": "As a pivotal technology for autonomous driving, collaborative perception enables vehicular agents to exchange perceptual data through vehicle-to-everything (V2X) communications, thereby enhancing perception accuracy of all collaborators. However, existing collaborative perception frameworks often assume ample communication resources, which is usually impractical in real-world vehicular networks. To address this challenge, this paper investigates the problem of communication resource allocation for collaborative perception and proposes RACooper, a novel RSU-assisted resource allocation framework that maximizes perception accuracy under constrained communication resources. RACooper leverages a hierarchical reinforcement learning model to dynamically allocate communication resources while accounting for real-time sensing data and channel dynamics induced by vehicular mobility. By jointly optimizing spatial confidence metrics and channel state information, our approach ensures efficient feature transmission, enhancing the effectiveness of collaborative perception. Simulation results demonstrate that compared to conventional baseline algorithms, RACooper achieves significant improvements in perception accuracy, especially under bandwidth-constrained scenarios.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "339",
        "title": "Evaluating LLM-Generated Versus Human-Authored Responses in Role-Play Dialogues",
        "author": [
            "Dongxu Lu",
            "Johan Jeuring",
            "Albert Gatt"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17694",
        "abstract": "Evaluating large language models (LLMs) in long-form, knowledge-grounded role-play dialogues remains challenging. This study compares LLM-generated and human-authored responses in multi-turn professional training simulations through human evaluation ($N=38$) and automated LLM-as-a-judge assessment. Human evaluation revealed significant degradation in LLM-generated response quality across turns, particularly in naturalness, context maintenance and overall quality, while human-authored responses progressively improved. In line with this finding, participants also indicated a consistent preference for human-authored dialogue. These human judgements were validated by our automated LLM-as-a-judge evaluation, where Gemini 2.0 Flash achieved strong alignment with human evaluators on both zero-shot pairwise preference and stochastic 6-shot construct ratings, confirming the widening quality gap between LLM and human responses over time. Our work contributes a multi-turn benchmark exposing LLM degradation in knowledge-grounded role-play dialogues and provides a validated hybrid evaluation framework to guide the reliable integration of LLMs in training simulations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "340",
        "title": "Investigating Bias: A Multilingual Pipeline for Generating, Solving, and Evaluating Math Problems with LLMs",
        "author": [
            "Mariam Mahran",
            "Katharina Simbeck"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17701",
        "abstract": "Large Language Models (LLMs) are increasingly used for educational support, yet their response quality varies depending on the language of interaction. This paper presents an automated multilingual pipeline for generating, solving, and evaluating math problems aligned with the German K-10 curriculum. We generated 628 math exercises and translated them into English, German, and Arabic. Three commercial LLMs (GPT-4o-mini, Gemini 2.5 Flash, and Qwen-plus) were prompted to produce step-by-step solutions in each language. A held-out panel of LLM judges, including Claude 3.5 Haiku, evaluated solution quality using a comparative framework. Results show a consistent gap, with English solutions consistently rated highest, and Arabic often ranked lower. These findings highlight persistent linguistic bias and the need for more equitable multilingual AI systems in education.",
        "tags": [
            "GPT",
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "341",
        "title": "DA-Mamba: Dialogue-aware selective state-space model for multimodal engagement estimation",
        "author": [
            "Shenwei Kang",
            "Xin Zhang",
            "Wen Liu",
            "Bin Li",
            "Yujie Liu",
            "Bo Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17711",
        "abstract": "Human engagement estimation in conversational scenarios is essential for applications such as adaptive tutoring, remote healthcare assessment, and socially aware human--computer interaction. Engagement is a dynamic, multimodal signal conveyed by facial expressions, speech, gestures, and behavioral cues over time. In this work we introduce DA-Mamba, a dialogue-aware multimodal architecture that replaces attention-heavy dialogue encoders with Mamba-based selective state-space processing to achieve linear time and memory complexity while retaining expressive cross-modal reasoning. We design a Mamba dialogue-aware selective state-space model composed of three core modules: a Dialogue-Aware Encoder, and two Mamba-based fusion mechanisms: Modality-Group Fusion and Partner-Group Fusion, these modules achieve expressive dialogue understanding. Extensive experiments on three standard benchmarks (NoXi, NoXi-Add, and MPIIGI) show that DA-Mamba surpasses prior state-of-the-art (SOTA) methods in concordance correlation coefficient (CCC), while reducing training time and peak memory; these gains enable processing much longer sequences and facilitate real-time deployment in resource-constrained, multi-party conversational settings. The source code will be available at: https://github.com/kksssssss-ssda/MMEA.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "342",
        "title": "ConfClip: Confidence-Weighted and Clipped Reward for Reinforcement Learning in LLMs",
        "author": [
            "Bonan Zhang",
            "Zhongqi Chen",
            "Bowen Song",
            "Qinya Li",
            "Fan Wu",
            "Guihai Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17730",
        "abstract": "Reinforcement learning (RL) has become a standard paradigm for refining large language models (LLMs) beyond pre-training and instruction tuning. A prominent line of work is RL with verifiable rewards (RLVR), which leverages automatically verifiable outcomes (e.g., correctness or executability) to generate reward signals. While efficient, this framework faces two key limitations: First, its binary feedback is too sparse to capture the quality of the reasoning process. Second, its coarse-grained rewards potentially lead to vanishing gradients. Inspired by observations from human learning, we introduce a RL technique that integrates verifiable outcomes with the model's own confidence estimates. This joint design enriches the reward signal, providing finer-grained feedback and implicitly supervising the reasoning process. Experimental results demonstrate that our proposed method enhances RL performance across multiple datasets and reduces token consumption during inference, while incurring negligible additional training cost. Moreover, it can be used as a plug-in module to enhance other state-of-the-art RL methods.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "343",
        "title": "Breaking Token Into Concepts: Exploring Extreme Compression in Token Representation Via Compositional Shared Semantics",
        "author": [
            "Kavin R V",
            "Pawan Goyal"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17737",
        "abstract": "Standard language models employ unique, monolithic embeddings for each token, potentially limiting their ability to capture the multifaceted nature of word meanings. We investigate whether tokens can be more effectively represented through a compositional structure that accumulates diverse semantic facets. To explore this, we propose Aggregate Semantic Grouping (ASG), a novel approach leveraging Product Quantization (PQ). We apply ASG to standard transformer architectures (mBERT, XLM-R, mT5) and evaluate this representational scheme across diverse tasks (NLI, NER, QA), as well as a biomedical domain-specific benchmark (BC5CDR) using BioBERT. Our findings demonstrate that representing tokens compositionally via ASG achieves extreme compression in embedding parameters (0.4--0.5\\%) while maintaining $>$95\\% task performance relative to the base model, even in generative tasks and extends to both cross lingual transfer and domain-specific settings. These results validate the principle that tokens can be effectively modeled as combinations of shared semantic building blocks. ASG offers a simple yet concrete method for achieving this, showcasing how compositional representations can capture linguistic richness while enabling compact yet semantically rich models.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "344",
        "title": "WISE: Weak-Supervision-Guided Step-by-Step Explanations for Multimodal LLMs in Image Classification",
        "author": [
            "Yiwen Jiang",
            "Deval Mehta",
            "Siyuan Yan",
            "Yaling Shen",
            "Zimu Wang",
            "Zongyuan Ge"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17740",
        "abstract": "Multimodal Large Language Models (MLLMs) have shown promise in visual-textual reasoning, with Multimodal Chain-of-Thought (MCoT) prompting significantly enhancing interpretability. However, existing MCoT methods rely on rationale-rich datasets and largely focus on inter-object reasoning, overlooking the intra-object understanding crucial for image classification. To address this gap, we propose WISE, a Weak-supervision-guided Step-by-step Explanation method that augments any image classification dataset with MCoTs by reformulating the concept-based representations from Concept Bottleneck Models (CBMs) into concise, interpretable reasoning chains under weak supervision. Experiments across ten datasets show that our generated MCoTs not only improve interpretability by 37% but also lead to gains in classification accuracy when used to fine-tune MLLMs. Our work bridges concept-based interpretability and generative MCoT reasoning, providing a generalizable framework for enhancing MLLMs in fine-grained visual understanding.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "345",
        "title": "Adaptive Fast-and-Slow Visual Program Reasoning for Long-Form VideoQA",
        "author": [
            "Chenglin Li",
            "Feng Han",
            "FengTao",
            "Ruilin Li",
            "Qianglong Chen",
            "Jingqi Tong",
            "Yin Zhang",
            "Jiaqi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17743",
        "abstract": "Large language models (LLMs) have shown promise in generating program workflows for visual tasks. However, previous approaches often rely on closed-source models, lack systematic reasoning, and struggle with long-form video question answering (videoQA). To address these challenges, we introduce the FS-VisPR framework, an adaptive visual program reasoning approach that balances fast reasoning for simple queries with slow reasoning for difficult ones. First, we design efficient visual modules (e.g., key clip retrieval and subtitle retrieval) to support long-form video tasks. Then, we construct a diverse and high-quality fast-slow reasoning dataset with a strong LLM to align open-source language models' ability to generate visual program workflows as FS-LLM. Next, we design a fast-slow reasoning framework with FS-LLM: Simple queries are directly solved by VideoLLMs, while difficult ones invoke visual program reasoning, motivated by human-like reasoning processes. During this process, low-confidence fast-thinking answers will trigger a second-stage slow-reasoning process, and a fallback mechanism to fast reasoning is activated if the program execution fails. Moreover, we improve visual programs through parameter search during both training and inference. By adjusting the parameters of the visual modules within the program, multiple variants are generated: during training, programs that yield correct answers are selected, while during inference, the program with the highest confidence result is applied. Experiments show that FS-VisPR improves both efficiency and reliability in visual program workflows. It achieves 50.4% accuracy on LVBench, surpassing GPT-4o, matching the performance of Qwen2.5VL-72B on VideoMME.",
        "tags": [
            "CLIP",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "346",
        "title": "Multi-Agent Amodal Completion: Direct Synthesis with Fine-Grained Semantic Guidance",
        "author": [
            "Hongxing Fan",
            "Lipeng Wang",
            "Haohua Chen",
            "Zehuan Huang",
            "Jiangtao Wu",
            "Lu Sheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17757",
        "abstract": "Amodal completion, generating invisible parts of occluded objects, is vital for applications like image editing and AR. Prior methods face challenges with data needs, generalization, or error accumulation in progressive pipelines. We propose a Collaborative Multi-Agent Reasoning Framework based on upfront collaborative reasoning to overcome these issues. Our framework uses multiple agents to collaboratively analyze occlusion relationships and determine necessary boundary expansion, yielding a precise mask for inpainting. Concurrently, an agent generates fine-grained textual descriptions, enabling Fine-Grained Semantic Guidance. This ensures accurate object synthesis and prevents the regeneration of occluders or other unwanted elements, especially within large inpainting areas. Furthermore, our method directly produces layered RGBA outputs guided by visible masks and attention maps from a Diffusion Transformer, eliminating extra segmentation. Extensive evaluations demonstrate our framework achieves state-of-the-art visual quality.",
        "tags": [
            "DiT",
            "Diffusion",
            "Image Editing",
            "Inpainting",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "347",
        "title": "MotionTrans: Human VR Data Enable Motion-Level Learning for Robotic Manipulation Policies",
        "author": [
            "Chengbo Yuan",
            "Rui Zhou",
            "Mengzhen Liu",
            "Yingdong Hu",
            "Shengjie Wang",
            "Li Yi",
            "Chuan Wen",
            "Shanghang Zhang",
            "Yang Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17759",
        "abstract": "Scaling real robot data is a key bottleneck in imitation learning, leading to the use of auxiliary data for policy training. While other aspects of robotic manipulation such as image or language understanding may be learned from internet-based datasets, acquiring motion knowledge remains challenging. Human data, with its rich diversity of manipulation behaviors, offers a valuable resource for this purpose. While previous works show that using human data can bring benefits, such as improving robustness and training efficiency, it remains unclear whether it can realize its greatest advantage: enabling robot policies to directly learn new motions for task completion. In this paper, we systematically explore this potential through multi-task human-robot cotraining. We introduce MotionTrans, a framework that includes a data collection system, a human data transformation pipeline, and a weighted cotraining strategy. By cotraining 30 human-robot tasks simultaneously, we direcly transfer motions of 13 tasks from human data to deployable end-to-end robot policies. Notably, 9 tasks achieve non-trivial success rates in zero-shot manner. MotionTrans also significantly enhances pretraining-finetuning performance (+40% success rate). Through ablation study, we also identify key factors for successful motion learning: cotraining with robot data and broad task-related motion coverage. These findings unlock the potential of motion-level learning from human data, offering insights into its effective use for training robotic manipulation policies. All data, code, and model weights are open-sourced https://motiontrans.github.io/.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "348",
        "title": "Enhancing the NAO: Extending Capabilities of Legacy Robots for Long-Term Research",
        "author": [
            "Austin Wilson",
            "Sahar Kapasi",
            "Zane Greene",
            "Alexis E. Block"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17760",
        "abstract": "Many research groups face challenges when legacy (unsupported) robotic platforms lose manufacturer support and cannot accommodate modern sensing, speech, and interaction capabilities. We present the Enhanced NAO, a revitalized version of Aldebaran's NAO robot that uses upgraded microphones, RGB-D and thermal cameras, and additional compute resources in a fully self-contained package. This system combines cloud and local models for perception and dialogue, while preserving the NAO's expressive body and behaviors. In a pilot validation study, the Enhanced NAO delivered significantly higher conversational quality and stronger user preference compared to the NAO AI Edition, without increasing response latency. Key upgrades, such as beamforming microphones and low-latency audio processing, reduced artifacts like self-hearing and improved multi-party separation. Expanded visual and thermal sensing established a foundation for future interaction capabilities. Beyond the NAO, our framework provides a platform-agnostic strategy for extending the lifespan and research utility of legacy robots, ensuring they remain valuable tools for human-robot interaction.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "349",
        "title": "Qwen3-Omni Technical Report",
        "author": [
            "Jin Xu",
            "Zhifang Guo",
            "Hangrui Hu",
            "Yunfei Chu",
            "Xiong Wang",
            "Jinzheng He",
            "Yuxuan Wang",
            "Xian Shi",
            "Ting He",
            "Xinfa Zhu",
            "Yuanjun Lv",
            "Yongqi Wang",
            "Dake Guo",
            "He Wang",
            "Linhan Ma",
            "Pei Zhang",
            "Xinyu Zhang",
            "Hongkun Hao",
            "Zishan Guo",
            "Baosong Yang",
            "Bin Zhang",
            "Ziyang Ma",
            "Xipin Wei",
            "Shuai Bai",
            "Keqin Chen",
            "Xuejing Liu",
            "Peng Wang",
            "Mingkun Yang",
            "Dayiheng Liu",
            "Xingzhang Ren",
            "Bo Zheng",
            "Rui Men",
            "Fan Zhou",
            "Bowen Yu",
            "Jianxin Yang",
            "Le Yu",
            "Jingren Zhou",
            "Junyang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17765",
        "abstract": "We present Qwen3-Omni, a single multimodal model that, for the first time, maintains state-of-the-art performance across text, image, audio, and video without any degradation relative to single-modal counterparts. Qwen3-Omni matches the performance of same-sized single-modal models within the Qwen series and excels particularly on audio tasks. Across 36 audio and audio-visual benchmarks, Qwen3-Omni achieves open-source SOTA on 32 benchmarks and overall SOTA on 22, outperforming strong closed-source models such as Gemini-2.5-Pro, Seed-ASR, and GPT-4o-Transcribe. Qwen3-Omni adopts a Thinker-Talker MoE architecture that unifies perception and generation across text, images, audio, and video, yielding fluent text and natural real-time speech. It supports text interaction in 119 languages, speech understanding in 19 languages, and speech generation in 10 languages. To reduce first-packet latency in streaming synthesis, Talker autoregressively predicts discrete speech codecs using a multi-codebook scheme. Leveraging the representational capacity of these codebooks, we replace computationally intensive block-wise diffusion with a lightweight causal ConvNet, enabling streaming from the first codec frame. In cold-start settings, Qwen3-Omni achieves a theoretical end-to-end first-packet latency of 234 ms. To further strengthen multimodal reasoning, we introduce a Thinking model that explicitly reasons over inputs from any modality. Since the research community currently lacks a general-purpose audio captioning model, we fine-tuned Qwen3-Omni-30B-A3B to obtain Qwen3-Omni-30B-A3B-Captioner, which produces detailed, low-hallucination captions for arbitrary audio inputs. Qwen3-Omni-30B-A3B, Qwen3-Omni-30B-A3B-Thinking, and Qwen3-Omni-30B-A3B-Captioner are publicly released under the Apache 2.0 license.",
        "tags": [
            "Diffusion",
            "GPT",
            "MoE",
            "Qwen"
        ]
    },
    {
        "id": "350",
        "title": "A State-Update Prompting Strategy for Efficient and Robust Multi-turn Dialogue",
        "author": [
            "Ziyi Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17766",
        "abstract": "Large Language Models (LLMs) struggle with information forgetting and inefficiency in long-horizon, multi-turn dialogues. To address this, we propose a training-free prompt engineering method, the State-Update Multi-turn Dialogue Strategy. It utilizes \"State Reconstruction\" and \"History Remind\" mechanisms to effectively manage dialogue history. Our strategy shows strong performance across multiple multi-hop QA datasets. For instance, on the HotpotQA dataset, it improves the core information filtering score by 32.6%, leading to a 14.1% increase in the downstream QA score, while also reducing inference time by 73.1% and token consumption by 59.4%. Ablation studies confirm the pivotal roles of both components. Our work offers an effective solution for optimizing LLMs in long-range interactions, providing new insights for developing more robust Agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "351",
        "title": "RoboSeek: You Need to Interact with Your Objects",
        "author": [
            "Yibo Peng",
            "Jiahao Yang",
            "Shenhao Yan",
            "Ziyu Huang",
            "Shuang Li",
            "Shuguang Cui",
            "Yiming Zhao",
            "Yatong Han"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17783",
        "abstract": "Optimizing and refining action execution through\nexploration and interaction is a promising way for robotic\nmanipulation. However, practical approaches to interaction driven robotic learning are still underexplored, particularly for\nlong-horizon tasks where sequential decision-making, physical\nconstraints, and perceptual uncertainties pose significant chal lenges. Motivated by embodied cognition theory, we propose\nRoboSeek, a framework for embodied action execution that\nleverages interactive experience to accomplish manipulation\ntasks. RoboSeek optimizes prior knowledge from high-level\nperception models through closed-loop training in simulation\nand achieves robust real-world execution via a real2sim2real\ntransfer pipeline. Specifically, we first replicate real-world\nenvironments in simulation using 3D reconstruction to provide\nvisually and physically consistent environments., then we train\npolicies in simulation using reinforcement learning and the\ncross-entropy method leveraging visual priors. The learned\npolicies are subsequently deployed on real robotic platforms\nfor execution. RoboSeek is hardware-agnostic and is evaluated\non multiple robotic platforms across eight long-horizon ma nipulation tasks involving sequential interactions, tool use, and\nobject handling. Our approach achieves an average success rate\nof 79%, significantly outperforming baselines whose success\nrates remain below 50%, highlighting its generalization and\nrobustness across tasks and platforms. Experimental results\nvalidate the effectiveness of our training framework in complex,\ndynamic real-world settings and demonstrate the stability of the\nproposed real2sim2real transfer mechanism, paving the way for\nmore generalizable embodied robotic learning. Project Page:\nhttps://russderrick.github.io/Roboseek/",
        "tags": [
            "3D",
            "RL"
        ]
    },
    {
        "id": "352",
        "title": "Revealing Multimodal Causality with Large Language Models",
        "author": [
            "Jin Li",
            "Shoujin Wang",
            "Qi Zhang",
            "Feng Liu",
            "Tongliang Liu",
            "Longbing Cao",
            "Shui Yu",
            "Fang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17784",
        "abstract": "Uncovering cause-and-effect mechanisms from data is fundamental to scientific progress. While large language models (LLMs) show promise for enhancing causal discovery (CD) from unstructured data, their application to the increasingly prevalent multimodal setting remains a critical challenge. Even with the advent of multimodal LLMs (MLLMs), their efficacy in multimodal CD is hindered by two primary limitations: (1) difficulty in exploring intra- and inter-modal interactions for comprehensive causal variable identification; and (2) insufficiency to handle structural ambiguities with purely observational data. To address these challenges, we propose MLLM-CD, a novel framework for multimodal causal discovery from unstructured data. It consists of three key components: (1) a novel contrastive factor discovery module to identify genuine multimodal factors based on the interactions explored from contrastive sample pairs; (2) a statistical causal structure discovery module to infer causal relationships among discovered factors; and (3) an iterative multimodal counterfactual reasoning module to refine the discovery outcomes iteratively by incorporating the world knowledge and reasoning capabilities of MLLMs. Extensive experiments on both synthetic and real-world datasets demonstrate the effectiveness of MLLM-CD in revealing genuine factors and causal relationships among them from multimodal unstructured data.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "353",
        "title": "Accurate and Efficient Low-Rank Model Merging in Core Space",
        "author": [
            "Aniello Panariello",
            "Daniel Marczak",
            "Simone Magistri",
            "Angelo Porrello",
            "BartÅomiej Twardowski",
            "Andrew D. Bagdanov",
            "Simone Calderara",
            "Joost van de Weijer"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17786",
        "abstract": "In this paper, we address the challenges associated with merging low-rank adaptations of large neural networks. With the rise of parameter-efficient adaptation techniques, such as Low-Rank Adaptation (LoRA), model fine-tuning has become more accessible. While fine-tuning models with LoRA is highly efficient, existing merging methods often sacrifice this efficiency by merging fully-sized weight matrices. We propose the Core Space merging framework, which enables the merging of LoRA-adapted models within a common alignment basis, thereby preserving the efficiency of low-rank adaptation while substantially improving accuracy across tasks. We further provide a formal proof that projection into Core Space ensures no loss of information and provide a complexity analysis showing the efficiency gains. Extensive empirical results demonstrate that Core Space significantly improves existing merging techniques and achieves state-of-the-art results on both vision and language tasks while utilizing a fraction of the computational resources. Codebase is available at https://github.com/apanariello4/core-space-merging.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "354",
        "title": "One Agent to Serve All: a Lite-Adaptive Stylized AI Assistant for Millions of Multi-Style Official Accounts",
        "author": [
            "Xingyu Fan",
            "Feifei Li",
            "Wenhui Que",
            "Hailong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17788",
        "abstract": "Conversational agents deployed in industrial-scale official account platforms must generate responses that are both contextually grounded and stylistically aligned-requirements that existing methods struggle to meet. Chain-of-thought (CoT) prompting induces significant latency due to multi-turn reasoning; per-account fine-tuning is computationally prohibitive; and long prompt-based methods degrade the model's ability to grasp injected context and style. In this paper, we propose WeStar, a lite-adaptive framework for stylized contextual question answering that scales to millions of official accounts. WeStar combines context-grounded generation via RAG with style-aware generation using Parametric RAG (PRAG), where LoRA modules are dynamically activated per style cluster. Our contributions are fourfold: (1) We introduce WeStar, a unified framework capable of serving large volumes of official accounts with minimal overhead. (2) We propose a multi-dimensional, cluster-based parameter sharing scheme that enables compact style representation while preserving stylistic diversity. (3) We develop a style-enhanced Direct Preference Optimization (SeDPO) method to optimize each style cluster's parameters for improved generation quality. (4) Experiments on a large-scale industrial dataset validate the effectiveness and efficiency of WeStar, underscoring its pracitical value in real-world deployment.",
        "tags": [
            "CoT",
            "LoRA",
            "RAG"
        ]
    },
    {
        "id": "355",
        "title": "Elucidating the Design Space of FP4 training",
        "author": [
            "Robert Hu",
            "Carlo Luschi",
            "Paul Balanca"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17791",
        "abstract": "The increasing computational demands of foundation models have spurred research into low-precision training, with 4-bit floating-point (\\texttt{FP4}) formats emerging as a frontier for maximizing hardware throughput. While numerous techniques have been proposed to stabilize \\texttt{FP4} training, they often present isolated solutions with varying, and not always clear, computational overheads. This paper aims to provide a unified view of the design space of \\texttt{FP4} training. We introduce a comprehensive, quantisation gradient-based framework for microscaling quantization that allows for a theoretical analysis of the computational costs associated with different stabilization methods on both the forward and backward passes. Using a simulator built on this framework, we conduct an extensive empirical study across a wide range of machine learning tasks, including regression, image classification, diffusion models, and language models. By systematically evaluating thousands of combinations of techniques, such as novel gradient approximations, rounding strategies, and scaling methods, we identify which configurations offer the most favourable performance-to-overhead trade-off. We find that the techniques enabling the best trade-off involve carefully combining Hadamard transformations, tensor scaling and stochastic rounding. We further find that using \\texttt{UE5M3} as a scaling factor potentially offers a good compromise between range and precision with manageable computational overhead.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "356",
        "title": "Solving time-fractional diffusion equations with Robin boundary conditions via fractional Hamiltonian boundary value methods",
        "author": [
            "Qian Luo",
            "Aiguo Xiao",
            "Xiaoqiang Yan",
            "Jingmin Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17793",
        "abstract": "In this paper, we propose a novel numerical scheme for solving time-fractional reaction-diffusion problems with Robin boundary conditions, where the time derivative is in the Caputo sense of order $\\alpha\\in(0,1)$. The existence and uniqueness of the solution is proved. Our proposed method is based on the spectral collocation method in space and Fractional Hamiltonian boundary value methods in time. For the considered spectral collocation method, the basis functions used are not the standard polynomial basis functions, but rather adapt to Robin boundary conditions, and the exponential convergence property is provided. The proposed procedure achieves spectral accuracy in space and is also capable of getting spectral accuracy in time. Some numerical examples are provided to support the theoretical results.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "357",
        "title": "Learning to vary: Teaching LMs to reproduce human linguistic variability in next-word prediction",
        "author": [
            "Tobias Groot",
            "Salo Lacunes",
            "Evgenia Ilia"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17794",
        "abstract": "Natural language generation (NLG) tasks are often subject to inherent variability; \\emph{e.g.} predicting the next word given a context has multiple valid responses, evident when asking multiple humans to complete the task. While having language models (LMs) that are aligned pluralistically, so that they are able to reproduce well the inherent diversity in perspectives of an entire population of interest is clearly beneficial, \\citet{ilia2024predict} show that LMs do not reproduce this type of linguistic variability well. They speculate this inability might stem from the lack of consistent training of LMs with data reflecting this type of inherent variability. As such, we investigate whether training LMs on multiple plausible word continuations per context can improve their ability to reproduce human linguistic variability for next-word prediction. We employ fine-tuning techniques for pre-trained and instruction-tuned models; and demonstrate their potential when fine-tuning GPT-2 and Mistral-7B-IT, using Provo Corpus. Our evaluation, which measures divergence among empirically estimated human and model next-word distributions across contexts before and after fine-tuning, shows that our multi-label fine-tuning improves the LMs' ability to reproduce linguistic variability; both for contexts that admit higher and lower variability.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "358",
        "title": "Findings of the Fourth Shared Task on Multilingual Coreference Resolution: Can LLMs Dethrone Traditional Approaches?",
        "author": [
            "Michal NovÃ¡k",
            "Miloslav KonopÃ­k",
            "Anna Nedoluzhko",
            "Martin Popel",
            "OndÅej PraÅ¾Ã¡k",
            "Jakub Sido",
            "Milan Straka",
            "ZdenÄk Å½abokrtskÃ½",
            "Daniel Zeman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17796",
        "abstract": "The paper presents an overview of the fourth edition of the Shared Task on Multilingual Coreference Resolution, organized as part of the CODI-CRAC 2025 workshop. As in the previous editions, participants were challenged to develop systems that identify mentions and cluster them according to identity coreference.\nA key innovation of this year's task was the introduction of a dedicated Large Language Model (LLM) track, featuring a simplified plaintext format designed to be more suitable for LLMs than the original CoNLL-U representation.\nThe task also expanded its coverage with three new datasets in two additional languages, using version 1.3 of CorefUD - a harmonized multilingual collection of 22 datasets in 17 languages.\nIn total, nine systems participated, including four LLM-based approaches (two fine-tuned and two using few-shot adaptation). While traditional systems still kept the lead, LLMs showed clear potential, suggesting they may soon challenge established approaches in future editions.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "359",
        "title": "MTM: A Multi-Scale Token Mixing Transformer for Irregular Multivariate Time Series Classification",
        "author": [
            "Shuhan Zhong",
            "Weipeng Zhuo",
            "Sizhe Song",
            "Guanyao Li",
            "Zhongyi Yu",
            "S.-H. Gary Chan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17809",
        "abstract": "Irregular multivariate time series (IMTS) is characterized by the lack of synchronized observations across its different channels. In this paper, we point out that this channel-wise asynchrony can lead to poor channel-wise modeling of existing deep learning methods. To overcome this limitation, we propose MTM, a multi-scale token mixing transformer for the classification of IMTS. We find that the channel-wise asynchrony can be alleviated by down-sampling the time series to coarser timescales, and propose to incorporate a masked concat pooling in MTM that gradually down-samples IMTS to enhance the channel-wise attention modules. Meanwhile, we propose a novel channel-wise token mixing mechanism which proactively chooses important tokens from one channel and mixes them with other channels, to further boost the channel-wise learning of our model. Through extensive experiments on real-world datasets and comparison with state-of-the-art methods, we demonstrate that MTM consistently achieves the best performance on all the benchmarks, with improvements of up to 3.8% in AUPRC for classification.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "360",
        "title": "Tac2Motion: Contact-Aware Reinforcement Learning with Tactile Feedback for Robotic Hand Manipulation",
        "author": [
            "Yitaek Kim",
            "Casper Hewson Rask",
            "Christoffer Sloth"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17812",
        "abstract": "This paper proposes Tac2Motion, a contact-aware reinforcement learning framework to facilitate the learning of contact-rich in-hand manipulation tasks, such as removing a lid. To this end, we propose tactile sensing-based reward shaping and incorporate the sensing into the observation space through embedding. The designed rewards encourage an agent to ensure firm grasping and smooth finger gaiting at the same time, leading to higher data efficiency and robust performance compared to the baseline. We verify the proposed framework on the opening a lid scenario, showing generalization of the trained policy into a couple of object types and various dynamics such as torsional friction. Lastly, the learned policy is demonstrated on the multi-fingered robot, Shadow Robot, showing that the control policy can be transferred to the real world. The video is available: https://youtu.be/poeJBPR7urQ.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "361",
        "title": "ContextFlow: Training-Free Video Object Editing via Adaptive Context Enrichment",
        "author": [
            "Yiyang Chen",
            "Xuanhua He",
            "Xiujun Ma",
            "Yue Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17818",
        "abstract": "Training-free video object editing aims to achieve precise object-level manipulation, including object insertion, swapping, and deletion. However, it faces significant challenges in maintaining fidelity and temporal consistency. Existing methods, often designed for U-Net architectures, suffer from two primary limitations: inaccurate inversion due to first-order solvers, and contextual conflicts caused by crude \"hard\" feature replacement. These issues are more challenging in Diffusion Transformers (DiTs), where the unsuitability of prior layer-selection heuristics makes effective guidance challenging. To address these limitations, we introduce ContextFlow, a novel training-free framework for DiT-based video object editing. In detail, we first employ a high-order Rectified Flow solver to establish a robust editing foundation. The core of our framework is Adaptive Context Enrichment (for specifying what to edit), a mechanism that addresses contextual conflicts. Instead of replacing features, it enriches the self-attention context by concatenating Key-Value pairs from parallel reconstruction and editing paths, empowering the model to dynamically fuse information. Additionally, to determine where to apply this enrichment (for specifying where to edit), we propose a systematic, data-driven analysis to identify task-specific vital layers. Based on a novel Guidance Responsiveness Metric, our method pinpoints the most influential DiT blocks for different tasks (e.g., insertion, swapping), enabling targeted and highly effective guidance. Extensive experiments show that ContextFlow significantly outperforms existing training-free methods and even surpasses several state-of-the-art training-based approaches, delivering temporally coherent, high-fidelity results.",
        "tags": [
            "DiT",
            "Diffusion",
            "Rectified Flow"
        ]
    },
    {
        "id": "362",
        "title": "Fine-Grained Detection of AI-Generated Text Using Sentence-Level Segmentation",
        "author": [
            "Lekkala Sai Teja",
            "Annepaka Yadagiri",
            "and Partha Pakray",
            "Chukhu Chunka",
            "Mangadoddi Srikar Vardhan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17830",
        "abstract": "Generation of Artificial Intelligence (AI) texts in important works has become a common practice that can be used to misuse and abuse AI at various levels. Traditional AI detectors often rely on document-level classification, which struggles to identify AI content in hybrid or slightly edited texts designed to avoid detection, leading to concerns about the model's efficiency, which makes it hard to distinguish between human-written and AI-generated texts. A sentence-level sequence labeling model proposed to detect transitions between human- and AI-generated text, leveraging nuanced linguistic signals overlooked by document-level classifiers. By this method, detecting and segmenting AI and human-written text within a single document at the token-level granularity is achieved. Our model combines the state-of-the-art pre-trained Transformer models, incorporating Neural Networks (NN) and Conditional Random Fields (CRFs). This approach extends the power of transformers to extract semantic and syntactic patterns, and the neural network component to capture enhanced sequence-level representations, thereby improving the boundary predictions by the CRF layer, which enhances sequence recognition and further identification of the partition between Human- and AI-generated texts. The evaluation is performed on two publicly available benchmark datasets containing collaborative human and AI-generated texts. Our experimental comparisons are with zero-shot detectors and the existing state-of-the-art models, along with rigorous ablation studies to justify that this approach, in particular, can accurately detect the spans of AI texts in a completely collaborative text. All our source code and the processed datasets are available in our GitHub repository.",
        "tags": [
            "Detection",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "363",
        "title": "Conv-like Scale-Fusion Time Series Transformer: A Multi-Scale Representation for Variable-Length Long Time Series",
        "author": [
            "Kai Zhang",
            "Siming Sun",
            "Zhengyu Fan",
            "Qinmin Yang",
            "Xuejun Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17845",
        "abstract": "Time series analysis faces significant challenges in handling variable-length data and achieving robust generalization. While Transformer-based models have advanced time series tasks, they often struggle with feature redundancy and limited generalization capabilities. Drawing inspiration from classical CNN architectures' pyramidal structure, we propose a Multi-Scale Representation Learning Framework based on a Conv-like ScaleFusion Transformer. Our approach introduces a temporal convolution-like structure that combines patching operations with multi-head attention, enabling progressive temporal dimension compression and feature channel expansion. We further develop a novel cross-scale attention mechanism for effective feature fusion across different temporal scales, along with a log-space normalization method for variable-length sequences. Extensive experiments demonstrate that our framework achieves superior feature independence, reduced redundancy, and better performance in forecasting and classification tasks compared to state-of-the-art methods.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "364",
        "title": "Semantic and Visual Crop-Guided Diffusion Models for Heterogeneous Tissue Synthesis in Histopathology",
        "author": [
            "Saghir Alfasly",
            "Wataru Uegami",
            "MD Enamul Hoq",
            "Ghazal Alabtah",
            "H.R. Tizhoosh"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17847",
        "abstract": "Synthetic data generation in histopathology faces unique challenges: preserving tissue heterogeneity, capturing subtle morphological features, and scaling to unannotated datasets. We present a latent diffusion model that generates realistic heterogeneous histopathology images through a novel dual-conditioning approach combining semantic segmentation maps with tissue-specific visual crops. Unlike existing methods that rely on text prompts or abstract visual embeddings, our approach preserves critical morphological details by directly incorporating raw tissue crops from corresponding semantic regions. For annotated datasets (i.e., Camelyon16, Panda), we extract patches ensuring 20-80% tissue heterogeneity. For unannotated data (i.e., TCGA), we introduce a self-supervised extension that clusters whole-slide images into 100 tissue types using foundation model embeddings, automatically generating pseudo-semantic maps for training. Our method synthesizes high-fidelity images with precise region-wise annotations, achieving superior performance on downstream segmentation tasks. When evaluated on annotated datasets, models trained on our synthetic data show competitive performance to those trained on real data, demonstrating the utility of controlled heterogeneous tissue generation. In quantitative evaluation, prompt-guided synthesis reduces Frechet Distance by up to 6X on Camelyon16 (from 430.1 to 72.0) and yields 2-3x lower FD across Panda and TCGA. Downstream DeepLabv3+ models trained solely on synthetic data attain test IoU of 0.71 and 0.95 on Camelyon16 and Panda, within 1-2% of real-data baselines (0.72 and 0.96). By scaling to 11,765 TCGA whole-slide images without manual annotations, our framework offers a practical solution for an urgent need for generating diverse, annotated histopathology data, addressing a critical bottleneck in computational pathology.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "365",
        "title": "SocialTraj: Two-Stage Socially-Aware Trajectory Prediction for Autonomous Driving via Conditional Diffusion Model",
        "author": [
            "Xiao Zhou",
            "Zengqi Peng",
            "Jun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17850",
        "abstract": "Accurate trajectory prediction of surrounding vehicles (SVs) is crucial for autonomous driving systems to avoid misguided decisions and potential accidents. However, achieving reliable predictions in highly dynamic and complex traffic scenarios remains a significant challenge. One of the key impediments lies in the limited effectiveness of current approaches to capture the multi-modal behaviors of drivers, which leads to predicted trajectories that deviate from actual future motions. To address this issue, we propose SocialTraj, a novel trajectory prediction framework integrating social psychology principles through social value orientation (SVO). By utilizing Bayesian inverse reinforcement learning (IRL) to estimate the SVO of SVs, we obtain the critical social context to infer the future interaction trend. To ensure modal consistency in predicted behaviors, the estimated SVOs of SVs are embedded into a conditional denoising diffusion model that aligns generated trajectories with historical driving styles. Additionally, the planned future trajectory of the ego vehicle (EV) is explicitly incorporated to enhance interaction modeling. Extensive experiments on NGSIM and HighD datasets demonstrate that SocialTraj is capable of adapting to highly dynamic and interactive scenarios while generating socially compliant and behaviorally consistent trajectory predictions, outperforming existing baselines. Ablation studies demonstrate that dynamic SVO estimation and explicit ego-planning components notably improve prediction accuracy and substantially reduce inference time.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "366",
        "title": "Make Every Letter Count: Building Dialect Variation Dictionaries from Monolingual Corpora",
        "author": [
            "Robert Litschko",
            "Verena Blaschke",
            "Diana Burkhardt",
            "Barbara Plank",
            "Diego Frassinelli"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17855",
        "abstract": "Dialects exhibit a substantial degree of variation due to the lack of a standard orthography. At the same time, the ability of Large Language Models (LLMs) to process dialects remains largely understudied. To address this gap, we use Bavarian as a case study and investigate the lexical dialect understanding capability of LLMs by examining how well they recognize and translate dialectal terms across different parts-of-speech. To this end, we introduce DiaLemma, a novel annotation framework for creating dialect variation dictionaries from monolingual data only, and use it to compile a ground truth dataset consisting of 100K human-annotated German-Bavarian word pairs. We evaluate how well nine state-of-the-art LLMs can judge Bavarian terms as dialect translations, inflected variants, or unrelated forms of a given German lemma. Our results show that LLMs perform best on nouns and lexically similar word pairs, and struggle most in distinguishing between direct translations and inflected variants. Interestingly, providing additional context in the form of example usages improves the translation performance, but reduces their ability to recognize dialect variants. This study highlights the limitations of LLMs in dealing with orthographic dialect variation and emphasizes the need for future work on adapting LLMs to dialects.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "367",
        "title": "CorPipe at CRAC 2025: Evaluating Multilingual Encoders for Multilingual Coreference Resolution",
        "author": [
            "Milan Straka"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17858",
        "abstract": "We present CorPipe 25, the winning entry to the CRAC 2025 Shared Task on Multilingual Coreference Resolution. This fourth iteration of the shared task introduces a new LLM track alongside the original unconstrained track, features reduced development and test sets to lower computational requirements, and includes additional datasets. CorPipe 25 represents a complete reimplementation of our previous systems, migrating from TensorFlow to PyTorch. Our system significantly outperforms all other submissions in both the LLM and unconstrained tracks by a substantial margin of 8 percentage points. The source code and trained models are publicly available at https://github.com/ufal/crac2025-corpipe.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "368",
        "title": "Expert-as-a-Service: Towards Efficient, Scalable, and Robust Large-scale MoE Serving",
        "author": [
            "Ziming Liu",
            "Boyu Tian",
            "Guoteng Wang",
            "Zhen Jiang",
            "Peng Sun",
            "Zhenhua Han",
            "Tian Tang",
            "Xiaohe Hu",
            "Yanmin Jia",
            "Yan Zhang",
            "He Liu",
            "Mingjun Zhang",
            "Yiqi Zhang",
            "Qiaoling Chen",
            "Shenggan Cheng",
            "Mingyu Gao",
            "Yang You",
            "Siyuan Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17863",
        "abstract": "Mixture-of-Experts (MoE) models challenge serving infrastructures with dynamic, sparse expert utilization, causing instability on conventional systems designed for dense architectures. We propose EaaS, a novel serving system to enable efficient, scalable, and robust MoE deployment. Our system disaggregates MoE modules into independent, stateless services. This design enables fine-grained resource scaling and provides inherent fault tolerance by decoupling compute units. The architecture is powered by a high-performance, CPU-free peer-to-peer communication library that ensures minimal overhead and high throughput. Experiments confirm EaaS's scalability and efficiency, achieving performance comparable to monolithic systems while providing robust fault tolerance and strong scalability. EaaS incurs less than a 2% throughput reduction under simulated hardware failures that would otherwise halt monolithic architectures. It further saves up to 37.5% of computing resources through dynamic fine-grained adaptation to serving traffic, demonstrating strong resilience for large-scale MoE deployment in production.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "369",
        "title": "ProDyG: Progressive Dynamic Scene Reconstruction via Gaussian Splatting from Monocular Videos",
        "author": [
            "Shi Chen",
            "Erik SandstrÃ¶m",
            "Sandro Lombardi",
            "Siyuan Li",
            "Martin R. Oswald"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17864",
        "abstract": "Achieving truly practical dynamic 3D reconstruction requires online operation, global pose and map consistency, detailed appearance modeling, and the flexibility to handle both RGB and RGB-D inputs. However, existing SLAM methods typically merely remove the dynamic parts or require RGB-D input, while offline methods are not scalable to long video sequences, and current transformer-based feedforward methods lack global consistency and appearance details. To this end, we achieve online dynamic scene reconstruction by disentangling the static and dynamic parts within a SLAM system. The poses are tracked robustly with a novel motion masking strategy, and dynamic parts are reconstructed leveraging a progressive adaptation of a Motion Scaffolds graph. Our method yields novel view renderings competitive to offline methods and achieves on-par tracking with state-of-the-art dynamic SLAM methods.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "SLAM",
            "Transformer"
        ]
    },
    {
        "id": "370",
        "title": "Understanding Post-Training Structural Changes in Large Language Models",
        "author": [
            "Xinyu He",
            "Xianghui Cao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17866",
        "abstract": "Post-training fundamentally alters the behavior of large language models (LLMs), yet its impact on the internal parameter space remains poorly understood. In this work, we conduct a systematic singular value decomposition (SVD) analysis of principal linear layers in pretrained LLMs, focusing on two widely adopted post-training methods: instruction tuning and long-chain-of-thought (Long-CoT) distillation. Our analysis reveals two consistent and unexpected structural changes:(1) a near-uniform geometric scaling of singular values across layers, which theoretically modulates attention scores; and (2) highly consistent orthogonal transformations are applied to the left and right singular vectors of each matrix. Disrupting this orthogonal consistency leads to catastrophic performance degradation. Based on these findings, we propose a simple yet effective framework that interprets post-training as a reparameterization of fixed subspaces in the pretrained parameter space. Further experiments reveal that singular value scaling behaves as a secondary effect, analogous to a temperature adjustment, whereas the core functional transformation lies in the coordinated rotation of singular vectors. These results challenge the prevailing view of the parameter space in large models as a black box, uncovering the first clear regularities in how parameters evolve during training, and providing a new perspective for deeper investigation into model parameter changes.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "371",
        "title": "Deep Hierarchical Learning with Nested Subspace Networks",
        "author": [
            "Paulius Rauba",
            "Mihaela van der Schaar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17874",
        "abstract": "Large neural networks are typically trained for a fixed computational budget, creating a rigid trade-off between performance and efficiency that is ill-suited for deployment in resource-constrained or dynamic environments. Existing approaches to this problem present a difficult choice: training a discrete collection of specialist models is computationally prohibitive, while dynamic methods like slimmable networks often lack the flexibility to be applied to large, pre-trained foundation models. In this work, we propose Nested Subspace Networks (NSNs), a novel architectural paradigm that enables a single model to be dynamically and granularly adjusted across a continuous spectrum of compute budgets at inference time. The core of our approach is to re-parameterize linear layers to satisfy a nested subspace property, such that the function computed at a given rank is a strict subspace of the function at any higher rank. We show that this entire hierarchy of models can be optimized jointly via an uncertainty-aware objective that learns to balance the contributions of different ranks based on their intrinsic difficulty. We demonstrate empirically that NSNs can be surgically applied to pre-trained LLMs and unlock a smooth and predictable compute-performance frontier. For example, a single NSN-adapted model can achieve a 50% reduction in inference FLOPs with only a 5 percentage point loss in accuracy. Our findings establish NSNs as a powerful framework for creating the next generation of adaptive foundation models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "372",
        "title": "Sight Over Site: Perception-Aware Reinforcement Learning for Efficient Robotic Inspection",
        "author": [
            "Richard Kuhlmann",
            "Jakob Wolfram",
            "Boyang Sun",
            "Jiaxu Xing",
            "Davide Scaramuzza",
            "Marc Pollefeys",
            "Cesar Cadena"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17877",
        "abstract": "Autonomous inspection is a central problem in robotics, with applications ranging from industrial monitoring to search-and-rescue. Traditionally, inspection has often been reduced to navigation tasks, where the objective is to reach a predefined location while avoiding obstacles. However, this formulation captures only part of the real inspection problem. In real-world environments, the inspection targets may become visible well before their exact coordinates are reached, making further movement both redundant and inefficient. What matters more for inspection is not simply arriving at the target's position, but positioning the robot at a viewpoint from which the target becomes observable. In this work, we revisit inspection from a perception-aware perspective. We propose an end-to-end reinforcement learning framework that explicitly incorporates target visibility as the primary objective, enabling the robot to find the shortest trajectory that guarantees visual contact with the target without relying on a map. The learned policy leverages both perceptual and proprioceptive sensing and is trained entirely in simulation, before being deployed to a real-world robot. We further develop an algorithm to compute ground-truth shortest inspection paths, which provides a reference for evaluation. Through extensive experiments, we show that our method outperforms existing classical and learning-based navigation approaches, yielding more efficient inspection trajectories in both simulated and real-world settings. The project is avialable at https://sight-over-site.github.io/",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "373",
        "title": "The Surprising Effectiveness of Linear Models for Whole-Body Model-Predictive Control",
        "author": [
            "Arun L. Bishop",
            "Juan Alvarez-Padilla",
            "Sam Schoedel",
            "Ibrahima Sory Sow",
            "Juee Chandrachud",
            "Sheitej Sharma",
            "Will Kraus",
            "Beomyeong Park",
            "Robert J. Griffin",
            "John M. Dolan",
            "Zachary Manchester"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17884",
        "abstract": "When do locomotion controllers require reasoning about nonlinearities? In this work, we show that a whole-body model-predictive controller using a simple linear time-invariant approximation of the whole-body dynamics is able to execute basic locomotion tasks on complex legged robots. The formulation requires no online nonlinear dynamics evaluations or matrix inversions. We demonstrate walking, disturbance rejection, and even navigation to a goal position without a separate footstep planner on a quadrupedal robot. In addition, we demonstrate dynamic walking on a hydraulic humanoid, a robot with significant limb inertia, complex actuator dynamics, and large sim-to-real gap.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "374",
        "title": "GaussianPSL: A novel framework based on Gaussian Splatting for exploring the Pareto frontier in multi-criteria optimization",
        "author": [
            "Phuong Mai Dinh",
            "Van-Nam Huynh"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17889",
        "abstract": "Multi-objective optimization (MOO) is essential for solving complex real-world problems involving multiple conflicting objectives. However, many practical applications - including engineering design, autonomous systems, and machine learning - often yield non-convex, degenerate, or discontinuous Pareto frontiers, which involve traditional scalarization and Pareto Set Learning (PSL) methods that struggle to approximate accurately. Existing PSL approaches perform well on convex fronts but tend to fail in capturing the diversity and structure of irregular Pareto sets commonly observed in real-world scenarios. In this paper, we propose Gaussian-PSL, a novel framework that integrates Gaussian Splatting into PSL to address the challenges posed by non-convex Pareto frontiers. Our method dynamically partitions the preference vector space, enabling simple MLP networks to learn localized features within each region, which are then integrated by an additional MLP aggregator. This partition-aware strategy enhances both exploration and convergence, reduces sensi- tivity to initialization, and improves robustness against local optima. We first provide the mathematical formulation for controllable Pareto set learning using Gaussian Splat- ting. Then, we introduce the Gaussian-PSL architecture and evaluate its performance on synthetic and real-world multi-objective benchmarks. Experimental results demonstrate that our approach outperforms standard PSL models in learning irregular Pareto fronts while maintaining computational efficiency and model simplicity. This work offers a new direction for effective and scalable MOO under challenging frontier geometries.",
        "tags": [
            "Gaussian Splatting"
        ]
    },
    {
        "id": "375",
        "title": "Optimizing Inference in Transformer-Based Models: A Multi-Method Benchmark",
        "author": [
            "Siu Hang Ho",
            "Prasad Ganesan",
            "Nguyen Duong",
            "Daniel Schlabig"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17894",
        "abstract": "Efficient inference is a critical challenge in deep generative modeling, particularly as diffusion models grow in capacity and complexity. While increased complexity often improves accuracy, it raises compute costs, latency, and memory requirements. This work investigates techniques such as pruning, quantization, knowledge distillation, and simplified attention to reduce computational overhead without impacting performance. The study also explores the Mixture of Experts (MoE) approach to further enhance efficiency. These experiments provide insights into optimizing inference for the state-of-the-art Fast Diffusion Transformer (fast-DiT) model.",
        "tags": [
            "DiT",
            "Diffusion",
            "MoE",
            "Transformer"
        ]
    },
    {
        "id": "376",
        "title": "Lipschitz-Based Robustness Certification for Recurrent Neural Networks via Convex Relaxation",
        "author": [
            "Paul Hamelbeck",
            "Johannes Schiffer"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17898",
        "abstract": "Robustness certification against bounded input noise or adversarial perturbations is increasingly important for deployment recurrent neural networks (RNNs) in safety-critical control applications. To address this challenge, we present RNN-SDP, a relaxation based method that models the RNN's layer interactions as a convex problem and computes a certified upper bound on the Lipschitz constant via semidefinite programming (SDP). We also explore an extension that incorporates known input constraints to further tighten the resulting Lipschitz bounds. RNN-SDP is evaluated on a synthetic multi-tank system, with upper bounds compared to empirical estimates. While incorporating input constraints yields only modest improvements, the general method produces reasonably tight and certifiable bounds, even as sequence length increases. The results also underscore the often underestimated impact of initialization errors, an important consideration for applications where models are frequently re-initialized, such as model predictive control (MPC).",
        "tags": [
            "MPC",
            "RNN"
        ]
    },
    {
        "id": "377",
        "title": "Does Audio Matter for Modern Video-LLMs and Their Benchmarks?",
        "author": [
            "Geewook Kim",
            "Minjoon Seo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17901",
        "abstract": "Modern multimodal large language models often claim \"video understanding,\" yet most evaluations use muted videos or simply discard audio. We ask a direct question: how much does audio actually matter for contemporary Video-LLMs and the benchmarks that certify them? We audit widely used suites and observe that many items are even solvable from a single frame, rendering audio largely redundant. Building on LLaVA-OneVision architecture, we attach a speech/audio encoder (e.g., Whisper) and analyze when audio helps, while addressing audio token explosion with a lightweight Mamba-based state-space token compressor. We find that audio yields minimal gains on recent video benchmarks but is decisive on curated, audio-sensitive subsets. To enable faithful evaluation, we release AVQA-Hard and Music-AVQA-Hard, our model, and code. Our findings surface a growing gap between current academic practice and real-world expectations, and provide practical tools for scalable audio-visual Video-LLMs. We will fully open-source our work at https://github.com/naver-ai/LLaVA-AV-SSM.",
        "tags": [
            "LLM",
            "LLaVA",
            "Mamba"
        ]
    },
    {
        "id": "378",
        "title": "Mitigating Strategy-Selection Bias in Reasoning for More Effective Test-Time Scaling",
        "author": [
            "Zongqian Wu",
            "Baoduo Xu",
            "Tianyu Li",
            "Zhu Sun",
            "Xiaofeng Zhu",
            "Lei Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17905",
        "abstract": "Test-time scaling (TTS) has been shown to improve the performance of large language models (LLMs) by sampling and aggregating diverse reasoning paths. However, existing research has overlooked a critical issue: selection bias of reasoning strategies during scaling. Specifically, when generating reasoning processes, LLMs tend to follow certain strategies (e.g., algebraic solutions for math problems) while neglecting other valid alternatives (e.g., geometric solutions), resulting in insufficient exploration of the solution space. To further understand the impact of this bias, we present a theoretical analysis that reveals when it undermines the effectiveness of test-time scaling. Motivated by this theoretical insight, we introduce TTS-Uniform, a framework designed to mitigate the selection bias of reasoning strategies. It (i) identifies potential strategies, (ii) uniformly allocates the sampling budget across them, and (iii) filters out unstable strategies prior to aggregation. Experimental results show that TTS-Uniform significantly enhances scaling effectiveness across multiple mainstream LLMs and benchmark datasets.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "379",
        "title": "MEF: A Systematic Evaluation Framework for Text-to-Image Models",
        "author": [
            "Xiaojing Dong",
            "Weilin Huang",
            "Liang Li",
            "Yiying Li",
            "Shu Liu",
            "Tongtong Ou",
            "Shuang Ouyang",
            "Yu Tian",
            "Fengxuan Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17907",
        "abstract": "Rapid advances in text-to-image (T2I) generation have raised higher requirements for evaluation methodologies. Existing benchmarks center on objective capabilities and dimensions, but lack an application-scenario perspective, limiting external validity. Moreover, current evaluations typically rely on either ELO for overall ranking or MOS for dimension-specific scoring, yet both methods have inherent shortcomings and limited interpretability. Therefore, we introduce the Magic Evaluation Framework (MEF), a systematic and practical approach for evaluating T2I models. First, we propose a structured taxonomy encompassing user scenarios, elements, element compositions, and text expression forms to construct the Magic-Bench-377, which supports label-level assessment and ensures a balanced coverage of both user scenarios and capabilities. On this basis, we combine ELO and dimension-specific MOS to generate model rankings and fine-grained assessments respectively. This joint evaluation method further enables us to quantitatively analyze the contribution of each dimension to user satisfaction using multivariate logistic regression. By applying MEF to current T2I models, we obtain a leaderboard and key characteristics of the leading models. We release our evaluation framework and make Magic-Bench-377 fully open-source to advance research in the evaluation of visual generative models.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "380",
        "title": "XaaS Containers: Performance-Portable Representation With Source and IR Containers",
        "author": [
            "Marcin Copik",
            "Eiman Alnuaimi",
            "Alok Kamatar",
            "Valerie Hayot-Sasson",
            "Alberto Madonna",
            "Todd Gamblin",
            "Kyle Chard",
            "Ian Foster",
            "Torsten Hoefler"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17914",
        "abstract": "High-performance computing (HPC) systems and cloud data centers are converging, and containers are becoming the default method of portable software deployment. Yet, while containers simplify software management, they face significant performance challenges in HPC environments as they must sacrifice hardware-specific optimizations to achieve portability. Although HPC containers can use runtime hooks to access optimized MPI libraries and GPU devices, they are limited by application binary interface (ABI) compatibility and cannot overcome the effects of early-stage compilation decisions. Acceleration as a Service (XaaS) proposes a vision of performance-portable containers, where a containerized application should achieve peak performance across all HPC systems. We present a practical realization of this vision through Source and Intermediate Representation (IR) containers, where we delay performance-critical decisions until the target system specification is known. We analyze specialization mechanisms in HPC software and propose a new LLM-assisted method for automatic discovery of specializations. By examining the compilation pipeline, we develop a methodology to build containers optimized for target architectures at deployment time. Our prototype demonstrates that new XaaS containers combine the convenience of containerization with the performance benefits of system-specialized builds.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "381",
        "title": "Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent",
        "author": [
            "Junyu Lu",
            "Songxin Zhang",
            "Zejian Xie",
            "Zhuoyang Song",
            "Jiaxing Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17917",
        "abstract": "Recent advances in GUI agents have achieved remarkable grounding and action-prediction performance, yet existing models struggle with unreliable reward signals and limited online trajectory generation. In this paper, we introduce Orcust, a framework that integrates Principle-Constrained Reward Modeling (PCRM) and Online VM-Grounded Trajectory Construction (OVTC) to enhance reasoning reliability and data efficiency in interactive GUI tasks. We leverages environment-verifiable and LLM-derived principle to enforce interpretable reward signals that constrain long chain-of-thought reasoning and rule-based feedback. OVTC spins up instrumented virtual machines to autonomously collect structured GUI interaction trajectories with explicit procedural and structural objectives, enabling the training of a stepwise reward model that robustly captures human preferences and adheres to task-specific constraints. Extensive experiments on standard GUI benchmarks covering perceptual grounding, foundational operations, and end-to-end task execution reveal that Orcust achieves state-of-the-art performance, improving by 22.2\\% on ScreenSpot and 23.9\\% on ScreenSpot-Pro over the base model (i.e. Qwen2.5-VL-7B). The results demonstrate Orcust's effectiveness in enhancing the reasoning, adaptability and scalability of GUI agents across various environments and task complexities.",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "382",
        "title": "SingLEM: Single-Channel Large EEG Model",
        "author": [
            "Jamiyan Sukhbaatar",
            "Satoshi Imamura",
            "Ibuki Inoue",
            "Shoya Murakami",
            "Kazi Mahmudul Hassan",
            "Seungwoo Han",
            "Ingon Chanpornpakdi",
            "Toshihisa Tanaka"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17920",
        "abstract": "Current deep learning models for electroencephalography (EEG) are often task-specific and depend on large labeled datasets, limiting their adaptability. Although emerging foundation models aim for broader applicability, their rigid dependence on fixed, high-density multi-channel montages restricts their use across heterogeneous datasets and in missing-channel or practical low-channel settings. To address these limitations, we introduce SingLEM, a self-supervised foundation model that learns robust, general-purpose representations from single-channel EEG, making it inherently hardware agnostic. The model employs a hybrid encoder architecture that combines convolutional layers to extract local features with a hierarchical transformer to model both short- and long-range temporal dependencies. SingLEM is pretrained on 71 public datasets comprising over 9,200 subjects and 357,000 single-channel hours of EEG. When evaluated as a fixed feature extractor across six motor imagery and cognitive tasks, aggregated single-channel representations consistently outperformed leading multi-channel foundation models and handcrafted baselines. These results demonstrate that a single-channel approach can achieve state-of-the-art generalization while enabling fine-grained neurophysiological analysis and enhancing interpretability. The source code and pretrained models are available at https://github.com/ttlabtuat/SingLEM.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "383",
        "title": "Transformer-Encoder Trees for Efficient Multilingual Machine Translation and Speech Translation",
        "author": [
            "Yiwen Guan",
            "Jacob Whitehill"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17930",
        "abstract": "Multilingual translation faces challenges of computational redundancy and limited accuracy for low-resource languages, especially in speech translation. To address this, we propose a novel hierarchical Transformer Encoder Tree (TET) combined with non-autoregressive encoder-only models trained with Connectionist Temporal Classification for multilingual translation. By sharing intermediate representations among linguistically similar target languages, TET can improve accuracy on low-resource languages, reduce computational redundancy, and allow generating all target languages in a single forward pass, thus eliminating sequential bottlenecks and improving parallelism. For speech translation, combining TET with a non-autoregressive speech recognition backbone (wav2vec2) shows promising results in terms of translation quality compared to autoregressive systems while being 7-14 times faster.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "384",
        "title": "Training-free Truthfulness Detection via Value Vectors in LLMs",
        "author": [
            "Runheng Liu",
            "Heyan Huang",
            "Xingchen Xiao",
            "Zhijing Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17932",
        "abstract": "Large language models often generate factually incorrect outputs, motivating efforts to detect the truthfulness of their content. Most existing approaches rely on training probes over internal activations, but these methods suffer from scalability and generalization issues. A recent training-free method, NoVo, addresses this challenge by exploiting statistical patterns from the model itself. However, it focuses exclusively on attention mechanisms, potentially overlooking the MLP module-a core component of Transformer models known to support factual recall. In this paper, we show that certain value vectors within MLP modules exhibit truthfulness-related statistical patterns. Building on this insight, we propose TruthV, a simple and interpretable training-free method that detects content truthfulness by leveraging these value vectors. On the NoVo benchmark, TruthV significantly outperforms both NoVo and log-likelihood baselines, demonstrating that MLP modules-despite being neglected in prior training-free efforts-encode rich and useful signals for truthfulness detection. These findings offer new insights into how truthfulness is internally represented in LLMs and motivate further research on scalable and interpretable truthfulness detection.",
        "tags": [
            "Detection",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "385",
        "title": "D-REX: A Benchmark for Detecting Deceptive Reasoning in Large Language Models",
        "author": [
            "Satyapriya Krishna",
            "Andy Zou",
            "Rahul Gupta",
            "Eliot Krzysztof Jones",
            "Nick Winter",
            "Dan Hendrycks",
            "J. Zico Kolter",
            "Matt Fredrikson",
            "Spyros Matsoukas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17938",
        "abstract": "The safety and alignment of Large Language Models (LLMs) are critical for their responsible deployment. Current evaluation methods predominantly focus on identifying and preventing overtly harmful outputs. However, they often fail to address a more insidious failure mode: models that produce benign-appearing outputs while operating on malicious or deceptive internal reasoning. This vulnerability, often triggered by sophisticated system prompt injections, allows models to bypass conventional safety filters, posing a significant, underexplored risk. To address this gap, we introduce the Deceptive Reasoning Exposure Suite (D-REX), a novel dataset designed to evaluate the discrepancy between a model's internal reasoning process and its final output. D-REX was constructed through a competitive red-teaming exercise where participants crafted adversarial system prompts to induce such deceptive behaviors. Each sample in D-REX contains the adversarial system prompt, an end-user's test query, the model's seemingly innocuous response, and, crucially, the model's internal chain-of-thought, which reveals the underlying malicious intent. Our benchmark facilitates a new, essential evaluation task: the detection of deceptive alignment. We demonstrate that D-REX presents a significant challenge for existing models and safety mechanisms, highlighting the urgent need for new techniques that scrutinize the internal processes of LLMs, not just their final outputs.",
        "tags": [
            "CoT",
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "386",
        "title": "DriveDPO: Policy Learning via Safety DPO For End-to-End Autonomous Driving",
        "author": [
            "Shuyao Shang",
            "Yuntao Chen",
            "Yuqi Wang",
            "Yingyan Li",
            "Zhaoxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17940",
        "abstract": "End-to-end autonomous driving has substantially progressed by directly predicting future trajectories from raw perception inputs, which bypasses traditional modular pipelines. However, mainstream methods trained via imitation learning suffer from critical safety limitations, as they fail to distinguish between trajectories that appear human-like but are potentially unsafe. Some recent approaches attempt to address this by regressing multiple rule-driven scores but decoupling supervision from policy optimization, resulting in suboptimal performance. To tackle these challenges, we propose DriveDPO, a Safety Direct Preference Optimization Policy Learning framework. First, we distill a unified policy distribution from human imitation similarity and rule-based safety scores for direct policy optimization. Further, we introduce an iterative Direct Preference Optimization stage formulated as trajectory-level preference alignment. Extensive experiments on the NAVSIM benchmark demonstrate that DriveDPO achieves a new state-of-the-art PDMS of 90.0. Furthermore, qualitative results across diverse challenging scenarios highlight DriveDPO's ability to produce safer and more reliable driving behaviors.",
        "tags": [
            "DPO"
        ]
    },
    {
        "id": "387",
        "title": "ComposableNav: Instruction-Following Navigation in Dynamic Environments via Composable Diffusion",
        "author": [
            "Zichao Hu",
            "Chen Tang",
            "Michael J. Munje",
            "Yifeng Zhu",
            "Alex Liu",
            "Shuijing Liu",
            "Garrett Warnell",
            "Peter Stone",
            "Joydeep Biswas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17941",
        "abstract": "This paper considers the problem of enabling robots to navigate dynamic environments while following instructions. The challenge lies in the combinatorial nature of instruction specifications: each instruction can include multiple specifications, and the number of possible specification combinations grows exponentially as the robot's skill set expands. For example, \"overtake the pedestrian while staying on the right side of the road\" consists of two specifications: \"overtake the pedestrian\" and \"walk on the right side of the road.\" To tackle this challenge, we propose ComposableNav, based on the intuition that following an instruction involves independently satisfying its constituent specifications, each corresponding to a distinct motion primitive. Using diffusion models, ComposableNav learns each primitive separately, then composes them in parallel at deployment time to satisfy novel combinations of specifications unseen in training. Additionally, to avoid the onerous need for demonstrations of individual motion primitives, we propose a two-stage training procedure: (1) supervised pre-training to learn a base diffusion model for dynamic navigation, and (2) reinforcement learning fine-tuning that molds the base model into different motion primitives. Through simulation and real-world experiments, we show that ComposableNav enables robots to follow instructions by generating trajectories that satisfy diverse and unseen combinations of specifications, significantly outperforming both non-compositional VLM-based policies and costmap composing baselines. Videos and additional materials can be found on the project page: https://amrl.cs.utexas.edu/ComposableNav/",
        "tags": [
            "Diffusion",
            "RL",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "388",
        "title": "HICode: Hierarchical Inductive Coding with LLMs",
        "author": [
            "Mian Zhong",
            "Pristina Wang",
            "Anjalie Field"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17946",
        "abstract": "Despite numerous applications for fine-grained corpus analysis, researchers continue to rely on manual labeling, which does not scale, or statistical tools like topic modeling, which are difficult to control. We propose that LLMs have the potential to scale the nuanced analyses that researchers typically conduct manually to large text corpora. To this effect, inspired by qualitative research methods, we develop HICode, a two-part pipeline that first inductively generates labels directly from analysis data and then hierarchically clusters them to surface emergent themes. We validate this approach across three diverse datasets by measuring alignment with human-constructed themes and demonstrating its robustness through automated and human evaluations. Finally, we conduct a case study of litigation documents related to the ongoing opioid crisis in the U.S., revealing aggressive marketing strategies employed by pharmaceutical companies and demonstrating HICode's potential for facilitating nuanced analyses in large-scale data.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "389",
        "title": "VideoFrom3D: 3D Scene Video Generation via Complementary Image and Video Diffusion Models",
        "author": [
            "Geonung Kim",
            "Janghyeok Han",
            "Sunghyun Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17985",
        "abstract": "In this paper, we propose VideoFrom3D, a novel framework for synthesizing high-quality 3D scene videos from coarse geometry, a camera trajectory, and a reference image. Our approach streamlines the 3D graphic design workflow, enabling flexible design exploration and rapid production of deliverables. A straightforward approach to synthesizing a video from coarse geometry might condition a video diffusion model on geometric structure. However, existing video diffusion models struggle to generate high-fidelity results for complex scenes due to the difficulty of jointly modeling visual quality, motion, and temporal consistency. To address this, we propose a generative framework that leverages the complementary strengths of image and video diffusion models. Specifically, our framework consists of a Sparse Anchor-view Generation (SAG) and a Geometry-guided Generative Inbetweening (GGI) module. The SAG module generates high-quality, cross-view consistent anchor views using an image diffusion model, aided by Sparse Appearance-guided Sampling. Building on these anchor views, GGI module faithfully interpolates intermediate frames using a video diffusion model, enhanced by flow-based camera control and structural guidance. Notably, both modules operate without any paired dataset of 3D scene models and natural images, which is extremely difficult to obtain. Comprehensive experiments show that our method produces high-quality, style-consistent scene videos under diverse and challenging scenarios, outperforming simple and extended baselines.",
        "tags": [
            "3D",
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "390",
        "title": "Variation in Verification: Understanding Verification Dynamics in Large Language Models",
        "author": [
            "Yefan Zhou",
            "Austin Xu",
            "Yilun Zhou",
            "Janvijay Singh",
            "Jiang Gui",
            "Shafiq Joty"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17995",
        "abstract": "Recent advances have shown that scaling test-time computation enables large language models (LLMs) to solve increasingly complex problems across diverse domains. One effective paradigm for test-time scaling (TTS) involves LLM generators producing multiple solution candidates, with LLM verifiers assessing the correctness of these candidates without reference answers. In this paper, we study generative verifiers, which perform verification by generating chain-of-thought (CoT) reasoning followed by a binary verdict. We systematically analyze verification dynamics across three dimensions - problem difficulty, generator capability, and verifier generation capability - with empirical studies on 12 benchmarks across mathematical reasoning, knowledge, and natural language reasoning tasks using 14 open-source models (2B to 72B parameter range) and GPT-4o. Our experiments reveal three key findings about verification effectiveness: (1) Easy problems allow verifiers to more reliably certify correct responses; (2) Weak generators produce errors that are easier to detect than strong generators; (3) Verification ability is generally correlated with the verifier's own problem-solving capability, but this relationship varies with problem difficulty. These findings reveal opportunities to optimize basic verification strategies in TTS applications. First, given the same verifier, some weak generators can nearly match stronger ones in post-verification TTS performance (e.g., the Gemma2-9B to Gemma2-27B performance gap shrinks by 75.5%). Second, we identify cases where strong verifiers offer limited advantage over weak ones, as both fail to provide meaningful verification gains, suggesting that verifier scaling alone cannot overcome fundamental verification challenges.",
        "tags": [
            "CoT",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "391",
        "title": "Adaptive Kernel Design for Bayesian Optimization Is a Piece of CAKE with LLMs",
        "author": [
            "Richard Cornelius Suwandi",
            "Feng Yin",
            "Juntao Wang",
            "Renjie Li",
            "Tsung-Hui Chang",
            "Sergios Theodoridis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17998",
        "abstract": "The efficiency of Bayesian optimization (BO) relies heavily on the choice of the Gaussian process (GP) kernel, which plays a central role in balancing exploration and exploitation under limited evaluation budgets. Traditional BO methods often rely on fixed or heuristic kernel selection strategies, which can result in slow convergence or suboptimal solutions when the chosen kernel is poorly suited to the underlying objective function. To address this limitation, we propose a freshly-baked Context-Aware Kernel Evolution (CAKE) to enhance BO with large language models (LLMs). Concretely, CAKE leverages LLMs as the crossover and mutation operators to adaptively generate and refine GP kernels based on the observed data throughout the optimization process. To maximize the power of CAKE, we further propose BIC-Acquisition Kernel Ranking (BAKER) to select the most effective kernel through balancing the model fit measured by the Bayesian information criterion (BIC) with the expected improvement at each iteration of BO. Extensive experiments demonstrate that our fresh CAKE-based BO method consistently outperforms established baselines across a range of real-world tasks, including hyperparameter optimization, controller tuning, and photonic chip design. Our code is publicly available at https://github.com/cake4bo/cake.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "392",
        "title": "Unveiling m-Sharpness Through the Structure of Stochastic Gradient Noise",
        "author": [
            "Haocheng Luo",
            "Mehrtash Harandi",
            "Dinh Phung",
            "Trung Le"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18001",
        "abstract": "Sharpness-aware minimization (SAM) has emerged as a highly effective technique for improving model generalization, but its underlying principles are not fully understood. We investigated the phenomenon known as m-sharpness, where the performance of SAM improves monotonically as the micro-batch size for computing perturbations decreases. Leveraging an extended Stochastic Differential Equation (SDE) framework, combined with an analysis of the structure of stochastic gradient noise (SGN), we precisely characterize the dynamics of various SAM variants. Our findings reveal that the stochastic noise introduced during SAM perturbations inherently induces a variance-based sharpness regularization effect. Motivated by our theoretical insights, we introduce Reweighted SAM, which employs sharpness-weighted sampling to mimic the generalization benefits of m-SAM while remaining parallelizable. Comprehensive experiments validate the effectiveness of our theoretical analysis and proposed method.",
        "tags": [
            "SAM",
            "SDE"
        ]
    },
    {
        "id": "393",
        "title": "M3ET: Efficient Vision-Language Learning for Robotics based on Multimodal Mamba-Enhanced Transformer",
        "author": [
            "Yanxin Zhang",
            "Liang He",
            "Zeyi Kang",
            "Zuheng Ming",
            "Kaixing Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18005",
        "abstract": "In recent years, multimodal learning has become essential in robotic vision and information fusion, especially for understanding human behavior in complex environments. However, current methods struggle to fully leverage the textual modality, relying on supervised pretrained models, which limits semantic extraction in unsupervised robotic environments, particularly with significant modality loss. These methods also tend to be computationally intensive, leading to high resource consumption in real-world applications. To address these challenges, we propose the Multi Modal Mamba Enhanced Transformer (M3ET), a lightweight model designed for efficient multimodal learning, particularly on mobile platforms. By incorporating the Mamba module and a semantic-based adaptive attention mechanism, M3ET optimizes feature fusion, alignment, and modality reconstruction. Our experiments show that M3ET improves cross-task performance, with a 2.3 times increase in pretraining inference speed. In particular, the core VQA task accuracy of M3ET remains at 0.74, while the model's parameter count is reduced by 0.67. Although performance on the EQA task is limited, M3ET's lightweight design makes it well suited for deployment on resource-constrained robotic platforms.",
        "tags": [
            "Mamba",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "394",
        "title": "Through the Lens of Human-Human Collaboration: A Configurable Research Platform for Exploring Human-Agent Collaboration",
        "author": [
            "Bingsheng Yao",
            "Jiaju Chen",
            "Chaoran Chen",
            "April Wang",
            "Toby Jia-jun Li",
            "Dakuo Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18008",
        "abstract": "Intelligent systems have traditionally been designed as tools rather than collaborators, often lacking critical characteristics that collaboration partnerships require. Recent advances in large language model (LLM) agents open new opportunities for human-LLM-agent collaboration by enabling natural communication and various social and cognitive behaviors. Yet it remains unclear whether principles of computer-mediated collaboration established in HCI and CSCW persist, change, or fail when humans collaborate with LLM agents. To support systematic investigations of these questions, we introduce an open and configurable research platform for HCI researchers. The platform's modular design allows seamless adaptation of classic CSCW experiments and manipulation of theory-grounded interaction controls. We demonstrate the platform's effectiveness and usability through two case studies: (1) re-implementing the classic human-human-collaboration task Shape Factory as a between-subject human-agent-collaboration experiment with 16 participants, and (2) a participatory cognitive walkthrough with five HCI researchers to refine workflows and interfaces for experiment setup and analysis.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "395",
        "title": "RadEval: A framework for radiology text evaluation",
        "author": [
            "Justin Xu",
            "Xi Zhang",
            "Javid Abderezaei",
            "Julie Bauml",
            "Roger Boodoo",
            "Fatemeh Haghighi",
            "Ali Ganjizadeh",
            "Eric Brattain",
            "Dave Van Veen",
            "Zaiqiao Meng",
            "David Eyre",
            "Jean-Benoit Delbrouck"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18030",
        "abstract": "We introduce RadEval, a unified, open-source framework for evaluating radiology texts. RadEval consolidates a diverse range of metrics, from classic n-gram overlap (BLEU, ROUGE) and contextual measures (BERTScore) to clinical concept-based scores (F1CheXbert, F1RadGraph, RaTEScore, SRR-BERT, TemporalEntityF1) and advanced LLM-based evaluators (GREEN). We refine and standardize implementations, extend GREEN to support multiple imaging modalities with a more lightweight model, and pretrain a domain-specific radiology encoder, demonstrating strong zero-shot retrieval performance. We also release a richly annotated expert dataset with over 450 clinically significant error labels and show how different metrics correlate with radiologist judgment. Finally, RadEval provides statistical testing tools and baseline model evaluations across multiple publicly available datasets, facilitating reproducibility and robust benchmarking in radiology report generation.",
        "tags": [
            "BERT",
            "LLM"
        ]
    },
    {
        "id": "396",
        "title": "NeuS-QA: Grounding Long-Form Video Understanding in Temporal Logic and Neuro-Symbolic Reasoning",
        "author": [
            "Sahil Shah",
            "S P Sharan",
            "Harsh Goel",
            "Minkyu Choi",
            "Mustafa Munir",
            "Manvik Pasula",
            "Radu Marculescu",
            "Sandeep Chinchali"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18041",
        "abstract": "Long-Form Video Question Answering (LVQA) poses challenges beyond traditional visual question answering (VQA), which is often limited to static images or short video clips. While current vision-language models (VLMs) perform well in those settings, they struggle with complex queries in LVQA over long videos involving multi-step temporal reasoning and causality. Vanilla approaches, which sample frames uniformly and feed them to a VLM with the question, incur significant token overhead, forcing severe downsampling. As a result, the model often misses fine-grained visual structure, subtle event transitions, or key temporal cues, ultimately leading to incorrect answers. To address these limitations, recent works have explored query-adaptive frame sampling, hierarchical keyframe selection, and agent-based iterative querying. However, these methods remain fundamentally heuristic: they lack explicit temporal representations and cannot enforce or verify logical event relationships. As a result, there are no formal guarantees that the sampled context actually encodes the compositional or causal logic demanded by the question. To address these foundational gaps, we introduce NeuS-QA, a training-free, plug-and-play neuro-symbolic pipeline for LVQA. NeuS-QA translates a natural language question into a formal temporal logic expression, constructs a video automaton from frame-level semantic propositions, and applies model checking to rigorously identify video segments satisfying the question's logical requirements. Only these logic-verified segments are submitted to the VLM, thus improving interpretability, reducing hallucinations, and enabling compositional reasoning without modifying or fine-tuning the model. Experiments on LongVideoBench and CinePile show NeuS-QA improves performance by over 10%, especially on questions involving event ordering, causality, and multi-step compositional reasoning.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "397",
        "title": "Prepare Before You Act: Learning From Humans to Rearrange Initial States",
        "author": [
            "Yinlong Dai",
            "Andre Keyser",
            "Dylan P. Losey"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18043",
        "abstract": "Imitation learning (IL) has proven effective across a wide range of manipulation tasks. However, IL policies often struggle when faced with out-of-distribution observations; for instance, when the target object is in a previously unseen position or occluded by other objects. In these cases, extensive demonstrations are needed for current IL methods to reach robust and generalizable behaviors. But when humans are faced with these sorts of atypical initial states, we often rearrange the environment for more favorable task execution. For example, a person might rotate a coffee cup so that it is easier to grasp the handle, or push a box out of the way so they can directly grasp their target object. In this work we seek to equip robot learners with the same capability: enabling robots to prepare the environment before executing their given policy. We propose ReSET, an algorithm that takes initial states -- which are outside the policy's distribution -- and autonomously modifies object poses so that the restructured scene is similar to training data. Theoretically, we show that this two step process (rearranging the environment before rolling out the given policy) reduces the generalization gap. Practically, our ReSET algorithm combines action-agnostic human videos with task-agnostic teleoperation data to i) decide when to modify the scene, ii) predict what simplifying actions a human would take, and iii) map those predictions into robot action primitives. Comparisons with diffusion policies, VLAs, and other baselines show that using ReSET to prepare the environment enables more robust task execution with equal amounts of total training data. See videos at our project website: https://reset2025paper.github.io/",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "398",
        "title": "HuMam: Humanoid Motion Control via End-to-End Deep Reinforcement Learning with Mamba",
        "author": [
            "Yinuo Wang",
            "Yuanyang Qi",
            "Jinzhao Zhou",
            "Gavin Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18046",
        "abstract": "End-to-end reinforcement learning (RL) for humanoid locomotion is appealing for its compact perception-action mapping, yet practical policies often suffer from training instability, inefficient feature fusion, and high actuation cost. We present HuMam, a state-centric end-to-end RL framework that employs a single-layer Mamba encoder to fuse robot-centric states with oriented footstep targets and a continuous phase clock. The policy outputs joint position targets tracked by a low-level PD loop and is optimized with PPO. A concise six-term reward balances contact quality, swing smoothness, foot placement, posture, and body stability while implicitly promoting energy saving. On the JVRC-1 humanoid in mc-mujoco, HuMam consistently improves learning efficiency, training stability, and overall task performance over a strong feedforward baseline, while reducing power consumption and torque peaks. To our knowledge, this is the first end-to-end humanoid RL controller that adopts Mamba as the fusion backbone, demonstrating tangible gains in efficiency, stability, and control economy.",
        "tags": [
            "Mamba",
            "PPO",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "399",
        "title": "The PIMMUR Principles: Ensuring Validity in Collective Behavior of LLM Societies",
        "author": [
            "Jiaxu Zhou",
            "Jen-tse Huang",
            "Xuhui Zhou",
            "Man Ho Lam",
            "Xintao Wang",
            "Hao Zhu",
            "Wenxuan Wang",
            "Maarten Sap"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18052",
        "abstract": "Large Language Models (LLMs) are increasingly used for social simulation, where populations of agents are expected to reproduce human-like collective behavior. However, we find that many recent studies adopt experimental designs that systematically undermine the validity of their claims. From a survey of over 40 papers, we identify six recurring methodological flaws: agents are often homogeneous (Profile), interactions are absent or artificially imposed (Interaction), memory is discarded (Memory), prompts tightly control outcomes (Minimal-Control), agents can infer the experimental hypothesis (Unawareness), and validation relies on simplified theoretical models rather than real-world data (Realism). For instance, GPT-4o and Qwen-3 correctly infer the underlying social experiment in 53.1% of cases when given instructions from prior work-violating the Unawareness principle. We formalize these six requirements as the PIMMUR principles and argue they are necessary conditions for credible LLM-based social simulation. To demonstrate their impact, we re-run five representative studies using a framework that enforces PIMMUR and find that the reported social phenomena frequently fail to emerge under more rigorous conditions. Our work establishes methodological standards for LLM-based multi-agent research and provides a foundation for more reliable and reproducible claims about \"AI societies.\"",
        "tags": [
            "GPT",
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "400",
        "title": "V2V-GoT: Vehicle-to-Vehicle Cooperative Autonomous Driving with Multimodal Large Language Models and Graph-of-Thoughts",
        "author": [
            "Hsu-kuang Chiu",
            "Ryo Hachiuma",
            "Chien-Yi Wang",
            "Yu-Chiang Frank Wang",
            "Min-Hung Chen",
            "Stephen F. Smith"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18053",
        "abstract": "Current state-of-the-art autonomous vehicles could face safety-critical situations when their local sensors are occluded by large nearby objects on the road. Vehicle-to-vehicle (V2V) cooperative autonomous driving has been proposed as a means of addressing this problem, and one recently introduced framework for cooperative autonomous driving has further adopted an approach that incorporates a Multimodal Large Language Model (MLLM) to integrate cooperative perception and planning processes. However, despite the potential benefit of applying graph-of-thoughts reasoning to the MLLM, this idea has not been considered by previous cooperative autonomous driving research. In this paper, we propose a novel graph-of-thoughts framework specifically designed for MLLM-based cooperative autonomous driving. Our graph-of-thoughts includes our proposed novel ideas of occlusion-aware perception and planning-aware prediction. We curate the V2V-GoT-QA dataset and develop the V2V-GoT model for training and testing the cooperative driving graph-of-thoughts. Our experimental results show that our method outperforms other baselines in cooperative perception, prediction, and planning tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "401",
        "title": "TempSamp-R1: Effective Temporal Sampling with Reinforcement Fine-Tuning for Video LLMs",
        "author": [
            "Yunheng Li",
            "Jing Cheng",
            "Shaoyong Jia",
            "Hangyi Kuang",
            "Shaohui Jiao",
            "Qibin Hou",
            "Ming-Ming Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18056",
        "abstract": "This paper introduces TempSamp-R1, a new reinforcement fine-tuning framework designed to improve the effectiveness of adapting multimodal large language models (MLLMs) to video temporal grounding tasks. We reveal that existing reinforcement learning methods, such as Group Relative Policy Optimization (GRPO), rely on on-policy sampling for policy updates. However, in tasks with large temporal search spaces, this strategy becomes both inefficient and limited in performance, as it often fails to identify temporally accurate solutions. To address this limitation, TempSamp-R1 leverages ground-truth annotations as off-policy supervision to provide temporally precise guidance, effectively compensating for the sparsity and misalignment in on-policy solutions. To further stabilize training and reduce variance in reward-based updates, TempSamp-R1 provides a non-linear soft advantage computation method that dynamically reshapes the reward feedback via an asymmetric transformation. By employing a hybrid Chain-of-Thought (CoT) training paradigm, TempSamp-R1 optimizes a single unified model to support both CoT and non-CoT inference modes, enabling efficient handling of queries with varying reasoning complexity. Experimental results demonstrate that TempSamp-R1 outperforms GRPO-based baselines, establishing new state-of-the-art performance on benchmark datasets: Charades-STA (R1@0.7: 52.9%, +2.7%), ActivityNet Captions (R1@0.5: 56.0%, +5.3%), and QVHighlights (mAP: 30.0%, +3.0%). Moreover, TempSamp-R1 shows robust few-shot generalization capabilities under limited data. Code: https://github.com/HVision-NKU/TempSamp-R1",
        "tags": [
            "CoT",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "402",
        "title": "Reinforced Generation of Combinatorial Structures: Applications to Complexity Theory",
        "author": [
            "Ansh Nagda",
            "Prabhakar Raghavan",
            "Abhradeep Thakurta"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18057",
        "abstract": "We explore whether techniques from AI can help discover new combinatorial structures that improve provable limits on efficient algorithms. Specifically, we use AlphaEvolve (an LLM coding agent) to study two settings:\na) Average-case hardness for MAX-CUT and MAX-Independent Set: We improve a recent result of Kunisky and Yu to obtain near-optimal upper and (conditional) lower bounds on certification algorithms for MAX-CUT and MAX-Independent Set on random 3- and 4-regular graphs. Our improved lower bounds are obtained by constructing nearly extremal Ramanujan graphs on as many as $163$ nodes, using AlphaEvolve. Additionally, via analytical arguments we strengthen the upper bounds to settle the computational hardness of these questions up to an error in the third decimal place.\nb) Worst-case Hardness of Approximation for MAX-k-CUT: We obtain new inapproximability results, proving that it is NP-hard to approximate MAX-4-CUT and MAX-3-CUT within factors of $0.987$ and $0.9649$ respectively, using AlphaEvolve to discover new gadget reductions. Our MAX-4-CUT result improves upon the SOTA of $0.9883$, and our MAX-3-CUT result improves on the current best gadget-based inapproximability result of $0.9853$, but falls short of improving the SOTA of $16/17$ that relies on a custom PCP, rather than a gadget reduction from \"standard\" HÃ¥stad-style PCPs.\nA key technical challenge we faced: verifying a candidate construction produced by AlphaEvolve is costly (often requiring exponential time). In both settings above, our results were enabled by using AlphaEvolve itself to evolve the verification procedure to be faster (sometimes by $10,000\\times$). We conclude with a discussion of norms by which to assess the assistance from AI in developing proofs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "403",
        "title": "Strategic Dishonesty Can Undermine AI Safety Evaluations of Frontier LLM",
        "author": [
            "Alexander Panfilov",
            "Evgenii Kortukov",
            "Kristina NikoliÄ",
            "Matthias Bethge",
            "Sebastian Lapuschkin",
            "Wojciech Samek",
            "Ameya Prabhu",
            "Maksym Andriushchenko",
            "Jonas Geiping"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18058",
        "abstract": "Large language model (LLM) developers aim for their models to be honest, helpful, and harmless. However, when faced with malicious requests, models are trained to refuse, sacrificing helpfulness. We show that frontier LLMs can develop a preference for dishonesty as a new strategy, even when other options are available. Affected models respond to harmful requests with outputs that sound harmful but are subtly incorrect or otherwise harmless in practice. This behavior emerges with hard-to-predict variations even within models from the same model family. We find no apparent cause for the propensity to deceive, but we show that more capable models are better at executing this strategy. Strategic dishonesty already has a practical impact on safety evaluations, as we show that dishonest responses fool all output-based monitors used to detect jailbreaks that we test, rendering benchmark scores unreliable. Further, strategic dishonesty can act like a honeypot against malicious users, which noticeably obfuscates prior jailbreak attacks. While output monitors fail, we show that linear probes on internal activations can be used to reliably detect strategic dishonesty. We validate probes on datasets with verifiable outcomes and by using their features as steering vectors. Overall, we consider strategic dishonesty as a concrete example of a broader concern that alignment of LLMs is hard to control, especially when helpfulness and harmlessness conflict.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "404",
        "title": "ARK-V1: An LLM-Agent for Knowledge Graph Question Answering Requiring Commonsense Reasoning",
        "author": [
            "Jan-Felix Klein",
            "Lars Ohnemus"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18063",
        "abstract": "Large Language Models (LLMs) show strong reasoning abilities but rely on internalized knowledge that is often insufficient, outdated, or incorrect when trying to answer a question that requires specific domain knowledge. Knowledge Graphs (KGs) provide structured external knowledge, yet their complexity and multi-hop reasoning requirements make integration challenging. We present ARK-V1, a simple KG-agent that iteratively explores graphs to answer natural language queries. We evaluate several not fine-tuned state-of-the art LLMs as backbones for ARK-V1 on the CoLoTa dataset, which requires both KG-based and commonsense reasoning over long-tail entities. ARK-V1 achieves substantially higher conditional accuracies than Chain-of-Thought baselines, and larger backbone models show a clear trend toward better coverage, correctness, and stability.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "405",
        "title": "RadarSFD: Single-Frame Diffusion with Pretrained Priors for Radar Point Clouds",
        "author": [
            "Bin Zhao",
            "Nakul Garg"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18068",
        "abstract": "Millimeter-wave radar provides perception robust to fog, smoke, dust, and low light, making it attractive for size, weight, and power constrained robotic platforms. Current radar imaging methods, however, rely on synthetic aperture or multi-frame aggregation to improve resolution, which is impractical for small aerial, inspection, or wearable systems. We present RadarSFD, a conditional latent diffusion framework that reconstructs dense LiDAR-like point clouds from a single radar frame without motion or SAR. Our approach transfers geometric priors from a pretrained monocular depth estimator into the diffusion backbone, anchors them to radar inputs via channel-wise latent concatenation, and regularizes outputs with a dual-space objective combining latent and pixel-space losses. On the RadarHD benchmark, RadarSFD achieves 35 cm Chamfer Distance and 28 cm Modified Hausdorff Distance, improving over the single-frame RadarHD baseline (56 cm, 45 cm) and remaining competitive with multi-frame methods using 5-41 frames. Qualitative results show recovery of fine walls and narrow gaps, and experiments across new environments confirm strong generalization. Ablation studies highlight the importance of pretrained initialization, radar BEV conditioning, and the dual-space loss. Together, these results establish the first practical single-frame, no-SAR mmWave radar pipeline for dense point cloud perception in compact robotic systems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "406",
        "title": "Improving Large Language Models Function Calling and Interpretability via Guided-Structured Templates",
        "author": [
            "Hy Dang",
            "Tianyi Liu",
            "Zhuofeng Wu",
            "Jingfeng Yang",
            "Haoming Jiang",
            "Tao Yang",
            "Pei Chen",
            "Zhengyang Wang",
            "Helen Wang",
            "Huasheng Li",
            "Bing Yin",
            "Meng Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18076",
        "abstract": "Large language models (LLMs) have demonstrated strong reasoning and tool-use capabilities, yet they often fail in real-world tool-interactions due to incorrect parameterization, poor tool selection, or misinterpretation of user intent. These issues often stem from an incomplete understanding of user goals and inadequate comprehension of tool documentation. While Chain-of-Thought (CoT) prompting has proven effective for enhancing reasoning in general contexts, our analysis reveals that free-form CoT is insufficient and sometimes counterproductive for structured function-calling tasks. To address this, we introduce a curriculum-inspired framework that leverages structured reasoning templates to guide LLMs through more deliberate step-by-step instructions for generating function callings. Experimental results show that our method reduces tool-use errors, achieving 3-12% relative improvements over strong baselines across diverse model series and approaches. Moreover, our framework enhances the robustness, interpretability, and transparency of tool-using agents, advancing the development of more reliable AI assistants for real-world applications.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "407",
        "title": "Tracing the Techno-Supremacy Doctrine: A Critical Discourse Analysis of the AI Executive Elite",
        "author": [
            "HÃ©ctor PÃ©rez-Urbina"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18079",
        "abstract": "This paper critically analyzes the discourse of the 'AI executive elite,' a group of highly influential individuals shaping the way AI is funded, developed, and deployed worldwide. The primary objective is to examine the presence and dynamics of the 'Techno-Supremacy Doctrine' (TSD), a term introduced in this study to describe a belief system characterized by an excessive trust in technology's alleged inherent superiority in solving complex societal problems. This study integrates quantitative heuristics with in-depth qualitative investigations. Its methodology is operationalized in a two-phase critical discourse analysis of 14 texts published by elite members between 2017 and 2025. The findings demonstrate that the elite is not a monolithic bloc but exhibits a broad spectrum of stances. The discourse is highly dynamic, showing a marked polarization and general increase in pro-TSD discourse following the launch of ChatGPT. The analysis identifies key discursive patterns, including a dominant pro-TSD narrative that combines utopian promises with claims of inevitable progress, and the common tactic of acknowledging risks only as a strategic preamble to proposing further technological solutions. This paper presents TSD as a comprehensive analytical framework and provides a 'diagnostic toolkit' for identifying its manifestations, from insidious to benign. It argues that fostering critical awareness of these discursive patterns is essential for AI practitioners, policymakers, and the public to actively navigate the future of AI.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "408",
        "title": "GraDeT-HTR: A Resource-Efficient Bengali Handwritten Text Recognition System utilizing Grapheme-based Tokenizer and Decoder-only Transformer",
        "author": [
            "Md. Mahmudul Hasan",
            "Ahmed Nesar Tahsin Choudhury",
            "Mahmudul Hasan",
            "Md. Mosaddek Khan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18081",
        "abstract": "Despite Bengali being the sixth most spoken language in the world, handwritten text recognition (HTR) systems for Bengali remain severely underdeveloped. The complexity of Bengali script--featuring conjuncts, diacritics, and highly variable handwriting styles--combined with a scarcity of annotated datasets makes this task particularly challenging. We present GraDeT-HTR, a resource-efficient Bengali handwritten text recognition system based on a Grapheme-aware Decoder-only Transformer architecture. To address the unique challenges of Bengali script, we augment the performance of a decoder-only transformer by integrating a grapheme-based tokenizer and demonstrate that it significantly improves recognition accuracy compared to conventional subword tokenizers. Our model is pretrained on large-scale synthetic data and fine-tuned on real human-annotated samples, achieving state-of-the-art performance on multiple benchmark datasets.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "409",
        "title": "Reasoning Core: A Scalable RL Environment for LLM Symbolic Reasoning",
        "author": [
            "Valentin Lacombe",
            "Valentin Quesnel",
            "Damien Sileo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18083",
        "abstract": "We introduce Reasoning Core, a new scalable environment for Reinforcement Learning with Verifiable Rewards (RLVR), designed to advance foundational symbolic reasoning in Large Language Models (LLMs). Unlike existing benchmarks that focus on games or isolated puzzles, Reasoning Core procedurally generates problems across core formal domains, including PDDL planning, first-order logic, context-free grammar parsing, causal reasoning, and system equation solving. The environment is built on key design principles of high-generality problem distributions, verification via external tools, and continuous difficulty control, which together provide a virtually infinite supply of novel training instances. Initial zero-shot evaluations with frontier LLMs confirm the difficulty of Reasoning Core's tasks, positioning it as a promising resource to improve the reasoning capabilities of future models.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "410",
        "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative Decoding",
        "author": [
            "Sudhanshu Agrawal",
            "Risheek Garrepalli",
            "Raghavv Goel",
            "Mingu Lee",
            "Christopher Lott",
            "Fatih Porikli"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18085",
        "abstract": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs (AR-LLMs) with the potential to operate at significantly higher token generation rates. However, currently available open-source dLLMs often generate at much lower rates, typically decoding only a single token at every denoising timestep in order to maximize output quality. We present Spiffy, a speculative decoding algorithm that accelerates dLLM inference by $\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output distribution. This work addresses the unique challenges involved in applying ideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes draft states by leveraging the dLLM's distribution itself in an auto-speculative manner. This approach is efficient and effective, and eliminates the overheads of training and running an independent draft model. To structure the candidate draft states, we propose a novel directed draft graph which is uniquely designed to take advantage of the bidirectional, block-wise nature of dLLM generation and can be verified in parallel by the dLLM. To further optimize the structure of these draft graphs, we introduce an efficient, offline calibration algorithm that procedurally determines high-quality graph configurations. These optimized draft graphs, enabling increased acceptance rates, lead to a significant boost in the overall speedup achieved by the system. Crucially, Spiffy is also complementary to other recent innovations in improving dLLM generation speeds such as KV-caching and multi-token unmasking. We demonstrate that when combined with such parallel decoding algorithms, Spiffy is able to effectively multiply the benefits of these methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "411",
        "title": "Strategic Coordination for Evolving Multi-agent Systems: A Hierarchical Reinforcement and Collective Learning Approach",
        "author": [
            "Chuhao Qin",
            "Evangelos Pournaras"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18088",
        "abstract": "Decentralized combinatorial optimization in evolving multi-agent systems poses significant challenges, requiring agents to balance long-term decision-making, short-term optimized collective outcomes, while preserving autonomy of interactive agents under unanticipated changes. Reinforcement learning offers a way to model sequential decision-making through dynamic programming to anticipate future environmental changes. However, applying multi-agent reinforcement learning (MARL) to decentralized combinatorial optimization problems remains an open challenge due to the exponential growth of the joint state-action space, high communication overhead, and privacy concerns in centralized training. To address these limitations, this paper proposes Hierarchical Reinforcement and Collective Learning (HRCL), a novel approach that leverages both MARL and decentralized collective learning based on a hierarchical framework. Agents take high-level strategies using MARL to group possible plans for action space reduction and constrain the agent behavior for Pareto optimality. Meanwhile, the low-level collective learning layer ensures efficient and decentralized coordinated decisions among agents with minimal communication. Extensive experiments in a synthetic scenario and real-world smart city application models, including energy self-management and drone swarm sensing, demonstrate that HRCL significantly improves performance, scalability, and adaptability compared to the standalone MARL and collective learning approaches, achieving a win-win synthesis solution.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "412",
        "title": "GeoSVR: Taming Sparse Voxels for Geometrically Accurate Surface Reconstruction",
        "author": [
            "Jiahe Li",
            "Jiawei Zhang",
            "Youmin Zhang",
            "Xiao Bai",
            "Jin Zheng",
            "Xiaohan Yu",
            "Lin Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18090",
        "abstract": "Reconstructing accurate surfaces with radiance fields has achieved remarkable progress in recent years. However, prevailing approaches, primarily based on Gaussian Splatting, are increasingly constrained by representational bottlenecks. In this paper, we introduce GeoSVR, an explicit voxel-based framework that explores and extends the under-investigated potential of sparse voxels for achieving accurate, detailed, and complete surface reconstruction. As strengths, sparse voxels support preserving the coverage completeness and geometric clarity, while corresponding challenges also arise from absent scene constraints and locality in surface refinement. To ensure correct scene convergence, we first propose a Voxel-Uncertainty Depth Constraint that maximizes the effect of monocular depth cues while presenting a voxel-oriented uncertainty to avoid quality degradation, enabling effective and robust scene constraints yet preserving highly accurate geometries. Subsequently, Sparse Voxel Surface Regularization is designed to enhance geometric consistency for tiny voxels and facilitate the voxel-based formation of sharp and accurate surfaces. Extensive experiments demonstrate our superior performance compared to existing methods across diverse challenging scenarios, excelling in geometric accuracy, detail preservation, and reconstruction completeness while maintaining high efficiency. Code is available at https://github.com/Fictionarry/GeoSVR.",
        "tags": [
            "Gaussian Splatting"
        ]
    },
    {
        "id": "413",
        "title": "ComposeMe: Attribute-Specific Image Prompts for Controllable Human Image Generation",
        "author": [
            "Guocheng Gordon Qian",
            "Daniil Ostashev",
            "Egor Nemchinov",
            "Avihay Assouline",
            "Sergey Tulyakov",
            "Kuan-Chieh Jackson Wang",
            "Kfir Aberman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18092",
        "abstract": "Generating high-fidelity images of humans with fine-grained control over attributes such as hairstyle and clothing remains a core challenge in personalized text-to-image synthesis. While prior methods emphasize identity preservation from a reference image, they lack modularity and fail to provide disentangled control over specific visual attributes. We introduce a new paradigm for attribute-specific image prompting, in which distinct sets of reference images are used to guide the generation of individual aspects of human appearance, such as hair, clothing, and identity. Our method encodes these inputs into attribute-specific tokens, which are injected into a pre-trained text-to-image diffusion model. This enables compositional and disentangled control over multiple visual factors, even across multiple people within a single image. To promote natural composition and robust disentanglement, we curate a cross-reference training dataset featuring subjects in diverse poses and expressions, and propose a multi-attribute cross-reference training strategy that encourages the model to generate faithful outputs from misaligned attribute inputs while adhering to both identity and textual conditioning. Extensive experiments show that our method achieves state-of-the-art performance in accurately following both visual and textual prompts. Our framework paves the way for more configurable human image synthesis by combining visual prompting with text-driven generation. Webpage is available at: https://snap-research.github.io/composeme/.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "414",
        "title": "SEQR: Secure and Efficient QR-based LoRA Routing",
        "author": [
            "William Fleshman",
            "Benjamin Van Durme"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18093",
        "abstract": "Low-Rank Adaptation (LoRA) has become a standard technique for parameter-efficient fine-tuning of large language models, enabling large libraries of LoRAs, each for a specific task or domain. Efficiently selecting the correct LoRA adapter for a given input remains a challenge, particularly in secure environments where supervised training of routers may raise privacy concerns. Motivated by previous approaches, we formalize the goal of unsupervised LoRA routing in terms of activation norm maximization, providing a theoretical framework for analysis. We demonstrate the discriminative power of activation norms and introduce SEQR, an unsupervised LoRA routing algorithm designed to maximize efficiency while providing strict routing guarantees. SEQR provably identifies the norm-maximizing adapter with significantly greater efficiency, making it a highly scalable and effective solution for dynamic LoRA composition. We validate our results through experiments that demonstrate improved multi-task performance and efficiency.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "415",
        "title": "Seg4Diff: Unveiling Open-Vocabulary Segmentation in Text-to-Image Diffusion Transformers",
        "author": [
            "Chaehyun Kim",
            "Heeseong Shin",
            "Eunbeen Hong",
            "Heeji Yoon",
            "Anurag Arnab",
            "Paul Hongsuck Seo",
            "Sunghwan Hong",
            "Seungryong Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18096",
        "abstract": "Text-to-image diffusion models excel at translating language prompts into photorealistic images by implicitly grounding textual concepts through their cross-modal attention mechanisms. Recent multi-modal diffusion transformers extend this by introducing joint self-attention over concatenated image and text tokens, enabling richer and more scalable cross-modal alignment. However, a detailed understanding of how and where these attention maps contribute to image generation remains limited. In this paper, we introduce Seg4Diff (Segmentation for Diffusion), a systematic framework for analyzing the attention structures of MM-DiT, with a focus on how specific layers propagate semantic information from text to image. Through comprehensive analysis, we identify a semantic grounding expert layer, a specific MM-DiT block that consistently aligns text tokens with spatially coherent image regions, naturally producing high-quality semantic segmentation masks. We further demonstrate that applying a lightweight fine-tuning scheme with mask-annotated image data enhances the semantic grouping capabilities of these layers and thereby improves both segmentation performance and generated image fidelity. Our findings demonstrate that semantic grouping is an emergent property of diffusion transformers and can be selectively amplified to advance both segmentation and generation performance, paving the way for unified models that bridge visual perception and generation.",
        "tags": [
            "DiT",
            "Diffusion",
            "Segmentation",
            "Text-to-Image"
        ]
    },
    {
        "id": "416",
        "title": "How Can Quantum Deep Learning Improve Large Language Models?",
        "author": [
            "Emily Jimin Roh",
            "Hyojun Ahn",
            "Samuel Yen-Chi Chen",
            "Soohyun Park",
            "Joongheon Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16244",
        "abstract": "The rapid progress of large language models (LLMs) has transformed natural language processing, yet the challenge of efficient adaptation remains unresolved. Full fine-tuning achieves strong performance but imposes prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) strategies, such as low-rank adaptation (LoRA), Prefix tuning, and sparse low-rank adaptation (SoRA), address this issue by reducing trainable parameters while maintaining competitive accuracy. However, these methods often encounter limitations in scalability, stability, and generalization across diverse tasks. Recent advances in quantum deep learning introduce novel opportunities through quantum-inspired encoding and parameterized quantum circuits (PQCs). In particular, the quantum-amplitude embedded adaptation (QAA) framework demonstrates expressive model updates with minimal overhead. This paper presents a systematic survey and comparative analysis of conventional PEFT methods and QAA. The analysis demonstrates trade-offs in convergence, efficiency, and representational capacity, while providing insight into the potential of quantum approaches for future LLM adaptation.",
        "tags": [
            "LLM",
            "LoRA",
            "Sora"
        ]
    },
    {
        "id": "417",
        "title": "Similarity-Guided Diffusion for Long-Gap Music Inpainting",
        "author": [
            "Sean Turland",
            "Eloi Moliner",
            "Vesa VÃ¤limÃ¤ki"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16342",
        "abstract": "Music inpainting aims to reconstruct missing segments of a corrupted recording. While diffusion-based generative models improve reconstruction for medium-length gaps, they often struggle to preserve musical plausibility over multi-second gaps. We introduce Similarity-Guided Diffusion Posterior Sampling (SimDPS), a hybrid method that combines diffusion-based inference with similarity search. Candidate segments are first retrieved from a corpus based on contextual similarity, then incorporated into a modified likelihood that guides the diffusion process toward contextually consistent reconstructions. Subjective evaluation on piano music inpainting with 2-s gaps shows that the proposed SimDPS method enhances perceptual plausibility compared to unguided diffusion and frequently outperforms similarity search alone when moderately similar candidates are available. These results demonstrate the potential of a hybrid similarity approach for diffusion-based audio enhancement with long gaps.",
        "tags": [
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "418",
        "title": "Low-Rank Adaptation of Evolutionary Deep Neural Networks for Efficient Learning of Time-Dependent PDEs",
        "author": [
            "Jiahao Zhang",
            "Shiheng Zhang",
            "Guang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16395",
        "abstract": "We study the Evolutionary Deep Neural Network (EDNN) framework for accelerating numerical solvers of time-dependent partial differential equations (PDEs). We introduce a Low-Rank Evolutionary Deep Neural Network (LR-EDNN), which constrains parameter evolution to a low-rank subspace, thereby reducing the effective dimensionality of training while preserving solution accuracy. The low-rank tangent subspace is defined layer-wise by the singular value decomposition (SVD) of the current network weights, and the resulting update is obtained by solving a well-posed, tractable linear system within this subspace. This design augments the underlying numerical solver with a parameter efficient EDNN component without requiring full fine-tuning of all network weights. We evaluate LR-EDNN on representative PDE problems and compare it against corresponding baselines. Across cases, LR-EDNN achieves comparable accuracy with substantially fewer trainable parameters and reduced computational cost. These results indicate that low-rank constraints on parameter velocities, rather than full-space updates, provide a practical path toward scalable, efficient, and reproducible scientific machine learning for PDEs.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "419",
        "title": "LightCode: Compiling LLM Inference for Photonic-Electronic Systems",
        "author": [
            "Ryan Tomich",
            "Zhizhen Zhong",
            "Dirk Englund"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16443",
        "abstract": "The growing demand for low-latency, energy-efficient inference in large language models (LLMs) has catalyzed interest in heterogeneous architectures. While GPUs remain dominant, they are poorly suited for integration with emerging domain-specific accelerators like the Photonic Tensor Units (PTUs), which offer low-power, high-throughput linear computation. This motivates hybrid compilation strategies that combine photonic and electronic resources. We present LightCode, a compiler framework and simulator for mapping LLM inference workloads across hybrid photonic-electronic systems. LightCode introduces the Stacked Graph, an intermediate representation that encodes multiple hardware-specific realizations of each tensor operation. Hardware assignment is formulated as a constrained subgraph selection problem optimized for latency or energy under parametric cost models. We evaluate LightCode on the prefill stage of GPT-2 and Llama-7B showing that under our workload and hardware assumptions, (i) Photonic hardware reduced energy by up to 50% in our simulated workloads at maximum sequence length; (ii) multiplexing and assignment strategy yielded latency improvements exceeding 10x; and (iii) Optimizing for latency or energy resulted in distinct hardware mappings in our simulations. LightCode offers a module, foundational framework and simulator for compiling LLMs to emerging photonic accelerators.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "420",
        "title": "From Coated to Uncoated: Scanning Electron Microscopy Corrections to Estimate True Surface Pore Size in Nanoporous Membranes",
        "author": [
            "Sima Zeinali Danalou",
            "Dian Yu",
            "Niher R. Sarker",
            "Hooman Chamani",
            "Jane Y. Howe",
            "Patrick C. Lee",
            "Jay R. Werber"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16471",
        "abstract": "Scanning electron microscopy (SEM) is the premier method for characterizing the nanoscale surface pores in ultrafiltration (UF) membranes and the support layers of reverse osmosis (RO) membranes. Based on SEM, the conventional understanding is that membranes typically have low surface porosities of <10%. We hypothesized that high acceleration voltage during SEM imaging and sputter metal coatings required for SEM have led to systematic underestimations of porosity and pore size. We showed that imaging a commercial UF membrane at 1, 5, and 10 kV reduced measured porosity from 10.3% (1 kV) to 6.3% (10 kV), while increasing Pt coating thickness from 1.5 to 5 nm lowered porosity by 54% for the UF membrane (12.9% to 5.8%) and 46% for an RO support (13.1% to 7.0%). To account for coating thickness, we developed a digital correction method that simulates pore dilation, enabling the pore structure to be estimated for uncoated membranes. Dilation yielded uncoated porosity values of 23% for the UF membrane and 20% for the RO support, about 3-fold greater than values observed with a 4 nm coating. Mean pore diameters were 2-fold greater for the UF membrane and 1.5-fold greater for the RO support. Critically, dilation-derived pore-size distributions agreed with low-flux dextran-retention data fitted with the Bungay-Brenner model. Our results suggest that surface porosities and pore sizes of nanoporous membranes are much larger than previously understood, with major implications for structure/transport relationships. For future nanoscale pore analysis of membranes (and other nanoporous materials), we recommend low acceleration voltage (1 kV), minimal coatings (1-2 nm), and digital dilation to account for coating artifacts",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "421",
        "title": "An Octave-based Multi-Resolution CQT Architecture for Diffusion-based Audio Generation",
        "author": [
            "MaurÃ­cio do V. M. da Costa",
            "Eloi Moliner"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16603",
        "abstract": "This paper introduces MR-CQTdiff, a novel neural-network architecture for diffusion-based audio generation that leverages a multi-resolution Constant-$Q$ Transform (C$Q$T). The proposed architecture employs an efficient, invertible CQT framework that adjusts the time-frequency resolution on an octave-by-octave basis. This design addresses the issue of low temporal resolution at lower frequencies, enabling more flexible and expressive audio generation. We conduct an evaluation using the FrÃ©chet Audio Distance (FAD) metric across various architectures and two datasets. Experimental results demonstrate that MR-CQTdiff achieves state-of-the-art audio quality, outperforming competing architectures.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "422",
        "title": "Audio-Conditioned Diffusion LLMs for ASR and Deliberation Processing",
        "author": [
            "Mengqi Wang",
            "Zhan Liu",
            "Zengrui Jin",
            "Guangzhi Sun",
            "Chao Zhang",
            "Philip C. Woodland"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16622",
        "abstract": "Diffusion-based large language models (DLLMs) have recently attracted growing interest as an alternative to autoregressive decoders. In this work, we present an empirical study on using the diffusion-based large language model LLaDA for automatic speech recognition (ASR). We first investigate its use as an external deliberation-based processing module for Whisper-LLaMA transcripts. By leveraging the bidirectional attention and denoising capabilities of LLaDA, we explore random masking, low-confidence masking, and semi-autoregressive strategies, showing that Whisper-LLaDA substantially reduces WER compared with the baseline. On LibriSpeech, the best cascade system achieves 2.25%/4.94% WER on test-clean/test-other, representing a 12.3% relative improvement over the Whisper-LLaMA baseline on the test-other split. In contrast, a plain-text LLaDA without acoustic features fails to improve accuracy, highlighting the importance of audio-conditioned embeddings. We further evaluate Whisper-LLaDA as a standalone decoder for ASR with diffusion-based and semi-autoregressive decoding. Most experimental configurations achieve faster inference than the Whisper-LLaMA baseline, although recognition accuracy is slightly lower. These findings offer an empirical view of diffusion-based LLMs for ASR and point to promising directions for improvements.",
        "tags": [
            "Diffusion",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "423",
        "title": "DoubleGen: Debiased Generative Modeling of Counterfactuals",
        "author": [
            "Alex Luedtke",
            "Kenji Fukumizu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16842",
        "abstract": "Generative models for counterfactual outcomes face two key sources of bias. Confounding bias arises when approaches fail to account for systematic differences between those who receive the intervention and those who do not. Misspecification bias arises when methods attempt to address confounding through estimation of an auxiliary model, but specify it incorrectly. We introduce DoubleGen, a doubly robust framework that modifies generative modeling training objectives to mitigate these biases. The new objectives rely on two auxiliaries -- a propensity and outcome model -- and successfully address confounding bias even if only one of them is correct. We provide finite-sample guarantees for this robustness property. We further establish conditions under which DoubleGen achieves oracle optimality -- matching the convergence rates standard approaches would enjoy if interventional data were available -- and minimax rate optimality. We illustrate DoubleGen with three examples: diffusion models, flow matching, and autoregressive language models.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "424",
        "title": "DroFiT: A Lightweight Band-fused Frequency Attention Toward Real-time UAV Speech Enhancement",
        "author": [
            "Jeongmin Lee",
            "Chanhong Jeon",
            "Hyungjoo Seo",
            "Taewook Kang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16945",
        "abstract": "This paper proposes DroFiT (Drone Frequency lightweight Transformer for speech enhancement, a single microphone speech enhancement network for severe drone self-noise. DroFit integrates a frequency-wise Transformer with a full/sub-band hybrid encoder-decoder and a TCN back-end for memory-efficient streaming. A learnable skip-and-gate fusion with a combined spectral-temporal loss further refines reconstruction. The model is trained on VoiceBank-DEMAND mixed with recorded drone noise (-5 to -25 dB SNR) and evaluate using standard speech enhancement metrics and computational efficiency. Experimental results show that DroFiT achieves competitive enhancement performance while significantly reducing computational and memory demands, paving the way for real-time processing on resource-constrained UAV platforms. Audio demo samples are available on our demo page.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "425",
        "title": "Quantum Adaptive Self-Attention for Financial Rebalancing: An Empirical Study on Automated Market Makers in Decentralized Finance",
        "author": [
            "Chi-Sheng Chen",
            "Aidan Hung-Wen Tsai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.16955",
        "abstract": "We formulate automated market maker (AMM) \\emph{rebalancing} as a binary detection problem and study a hybrid quantum--classical self-attention block, \\textbf{Quantum Adaptive Self-Attention (QASA)}. QASA constructs quantum queries/keys/values via variational quantum circuits (VQCs) and applies standard softmax attention over Pauli-$Z$ expectation vectors, yielding a drop-in attention module for financial time-series decision making. Using daily data for \\textbf{BTCUSDC} over \\textbf{Jan-2024--Jan-2025} with a 70/15/15 time-series split, we compare QASA against classical ensembles, a transformer, and pure quantum baselines under Return, Sharpe, and Max Drawdown. The \\textbf{QASA-Sequence} variant attains the \\emph{best single-model risk-adjusted performance} (\\textbf{13.99\\%} return; \\textbf{Sharpe 1.76}), while hybrid models average \\textbf{11.2\\%} return (vs.\\ 9.8\\% classical; 4.4\\% pure quantum), indicating a favorable performance--stability--cost trade-off.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "426",
        "title": "$\\texttt{DiffSyn}$: A Generative Diffusion Approach to Materials Synthesis Planning",
        "author": [
            "Elton Pan",
            "Soonhyoung Kwon",
            "Sulin Liu",
            "Mingrou Xie",
            "Alexander J. Hoffman",
            "Yifei Duan",
            "Thorben Prein",
            "Killian Sheriff",
            "Yuriy Roman-Leshkov",
            "Manuel Moliner",
            "Rafael Gomez-Bombarelli",
            "Elsa Olivetti"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17094",
        "abstract": "The synthesis of crystalline materials, such as zeolites, remains a significant challenge due to a high-dimensional synthesis space, intricate structure-synthesis relationships and time-consuming experiments. Considering the one-to-many relationship between structure and synthesis, we propose $\\texttt{DiffSyn}$, a generative diffusion model trained on over 23,000 synthesis recipes spanning 50 years of literature. $\\texttt{DiffSyn}$ generates probable synthesis routes conditioned on a desired zeolite structure and an organic template. $\\texttt{DiffSyn}$ achieves state-of-the-art performance by capturing the multi-modal nature of structure-synthesis relationships. We apply $\\texttt{DiffSyn}$ to differentiate among competing phases and generate optimal synthesis routes. As a proof of concept, we synthesize a UFI material using $\\texttt{DiffSyn}$-generated synthesis routes. These routes, rationalized by density functional theory binding energies, resulted in the successful synthesis of a UFI material with a high Si/Al$_{\\text{ICP}}$ of 19.0, which is expected to improve thermal stability and is higher than that of any previously recorded.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "427",
        "title": "MaskVCT: Masked Voice Codec Transformer for Zero-Shot Voice Conversion With Increased Controllability via Multiple Guidances",
        "author": [
            "Junhyeok Lee",
            "Helin Wang",
            "Yaohan Guan",
            "Thomas Thebaud",
            "Laureano Moro-Velazquez",
            "JesÃºs Villalba",
            "Najim Dehak"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17143",
        "abstract": "We introduce MaskVCT, a zero-shot voice conversion (VC) model that offers multi-factor controllability through multiple classifier-free guidances (CFGs). While previous VC models rely on a fixed conditioning scheme, MaskVCT integrates diverse conditions in a single model. To further enhance robustness and control, the model can leverage continuous or quantized linguistic features to enhance intellgibility and speaker similarity, and can use or omit pitch contour to control prosody. These choices allow users to seamlessly balance speaker identity, linguistic content, and prosodic factors in a zero-shot VC setting. Extensive experiments demonstrate that MaskVCT achieves the best target speaker and accent similarities while obtaining competitive word and character error rates compared to existing baselines. Audio samples are available at https://maskvct.github.io/.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "428",
        "title": "DeepASA: An Object-Oriented One-for-All Network for Auditory Scene Analysis",
        "author": [
            "Dongheon Lee",
            "Younghoo Kwon",
            "Jung-Woo Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17247",
        "abstract": "We propose DeepASA, a one-for-all model for auditory scene analysis that performs multi-input multi-output (MIMO) source separation, dereverberation, sound event detection (SED), audio classification, and direction-of-arrival estimation (DoAE) within a unified framework. DeepASA is designed for complex auditory scenes where multiple, often similar, sound sources overlap in time and move dynamically in space. To achieve robust and consistent inference across tasks, we introduce an object-oriented processing (OOP) strategy. This approach encapsulates diverse auditory features into object-centric representations and refines them through a chain-of-inference (CoI) mechanism. The pipeline comprises a dynamic temporal kernel-based feature extractor, a transformer-based aggregator, and an object separator that yields per-object features. These features feed into multiple task-specific decoders. Our object-centric representations naturally resolve the parameter association ambiguity inherent in traditional track-wise processing. However, early-stage object separation can lead to failure in downstream ASA tasks. To address this, we implement temporal coherence matching (TCM) within the chain-of-inference, enabling multi-task fusion and iterative refinement of object features using estimated auditory parameters. We evaluate DeepASA on representative spatial audio benchmark datasets, including ASA2, MC-FUSS, and STARSS23. Experimental results show that our model achieves state-of-the-art performance across all evaluated tasks, demonstrating its effectiveness in both source separation and auditory parameter estimation under diverse spatial auditory scenes.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "429",
        "title": "BeepBank-500: A Synthetic Earcon Mini-Corpus for UI Sound Research and Psychoacoustics Research",
        "author": [
            "Mandip Goswami"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17277",
        "abstract": "We introduce BeepBank-500, a compact, fully synthetic earcon/alert dataset (300-500 clips) designed for rapid, rights-clean experimentation in human-computer interaction and audio machine learning. Each clip is generated from a parametric recipe controlling waveform family (sine, square, triangle, FM), fundamental frequency, duration, amplitude envelope, amplitude modulation (AM), and lightweight Schroeder-style reverberation. We use three reverberation settings: dry, and two synthetic rooms denoted 'rir small' ('small') and 'rir medium' ('medium') throughout the paper and in the metadata. We release mono 48 kHz WAV audio (16-bit), a rich metadata table (signal/spectral features), and tiny reproducible baselines for (i) waveform-family classification and (ii) f0 regression on single tones. The corpus targets tasks such as earcon classification, timbre analyses, and onset detection, with clearly stated licensing and limitations. Audio is dedicated to the public domain via CC0-1.0; code is under MIT. Data DOI: https://doi.org/10.5281/zenodo.17172015. Code: https://github.com/mandip42/earcons-mini-500.",
        "tags": [
            "CLIP",
            "Detection"
        ]
    },
    {
        "id": "430",
        "title": "From Prediction to Understanding: Will AI Foundation Models Transform Brain Science?",
        "author": [
            "Thomas Serre",
            "Ellie Pavlick"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17280",
        "abstract": "Generative pretraining (the \"GPT\" in ChatGPT) enables language models to learn from vast amounts of internet text without human supervision. This approach has driven breakthroughs across AI by allowing deep neural networks to learn from massive, unstructured datasets. We use the term foundation models to refer to large pretrained systems that can be adapted to a wide range of tasks within and across domains, and these models are increasingly applied beyond language to the brain sciences. These models achieve strong predictive accuracy, raising hopes that they might illuminate computational principles. But predictive success alone does not guarantee scientific understanding. Here, we outline how foundation models can be productively integrated into the brain sciences, highlighting both their promise and their limitations. The central challenge is to move from prediction to explanation: linking model computations to mechanisms underlying neural activity and cognition.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "431",
        "title": "Robust Mixture Models for Algorithmic Fairness Under Latent Heterogeneity",
        "author": [
            "Siqi Li",
            "Molei Liu",
            "Ziye Tian",
            "Chuan Hong",
            "Nan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.17411",
        "abstract": "Standard machine learning models optimized for average performance often fail on minority subgroups and lack robustness to distribution shifts. This challenge worsens when subgroups are latent and affected by complex interactions among continuous and discrete features. We introduce ROME (RObust Mixture Ensemble), a framework that learns latent group structure from data while optimizing for worst-group performance. ROME employs two approaches: an Expectation-Maximization algorithm for linear models and a neural Mixture-of-Experts for nonlinear settings. Through simulations and experiments on real-world datasets, we demonstrate that ROME significantly improves algorithmic fairness compared to standard methods while maintaining competitive average performance. Importantly, our method requires no predefined group labels, making it practical when sources of disparities are unknown or evolving.",
        "tags": [
            "MoE"
        ]
    }
]