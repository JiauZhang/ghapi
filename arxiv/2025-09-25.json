[
    {
        "id": "1",
        "title": "A Cost-Benefit Analysis of On-Premise Large Language Model Deployment: Breaking Even with Commercial LLM Services",
        "author": [
            "Guanzhong Pan",
            "Haibo Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18101",
        "abstract": "Large language models (LLMs) are becoming increasingly widespread. Organizations that want to use AI for productivity now face an important decision. They can subscribe to commercial LLM services or deploy models on their own infrastructure. Cloud services from providers such as OpenAI, Anthropic, and Google are attractive because they provide easy access to state-of-the-art models and are easy to scale. However, concerns about data privacy, the difficulty of switching service providers, and long-term operating costs have driven interest in local deployment of open-source models. This paper presents a cost-benefit analysis framework to help organizations determine when on-premise LLM deployment becomes economically viable compared to commercial subscription services. We consider the hardware requirements, operational expenses, and performance benchmarks of the latest open-source models, including Qwen, Llama, Mistral, and etc. Then we compare the total cost of deploying these models locally with the major cloud providers subscription fee. Our findings provide an estimated breakeven point based on usage levels and performance needs. These results give organizations a practical framework for planning their LLM strategies.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "2",
        "title": "XMUspeech Systems for the ASVspoof 5 Challenge",
        "author": [
            "Wangjie Li",
            "Xingjia Xie",
            "Yishuang Li",
            "Wenhao Guan",
            "Kaidi Wang",
            "Pengyu Ren",
            "Lin Li",
            "Qingyang Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18102",
        "abstract": "In this paper, we present our submitted XMUspeech systems to the speech deepfake detection track of the ASVspoof 5 Challenge. Compared to previous challenges, the audio duration in ASVspoof 5 database has significantly increased. And we observed that merely adjusting the input audio length can substantially improve system performance. To capture artifacts at multiple levels, we explored the performance of AASIST, HM-Conformer, Hubert, and Wav2vec2 with various input features and loss functions. Specifically, in order to obtain artifact-related information, we trained self-supervised models on the dataset containing spoofing utterances as the feature extractors. And we applied an adaptive multi-scale feature fusion (AMFF) method to integrate features from multiple Transformer layers with the hand-crafted feature to enhance the detection capability. In addition, we conducted extensive experiments on one-class loss functions and provided optimized configurations to better align with the anti-spoofing task. Our fusion system achieved a minDCF of 0.4783 and an EER of 20.45% in the closed condition, and a minDCF of 0.2245 and an EER of 9.36% in the open condition.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "3",
        "title": "Solve it with EASE",
        "author": [
            "Adam Viktorin",
            "Tomas Kadavy",
            "Jozef Kovac",
            "Michal Pluhacek",
            "Roman Senkerik"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18108",
        "abstract": "This paper presents EASE (Effortless Algorithmic Solution Evolution), an open-source and fully modular framework for iterative algorithmic solution generation leveraging large language models (LLMs). EASE integrates generation, testing, analysis, and evaluation into a reproducible feedback loop, giving users full control over error handling, analysis, and quality assessment. Its architecture supports the orchestration of multiple LLMs in complementary roles-such as generator, analyst, and evaluator. By abstracting the complexity of prompt design and model management, EASE provides a transparent and extensible platform for researchers and practitioners to co-design algorithms and other generative solutions across diverse domains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "4",
        "title": "Prompt Optimization Meets Subspace Representation Learning for Few-shot Out-of-Distribution Detection",
        "author": [
            "Faizul Rakib Sayem",
            "Shahana Ibrahim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18111",
        "abstract": "The reliability of artificial intelligence (AI) systems in open-world settings depends heavily on their ability to flag out-of-distribution (OOD) inputs unseen during training. Recent advances in large-scale vision-language models (VLMs) have enabled promising few-shot OOD detection frameworks using only a handful of in-distribution (ID) samples. However, existing prompt learning-based OOD methods rely solely on softmax probabilities, overlooking the rich discriminative potential of the feature embeddings learned by VLMs trained on millions of samples. To address this limitation, we propose a novel context optimization (CoOp)-based framework that integrates subspace representation learning with prompt tuning. Our approach improves ID-OOD separability by projecting the ID features into a subspace spanned by prompt vectors, while projecting ID-irrelevant features into an orthogonal null space. To train such OOD detection framework, we design an easy-to-handle end-to-end learning criterion that ensures strong OOD detection performance as well as high ID classification accuracy. Experiments on real-world datasets showcase the effectiveness of our approach.",
        "tags": [
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "5",
        "title": "Large language models surpass domain-specific architectures for antepartum electronic fetal monitoring analysis",
        "author": [
            "Sheng Wong",
            "Ravi Shankar",
            "Beth Albert",
            "Gabriel Davis Jones"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18112",
        "abstract": "Foundation models (FMs) and large language models (LLMs) demonstrate remarkable capabilities across diverse domains through training on massive datasets. These models have demonstrated exceptional performance in healthcare applications, yet their potential for electronic fetal monitoring (EFM)/cardiotocography (CTG) analysis, a critical technology for evaluating fetal well-being, remains largely underexplored. Antepartum CTG interpretation presents unique challenges due to the complex nature of fetal heart rate (FHR) patterns and uterine activity, requiring sophisticated analysis of long time-series data. The assessment of CTG is heavily based on subjective clinical interpretation, often leading to variability in diagnostic accuracy and deviation from timely pregnancy care. This study presents the first comprehensive comparison of state-of-the-art AI approaches for automated antepartum CTG analysis. We systematically compare time-series FMs and LLMs against established CTG-specific architectures. Our evaluation encompasses over 500 CTG recordings of varying durations reflecting real-world clinical recordings, providing robust performance benchmarks across different modelling paradigms. Our results demonstrate that fine-tuned LLMs achieve superior performance compared to both foundation models and domain-specific approaches, offering a promising alternative pathway for clinical CTG interpretation. These findings provide critical insights into the relative strengths of different AI methodologies for fetal monitoring applications and establish a foundation for future clinical AI development in prenatal care.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "Dynamic Prompt Fusion for Multi-Task and Cross-Domain Adaptation in LLMs",
        "author": [
            "Xin Hu",
            "Yue Kang",
            "Guanzi Yao",
            "Tianze Kang",
            "Mengjie Wang",
            "Heyao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18113",
        "abstract": "This study addresses the generalization limitations commonly observed in large language models under multi-task and cross-domain settings. Unlike prior methods such as SPoT, which depends on fixed prompt templates, our study introduces a unified multi-task learning framework with dynamic prompt scheduling mechanism. By introducing a prompt pool and a task-aware scheduling strategy, the method dynamically combines and aligns prompts for different tasks. This enhances the model's ability to capture semantic differences across tasks. During prompt fusion, the model uses task embeddings and a gating mechanism to finely control the prompt signals. This ensures alignment between prompt content and task-specific demands. At the same time, it builds flexible sharing pathways across tasks. In addition, the proposed optimization objective centers on joint multi-task learning. It incorporates an automatic learning strategy for scheduling weights, which effectively mitigates task interference and negative transfer. To evaluate the effectiveness of the method, a series of sensitivity experiments were conducted. These experiments examined the impact of prompt temperature parameters and task number variation. The results confirm the advantages of the proposed mechanism in maintaining model stability and enhancing transferability. Experimental findings show that the prompt scheduling method significantly improves performance on a range of language understanding and knowledge reasoning tasks. These results fully demonstrate its applicability and effectiveness in unified multi-task modeling and cross-domain adaptation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "7",
        "title": "A Study of Skews, Imbalances, and Pathological Conditions in LLM Inference Deployment on GPU Clusters detectable from DPU",
        "author": [
            "Javed I. Khan an Henry Uwabor Moye"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18114",
        "abstract": "Autoregressive inference in large transformer-based language models (LLMs) presents significant challenges for runtime efficiency, particularly during the decode phase where load imbalance across GPU shards can cause throughput degradation and latency spikes. A DPU-assisted framework leveraged by BlueField-3 Data Processing Units can enable real-time detection and mitigation of load imbalance in multi-node tensor-parallel inference. By offloading monitoring tasks to the DPU and analyzing GPU telemetry and inter-node communication patterns, the resulting system can provide actionable feedback to inference controllers and schedulers. The goal of this study is three-fold i) identify the reported skews/imbalances/pathological conditions that arise in muti-GPU execution of a) LLM tensor computing (both during training and inference), b) identify their impact on computational performance, and c) make a critical assessment if those can be tracked for potential mitigation from a DPU network.",
        "tags": [
            "Detection",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "8",
        "title": "Amortized Latent Steering: Low-Cost Alternative to Test-Time Optimization",
        "author": [
            "Nathan Egbuna",
            "Saatvik Gaur",
            "Sunishchal Dev",
            "Ashwinee Panda",
            "Maheep Chaudhary"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18116",
        "abstract": "Test-time optimization remains impractical at scale due to prohibitive inference costs\\textemdash techniques like iterative refinement and multi-step verification can require $10$--$100\\times$ more compute per query than standard decoding. Latent space test-time optimization methods like LatentSeek offer a more direct approach by steering hidden representations, but still demand expensive per-query optimization loops with multiple backward passes. We propose Amortized Latent Steering (ALS), which collapses this iterative optimization into a single offline-computed vector applied at constant cost during inference. ALS computes the mean difference between hidden states from successful versus unsuccessful generations, then uses this direction to calibrate the model's hidden representations: when decoding drifts away from the success manifold, ALS nudges activations back toward it. Across GSM8K and MATH-$500$ benchmarks, ALS achieves $2$--$5\\times$ speedup over iterative methods while matching or surpassing greedy Chain-of-Thought (CoT) and Self-Consistency baselines, yielding up to 101\\% improvement in efficiency--accuracy trade-off. These results show that much of latent optimization's benefit can be captured offline, making sophisticated reasoning techniques viable for production deployment. Code is available at~\\href{https://anonymous.4open.science/r/steering-17F2}{https://anonymous.4open.science/r/steering-17F2}",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "9",
        "title": "MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents",
        "author": [
            "Yifan Xu",
            "Xiao Liu",
            "Xinghan Liu",
            "Jiaqi Fu",
            "Hanchen Zhang",
            "Bohao Jing",
            "Shudan Zhang",
            "Yuting Wang",
            "Wenyi Zhao",
            "Yuxiao Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18119",
        "abstract": "Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MOBILERL to enhance GUI agents in mobile environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (75.8%) and AndroidLab (46.8%). The MOBILERL framework is adopted in the AutoGLM products, and also open-sourced at https://github.com/THUDM/MobileRL.",
        "tags": [
            "GLM",
            "GRPO",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "10",
        "title": "GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models",
        "author": [
            "Yue Zhang",
            "Jiaxin Zhang",
            "Qiuyu Ren",
            "Tahsin Saffat",
            "Xiaoxuan Liu",
            "Zitong Yang",
            "Banghua Zhu",
            "Yi Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18122",
        "abstract": "We introduce \\textbf{GAUSS} (\\textbf{G}eneral \\textbf{A}ssessment of \\textbf{U}nderlying \\textbf{S}tructured \\textbf{S}kills in Mathematics), a benchmark that evaluates LLMs' mathematical abilities across twelve core skill dimensions, grouped into three domains: knowledge and understanding, problem solving and communication, and meta-skills and creativity. By categorizing problems according to cognitive skills and designing tasks that isolate specific abilities, GAUSS constructs comprehensive, fine-grained, and interpretable profiles of models' mathematical abilities. These profiles faithfully represent their underlying mathematical intelligence. To exemplify how to use the \\textsc{GAUSS} benchmark, we have derived the skill profile of \\textsc{GPT-5-thinking}, revealing its strengths and weaknesses as well as its differences relative to \\textsc{o4-mini-high}, thereby underscoring the value of multidimensional, skill-based evaluation.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "11",
        "title": "NurseSchedRL: Attention-Guided Reinforcement Learning for Nurse-Patient Assignment",
        "author": [
            "Harsha Koduri"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18125",
        "abstract": "Healthcare systems face increasing pressure to allocate limited nursing resources efficiently while accounting for skill heterogeneity, patient acuity, staff fatigue, and continuity of care. Traditional optimization and heuristic scheduling methods struggle to capture these dynamic, multi-constraint environments. I propose NurseSchedRL, a reinforcement learning framework for nurse-patient assignment that integrates structured state encoding, constrained action masking, and attention-based representations of skills, fatigue, and geographical context. NurseSchedRL uses Proximal Policy Optimization (PPO) with feasibility masks to ensure assignments respect real-world constraints, while dynamically adapting to patient arrivals and varying nurse availability. In simulation with realistic nurse and patient data, NurseSchedRL achieves improved scheduling efficiency, better alignment of skills to patient needs, and reduced fatigue compared to baseline heuristic and unconstrained RL approaches. These results highlight the potential of reinforcement learning for decision support in complex, high-stakes healthcare workforce management.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "12",
        "title": "Safe-SAIL: Towards a Fine-grained Safety Landscape of Large Language Models via Sparse Autoencoder Interpretation Framework",
        "author": [
            "Jiaqi Weng",
            "Han Zheng",
            "Hanyu Zhang",
            "Qinqin He",
            "Jialing Tao",
            "Hui Xue",
            "Zhixuan Chu",
            "Xiting Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18127",
        "abstract": "Increasing deployment of large language models (LLMs) in real-world applications raises significant safety concerns. Most existing safety research focuses on evaluating LLM outputs or specific safety tasks, limiting their ability to ad- dress broader, undefined risks. Sparse Autoencoders (SAEs) facilitate interpretability research to clarify model behavior by explaining single-meaning atomic features decomposed from entangled signals. jHowever, prior applications on SAEs do not interpret features with fine-grained safety-related con- cepts, thus inadequately addressing safety-critical behaviors, such as generating toxic responses and violating safety regu- lations. For rigorous safety analysis, we must extract a rich and diverse set of safety-relevant features that effectively capture these high-risk behaviors, yet face two challenges: identifying SAEs with the greatest potential for generating safety concept-specific neurons, and the prohibitively high cost of detailed feature explanation. In this paper, we pro- pose Safe-SAIL, a framework for interpreting SAE features within LLMs to advance mechanistic understanding in safety domains. Our approach systematically identifies SAE with best concept-specific interpretability, explains safety-related neurons, and introduces efficient strategies to scale up the in- terpretation process. We will release a comprehensive toolkit including SAE checkpoints and human-readable neuron ex- planations, which supports empirical analysis of safety risks to promote research on LLM safety.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "13",
        "title": "Two ways to knowledge?",
        "author": [
            "Jean-Michel Tucny",
            "Abhisek Ganguly",
            "Santosh Ansumali",
            "Sauro Succi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18131",
        "abstract": "It is shown that the weight matrices of transformer-based machine learning applications to the solution of two representative physical applications show a random-like character which bears no directly recognizable link to the physical and mathematical structure of the physical problem under study. This suggests that machine learning and the scientific method may represent two distinct and potentially complementary paths to knowledge, even though a strict notion of explainability in terms of direct correspondence between network parameters and physical structures may remain out of reach. It is also observed that drawing a parallel between transformer operation and (generalized) path-integration techniques may account for the random-like nature of the weights, but still does not resolve the tension with explainability. We conclude with some general comments on the hazards of gleaning knowledge without the benefit of Insight.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "14",
        "title": "Self-Evolving LLMs via Continual Instruction Tuning",
        "author": [
            "Le Huang",
            "Jiazheng Kang",
            "Cheng Hou",
            "Zhe Zhao",
            "Zhenxiang Yan",
            "Chuan Shi",
            "Ting Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18133",
        "abstract": "In real-world industrial settings, large language models (LLMs) must learn continually to keep pace with diverse and evolving tasks, requiring self-evolution to refine knowledge under dynamic data distributions. However, existing continual learning (CL) approaches, such as replay and parameter isolation, often suffer from catastrophic forgetting: training on new tasks degrades performance on earlier ones by overfitting to the new distribution and weakening http://generalization.We propose MoE-CL, a parameter-efficient adversarial mixture-of-experts framework for industrial-scale, self-evolving continual instruction tuning of LLMs. MoE-CL uses a dual-expert design: (1) a dedicated LoRA expert per task to preserve task-specific knowledge via parameter independence, mitigating forgetting; and (2) a shared LoRA expert to enable cross-task transfer. To prevent transferring task-irrelevant noise through the shared pathway, we integrate a task-aware discriminator within a GAN. The discriminator encourages the shared expert to pass only task-aligned information during sequential training. Through adversarial learning, the shared expert acquires generalized representations that mimic the discriminator, while dedicated experts retain task-specific details, balancing knowledge retention and cross-task generalization and thereby supporting http://self-evolution.Extensive experiments on the public MTL5 benchmark and an industrial Tencent3 benchmark validate the effectiveness of MoE-CL for continual instruction tuning. In real-world A/B testing for content compliance review on the Tencent Video platform, MoE-CL reduced manual review costs by 15.3%. These results demonstrate that MoE-CL is practical for large-scale industrial deployment where continual adaptation and stable transfer are critical.",
        "tags": [
            "GAN",
            "LLM",
            "LoRA",
            "MoE"
        ]
    },
    {
        "id": "15",
        "title": "From Parameters to Performance: A Data-Driven Study on LLM Structure and Development",
        "author": [
            "Suqing Wang",
            "Zuchao Li",
            "Luohe Shi",
            "Bo Du",
            "Hai Zhao",
            "Yun Li",
            "Qianren Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18136",
        "abstract": "Large language models (LLMs) have achieved remarkable success across various domains, driving significant technological advancements and innovations. Despite the rapid growth in model scale and capability, systematic, data-driven research on how structural configurations affect performance remains scarce. To address this gap, we present a large-scale dataset encompassing diverse open-source LLM structures and their performance across multiple benchmarks. Leveraging this dataset, we conduct a systematic, data mining-driven analysis to validate and quantify the relationship between structural configurations and performance. Our study begins with a review of the historical development of LLMs and an exploration of potential future trends. We then analyze how various structural choices impact performance across benchmarks and further corroborate our findings using mechanistic interpretability techniques. By providing data-driven insights into LLM optimization, our work aims to guide the targeted development and application of future models. We will release our dataset at https://huggingface.co/datasets/DX0369/LLM-Structure-Performance-Dataset",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "16",
        "title": "LoRALib: A Standardized Benchmark for Evaluating LoRA-MoE Methods",
        "author": [
            "Shaoheng Wang",
            "Yao Lu",
            "Yuqi Li",
            "Yaxin Gao",
            "Jiaqi Nie",
            "Shanqing Yu",
            "Yingli Tian",
            "Qi Xuan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18137",
        "abstract": "As a parameter efficient fine-tuning (PEFT) method, low-rank adaptation (LoRA) can save significant costs in storage and computing, but its strong adaptability to a single task is often accompanied by insufficient cross-task generalization capabilities. To improve this, existing work combines LoRA with mixture-of-experts (MoE) to enhance the model's adaptability through expert modules and routing mechanisms. However, existing LoRA-MoE methods lack unified standards in models, datasets, hyperparameters, and evaluation methods, making it difficult to conduct fair comparisons between different methods. To this end, we proposed a unified benchmark named LoRALib. Specifically, we standardized datasets from $40$ downstream tasks into a unified format, fine-tuned them using the same hyperparameters and obtained $680$ LoRA modules across $17$ model architectures. Based on this LoRA library, we conduct large-scale experiments on $3$ representative LoRA-MoE methods and different LoRA selection mechanisms using the open-sourced testing tool OpenCompass. Extensive experiments show that LoRAMoE performs best, and that prioritizing LoRAs relevant to the target task can further improve the performance of MoE. We hope these findings will inspire future work. Our datasets and LoRA library are available at https://huggingface.co/datasets/YaoLuzjut/LoRAOcean_dataset and https://huggingface.co/YaoLuzjut/models.",
        "tags": [
            "LoRA",
            "MoE"
        ]
    },
    {
        "id": "17",
        "title": "AdaSTI: Conditional Diffusion Models with Adaptive Dependency Modeling for Spatio-Temporal Imputation",
        "author": [
            "Yubo Yang",
            "Yichen Zhu",
            "Bo Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18144",
        "abstract": "Spatio-temporal data abounds in domain like traffic and environmental monitoring. However, it often suffers from missing values due to sensor malfunctions, transmission failures, etc. Recent years have seen continued efforts to improve spatio-temporal data imputation performance. Recently diffusion models have outperformed other approaches in various tasks, including spatio-temporal imputation, showing competitive performance. Extracting and utilizing spatio-temporal dependencies as conditional information is vital in diffusion-based methods. However, previous methods introduce error accumulation in this process and ignore the variability of the dependencies in the noisy data at different diffusion steps. In this paper, we propose AdaSTI (Adaptive Dependency Model in Diffusion-based Spatio-Temporal Imputation), a novel spatio-temporal imputation approach based on conditional diffusion model. Inside AdaSTI, we propose a BiS4PI network based on a bi-directional S4 model for pre-imputation with the imputed result used to extract conditional information by our designed Spatio-Temporal Conditionalizer (STC)network. We also propose a Noise-Aware Spatio-Temporal (NAST) network with a gated attention mechanism to capture the variant dependencies across diffusion steps. Extensive experiments on three real-world datasets show that AdaSTI outperforms existing methods in all the settings, with up to 46.4% reduction in imputation error.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "18",
        "title": "Sparse Training Scheme for Multimodal LLM",
        "author": [
            "Kean Shi",
            "Liang Chen",
            "Haozhe Zhao",
            "Baobao Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18150",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated outstanding performance across a variety of domains. However, training MLLMs is often inefficient due to the significantly longer input sequences introduced by multimodal data and the low utilization of inter-layer computations. To address this challenge, we shift the focus to the training process itself and propose a novel training-efficient framework based on sparse representations, termed the Sparse Training Scheme (STS). This scheme consists of two key components: the Visual Token Compressor, which reduces the information load by compressing visual tokens, and the Layer Dynamic Skipper, which mitigates the computational overhead by dynamically skipping unnecessary layers in the language model during both forward and backward passes. Our approach is broadly applicable to diverse MLLM architectures and has been extensively evaluated on multiple benchmarks, demonstrating its effectiveness and efficiency.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "19",
        "title": "MiniCPM-V 4.5: Cooking Efficient MLLMs via Architecture, Data, and Training Recipe",
        "author": [
            "Tianyu Yu",
            "Zefan Wang",
            "Chongyi Wang",
            "Fuwei Huang",
            "Wenshuo Ma",
            "Zhihui He",
            "Tianchi Cai",
            "Weize Chen",
            "Yuxiang Huang",
            "Yuanqian Zhao",
            "Bokai Xu",
            "Junbo Cui",
            "Yingjing Xu",
            "Liqing Ruan",
            "Luoyuan Zhang",
            "Hanyu Liu",
            "Jingkun Tang",
            "Hongyuan Liu",
            "Qining Guo",
            "Wenhao Hu",
            "Bingxiang He",
            "Jie Zhou",
            "Jie Cai",
            "Ji Qi",
            "Zonghao Guo",
            "Chi Chen",
            "Guoyang Zeng",
            "Yuxuan Li",
            "Ganqu Cui",
            "Ning Ding",
            "Xu Han",
            "Yuan Yao",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18154",
        "abstract": "Multimodal Large Language Models (MLLMs) are undergoing rapid progress and represent the frontier of AI development. However, their training and inference efficiency have emerged as a core bottleneck in making MLLMs more accessible and scalable. To address the challenges, we present MiniCPM-V 4.5, an 8B parameter model designed for high efficiency and strong performance. We introduce three core improvements in model architecture, data strategy and training method: a unified 3D-Resampler model architecture for highly compact encoding over images and videos, a unified learning paradigm for document knowledge and text recognition without heavy data engineering, and a hybrid reinforcement learning strategy for proficiency in both short and long reasoning modes. Comprehensive experimental results in OpenCompass evaluation show that MiniCPM-V 4.5 surpasses widely used proprietary models such as GPT-4o-latest, and significantly larger open-source models such as Qwen2.5-VL 72B. Notably, the strong performance is achieved with remarkable efficiency. For example, on the widely adopted VideoMME benchmark, MiniCPM-V 4.5 achieves state-of-the-art performance among models under 30B size, using just 46.7\\% GPU memory cost and 8.7\\% inference time of Qwen2.5-VL 7B.",
        "tags": [
            "3D",
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "20",
        "title": "Event Causality Identification with Synthetic Control",
        "author": [
            "Haoyu Wang",
            "Fengze Liu",
            "Jiayao Zhang",
            "Dan Roth",
            "Kyle Richardson"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18156",
        "abstract": "Event causality identification (ECI), a process that extracts causal relations between events from text, is crucial for distinguishing causation from correlation. Traditional approaches to ECI have primarily utilized linguistic patterns and multi-hop relational inference, risking false causality identification due to informal usage of causality and specious graphical inference. In this paper, we adopt the Rubin Causal Model to identify event causality: given two temporally ordered events, we see the first event as the treatment and the second one as the observed outcome. Determining their causality involves manipulating the treatment and estimating the resultant change in the likelihood of the outcome. Given that it is only possible to implement manipulation conceptually in the text domain, as a work-around, we try to find a twin for the protagonist from existing corpora. This twin should have identical life experiences with the protagonist before the treatment but undergoes an intervention of treatment. However, the practical difficulty of locating such a match limits its feasibility. Addressing this issue, we use the synthetic control method to generate such a twin' from relevant historical data, leveraging text embedding synthesis and inversion techniques. This approach allows us to identify causal relations more robustly than previous methods, including GPT-4, which is demonstrated on a causality benchmark, COPES-hard.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "21",
        "title": "ZERA: Zero-init Instruction Evolving Refinement Agent - From Zero Instructions to Structured Prompts via Principle-based Optimization",
        "author": [
            "Seungyoun Yi",
            "Minsoo Khang",
            "Sungrae Park"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18158",
        "abstract": "Automatic Prompt Optimization (APO) improves large language model (LLM) performance by refining prompts for specific tasks. However, prior APO methods typically focus only on user prompts, rely on unstructured feedback, and require large sample sizes and long iteration cycles-making them costly and brittle. We propose ZERA (Zero-init Instruction Evolving Refinement Agent), a novel framework that jointly optimizes both system and user prompts through principled, low-overhead refinement. ZERA scores prompts using eight generalizable criteria with automatically inferred weights, and revises prompts based on these structured critiques. This enables fast convergence to high-quality prompts using minimal examples and short iteration cycles. We evaluate ZERA across five LLMs and nine diverse datasets spanning reasoning, summarization, and code generation tasks. Experimental results demonstrate consistent improvements over strong baselines. Further ablation studies highlight the contribution of each component to more effective prompt construction. Our implementation including all prompts is publicly available at https://github.com/younatics/zera-agent.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "22",
        "title": "A Simple and Reproducible Hybrid Solver for a Truck-Drone VRP with Recharge",
        "author": [
            "Meraryslan Meraliyev",
            "Cemil Turan",
            "Shirali Kadyrov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18162",
        "abstract": "We study last-mile delivery with one truck and one drone under explicit battery management: the drone flies at twice the truck speed; each sortie must satisfy an endurance budget; after every delivery the drone recharges on the truck before the next launch. We introduce a hybrid reinforcement learning (RL) solver that couples an ALNS-based truck tour (with 2/3-opt and Or-opt) with a small pointer/attention policy that schedules drone sorties. The policy decodes launch--serve--rendezvous triplets with hard feasibility masks for endurance and post-delivery recharge; a fast, exact timeline simulator enforces launch/recovery handling and computes the true makespan used by masked greedy/beam decoding. On Euclidean instances with $N{=}50$, $E{=}0.7$, and $R{=}0.1$, the method achieves an average makespan of \\textbf{5.203}$\\pm$0.093, versus \\textbf{5.349}$\\pm$0.038 for ALNS and \\textbf{5.208}$\\pm$0.124 for NN -- i.e., \\textbf{2.73\\%} better than ALNS on average and within \\textbf{0.10\\%} of NN. Per-seed, the RL scheduler never underperforms ALNS on the same instance and ties or beats NN on two of three seeds. A decomposition of the makespan shows the expected truck--wait trade-off across heuristics; the learned scheduler balances both to minimize the total completion time. We provide a config-first implementation with plotting and significance-test utilities to support replication.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "23",
        "title": "Thinking in a Crowd: How Auxiliary Information Shapes LLM Reasoning",
        "author": [
            "Haodong Zhao",
            "Chenyan Zhao",
            "Yansi Li",
            "Zhuosheng Zhang",
            "Gongshen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18163",
        "abstract": "The capacity of Large Language Models (LLMs) to reason is fundamental to their application in complex, knowledge-intensive domains. In real-world scenarios, LLMs are often augmented with external information that can be helpful, irrelevant, or even misleading. This paper investigates the causal impact of such auxiliary information on the reasoning process of LLMs with explicit step-by-step thinking capabilities. We introduce SciAux, a new dataset derived from ScienceQA, to systematically test the robustness of the model against these types of information. Our findings reveal a critical vulnerability: the model's deliberative \"thinking mode\" is a double-edged sword. While helpful context improves accuracy, misleading information causes a catastrophic drop in performance, which is amplified by the thinking process. Instead of conferring robustness, thinking reinforces the degree of error when provided with misinformation. This highlights that the challenge is not merely to make models \"think\", but to endow them with the critical faculty to evaluate the information upon which their reasoning is based. The SciAux dataset is available at https://huggingface.co/datasets/billhdzhao/SciAux.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "24",
        "title": "DSFT: Inspiring Diffusion Large Language Models to Comprehend Mathematical and Logical Patterns",
        "author": [
            "Ranfei Chen",
            "Ming Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18164",
        "abstract": "Diffusion large language models (dLLMs) have emerged as a new architecture following auto regressive models. Their denoising process offers a powerful generative advantage, but they present significant challenges in learning and understanding numerically sensitive mathematical and order-sensitive logical tasks. Current training methods, including pre-training, fine-tuning, and reinforcement learning, focus primarily on improving general knowledge retention and reasoning abilities, but lack a comprehensive understanding of mathematical and logical patterns. We propose DSFT, a simple yet effective Diffusion SFT strategy, by adjusting the masking strategy and loss function, guiding models to understand mathematical and logical patterns. This strategy can be flexibly combined with pre-training, reinforcement learning, and other training methods. Validated on models such as LLaDA and Dream series, we prove that DSFT on small-scale data can achieve improvements of 5-10% and approximately 2% on mathematical and logical problems, respectively. This inspiring masking approach offers insights for future learning of specific patterns, which can be easily and efficiently combined with other training methods and applied to various dLLMs. Our code is publicly available at https://anonymous.4open.science/r/DSFT-0FFB/",
        "tags": [
            "Diffusion",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "25",
        "title": "SIRAG: Towards Stable and Interpretable RAG with A Process-Supervised Multi-Agent Framework",
        "author": [
            "Junlin Wang",
            "Zehao Wu",
            "Shaowei Lu",
            "Yanlan Li",
            "Xinghao Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18167",
        "abstract": "Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to access external knowledge sources, but the effectiveness of RAG relies on the coordination between the retriever and the generator. Since these components are developed independently, their interaction is often suboptimal: the retriever may return irrelevant or redundant documents, while the generator may fail to fully leverage retrieved evidence. In this work, we propose a process-supervised multi-agent framework to bridge the gap between retriever and generator. The framework introduces two lightweight agents: a Decision Maker, which determines when to continue retrieval or stop for answer generation, and a Knowledge Selector, which filters retrieved documents to retain only the most useful evidence. To provide fine-grained supervision, we employ an LLM-as-a-Judge that evaluates each intermediate action with process-level rewards, ensuring more accurate credit assignment than relying solely on final answer correctness. We further adopt a tree-structured rollout strategy to explore diverse reasoning paths, and train both agents with Proximal Policy Optimization (PPO) in an end-to-end manner. Experiments on single-hop and multi-hop question answering benchmarks show that our approach achieves higher accuracy, more stable convergence, and produces more interpretable reasoning trajectories compared with standard RAG baselines. Importantly, the proposed framework is modular and plug-and-play, requiring no modification to the retriever or generator, making it practical for real-world RAG applications.",
        "tags": [
            "LLM",
            "PPO",
            "RAG"
        ]
    },
    {
        "id": "26",
        "title": "PiMoE: Token-Level Routing for Integrating High-Precision Computation and Reasoning",
        "author": [
            "Hengbo Xiao",
            "Jingyuan Fan",
            "Xin Tong",
            "Jingzhao Zhang",
            "Chao Lu",
            "Guannan He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18169",
        "abstract": "Complex systems typically rely on high-precision numerical computation to support decisions, but current large language models (LLMs) cannot yet incorporate such computations as an intrinsic and interpretable capability with existing architectures. Mainstream multi-agent approaches can leverage external experts, but inevitably introduce communication overhead and suffer from inefficient multimodal emergent capability and limited scalability. To this end, we propose PiMoE (Physically-isolated Mixture of Experts), a training and inference architecture for integrating computation and reasoning. Instead of the workflow paradigm of tool invocation, PiMoE endogenously integrates computational capabilities into neural networks after separately training experts, a text-to-computation module, and a router. At inference, the router directs computation and reasoning at the token level, thereby enabling iterative alternation within a single chain of thought. We evaluate PiMoE on two reasoning-computation tasks against LLM finetuning and the multi-agent system approaches. Results show that the PiMoE architecture achieves not only higher accuracy than directly finetuning LLMs but also significant improvements in response latency, token usage, and GPU energy consumption compared with mainstream multi-agent approaches. PiMoE offers an efficient, interpretable, and scalable paradigm for next-generation scientific or industrial intelligent systems.",
        "tags": [
            "CoT",
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "27",
        "title": "SBVR: Summation of BitVector Representation for Efficient LLM Quantization",
        "author": [
            "Wonjun Bang",
            "Jongseok Park",
            "Hongseung Yu",
            "Kyungmin Bin",
            "Kyunghan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18172",
        "abstract": "With the advent of large language models (LLMs), numerous Post-Training Quantization (PTQ) strategies have been proposed to alleviate deployment barriers created by their enormous parameter counts. Quantization achieves compression by limiting the number of representable points in the data. Therefore, the key to achieving efficient quantization is selecting the optimal combination of representation points, or codes, for the given data. Existing PTQ solutions adopt two major approaches to this problem: Round-To-Nearest (RTN)-based methods and codebook-based methods. RTN-based methods map LLM weights onto uniformly distributed integer grids, failing to account for the Gaussian-like weight distribution of LLM weights. Codebook-based methods mitigate this issue by constructing distribution-aware codebooks; however, they suffer from random and strided memory access patterns, resulting in degraded inference speed that is exacerbated by the limited size of GPU L1 cache. To overcome these limitations, we propose a novel LLM quantization method, SBVR (Summation of BitVector Representation), that enables Gaussian-like code representation in a hardware-friendly manner for fast inference. SBVR maps weight values to non-uniform representation points whose distribution follows the actual distribution of LLM weights, enabling more accurate compression. Additionally, we design a custom CUDA kernel that allows matrix-vector multiplication directly in the SBVR format without decompression, thereby enabling high-performance execution of SBVR-compressed models. Our evaluations of SBVR on various models demonstrate state-of-the-art perplexity and accuracy benchmark performance while delivering a 2.21x- 3.04x end-to-end token-generation speedup over naive FP16 models in the 4-bit quantization regime.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "28",
        "title": "TurnBack: A Geospatial Route Cognition Benchmark for Large Language Models through Reverse Route",
        "author": [
            "Hongyi Luo",
            "Qing Cheng",
            "Daniel Matos",
            "Hari Krishna Gadi",
            "Yanfeng Zhang",
            "Lu Liu",
            "Yongliang Wang",
            "Niclas Zeller",
            "Daniel Cremers",
            "Liqiu Meng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18173",
        "abstract": "Humans can interpret geospatial information through natural language, while the geospatial cognition capabilities of Large Language Models (LLMs) remain underexplored. Prior research in this domain has been constrained by non-quantifiable metrics, limited evaluation datasets and unclear research hierarchies. Therefore, we propose a large-scale benchmark and conduct a comprehensive evaluation of the geospatial route cognition of LLMs. We create a large-scale evaluation dataset comprised of 36000 routes from 12 metropolises worldwide. Then, we introduce PathBuilder, a novel tool for converting natural language instructions into navigation routes, and vice versa, bridging the gap between geospatial information and natural language. Finally, we propose a new evaluation framework and metrics to rigorously assess 11 state-of-the-art (SOTA) LLMs on the task of route reversal. The benchmark reveals that LLMs exhibit limitation to reverse routes: most reverse routes neither return to the starting point nor are similar to the optimal route. Additionally, LLMs face challenges such as low robustness in route generation and high confidence for their incorrect answers. Code\\ \\&\\ Data available here: \\href{https://github.com/bghjmn32/EMNLP2025_Turnback}{TurnBack.}",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "29",
        "title": "Baseer: A Vision-Language Model for Arabic Document-to-Markdown OCR",
        "author": [
            "Khalil Hennara",
            "Muhammad Hreden",
            "Mohamed Motasim Hamed",
            "Ahmad Bastati",
            "Zeina Aldallal",
            "Sara Chrouf",
            "Safwan AlModhayan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18174",
        "abstract": "Arabic document OCR remains a challenging task due to the language's cursive script, diverse fonts, diacritics, and right-to-left orientation. While modern Multimodal Large Language Models (MLLMs) have advanced document understanding for high-resource languages, their performance on Arabic remains limited. In this work, we introduce Baseer, a vision-language model fine- tuned specifically for Arabic document OCR. Leveraging a large-scale dataset combining synthetic and real-world documents, Baseer is trained using a decoder-only fine-tuning strategy to adapt a pre-trained MLLM while preserving general visual features. We also present Misraj-DocOCR, a high-quality, expert-verified benchmark designed for rigorous evaluation of Arabic OCR systems. Our experiments show that Baseer significantly outperforms existing open-source and commercial solutions, achieving a WER of 0.25 and establishing a new state-of-the-art in the domain of Arabic document OCR. Our results highlight the benefits of domain-specific adaptation of general-purpose MLLMs and establish a strong baseline for high-accuracy OCR on morphologically rich languages like Arabic.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "30",
        "title": "The Describe-Then-Generate Bottleneck: How VLM Descriptions Alter Image Generation Outcomes",
        "author": [
            "Sai Varun Kodathala",
            "Rakesh Vunnam"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18179",
        "abstract": "With the increasing integration of multimodal AI systems in creative workflows, understanding information loss in vision-language-vision pipelines has become important for evaluating system limitations. However, the degradation that occurs when visual content passes through textual intermediation remains poorly quantified. In this work, we provide empirical analysis of the describe-then-generate bottleneck, where natural language serves as an intermediate representation for visual information. We generated 150 image pairs through the describe-then-generate pipeline and applied existing metrics (LPIPS, SSIM, and color distance) to measure information preservation across perceptual, structural, and chromatic dimensions. Our evaluation reveals that 99.3% of samples exhibit substantial perceptual degradation and 91.5% demonstrate significant structural information loss, providing empirical evidence that the describe-then-generate bottleneck represents a measurable and consistent limitation in contemporary multimodal systems.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "31",
        "title": "Large Language Models and Operations Research: A Structured Survey",
        "author": [
            "Yang Wang",
            "Kai Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18180",
        "abstract": "Operations research (OR) provides fundamental methodologies for complex system decision-making, with established applications in transportation, supply chain management, and production scheduling. Traditional approaches, which depend on expert-based modeling and manual parameter adjustment, often face challenges in handling large-scale, dynamic, and multi-constraint problems. Recently, large language models (LLMs) have shown potential to address these limitations through semantic understanding, structured generation, and reasoning control. LLMs can translate natural language descriptions into mathematical models or executable code, generate heuristics, evolve algorithms, and directly tackle optimization tasks. This paper surveys recent progress on the integration of LLMs into OR, organizing methods into three main directions: automatic modeling, auxiliary optimization, and direct solving. It further reviews evaluation benchmarks and domain-specific applications, and summarizes key open issues such as unstable semantic-to-structure mapping, fragmented research progress, limited generalization, and insufficient evaluation systems. Finally, the survey outlines possible research avenues for advancing the role of LLMs in OR.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "32",
        "title": "Synthesizing Attitudes, Predicting Actions (SAPA): Behavioral Theory-Guided LLMs for Ridesourcing Mode Choice Modeling",
        "author": [
            "Mustafa Sameen",
            "Xiaojian Zhang",
            "Xilei Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18181",
        "abstract": "Accurate modeling of ridesourcing mode choices is essential for designing and implementing effective traffic management policies for reducing congestion, improving mobility, and allocating resources more efficiently. Existing models for predicting ridesourcing mode choices often suffer from limited predictive accuracy due to their inability to capture key psychological factors, and are further challenged by severe class imbalance, as ridesourcing trips comprise only a small fraction of individuals' daily travel. To address these limitations, this paper introduces the Synthesizing Attitudes, Predicting Actions (SAPA) framework, a hierarchical approach that uses Large Language Models (LLMs) to synthesize theory-grounded latent attitudes to predict ridesourcing choices. SAPA first uses an LLM to generate qualitative traveler personas from raw travel survey data and then trains a propensity-score model on demographic and behavioral features, enriched by those personas, to produce an individual-level score. Next, the LLM assigns quantitative scores to theory-driven latent variables (e.g., time and cost sensitivity), and a final classifier integrates the propensity score, latent-variable scores (with their interaction terms), and observable trip attributes to predict ridesourcing mode choice. Experiments on a large-scale, multi-year travel survey show that SAPA significantly outperforms state-of-the-art baselines, improving ridesourcing choice predictions by up to 75.9% in terms of PR-AUC on a held-out test set. This study provides a powerful tool for accurately predicting ridesourcing mode choices, and provides a methodology that is readily transferable to various applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "33",
        "title": "Qianfan-VL: Domain-Enhanced Universal Vision-Language Models",
        "author": [
            "Daxiang Dong",
            "Mingming Zheng",
            "Dong Xu",
            "Bairong Zhuang",
            "Wenyu Zhang",
            "Chunhua Luo",
            "Haoran Wang",
            "Zijian Zhao",
            "Jie Li",
            "Yuxuan Li",
            "Hanjun Zhong",
            "Mengyue Liu",
            "Jieting Chen",
            "Shupeng Li",
            "Lun Tian",
            "Yaping Feng",
            "Xin Li",
            "Donggang Jiang",
            "Yong Chen",
            "Yehua Xu",
            "Duohao Qin",
            "Chen Feng",
            "Dan Wang",
            "Henghua Zhang",
            "Jingjing Ha",
            "Jinhui He",
            "Yanfeng Zhai",
            "Chengxin Zheng",
            "Jiayi Mao",
            "Jiacheng Chen",
            "Ruchang Yao",
            "Ziye Yuan",
            "Jianmin Wu",
            "Guangjun Xie",
            "Dou Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18189",
        "abstract": "We present Qianfan-VL, a series of multimodal large language models ranging from 3B to 70B parameters, achieving state-of-the-art performance through innovative domain enhancement techniques. Our approach employs multi-stage progressive training and high-precision data synthesis pipelines, which prove to be critical technologies for enhancing domain-specific capabilities while maintaining strong general performance. Qianfan-VL achieves comparable results to leading open-source models on general benchmarks, with state-of-the-art performance on benchmarks such as CCBench, SEEDBench IMG, ScienceQA, and MMStar. The domain enhancement strategy delivers significant advantages in OCR and document understanding, validated on both public benchmarks (OCRBench 873, DocVQA 94.75%) and in-house evaluations. Notably, Qianfan-VL-8B and 70B variants incorporate long chain-of-thought capabilities, demonstrating superior performance on mathematical reasoning (MathVista 78.6%) and logical inference tasks. All models are trained entirely on Baidu's Kunlun P800 chips, validating the capability of large-scale AI infrastructure to train SOTA-level multimodal models with over 90% scaling efficiency on 5000 chips for a single task. This work establishes an effective methodology for developing domain-enhanced multimodal models suitable for diverse enterprise deployment scenarios.",
        "tags": [
            "CoT",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "34",
        "title": "HazeFlow: Revisit Haze Physical Model as ODE and Non-Homogeneous Haze Generation for Real-World Dehazing",
        "author": [
            "Junseong Shin",
            "Seungwoo Chung",
            "Yunjeong Yang",
            "Tae Hyun Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18190",
        "abstract": "Dehazing involves removing haze or fog from images to restore clarity and improve visibility by estimating atmospheric scattering effects. While deep learning methods show promise, the lack of paired real-world training data and the resulting domain gap hinder generalization to real-world scenarios. In this context, physics-grounded learning becomes crucial; however, traditional methods based on the Atmospheric Scattering Model (ASM) often fall short in handling real-world complexities and diverse haze patterns. To solve this problem, we propose HazeFlow, a novel ODE-based framework that reformulates ASM as an ordinary differential equation (ODE). Inspired by Rectified Flow (RF), HazeFlow learns an optimal ODE trajectory to map hazy images to clean ones, enhancing real-world dehazing performance with only a single inference step. Additionally, we introduce a non-homogeneous haze generation method using Markov Chain Brownian Motion (MCBM) to address the scarcity of paired real-world data. By simulating realistic haze patterns through MCBM, we enhance the adaptability of HazeFlow to diverse real-world scenarios. Through extensive experiments, we demonstrate that HazeFlow achieves state-of-the-art performance across various real-world dehazing benchmark datasets.",
        "tags": [
            "ODE",
            "Rectified Flow"
        ]
    },
    {
        "id": "35",
        "title": "Conversational Orientation Reasoning: Egocentric-to-Allocentric Navigation with Multimodal Chain-of-Thought",
        "author": [
            "Yu Ti Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18200",
        "abstract": "Conversational agents must translate egocentric utterances (e.g., \"on my right\") into allocentric orientations (N/E/S/W). This challenge is particularly critical in indoor or complex facilities where GPS signals are weak and detailed maps are unavailable. While chain-of-thought (CoT) prompting has advanced reasoning in language and vision tasks, its application to multimodal spatial orientation remains underexplored. We introduce Conversational Orientation Reasoning (COR), a new benchmark designed for Traditional Chinese conversational navigation projected from real-world environments, addressing egocentric-to-allocentric reasoning in non-English and ASR-transcribed scenarios. We propose a multimodal chain-of-thought (MCoT) framework, which integrates ASR-transcribed speech with landmark coordinates through a structured three-step reasoning process: (1) extracting spatial relations, (2) mapping coordinates to absolute directions, and (3) inferring user orientation. A curriculum learning strategy progressively builds these capabilities on Taiwan-LLM-13B-v2.0-Chat, a mid-sized model representative of resource-constrained settings. Experiments show that MCoT achieves 100% orientation accuracy on clean transcripts and 98.1% with ASR transcripts, substantially outperforming unimodal and non-structured baselines. Moreover, MCoT demonstrates robustness under noisy conversational conditions, including ASR recognition errors and multilingual code-switching. The model also maintains high accuracy in cross-domain evaluation and resilience to linguistic variation, domain shift, and referential ambiguity. These findings highlight the potential of structured MCoT spatial reasoning as a path toward interpretable and resource-efficient embodied navigation.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "36",
        "title": "Similarity Field Theory: A Mathematical Framework for Intelligence",
        "author": [
            "Kei-Sing Ng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18218",
        "abstract": "We posit that persisting and transforming similarity relations form the structural basis of any comprehensible dynamic system. This paper introduces Similarity Field Theory, a mathematical framework that formalizes the principles governing similarity values among entities and their evolution. We define: (1) a similarity field $S: U \\times U \\to [0,1]$ over a universe of entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed relational field (asymmetry and non-transitivity are allowed); (2) the evolution of a system through a sequence $Z_p = (X_p, S^{(p)})$ indexed by $p=0,1,2,\\ldots$; (3) concepts $K$ as entities that induce fibers $F_{\\alpha}(K) = { E \\in U \\mid S(E,K) \\ge \\alpha }$, i.e., superlevel sets of the unary map $S_K(E) := S(E,K)$; and (4) a generative operator $G$ that produces new entities. Within this framework, we formalize a generative definition of intelligence: an operator $G$ is intelligent with respect to a concept $K$ if, given a system containing entities belonging to the fiber of $K$, it generates new entities that also belong to that fiber. Similarity Field Theory thus offers a foundational language for characterizing, comparing, and constructing intelligent systems. We prove two theorems: (i) asymmetry blocks mutual inclusion; and (ii) stability requires either an anchor coordinate or eventual confinement within a level set of $f$. These results ensure that the evolution of similarity fields is both constrained and interpretable, culminating in an exploration of how the framework allows us to interpret large language models and use them as experimental probes into societal cognition.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "37",
        "title": "An N-Plus-1 GPT Agency for Critical Solution of Mechanical Engineering Analysis Problems",
        "author": [
            "Anthony Patera",
            "Rohan Abeyaratne"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18229",
        "abstract": "Generative AI, and specifically GPT, can produce a remarkable solution to a mechanical engineering analysis problem - but also, on occasion, a flawed solution. For example, an elementary mechanics problem is solved flawlessly in one GPT instance and incorrectly in a subsequent GPT instance, with a success probability of only 85%. This unreliability renders \"out-of-the-box\" GPT unsuitable for deployment in education or engineering practice. We introduce an \"N-Plus-1\" GPT Agency for Initial (Low-Cost) Analysis of mechanical engineering Problem Statements. Agency first launches N instantiations of Agent Solve to yield N independent Proposed Problem Solution Realizations; Agency then invokes Agent Compare to summarize and compare the N Proposed Problem Solution Realizations and to provide a Recommended Problem Solution. We argue from Condorcet's Jury Theorem that, for a Problem Statement characterized by per-Solve success probability greater than 1/2 (and N sufficiently large), the Predominant (Agent Compare) Proposed Problem Solution will, with high probability, correspond to a Correct Proposed Problem Solution. Furthermore, Agent Compare can also incorporate aspects of Secondary (Agent Compare) Proposed Problem Solutions, in particular when the latter represent alternative Problem Statement interpretations - different Mathematical Models - or alternative Mathematical Solution Procedures. Comparisons to Grok Heavy, a commercial multi-agent model, show similarities in design and performance, but also important differences in emphasis: our Agency focuses on transparency and pedagogical value.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "38",
        "title": "Towards General Computer Control with Hierarchical Agents and Multi-Level Action Spaces",
        "author": [
            "Zihan Dong",
            "Xinyu Fan",
            "Zixiang Tang",
            "Yunqing Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18230",
        "abstract": "Controlling desktop applications via software remains a fundamental yet under-served problem. Existing multi-modal large language models (MLLMs) ingest screenshots and task instructions to generate keystrokes and mouse events, but they suffer from prohibitive inference latency, poor sample efficiency on long-horizon sparse-reward tasks, and infeasible on-device deployment. We introduce a lightweight hierarchical reinforcement learning framework, ComputerAgent, that formulates OS control as a two-level option process (manager and subpolicy), employs a triple-modal state encoder (screenshot, task ID, numeric state) to handle visual and contextual diversity, integrates meta-actions with an early-stop mechanism to reduce wasted interactions, and uses a compact vision backbone plus small policy networks for on-device inference (15M parameters). On a suite of 135 real-world desktop tasks, ComputerAgent attains 92.1% success on simple tasks (<8 steps) and 58.8% on hard tasks (>=8 steps), matching or exceeding 200B-parameter MLLM baselines on simple scenarios while reducing model size by over four orders of magnitude and halving inference time. These results demonstrate that hierarchical RL offers a practical, scalable alternative to monolithic MLLM-based automation for computer control.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "39",
        "title": "PEEK: Guiding and Minimal Image Representations for Zero-Shot Generalization of Robot Manipulation Policies",
        "author": [
            "Jesse Zhang",
            "Marius Memmel",
            "Kevin Kim",
            "Dieter Fox",
            "Jesse Thomason",
            "Fabio Ramos",
            "Erdem BÄ±yÄ±k",
            "Abhishek Gupta",
            "Anqi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18282",
        "abstract": "Robotic manipulation policies often fail to generalize because they must simultaneously learn where to attend, what actions to take, and how to execute them. We argue that high-level reasoning about where and what can be offloaded to vision-language models (VLMs), leaving policies to specialize in how to act. We present PEEK (Policy-agnostic Extraction of Essential Keypoints), which fine-tunes VLMs to predict a unified point-based intermediate representation: 1. end-effector paths specifying what actions to take, and 2. task-relevant masks indicating where to focus. These annotations are directly overlaid onto robot observations, making the representation policy-agnostic and transferable across architectures. To enable scalable training, we introduce an automatic annotation pipeline, generating labeled data across 20+ robot datasets spanning 9 embodiments. In real-world evaluations, PEEK consistently boosts zero-shot generalization, including a 41.4x real-world improvement for a 3D policy trained only in simulation, and 2-3.5x gains for both large VLAs and small manipulation policies. By letting VLMs absorb semantic and visual complexity, PEEK equips manipulation policies with the minimal cues they need--where, what, and how. Website at https://peek-robot.github.io/.",
        "tags": [
            "3D",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "40",
        "title": "Evaluating Large Language Models for Detecting Antisemitism",
        "author": [
            "Jay Patel",
            "Hrudayangam Mehta",
            "Jeremy Blackburn"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18293",
        "abstract": "Detecting hateful content is a challenging and important problem. Automated tools, like machine-learning models, can help, but they require continuous training to adapt to the ever-changing landscape of social media. In this work, we evaluate eight open-source LLMs' capability to detect antisemitic content, specifically leveraging in-context definition as a policy guideline. We explore various prompting techniques and design a new CoT-like prompt, Guided-CoT. Guided-CoT handles the in-context policy well, increasing performance across all evaluated models, regardless of decoding configuration, model sizes, or reasoning capability. Notably, Llama 3.1 70B outperforms fine-tuned GPT-3.5. Additionally, we examine LLM errors and introduce metrics to quantify semantic divergence in model-generated rationales, revealing notable differences and paradoxical behaviors among LLMs. Our experiments highlight the differences observed across LLMs' utility, explainability, and reliability.",
        "tags": [
            "CoT",
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "41",
        "title": "Rethinking Pulmonary Embolism Segmentation: A Study of Current Approaches and Challenges with an Open Weight Model",
        "author": [
            "Yixin Zhang",
            "Ryan Chamberlain",
            "Lawrance Ngo",
            "Kevin Kramer",
            "Maciej A. Mazurowski"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18308",
        "abstract": "In this study, we curated a densely annotated in-house dataset comprising 490 CTPA scans. Using this dataset, we systematically evaluated nine widely used segmentation architectures from both the CNN and Vision Transformer (ViT) families, initialized with either pretrained or random weights, under a unified testing framework as a performance audit. Our study leads to several important observations: (1) 3D U-Net with a ResNet encoder remains a highly effective architecture for PE segmentation; (2) 3D models are particularly well-suited to this task given the morphological characteristics of emboli; (3) CNN-based models generally yield superior performance compared to their ViT-based counterparts in PE segmentation; (4) classification-based pretraining, even on large PE datasets, can adversely impact segmentation performance compared to training from scratch, suggesting that PE classification and segmentation may rely on different sets of discriminative features; (5) different model architectures show a highly consistent pattern of segmentation performance when trained on the same data; and (6) while central and large emboli can be segmented with satisfactory accuracy, distal emboli remain challenging due to both task complexity and the scarcity of high-quality datasets. Besides these findings, our best-performing model achieves a mean Dice score of 0.7131 for segmentation. It detects 181 emboli with 49 false positives and 28 false negatives from 60 in-house testing scans. Its generalizability is further validated on public datasets.",
        "tags": [
            "3D",
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "42",
        "title": "Fine-Tuning Robot Policies While Maintaining User Privacy",
        "author": [
            "Benjamin A. Christie",
            "Sagar Parekh",
            "Dylan P. Losey"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18311",
        "abstract": "Recent works introduce general-purpose robot policies. These policies provide a strong prior over how robots should behave -- e.g., how a robot arm should manipulate food items. But in order for robots to match an individual person's needs, users typically fine-tune these generalized policies -- e.g., showing the robot arm how to make their own preferred dinners. Importantly, during the process of personalizing robots, end-users leak data about their preferences, habits, and styles (e.g., the foods they prefer to eat). Other agents can simply roll-out the fine-tuned policy and see these personally-trained behaviors. This leads to a fundamental challenge: how can we develop robots that personalize actions while keeping learning private from external agents? We here explore this emerging topic in human-robot interaction and develop PRoP, a model-agnostic framework for personalized and private robot policies. Our core idea is to equip each user with a unique key; this key is then used to mathematically transform the weights of the robot's network. With the correct key, the robot's policy switches to match that user's preferences -- but with incorrect keys, the robot reverts to its baseline behaviors. We show the general applicability of our method across multiple model types in imitation learning, reinforcement learning, and classification tasks. PRoP is practically advantageous because it retains the architecture and behaviors of the original policy, and experimentally outperforms existing encoder-based approaches. See videos and code here: https://prop-icra26.github.io.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "43",
        "title": "Haptic Communication in Human-Human and Human-Robot Co-Manipulation",
        "author": [
            "Katherine H. Allen",
            "Chris Rogers",
            "Elaine S. Short"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18327",
        "abstract": "When a human dyad jointly manipulates an object, they must communicate about their intended motion plans. Some of that collaboration is achieved through the motion of the manipulated object itself, which we call \"haptic communication.\" In this work, we captured the motion of human-human dyads moving an object together with one participant leading a motion plan about which the follower is uninformed. We then captured the same human participants manipulating the same object with a robot collaborator. By tracking the motion of the shared object using a low-cost IMU, we can directly compare human-human shared manipulation to the motion of those same participants interacting with the robot. Intra-study and post-study questionnaires provided participant feedback on the collaborations, indicating that the human-human collaborations are significantly more fluent, and analysis of the IMU data indicates that it captures objective differences in the motion profiles of the conditions. The differences in objective and subjective measures of accuracy and fluency between the human-human and human-robot trials motivate future research into improving robot assistants for physical tasks by enabling them to send and receive anthropomorphic haptic signals.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "44",
        "title": "CoRaCMG: Contextual Retrieval-Augmented Framework for Commit Message Generation",
        "author": [
            "Bo Xiong",
            "Linghao Zhang",
            "Chong Wang",
            "Peng Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18337",
        "abstract": "Commit messages play a key role in documenting the intent behind code changes. However, they are often low-quality, vague, or incomplete, limiting their usefulness. Commit Message Generation (CMG) aims to automatically generate descriptive commit messages from code diffs to reduce developers' effort and improve message quality. Although recent advances in LLMs have shown promise in automating CMG, their performance remains limited. This paper aims to enhance CMG performance by retrieving similar diff-message pairs to guide LLMs to generate commit messages that are more precise and informative. We proposed CoRaCMG, a Contextual Retrieval-augmented framework for Commit Message Generation, structured in three phases: (1) Retrieve: retrieving the similar diff-message pairs; (2) Augment: combining them with the query diff into a structured prompt; and (3) Generate: generating commit messages corresponding to the query diff via LLMs. CoRaCMG enables LLMs to learn project-specific terminologies and writing styles from the retrieved diff-message pairs, thereby producing high-quality commit messages. We evaluated our method on various LLMs, including closed-source GPT models and open-source DeepSeek models. Experimental results show that CoRaCMG significantly boosts LLM performance across four metrics (BLEU, Rouge-L, METEOR, and CIDEr). Specifically, DeepSeek-R1 achieves relative improvements of 76% in BLEU and 71% in CIDEr when augmented with a single retrieved example pair. After incorporating the single example pair, GPT-4o achieves the highest improvement rate, with BLEU increasing by 89%. Moreover, performance gains plateau after more than three examples are used, indicating diminishing returns. Further analysis shows that the improvements are attributed to the model's ability to capture the terminologies and writing styles of human-written commit messages from the retrieved example pairs.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "45",
        "title": "Semantic-Aware Particle Filter for Reliable Vineyard Robot Localisation",
        "author": [
            "Rajitha de Silva",
            "Jonathan Cox",
            "James R. Heselden",
            "Marija Popovic",
            "Cesar Cadena",
            "Riccardo Polvara"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18342",
        "abstract": "Accurate localisation is critical for mobile robots in structured outdoor environments, yet LiDAR-based methods often fail in vineyards due to repetitive row geometry and perceptual aliasing. We propose a semantic particle filter that incorporates stable object-level detections, specifically vine trunks and support poles into the likelihood estimation process. Detected landmarks are projected into a birds eye view and fused with LiDAR scans to generate semantic observations. A key innovation is the use of semantic walls, which connect adjacent landmarks into pseudo-structural constraints that mitigate row aliasing. To maintain global consistency in headland regions where semantics are sparse, we introduce a noisy GPS prior that adaptively supports the filter. Experiments in a real vineyard demonstrate that our approach maintains localisation within the correct row, recovers from deviations where AMCL fails, and outperforms vision-based SLAM methods such as RTAB-Map.",
        "tags": [
            "Robotics",
            "SLAM"
        ]
    },
    {
        "id": "46",
        "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for Offloaded LLMs via Substitute Speculative Decoding",
        "author": [
            "Pei-Shuo Wang",
            "Jian-Jia Chen",
            "Chun-Che Yang",
            "Chi-Chih Chang",
            "Ning-Chi Huang",
            "Mohamed S. Abdelfattah",
            "Kai-Chiang Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18344",
        "abstract": "The immense model sizes of large language models (LLMs) challenge deployment on memory-limited consumer GPUs. Although model compression and parameter offloading are common strategies to address memory limitations, compression can degrade quality, and offloading maintains quality but suffers from slow inference. Speculative decoding presents a promising avenue to accelerate parameter offloading, utilizing a fast draft model to propose multiple draft tokens, which are then verified by the target LLM in parallel with a single forward pass. This method reduces the time-consuming data transfers in forward passes that involve offloaded weight transfers. Existing methods often rely on pretrained weights of the same family, but require additional training to align with custom-trained models. Moreover, approaches that involve draft model training usually yield only modest speedups. This limitation arises from insufficient alignment with the target model, preventing higher token acceptance lengths. To address these challenges and achieve greater speedups, we propose SubSpec, a plug-and-play method to accelerate parameter offloading that is lossless and training-free. SubSpec constructs a highly aligned draft model by generating low-bit quantized substitute layers from offloaded target LLM portions. Additionally, our method shares the remaining GPU-resident layers and the KV-Cache, further reducing memory overhead and enhance alignment. SubSpec achieves a high average acceptance length, delivering 9.1x speedup for Qwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for Qwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "47",
        "title": "A mixed formulation for the fractional Poisson problem",
        "author": [
            "Juan Pablo Borthagaray",
            "Nahuel de LeÃ³n"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18348",
        "abstract": "The mixed formulation of the classical Poisson problem introduces the flux as an additional variable, leading to a system of coupled equations. Using fractional calculus identities, in this work we explore a mixed formulation of the fractional Poisson problem and establish its well-posedness. Since a direct discretization of this problem appears to be out of reach, we adapt a stabilized approach by Hughes and Masud, which yields a coercive and well-posed formulation. The coercivity ensures the stability of any conforming finite element discretization. We further prove the convergence of this discretization, derive convergence rates, and discuss implementation aspects. Finally, we present numerical experiments that highlight both the importance of stabilization and the accuracy of our theoretical results.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "48",
        "title": "FastMTP: Accelerating LLM Inference with Enhanced Multi-Token Prediction",
        "author": [
            "Yuxuan Cai",
            "Xiaozhuan Liang",
            "Xinghua Wang",
            "Jin Ma",
            "Haijin Liang",
            "Jinwen Luo",
            "Xinyu Zuo",
            "Lisheng Duan",
            "Yuyang Yin",
            "Xi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18362",
        "abstract": "As large language models (LLMs) become increasingly powerful, the sequential nature of autoregressive generation creates a fundamental throughput bottleneck that limits the practical deployment. While Multi-Token Prediction (MTP) has demonstrated remarkable benefits for model training efficiency and performance, its inherent potential for inference acceleration remains largely unexplored. This paper introduces FastMTP, a simple yet effective method that improves multi-step draft quality by aligning MTP training with its inference pattern, significantly enhancing speculative decoding performance. Our approach fine-tunes a single MTP head with position-shared weights on self-distilled data, enabling it to capture dependencies among consecutive future tokens and maintain high acceptance rates across multiple recursive draft steps. By integrating language-aware dynamic vocabulary compression into the MTP head, we further reduce computational overhead in the drafting process. Experimental results across seven diverse benchmarks demonstrate that FastMTP achieves an average of 2.03x speedup compared to standard next token prediction with lossless output quality, outperforming vanilla MTP by 82%. FastMTP requires only lightweight training and seamlessly integrates with existing inference frameworks, offering a practical and rapidly deployable solution for accelerating LLM inference.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "49",
        "title": "Align Where the Words Look: Cross-Attention-Guided Patch Alignment with Contrastive and Transport Regularization for Bengali Captioning",
        "author": [
            "Riad Ahmed Anonto",
            "Sardar Md. Saffat Zabin",
            "M. Saifur Rahman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18369",
        "abstract": "Grounding vision--language models in low-resource languages remains challenging, as they often produce fluent text about the wrong objects. This stems from scarce paired data, translation pivots that break alignment, and English-centric pretraining that ignores target-language semantics. We address this with a compute-aware Bengali captioning pipeline trained on LaBSE-verified EN--BN pairs and 110k bilingual-prompted synthetic images. A frozen MaxViT yields stable visual patches, a Bengali-native mBART-50 decodes, and a lightweight bridge links the modalities. Our core novelty is a tri-loss objective: Patch-Alignment Loss (PAL) aligns real and synthetic patch descriptors using decoder cross-attention, InfoNCE enforces global real--synthetic separation, and Sinkhorn-based OT ensures balanced fine-grained patch correspondence. This PAL+InfoNCE+OT synergy improves grounding, reduces spurious matches, and drives strong gains on Flickr30k-1k (BLEU-4 12.29, METEOR 27.98, BERTScore-F1 71.20) and MSCOCO-1k (BLEU-4 12.00, METEOR 28.14, BERTScore-F1 75.40), outperforming strong CE baselines and narrowing the real--synthetic centroid gap by 41%.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "50",
        "title": "Policy Gradient with Self-Attention for Model-Free Distributed Nonlinear Multi-Agent Games",
        "author": [
            "Eduardo SebastiÃ¡n",
            "Maitrayee Keskar",
            "Eeman Iqbal",
            "Eduardo Montijano",
            "Carlos SagÃ¼Ã©s",
            "Nikolay Atanasov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18371",
        "abstract": "Multi-agent games in dynamic nonlinear settings are challenging due to the time-varying interactions among the agents and the non-stationarity of the (potential) Nash equilibria. In this paper we consider model-free games, where agent transitions and costs are observed without knowledge of the transition and cost functions that generate them. We propose a policy gradient approach to learn distributed policies that follow the communication structure in multi-team games, with multiple agents per team. Our formulation is inspired by the structure of distributed policies in linear quadratic games, which take the form of time-varying linear feedback gains. In the nonlinear case, we model the policies as nonlinear feedback gains, parameterized by self-attention layers to account for the time-varying multi-agent communication topology. We demonstrate that our distributed policy gradient approach achieves strong performance in several settings, including distributed linear and nonlinear regulation, and simulated and real multi-robot pursuit-and-evasion games.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "51",
        "title": "GnnXemplar: Exemplars to Explanations - Natural Language Rules for Global GNN Interpretability",
        "author": [
            "Burouj Armgaan",
            "Eshan Jain",
            "Harsh Pandey",
            "Mahesh Chandran",
            "Sayan Ranu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18376",
        "abstract": "Graph Neural Networks (GNNs) are widely used for node classification, yet their opaque decision-making limits trust and adoption. While local explanations offer insights into individual predictions, global explanation methods, those that characterize an entire class, remain underdeveloped. Existing global explainers rely on motif discovery in small graphs, an approach that breaks down in large, real-world settings where subgraph repetition is rare, node attributes are high-dimensional, and predictions arise from complex structure-attribute interactions. We propose GnnXemplar, a novel global explainer inspired from Exemplar Theory from cognitive science. GnnXemplar identifies representative nodes in the GNN embedding space, exemplars, and explains predictions using natural language rules derived from their neighborhoods. Exemplar selection is framed as a coverage maximization problem over reverse k-nearest neighbors, for which we provide an efficient greedy approximation. To derive interpretable rules, we employ a self-refining prompt strategy using large language models (LLMs). Experiments across diverse benchmarks show that GnnXemplar significantly outperforms existing methods in fidelity, scalability, and human interpretability, as validated by a user study with 60 participants.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "52",
        "title": "Interactive Real-Time Speaker Diarization Correction with Human Feedback",
        "author": [
            "Xinlu He",
            "Yiwen Guan",
            "Badrivishal Paurana",
            "Zilin Dai",
            "Jacob Whitehill"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18377",
        "abstract": "Most automatic speech processing systems operate in \"open loop\" mode without user feedback about who said what; yet, human-in-the-loop workflows can potentially enable higher accuracy. We propose an LLM-assisted speaker diarization correction system that lets users fix speaker attribution errors in real time. The pipeline performs streaming ASR and diarization, uses an LLM to deliver concise summaries to the users, and accepts brief verbal feedback that is immediately incorporated without disrupting interactions. Moreover, we develop techniques to make the workflow more effective: First, a split-when-merged (SWM) technique detects and splits multi-speaker segments that the ASR erroneously attributes to just a single speaker. Second, online speaker enrollments are collected based on users' diarization corrections, thus helping to prevent speaker diarization errors from occurring in the future. LLM-driven simulations on the AMI test set indicate that our system substantially reduces DER by 9.92% and speaker confusion error by 44.23%. We further analyze correction efficacy under different settings, including summary vs full transcript display, the number of online enrollments limitation, and correction frequency.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "53",
        "title": "Evaluating the Safety and Skill Reasoning of Large Reasoning Models Under Compute Constraints",
        "author": [
            "Adarsha Balaji",
            "Le Chen",
            "Rajeev Thakur",
            "Franck Cappello",
            "Sandeep Madireddy"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18382",
        "abstract": "Test-time compute scaling has demonstrated the ability to improve the performance of reasoning language models by generating longer chain-of-thought (CoT) sequences. However, this increase in performance comes with a significant increase in computational cost. In this work, we investigate two compute constraint strategies: (1) reasoning length constraint and (2) model quantization, as methods to reduce the compute demand of reasoning models and study their impact on their safety performance. Specifically, we explore two approaches to apply compute constraints to reasoning models: (1) fine-tuning reasoning models using a length controlled policy optimization (LCPO) based reinforcement learning method to satisfy a user-defined CoT reasoning length, and (2) applying quantization to maximize the generation of CoT sequences within a user-defined compute constraint. Furthermore, we study the trade-off between the computational efficiency and the safety of the model.",
        "tags": [
            "CoT",
            "RL"
        ]
    },
    {
        "id": "54",
        "title": "GÃ¶del Test: Can Large Language Models Solve Easy Conjectures?",
        "author": [
            "Moran Feldman",
            "Amin Karbasi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18383",
        "abstract": "Recent announcements from frontier AI model labs have highlighted strong results on high-school and undergraduate math competitions. Yet it remains unclear whether large language models can solve new, simple conjectures in more advanced areas of mathematics. We propose the GÃ¶del Test: evaluating whether a model can produce correct proofs for very simple, previously unsolved conjectures. To this end, we study the performance of GPT-5 on five conjectures in combinatorial optimization. For each problem, we provided one or two source papers from which the conjecture arose, withheld our own conjecture, and then assessed the model's reasoning in detail. On the three easier problems, GPT-5 produced nearly correct solutions; for Problem 2 it even derived a different approximation guarantee that, upon checking, refuted our conjecture while providing a valid solution. The model failed on Problem 4, which required combining results from two papers. On Problem 5, a harder case without a validated conjecture, GPT-5 proposed the same algorithm we had in mind but failed in the analysis, suggesting the proof is more challenging than expected. Although our sample is small, the results point to meaningful progress on routine reasoning, occasional flashes of originality, and clear limitations when cross-paper synthesis is required. GPT-5 may represent an early step toward frontier models eventually passing the GÃ¶del Test.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "55",
        "title": "AD-VF: LLM-Automatic Differentiation Enables Fine-Tuning-Free Robot Planning from Formal Methods Feedback",
        "author": [
            "Yunhao Yang",
            "Junyuan Hong",
            "Gabriel Jacob Perin",
            "Zhiwen Fan",
            "Li Yin",
            "Zhangyang Wang",
            "Ufuk Topcu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18384",
        "abstract": "Large language models (LLMs) can translate natural language instructions into executable action plans for robotics, autonomous driving, and other domains. Yet, deploying LLM-driven planning in the physical world demands strict adherence to safety and regulatory constraints, which current models often violate due to hallucination or weak alignment. Traditional data-driven alignment methods, such as Direct Preference Optimization (DPO), require costly human labeling, while recent formal-feedback approaches still depend on resource-intensive fine-tuning. In this paper, we propose LAD-VF, a fine-tuning-free framework that leverages formal verification feedback for automated prompt engineering. By introducing a formal-verification-informed text loss integrated with LLM-AutoDiff, LAD-VF iteratively refines prompts rather than model parameters. This yields three key benefits: (i) scalable adaptation without fine-tuning; (ii) compatibility with modular LLM architectures; and (iii) interpretable refinement via auditable prompts. Experiments in robot navigation and manipulation tasks demonstrate that LAD-VF substantially enhances specification compliance, improving success rates from 60% to over 90%. Our method thus presents a scalable and interpretable pathway toward trustworthy, formally-verified LLM-driven control systems.",
        "tags": [
            "DPO",
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "56",
        "title": "Towards Provable Emergence of In-Context Reinforcement Learning",
        "author": [
            "Jiuqi Wang",
            "Rohan Chandra",
            "Shangtong Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18389",
        "abstract": "Typically, a modern reinforcement learning (RL) agent solves a task by updating its neural network parameters to adapt its policy to the task. Recently, it has been observed that some RL agents can solve a wide range of new out-of-distribution tasks without parameter updates after pretraining on some task distribution. When evaluated in a new task, instead of making parameter updates, the pretrained agent conditions its policy on additional input called the context, e.g., the agent's interaction history in the new task. The agent's performance increases as the information in the context increases, with the agent's parameters fixed. This phenomenon is typically called in-context RL (ICRL). The pretrained parameters of the agent network enable the remarkable ICRL phenomenon. However, many ICRL works perform the pretraining with standard RL algorithms. This raises the central question this paper aims to address: Why can the RL pretraining algorithm generate network parameters that enable ICRL? We hypothesize that the parameters capable of ICRL are minimizers of the pretraining loss. This work provides initial support for this hypothesis through a case study. In particular, we prove that when a Transformer is pretrained for policy evaluation, one of the global minimizers of the pretraining loss can enable in-context temporal difference learning.",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "57",
        "title": "An Artificial Intelligence Value at Risk Approach: Metrics and Models",
        "author": [
            "Luis Enriquez Alvarez"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18394",
        "abstract": "Artificial intelligence risks are multidimensional in nature, as the same risk scenarios may have legal, operational, and financial risk dimensions. With the emergence of new AI regulations, the state of the art of artificial intelligence risk management seems to be highly immature due to upcoming AI regulations. Despite the appearance of several methodologies and generic criteria, it is rare to find guidelines with real implementation value, considering that the most important issue is customizing artificial intelligence risk metrics and risk models for specific AI risk scenarios. Furthermore, the financial departments, legal departments and Government Risk Compliance teams seem to remain unaware of many technical aspects of AI systems, in which data scientists and AI engineers emerge as the most appropriate implementers. It is crucial to decompose the problem of artificial intelligence risk in several dimensions: data protection, fairness, accuracy, robustness, and information security. Consequently, the main task is developing adequate metrics and risk models that manage to reduce uncertainty for decision-making in order to take informed decisions concerning the risk management of AI systems.\nThe purpose of this paper is to orientate AI stakeholders about the depths of AI risk management. Although it is not extremely technical, it requires a basic knowledge of risk management, quantifying uncertainty, the FAIR model, machine learning, large language models and AI context engineering. The examples presented pretend to be very basic and understandable, providing simple ideas that can be developed regarding specific AI customized environments. There are many issues to solve in AI risk management, and this paper will present a holistic overview of the inter-dependencies of AI risks, and how to model them together, within risk scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "58",
        "title": "NormGenesis: Multicultural Dialogue Generation via Exemplar-Guided Social Norm Modeling and Violation Recovery",
        "author": [
            "Minki Hong",
            "Jangho Choi",
            "Jihie Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18395",
        "abstract": "Social norms govern culturally appropriate behavior in communication, enabling dialogue systems to produce responses that are not only coherent but also socially acceptable. We present NormGenesis, a multicultural framework for generating and annotating socially grounded dialogues across English, Chinese, and Korean. To model the dynamics of social interaction beyond static norm classification, we propose a novel dialogue type, Violation-to-Resolution (V2R), which models the progression of conversations following norm violations through recognition and socially appropriate repair. To improve pragmatic consistency in underrepresented languages, we implement an exemplar-based iterative refinement early in the dialogue synthesis process. This design introduces alignment with linguistic, emotional, and sociocultural expectations before full dialogue generation begins. Using this framework, we construct a dataset of 10,800 multi-turn dialogues annotated at the turn level for norm adherence, speaker intent, and emotional response. Human and LLM-based evaluations demonstrate that NormGenesis significantly outperforms existing datasets in refinement quality, dialogue naturalness, and generalization performance. We show that models trained on our V2R-augmented data exhibit improved pragmatic competence in ethically sensitive contexts. Our work establishes a new benchmark for culturally adaptive dialogue modeling and provides a scalable methodology for norm-aware generation across linguistically and culturally diverse languages.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "59",
        "title": "ATLAS: Benchmarking and Adapting LLMs for Global Trade via Harmonized Tariff Code Classification",
        "author": [
            "Pritish Yuvraj",
            "Siva Devarakonda"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18400",
        "abstract": "Accurate classification of products under the Harmonized Tariff Schedule (HTS) is a critical bottleneck in global trade, yet it has received little attention from the machine learning community. Misclassification can halt shipments entirely, with major postal operators suspending deliveries to the U.S. due to incomplete customs documentation. We introduce the first benchmark for HTS code classification, derived from the U.S. Customs Rulings Online Search System (CROSS). Evaluating leading LLMs, we find that our fine-tuned Atlas model (LLaMA-3.3-70B) achieves 40 percent fully correct 10-digit classifications and 57.5 percent correct 6-digit classifications, improvements of 15 points over GPT-5-Thinking and 27.5 points over Gemini-2.5-Pro-Thinking. Beyond accuracy, Atlas is roughly five times cheaper than GPT-5-Thinking and eight times cheaper than Gemini-2.5-Pro-Thinking, and can be self-hosted to guarantee data privacy in high-stakes trade and compliance workflows. While Atlas sets a strong baseline, the benchmark remains highly challenging, with only 40 percent 10-digit accuracy. By releasing both dataset and model, we aim to position HTS classification as a new community benchmark task and invite future work in retrieval, reasoning, and alignment.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "60",
        "title": "Evaluating the Creativity of LLMs in Persian Literary Text Generation",
        "author": [
            "Armin Tourajmehr",
            "Mohammad Reza Modarres",
            "Yadollah Yaghoobzadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18401",
        "abstract": "Large language models (LLMs) have demonstrated notable creative abilities in generating literary texts, including poetry and short stories. However, prior research has primarily centered on English, with limited exploration of non-English literary traditions and without standardized methods for assessing creativity. In this paper, we evaluate the capacity of LLMs to generate Persian literary text enriched with culturally relevant expressions. We build a dataset of user-generated Persian literary spanning 20 diverse topics and assess model outputs along four creativity dimensions-originality, fluency, flexibility, and elaboration-by adapting the Torrance Tests of Creative Thinking. To reduce evaluation costs, we adopt an LLM as a judge for automated scoring and validate its reliability against human judgments using intraclass correlation coefficients, observing strong agreement. In addition, we analyze the models' ability to understand and employ four core literary devices: simile, metaphor, hyperbole, and antithesis. Our results highlight both the strengths and limitations of LLMs in Persian literary text generation, underscoring the need for further refinement.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "61",
        "title": "Check Field Detection Agent (CFD-Agent) using Multimodal Large Language and Vision Language Models",
        "author": [
            "Sourav Halder",
            "Jinjun Tong",
            "Xinyu Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18405",
        "abstract": "Checks remain a foundational instrument in the financial ecosystem, facilitating substantial transaction volumes across institutions. However, their continued use also renders them a persistent target for fraud, underscoring the importance of robust check fraud detection mechanisms. At the core of such systems lies the accurate identification and localization of critical fields, such as the signature, magnetic ink character recognition (MICR) line, courtesy amount, legal amount, payee, and payer, which are essential for subsequent verification against reference checks belonging to the same customer. This field-level detection is traditionally dependent on object detection models trained on large, diverse, and meticulously labeled datasets, a resource that is scarce due to proprietary and privacy concerns. In this paper, we introduce a novel, training-free framework for automated check field detection, leveraging the power of a vision language model (VLM) in conjunction with a multimodal large language model (MLLM). Our approach enables zero-shot detection of check components, significantly lowering the barrier to deployment in real-world financial settings. Quantitative evaluation of our model on a hand-curated dataset of 110 checks spanning multiple formats and layouts demonstrates strong performance and generalization capability. Furthermore, this framework can serve as a bootstrap mechanism for generating high-quality labeled datasets, enabling the development of specialized real-time object detection models tailored to institutional needs.",
        "tags": [
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "62",
        "title": "Instruction-Following Evaluation in Function Calling for Large Language Models",
        "author": [
            "Nikolai Skripko"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18420",
        "abstract": "Function calling is a core capability of large language models, essential for AI agents. Existing benchmarks such as the Berkeley Function Calling Leaderboard (BFCL), tau^2-Bench (https://arxiv.org/abs/2506.07982), and ACEBench (https://arxiv.org/abs/2501.12851) evaluate argument correctness but do not test adherence to format instructions embedded in parameter descriptions, such as enclosing values in double quotes or using ISO date formats.\nWe introduce IFEval-FC, a benchmark inspired by IFEval (https://arxiv.org/abs/2311.07911) that assesses precise instruction following in function calling. IFEval-FC encodes verifiable formats directly within JSON schema descriptions, for example specifying that a value must not contain punctuation. It includes 750 test cases, each consisting of a function with an embedded format for one of its input parameters and a corresponding user query. Evaluation is fully algorithmic, ensuring objectivity, reproducibility, and scalability.\nOur results show that even state-of-the-art proprietary models, including GPT-5 and Claude 4.1 Opus, frequently fail to follow basic formatting rules, highlighting a practical limitation for real-world agent systems. The complete codebase and data are publicly available at https://github.com/Skripkon/IFEval-FC.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "63",
        "title": "Scattering Transformer: A Training-Free Transformer Architecture for Heart Murmur Detection",
        "author": [
            "Rami Zewail"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18424",
        "abstract": "In an attempt to address the need for skilled clinicians in heart sound interpretation, recent research efforts on automating cardiac auscultation have explored deep learning approaches. The majority of these approaches have been based on supervised learning that is always challenged in occasions where training data is limited. More recently, there has been a growing interest in potentials of pre-trained self-supervised audio foundation models for biomedical end tasks. Despite exhibiting promising results, these foundational models are typically computationally intensive. Within the context of automatic cardiac auscultation, this study explores a lightweight alternative to these general-purpose audio foundation models by introducing the Scattering Transformer, a novel, training-free transformer architecture for heart murmur detection. The proposed method leverages standard wavelet scattering networks by introducing contextual dependencies in a transformer-like architecture without any backpropagation. We evaluate our approach on the public CirCor DigiScope dataset, directly comparing it against leading general-purpose foundational models. The Scattering Transformer achieves a Weighted Accuracy(WAR) of 0.786 and an Unweighted Average Recall(UAR) of 0.697, demonstrating performance highly competitive with contemporary state of the art methods. This study establishes the Scattering Transformer as a viable and promising alternative in resource-constrained setups.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "64",
        "title": "Losing the Plot: How VLM responses degrade on imperfect charts",
        "author": [
            "Philip Wootaek Shin",
            "Jack Sampson",
            "Vijaykrishnan Narayanan",
            "Andres Marquez",
            "Mahantesh Halappanavar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18425",
        "abstract": "Vision language models (VLMs) show strong results on chart understanding, yet existing benchmarks assume clean figures and fact based queries. Real world charts often contain distortions and demand reasoning beyond simple matching. We evaluate ChatGPT 4o, Claude Sonnet 4, and Gemini 2.5 Pro, finding sharp performance drops under corruption or occlusion, with hallucinations such as value fabrication, trend misinterpretation, and entity confusion becoming more frequent. Models remain overconfident in degraded settings, generating plausible but unsupported explanations.\nTo address this gap, we introduce CHART NOISe(Chart Hallucinations, Answers, and Reasoning Testing on Noisy and Occluded Input Selections), a dataset combining chart corruptions, occlusions, and exam style multiple choice questions inspired by Korea's CSAT English section. A key innovation is prompt reverse inconsistency, where models contradict themselves when asked to confirm versus deny the same statement. Our contributions are threefold: (1) benchmarking state of the art VLMs, exposing systematic vulnerabilities in chart reasoning; (2) releasing CHART NOISe, the first dataset unifying corruption, occlusion, and reverse inconsistency; and (3) proposing baseline mitigation strategies such as quality filtering and occlusion detection. Together, these efforts establish a rigorous testbed for advancing robustness and reliability in chart understanding.",
        "tags": [
            "Detection",
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "65",
        "title": "Latent Action Pretraining Through World Modeling",
        "author": [
            "Bahey Tharwat",
            "Yara Nasser",
            "Ali Abouzeid",
            "Ian Reid"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18428",
        "abstract": "Vision-Language-Action (VLA) models have gained popularity for learning robotic manipulation tasks that follow language instructions. State-of-the-art VLAs, such as OpenVLA and $\\pi_{0}$, were trained on large-scale, manually labeled action datasets collected through teleoperation. More recent approaches, including LAPA and villa-X, introduce latent action representations that enable unsupervised pretraining on unlabeled datasets by modeling abstract visual changes between frames. Although these methods have shown strong results, their large model sizes make deployment in real-world settings challenging. In this work, we propose LAWM, a model-agnostic framework to pretrain imitation learning models in a self-supervised way, by learning latent action representations from unlabeled video data through world modeling. These videos can be sourced from robot recordings or videos of humans performing actions with everyday objects. Our framework is designed to be effective for transferring across tasks, environments, and embodiments. It outperforms models trained with ground-truth robotics actions and similar pretraining methods on the LIBERO benchmark and real-world setup, while being significantly more efficient and practical for real-world settings.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "66",
        "title": "Diffusion Policies with Offline and Inverse Reinforcement Learning for Promoting Physical Activity in Older Adults Using Wearable Sensors",
        "author": [
            "Chang Liu",
            "Ladda Thiamwong",
            "Yanjie Fu",
            "Rui Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18433",
        "abstract": "Utilizing offline reinforcement learning (RL) with real-world clinical data is getting increasing attention in AI for healthcare. However, implementation poses significant challenges. Defining direct rewards is difficult, and inverse RL (IRL) struggles to infer accurate reward functions from expert behavior in complex environments. Offline RL also encounters challenges in aligning learned policies with observed human behavior in healthcare applications. To address challenges in applying offline RL to physical activity promotion for older adults at high risk of falls, based on wearable sensor activity monitoring, we introduce Kolmogorov-Arnold Networks and Diffusion Policies for Offline Inverse Reinforcement Learning (KANDI). By leveraging the flexible function approximation in Kolmogorov-Arnold Networks, we estimate reward functions by learning free-living environment behavior from low-fall-risk older adults (experts), while diffusion-based policies within an Actor-Critic framework provide a generative approach for action refinement and efficiency in offline RL. We evaluate KANDI using wearable activity monitoring data in a two-arm clinical trial from our Physio-feedback Exercise Program (PEER) study, emphasizing its practical application in a fall-risk intervention program to promote physical activity among older adults. Additionally, KANDI outperforms state-of-the-art methods on the D4RL benchmark. These results underscore KANDI's potential to address key challenges in offline RL for healthcare applications, offering an effective solution for activity promotion intervention strategies in healthcare.",
        "tags": [
            "Diffusion",
            "KAN",
            "RL"
        ]
    },
    {
        "id": "67",
        "title": "Developing an AI framework to automatically detect shared decision-making in patient-doctor conversations",
        "author": [
            "Oscar J. Ponce-Ponte",
            "David Toro-Tobon",
            "Luis F. Figueroa",
            "Michael Gionfriddo",
            "Megan Branda",
            "Victor M. Montori",
            "Saturnino Luz",
            "Juan P. Brito"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18439",
        "abstract": "Shared decision-making (SDM) is necessary to achieve patient-centred care. Currently no methodology exists to automatically measure SDM at scale. This study aimed to develop an automated approach to measure SDM by using language modelling and the conversational alignment (CA) score. A total of 157 video-recorded patient-doctor conversations from a randomized multi-centre trial evaluating SDM decision aids for anticoagulation in atrial fibrillations were transcribed and segmented into 42,559 sentences. Context-response pairs and negative sampling were employed to train deep learning (DL) models and fine-tuned BERT models via the next sentence prediction (NSP) task. Each top-performing model was used to calculate four types of CA scores. A random-effects analysis by clinician, adjusting for age, sex, race, and trial arm, assessed the association between CA scores and SDM outcomes: the Decisional Conflict Scale (DCS) and the Observing Patient Involvement in Decision-Making 12 (OPTION12) scores. p-values were corrected for multiple comparisons with the Benjamini-Hochberg method. Among 157 patients (34% female, mean age 70 SD 10.8), clinicians on average spoke more words than patients (1911 vs 773). The DL model without the stylebook strategy achieved a recall@1 of 0.227, while the fine-tuned BERTbase (110M) achieved the highest recall@1 with 0.640. The AbsMax (18.36 SE7.74 p=0.025) and Max CA (21.02 SE7.63 p=0.012) scores generated with the DL without stylebook were associated with OPTION12. The Max CA score generated with the fine-tuned BERTbase (110M) was associated with the DCS score (-27.61 SE12.63 p=0.037). BERT model sizes did not have an impact the association between CA scores and SDM. This study introduces an automated, scalable methodology to measure SDM in patient-doctor conversations through explainable CA scores, with potential to evaluate SDM strategies at scale.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "68",
        "title": "Large-Scale, Longitudinal Study of Large Language Models During the 2024 US Election Season",
        "author": [
            "Sarah H. Cen",
            "Andrew Ilyas",
            "Hedi Driss",
            "Charlotte Park",
            "Aspen Hopkins",
            "Chara Podimata",
            "Aleksander MÄdry"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18446",
        "abstract": "The 2024 US presidential election is the first major contest to occur in the US since the popularization of large language models (LLMs). Building on lessons from earlier shifts in media (most notably social media's well studied role in targeted messaging and political polarization) this moment raises urgent questions about how LLMs may shape the information ecosystem and influence political discourse. While platforms have announced some election safeguards, how well they work in practice remains unclear. Against this backdrop, we conduct a large-scale, longitudinal study of 12 models, queried using a structured survey with over 12,000 questions on a near-daily cadence from July through November 2024. Our design systematically varies content and format, resulting in a rich dataset that enables analyses of the models' behavior over time (e.g., across model updates), sensitivity to steering, responsiveness to instructions, and election-related knowledge and \"beliefs.\" In the latter half of our work, we perform four analyses of the dataset that (i) study the longitudinal variation of model behavior during election season, (ii) illustrate the sensitivity of election-related responses to demographic steering, (iii) interrogate the models' beliefs about candidates' attributes, and (iv) reveal the models' implicit predictions of the election outcome. To facilitate future evaluations of LLMs in electoral contexts, we detail our methodology, from question generation to the querying pipeline and third-party tooling. We also publicly release our dataset at https://huggingface.co/datasets/sarahcen/llm-election-data-2024",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "69",
        "title": "PrioriTouch: Adapting to User Contact Preferences for Whole-Arm Physical Human-Robot Interaction",
        "author": [
            "Rishabh Madan",
            "Jiawei Lin",
            "Mahika Goel",
            "Angchen Xie",
            "Xiaoyu Liang",
            "Marcus Lee",
            "Justin Guo",
            "Pranav N. Thakkar",
            "Rohan Banerjee",
            "Jose Barreiros",
            "Kate Tsui",
            "Tom Silver",
            "Tapomayukh Bhattacharjee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18447",
        "abstract": "Physical human-robot interaction (pHRI) requires robots to adapt to individual contact preferences, such as where and how much force is applied. Identifying preferences is difficult for a single contact; with whole-arm interaction involving multiple simultaneous contacts between the robot and human, the challenge is greater because different body parts can impose incompatible force requirements. In caregiving tasks, where contact is frequent and varied, such conflicts are unavoidable. With multiple preferences across multiple contacts, no single solution can satisfy all objectives--trade-offs are inherent, making prioritization essential. We present PrioriTouch, a framework for ranking and executing control objectives across multiple contacts. PrioriTouch can prioritize from a general collection of controllers, making it applicable not only to caregiving scenarios such as bed bathing and dressing but also to broader multi-contact settings. Our method combines a novel learning-to-rank approach with hierarchical operational space control, leveraging simulation-in-the-loop rollouts for data-efficient and safe exploration. We conduct a user study on physical assistance preferences, derive personalized comfort thresholds, and incorporate them into PrioriTouch. We evaluate PrioriTouch through extensive simulation and real-world experiments, demonstrating its ability to adapt to user contact preferences, maintain task performance, and enhance safety and comfort. Website: https://emprise.cs.cornell.edu/prioritouch.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "70",
        "title": "An Analysis of Kalman Filter based Object Tracking Methods for Fast-Moving Tiny Objects",
        "author": [
            "Prithvi Raj Singh",
            "Raju Gottumukkala",
            "Anthony Maida"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18451",
        "abstract": "Unpredictable movement patterns and small visual mark make precise tracking of fast-moving tiny objects like a racquetball one of the challenging problems in computer vision. This challenge is particularly relevant for sport robotics applications, where lightweight and accurate tracking systems can improve robot perception and planning capabilities. While Kalman filter-based tracking methods have shown success in general object tracking scenarios, their performance degrades substantially when dealing with rapidly moving objects that exhibit irregular bouncing behavior. In this study, we evaluate the performance of five state-of-the-art Kalman filter-based tracking methods-OCSORT, DeepOCSORT, ByteTrack, BoTSORT, and StrongSORT-using a custom dataset containing 10,000 annotated racquetball frames captured at 720p-1280p resolution. We focus our analysis on two critical performance factors: inference speed and update frequency per image, examining how these parameters affect tracking accuracy and reliability for fast-moving tiny objects. Our experimental evaluation across four distinct scenarios reveals that DeepOCSORT achieves the lowest tracking error with an average ADE of 31.15 pixels compared to ByteTrack's 114.3 pixels, while ByteTrack demonstrates the fastest processing at 26.6ms average inference time versus DeepOCSORT's 26.8ms. However, our results show that all Kalman filter-based trackers exhibit significant tracking drift with spatial errors ranging from 3-11cm (ADE values: 31-114 pixels), indicating fundamental limitations in handling the unpredictable motion patterns of fast-moving tiny objects like racquetballs. Our analysis demonstrates that current tracking approaches require substantial improvements, with error rates 3-4x higher than standard object tracking benchmarks, highlighting the need for specialized methodologies for fast-moving tiny object tracking applications.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "71",
        "title": "SC2Tools: StarCraft II Toolset and Dataset API",
        "author": [
            "Andrzej BiaÅecki",
            "Piotr BiaÅecki",
            "Piotr SowiÅski",
            "Mateusz Budziak",
            "Jan Gajewski"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18454",
        "abstract": "Computer games, as fully controlled simulated environments, have been utilized in significant scientific studies demonstrating the application of Reinforcement Learning (RL). Gaming and esports are key areas influenced by the application of Artificial Intelligence (AI) and Machine Learning (ML) solutions at scale. Tooling simplifies scientific workloads and is essential for developing the gaming and esports research area.\nIn this work, we present ``SC2Tools'', a toolset containing multiple submodules responsible for working with, and producing larger datasets. We provide a modular structure of the implemented tooling, leaving room for future extensions where needed. Additionally, some of the tools are not StarCraft~2 exclusive and can be used with other types of data for dataset creation.\nThe tools we present were leveraged in creating one of the largest StarCraft~2 tournament datasets to date with a separate PyTorch and PyTorch Lightning application programming interface (API) for easy access to the data.\nWe conclude that alleviating the burden of data collection, preprocessing, and custom code development is essential for less technically proficient researchers to engage in the growing gaming and esports research area. Finally, our solution provides some foundational work toward normalizing experiment workflow in StarCraft~2",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "72",
        "title": "Learning Geometry-Aware Nonprehensile Pushing and Pulling with Dexterous Hands",
        "author": [
            "Yunshuang Li",
            "Yiyang Ling",
            "Gaurav S. Sukhatme",
            "Daniel Seita"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18455",
        "abstract": "Nonprehensile manipulation, such as pushing and pulling, enables robots to move, align, or reposition objects that may be difficult to grasp due to their geometry, size, or relationship to the robot or the environment. Much of the existing work in nonprehensile manipulation relies on parallel-jaw grippers or tools such as rods and spatulas. In contrast, multi-fingered dexterous hands offer richer contact modes and versatility for handling diverse objects to provide stable support over the objects, which compensates for the difficulty of modeling the dynamics of nonprehensile manipulation. Therefore, we propose Geometry-aware Dexterous Pushing and Pulling (GD2P) for nonprehensile manipulation with dexterous robotic hands. We study pushing and pulling by framing the problem as synthesizing and learning pre-contact dexterous hand poses that lead to effective manipulation. We generate diverse hand poses via contact-guided sampling, filter them using physics simulation, and train a diffusion model conditioned on object geometry to predict viable poses. At test time, we sample hand poses and use standard motion planners to select and execute pushing and pulling actions. We perform 840 real-world experiments with an Allegro Hand, comparing our method to baselines. The results indicate that GD2P offers a scalable route for training dexterous nonprehensile manipulation policies. We further demonstrate GD2P on a LEAP Hand, highlighting its applicability to different hand morphologies. Our pre-trained models and dataset, including 1.3 million hand poses across 2.3k objects, will be open-source to facilitate further research. Our project website is available at: http://geodex2p.github.io.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "73",
        "title": "GluMind: Multimodal Parallel Attention and Knowledge Retention for Robust Cross-Population Blood Glucose Forecasting",
        "author": [
            "Ebrahim Farahmand",
            "Reza Rahimi Azghan",
            "Nooshin Taheri Chatrudi",
            "Velarie Yaa Ansu-Baidoo",
            "Eric Kim",
            "Gautham Krishna Gudur",
            "Mohit Malu",
            "Owen Krueger",
            "Edison Thomaz",
            "Giulia Pedrielli",
            "Pavan Turaga",
            "Hassan Ghasemzadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18457",
        "abstract": "This paper proposes GluMind, a transformer-based multimodal framework designed for continual and long-term blood glucose forecasting. GluMind devises two attention mechanisms, including cross-attention and multi-scale attention, which operate in parallel and deliver accurate predictive performance. Cross-attention effectively integrates blood glucose data with other physiological and behavioral signals such as activity, stress, and heart rate, addressing challenges associated with varying sampling rates and their adverse impacts on robust prediction. Moreover, the multi-scale attention mechanism captures long-range temporal dependencies. To mitigate catastrophic forgetting, GluMind incorporates a knowledge retention technique into the transformer-based forecasting model. The knowledge retention module not only enhances the model's ability to retain prior knowledge but also boosts its overall forecasting performance. We evaluate GluMind on the recently released AIREADI dataset, which contains behavioral and physiological data collected from healthy people, individuals with prediabetes, and those with type 2 diabetes. We examine the performance stability and adaptability of GluMind in learning continuously as new patient cohorts are introduced. Experimental results show that GluMind consistently outperforms other state-of-the-art forecasting models, achieving approximately 15% and 9% improvements in root mean squared error (RMSE) and mean absolute error (MAE), respectively.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "74",
        "title": "CogniLoad: A Synthetic Natural Language Reasoning Benchmark With Tunable Length, Intrinsic Difficulty, and Distractor Density",
        "author": [
            "Daniel Kaiser",
            "Arnoldo Frigessi",
            "Ali Ramezani-Kebrya",
            "Benjamin Ricaud"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18458",
        "abstract": "Current benchmarks for long-context reasoning in Large Language Models (LLMs) often blur critical factors like intrinsic task complexity, distractor interference, and task length. To enable more precise failure analysis, we introduce CogniLoad, a novel synthetic benchmark grounded in Cognitive Load Theory (CLT). CogniLoad generates natural-language logic puzzles with independently tunable parameters that reflect CLT's core dimensions: intrinsic difficulty ($d$) controls intrinsic load; distractor-to-signal ratio ($\\rho$) regulates extraneous load; and task length ($N$) serves as an operational proxy for conditions demanding germane load. Evaluating 22 SotA reasoning LLMs, CogniLoad reveals distinct performance sensitivities, identifying task length as a dominant constraint and uncovering varied tolerances to intrinsic complexity and U-shaped responses to distractor ratios. By offering systematic, factorial control over these cognitive load dimensions, CogniLoad provides a reproducible, scalable, and diagnostically rich tool for dissecting LLM reasoning limitations and guiding future model development.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "75",
        "title": "Robotic Skill Diversification via Active Mutation of Reward Functions in Reinforcement Learning During a Liquid Pouring Task",
        "author": [
            "Jannick van Buuren",
            "Roberto Giglio",
            "Loris Roveda",
            "Luka Peternel"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18463",
        "abstract": "This paper explores how deliberate mutations of reward function in reinforcement learning can produce diversified skill variations in robotic manipulation tasks, examined with a liquid pouring use case. To this end, we developed a new reward function mutation framework that is based on applying Gaussian noise to the weights of the different terms in the reward function. Inspired by the cost-benefit tradeoff model from human motor control, we designed the reward function with the following key terms: accuracy, time, and effort. The study was performed in a simulation environment created in NVIDIA Isaac Sim, and the setup included Franka Emika Panda robotic arm holding a glass with a liquid that needed to be poured into a container. The reinforcement learning algorithm was based on Proximal Policy Optimization. We systematically explored how different configurations of mutated weights in the rewards function would affect the learned policy. The resulting policies exhibit a wide range of behaviours: from variations in execution of the originally intended pouring task to novel skills useful for unexpected tasks, such as container rim cleaning, liquid mixing, and watering. This approach offers promising directions for robotic systems to perform diversified learning of specific tasks, while also potentially deriving meaningful skills for future tasks.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "76",
        "title": "RL-augmented Adaptive Model Predictive Control for Bipedal Locomotion over Challenging Terrain",
        "author": [
            "Junnosuke Kamohara",
            "Feiyang Wu",
            "Chinmayee Wamorkar",
            "Seth Hutchinson",
            "Ye Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18466",
        "abstract": "Model predictive control (MPC) has demonstrated effectiveness for humanoid bipedal locomotion; however, its applicability in challenging environments, such as rough and slippery terrain, is limited by the difficulty of modeling terrain interactions. In contrast, reinforcement learning (RL) has achieved notable success in training robust locomotion policies over diverse terrain, yet it lacks guarantees of constraint satisfaction and often requires substantial reward shaping. Recent efforts in combining MPC and RL have shown promise of taking the best of both worlds, but they are primarily restricted to flat terrain or quadrupedal robots. In this work, we propose an RL-augmented MPC framework tailored for bipedal locomotion over rough and slippery terrain. Our method parametrizes three key components of single-rigid-body-dynamics-based MPC: system dynamics, swing leg controller, and gait frequency. We validate our approach through bipedal robot simulations in NVIDIA IsaacLab across various terrains, including stairs, stepping stones, and low-friction surfaces. Experimental results demonstrate that our RL-augmented MPC framework produces significantly more adaptive and robust behaviors compared to baseline MPC and RL.",
        "tags": [
            "MPC",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "77",
        "title": "LAWCAT: Efficient Distillation from Quadratic to Linear Attention with Convolution across Tokens for Long Context Modeling",
        "author": [
            "Zeyu Liu",
            "Souvik Kundu",
            "Lianghao Jiang",
            "Anni Li",
            "Srikanth Ronanki",
            "Sravan Bodapati",
            "Gourav Datta",
            "Peter A. Beerel"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18467",
        "abstract": "Although transformer architectures have achieved state-of-the-art performance across diverse domains, their quadratic computational complexity with respect to sequence length remains a significant bottleneck, particularly for latency-sensitive long-context applications. While recent linear-complexity alternatives are increasingly powerful, effectively training them from scratch is still resource-intensive. To overcome these limitations, we propose LAWCAT (Linear Attention with Convolution Across Time), a novel linearization framework designed to efficiently transfer the capabilities of pre-trained transformers into a performant linear attention architecture. LAWCAT integrates causal Conv1D layers to enhance local dependency modeling and employs normalized gated linear attention to improve generalization across varying context lengths. Our comprehensive evaluations demonstrate that, distilling Mistral-7B with only 1K-length sequences yields over 90\\% passkey retrieval accuracy up to 22K tokens, significantly extending its effective context window. Similarly, Llama3.2-1B LAWCAT variant achieves competitive performance on S-NIAH 1\\&2\\&3 tasks (1K-8K context length) and BABILong benchmark (QA2\\&QA3, 0K-16K context length), requiring less than 0.1\\% pre-training tokens compared with pre-training models. Furthermore, LAWCAT exhibits faster prefill speeds than FlashAttention-2 for sequences exceeding 8K tokens. LAWCAT thus provides an efficient pathway to high-performance, long-context linear models suitable for edge deployment, reducing reliance on extensive long-sequence training data and computational resources.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "78",
        "title": "Discrete-time diffusion-like models for speech synthesis",
        "author": [
            "Xiaozhou Tan",
            "Minghui Zhao",
            "Mattias Cross",
            "Anton Ragni"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18470",
        "abstract": "Diffusion models have attracted a lot of attention in recent years. These models view speech generation as a continuous-time process. For efficient training, this process is typically restricted to additive Gaussian noising, which is limiting. For inference, the time is typically discretized, leading to the mismatch between continuous training and discrete sampling conditions. Recently proposed discrete-time processes, on the other hand, usually do not have these limitations, may require substantially fewer inference steps, and are fully consistent between training/inference conditions. This paper explores some diffusion-like discrete-time processes and proposes some new variants. These include processes applying additive Gaussian noise, multiplicative Gaussian noise, blurring noise and a mixture of blurring and Gaussian noises. The experimental results suggest that discrete-time processes offer comparable subjective and objective speech quality to their widely popular continuous counterpart, with more efficient and consistent training and inference schemas.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "79",
        "title": "Individualized non-uniform quantization for vector search",
        "author": [
            "Mariano Tepper",
            "Ted Willke"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18471",
        "abstract": "Embedding vectors are widely used for representing unstructured data and searching through it for semantically similar items. However, the large size of these vectors, due to their high-dimensionality, creates problems for modern vector search techniques: retrieving large vectors from memory/storage is expensive and their footprint is costly. In this work, we present NVQ (non-uniform vector quantization), a new vector compression technique that is computationally and spatially efficient in the high-fidelity regime. The core in NVQ is to use novel parsimonious and computationally efficient nonlinearities for building non-uniform vector quantizers. Critically, these quantizers are \\emph{individually} learned for each indexed vector. Our experimental results show that NVQ exhibits improved accuracy compared to the state of the art with a minimal computational cost.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "80",
        "title": "MoCrop: Training Free Motion Guided Cropping for Efficient Video Action Recognition",
        "author": [
            "Binhua Huang",
            "Wendong Yao",
            "Shaowu Chen",
            "Guoxin Wang",
            "Qingyuan Wang",
            "Soumyabrata Dev"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18473",
        "abstract": "We introduce MoCrop, a motion-aware adaptive cropping module for efficient video action recognition in the compressed domain. MoCrop uses motion vectors that are available in H.264 video to locate motion-dense regions and produces a single clip-level crop that is applied to all I-frames at inference. The module is training free, adds no parameters, and can be plugged into diverse backbones. A lightweight pipeline that includes denoising & merge (DM), Monte Carlo sampling (MCS), and adaptive cropping (AC) via a motion-density submatrix search yields robust crops with negligible overhead. On UCF101, MoCrop improves accuracy or reduces compute. With ResNet-50, it delivers +3.5% Top-1 accuracy at equal FLOPs (attention setting), or +2.4% Top-1 accuracy with 26.5% fewer FLOPs (efficiency setting). Applied to CoViAR, it reaches 89.2% Top-1 accuracy at the original cost and 88.5% Top-1 accuracy while reducing compute from 11.6 to 8.5 GFLOPs. Consistent gains on MobileNet-V3, EfficientNet-B1, and Swin-B indicate strong generality and make MoCrop practical for real-time deployment in the compressed domain. Our code and models are available at https://github.com/microa/MoCrop.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "81",
        "title": "Codebook-Based Adaptive Feature Compression With Semantic Enhancement for Edge-Cloud Systems",
        "author": [
            "Xinyu Wang",
            "Zikun Zhou",
            "Yingjian Li",
            "Xin An",
            "Hongpeng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18481",
        "abstract": "Coding images for machines with minimal bitrate and strong analysis performance is key to effective edge-cloud systems. Several approaches deploy an image codec and perform analysis on the reconstructed image. Other methods compress intermediate features using entropy models and subsequently perform analysis on the decoded features. Nevertheless, these methods both perform poorly under low-bitrate conditions, as they retain many redundant details or learn over-concentrated symbol distributions. In this paper, we propose a Codebook-based Adaptive Feature Compression framework with Semantic Enhancement, named CAFC-SE. It maps continuous visual features to discrete indices with a codebook at the edge via Vector Quantization (VQ) and selectively transmits them to the cloud. The VQ operation that projects feature vectors onto the nearest visual primitives enables us to preserve more informative visual patterns under low-bitrate conditions. Hence, CAFC-SE is less vulnerable to low-bitrate conditions. Extensive experiments demonstrate the superiority of our method in terms of rate and accuracy.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "82",
        "title": "Physics-informed time series analysis with Kolmogorov-Arnold Networks under Ehrenfest constraints",
        "author": [
            "Abhijit Sen",
            "Illya V. Lukin",
            "Kurt Jacobs",
            "Lev Kaplan",
            "Andrii G. Sotnikov",
            "Denys I. Bondar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18483",
        "abstract": "The prediction of quantum dynamical responses lies at the heart of modern physics. Yet, modeling these time-dependent behaviors remains a formidable challenge because quantum systems evolve in high-dimensional Hilbert spaces, often rendering traditional numerical methods computationally prohibitive. While large language models have achieved remarkable success in sequential prediction, quantum dynamics presents a fundamentally different challenge: forecasting the entire temporal evolution of quantum systems rather than merely the next element in a sequence. Existing neural architectures such as recurrent and convolutional networks often require vast training datasets and suffer from spurious oscillations that compromise physical interpretability. In this work, we introduce a fundamentally new approach: Kolmogorov Arnold Networks (KANs) augmented with physics-informed loss functions that enforce the Ehrenfest theorems. Our method achieves superior accuracy with significantly less training data: it requires only 5.4 percent of the samples (200) compared to Temporal Convolution Networks (3,700). We further introduce the Chain of KANs, a novel architecture that embeds temporal causality directly into the model design, making it particularly well-suited for time series modeling. Our results demonstrate that physics-informed KANs offer a compelling advantage over conventional black-box models, maintaining both mathematical rigor and physical consistency while dramatically reducing data requirements.",
        "tags": [
            "KAN",
            "LLM"
        ]
    },
    {
        "id": "83",
        "title": "Differentiable Light Transport with Gaussian Surfels via Adapted Radiosity for Efficient Relighting and Geometry Reconstruction",
        "author": [
            "Kaiwen Jiang",
            "Jia-Mu Sun",
            "Zilu Li",
            "Dan Wang",
            "Tzu-Mao Li",
            "Ravi Ramamoorthi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18497",
        "abstract": "Radiance fields have gained tremendous success with applications ranging from novel view synthesis to geometry reconstruction, especially with the advent of Gaussian splatting. However, they sacrifice modeling of material reflective properties and lighting conditions, leading to significant geometric ambiguities and the inability to easily perform relighting. One way to address these limitations is to incorporate physically-based rendering, but it has been prohibitively expensive to include full global illumination within the inner loop of the optimization. Therefore, previous works adopt simplifications that make the whole optimization with global illumination effects efficient but less accurate. In this work, we adopt Gaussian surfels as the primitives and build an efficient framework for differentiable light transport, inspired from the classic radiosity theory. The whole framework operates in the coefficient space of spherical harmonics, enabling both diffuse and specular materials. We extend the classic radiosity into non-binary visibility and semi-opaque primitives, propose novel solvers to efficiently solve the light transport, and derive the backward pass for gradient optimizations, which is more efficient than auto-differentiation. During inference, we achieve view-independent rendering where light transport need not be recomputed under viewpoint changes, enabling hundreds of FPS for global illumination effects, including view-dependent reflections using a spherical harmonics representation. Through extensive qualitative and quantitative experiments, we demonstrate superior geometry reconstruction, view synthesis and relighting than previous inverse rendering baselines, or data-driven baselines given relatively sparse datasets with known or unknown lighting conditions.",
        "tags": [
            "Gaussian Splatting"
        ]
    },
    {
        "id": "84",
        "title": "Spatial Envelope MPC: High Performance Driving without a Reference",
        "author": [
            "Siyuan Yu",
            "Congkai Shen",
            "Yufei Xi",
            "James Dallas",
            "Michael Thompson",
            "John Subosits",
            "Hiroshi Yasuda",
            "Tulga Ersal"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18506",
        "abstract": "This paper presents a novel envelope based model predictive control (MPC) framework designed to enable autonomous vehicles to handle high performance driving across a wide range of scenarios without a predefined reference. In high performance autonomous driving, safe operation at the vehicle's dynamic limits requires a real time planning and control framework capable of accounting for key vehicle dynamics and environmental constraints when following a predefined reference trajectory is suboptimal or even infeasible. State of the art planning and control frameworks, however, are predominantly reference based, which limits their performance in such situations. To address this gap, this work first introduces a computationally efficient vehicle dynamics model tailored for optimization based control and a continuously differentiable mathematical formulation that accurately captures the entire drivable envelope. This novel model and formulation allow for the direct integration of dynamic feasibility and safety constraints into a unified planning and control framework, thereby removing the necessity for predefined references. The challenge of envelope planning, which refers to maximally approximating the safe drivable area, is tackled by combining reinforcement learning with optimization techniques. The framework is validated through both simulations and real world experiments, demonstrating its high performance across a variety of tasks, including racing, emergency collision avoidance and off road navigation. These results highlight the framework's scalability and broad applicability across a diverse set of scenarios.",
        "tags": [
            "MPC",
            "RL"
        ]
    },
    {
        "id": "85",
        "title": "A Rhythm-Aware Phrase Insertion for Classical Arabic Poetry Composition",
        "author": [
            "Mohamad Elzohbi",
            "Richard Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18514",
        "abstract": "This paper presents a methodology for inserting phrases in Arabic poems to conform to a specific rhythm using ByT5, a byte-level multilingual transformer-based model. Our work discusses a rule-based grapheme-to-beat transformation tailored for extracting the rhythm from fully diacritized Arabic script. Our approach employs a conditional denoising objective to fine-tune ByT5, where the model reconstructs masked words to match a target rhythm. We adopt a curriculum learning strategy, pre-training on a general Arabic dataset before fine-tuning on poetic dataset, and explore cross-lingual transfer from English to Arabic. Experimental results demonstrate that our models achieve high rhythmic alignment while maintaining semantic coherence. The proposed model has the potential to be used in co-creative applications in the process of composing classical Arabic poems.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "86",
        "title": "Coherence-driven inference for cybersecurity",
        "author": [
            "Steve Huntsman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18520",
        "abstract": "Large language models (LLMs) can compile weighted graphs on natural language data to enable automatic coherence-driven inference (CDI) relevant to red and blue team operations in cybersecurity. This represents an early application of automatic CDI that holds near- to medium-term promise for decision-making in cybersecurity and eventually also for autonomous blue team operations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "87",
        "title": "APRIL: Active Partial Rollouts in Reinforcement Learning to tame long-tail generation",
        "author": [
            "Yuzhen Zhou",
            "Jiajun Li",
            "Yusheng Su",
            "Gowtham Ramesh",
            "Zilin Zhu",
            "Xiang Long",
            "Chenyang Zhao",
            "Jin Pan",
            "Xiaodong Yu",
            "Ze Wang",
            "Kangrui Du",
            "Jialian Wu",
            "Ximeng Sun",
            "Jiang Liu",
            "Qiaolin Yu",
            "Hao Chen",
            "Zicheng Liu",
            "Emad Barsoum"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18521",
        "abstract": "Reinforcement learning (RL) has become a cornerstone in advancing large-scale pre-trained language models (LLMs). Successive generations, including GPT-o series, DeepSeek-R1, Kimi-K1.5, Grok 4, and GLM-4.5, have relied on large-scale RL training to enhance reasoning and coding capabilities. To meet the community's growing RL needs, numerous RL frameworks have been proposed. Most of these frameworks primarily rely on inference engines for rollout generation and training engines for policy updates. However, RL training remains computationally expensive, with rollout generation accounting for more than 90% of total runtime. In addition, its efficiency is often constrained by the long-tail distribution of rollout response lengths, where a few lengthy responses stall entire batches, leaving GPUs idle and underutilized. As model and rollout sizes continue to grow, this bottleneck increasingly limits scalability. To address this challenge, we propose Active Partial Rollouts in Reinforcement Learning (APRIL), which mitigates long-tail inefficiency. In the rollout phase, APRIL over-provisions rollout requests, terminates once the target number of responses is reached, and recycles incomplete responses for continuation in future steps. This strategy ensures that no rollouts are discarded while substantially reducing GPU idle time. Experiments show that APRIL improves rollout throughput by at most 44% across commonly used RL algorithms (GRPO, DAPO, GSPO), accelerates convergence, and achieves at most 8% higher final accuracy across tasks. Moreover, APRIL is both framework and hardware agnostic, already integrated into the slime RL framework, and deployable on NVIDIA and AMD GPUs alike. Taken together, this work unifies system-level and algorithmic considerations in proposing APRIL, with the aim of advancing RL training efficiency and inspiring further optimizations in RL systems.",
        "tags": [
            "DeepSeek",
            "GLM",
            "GPT",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "88",
        "title": "Automatic coherence-driven inference on arguments",
        "author": [
            "Steve Huntsman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18523",
        "abstract": "Inconsistencies are ubiquitous in law, administration, and jurisprudence. Though a cure is too much to hope for, we propose a technological remedy. Large language models (LLMs) can accurately extract propositions from arguments and compile them into natural data structures that enable coherence-driven inference (CDI) via combinatorial optimization. This neurosymbolic architecture naturally separates concerns and enables meaningful judgments about the coherence of arguments that can inform legislative and policy analysis and legal reasoning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "89",
        "title": "AI Agent Access (A\\^3) Network: An Embodied, Communication-Aware Multi-Agent Framework for 6G Coverage",
        "author": [
            "Han Zeng",
            "Haibo Wang",
            "Luhao Fan",
            "Bingcheng Zhu",
            "Xiaohu You",
            "Zaichen Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18526",
        "abstract": "The vision of 6G communication demands autonomous and resilient networking in environments without fixed infrastructure. Yet most multi-agent reinforcement learning (MARL) approaches focus on isolated stages - exploration, relay formation, or access - under static deployments and centralized control, limiting adaptability. We propose the AI Agent Access (A\\^3) Network, a unified, embodied intelligence-driven framework that transforms multi-agent networking into a dynamic, decentralized, and end-to-end system. Unlike prior schemes, the A\\^3 Network integrates exploration, target user access, and backhaul maintenance within a single learning process, while supporting on-demand agent addition during runtime. Its decentralized policies ensure that even a single agent can operate independently with limited observations, while coordinated agents achieve scalable, communication-optimized coverage. By embedding link-level communication metrics into actor-critic learning, the A\\^3 Network couples topology formation with robust decision-making. Numerical simulations demonstrate that the A\\^3 Network not only balances exploration and communication efficiency but also delivers system-level adaptability absent in existing MARL frameworks, offering a new paradigm for 6G multi-agent networks.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "90",
        "title": "FERA: Foil Fencing Referee Assistant Using Pose-Based Multi-Label Move Recognition and Rule Reasoning",
        "author": [
            "Ziwen Chen",
            "Zhong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18527",
        "abstract": "The sport of fencing, like many other sports, faces challenges in refereeing: subjective calls, human errors, bias, and limited availability in practice environments. We present FERA (Fencing Referee Assistant), a prototype AI referee for foil fencing which integrates pose-based multi-label action recognition and rule-based reasoning. FERA extracts 2D joint positions from video, normalizes them, computes a 101-dimensional kinematic feature set, and applies a Transformer for multi-label move and blade classification. To determine priority and scoring, FERA applies a distilled language model with encoded right-of-way rules, producing both a decision and an explanation for each exchange. With limited hand-labeled data, a 5-fold cross-validation achieves an average macro-F1 score of 0.549, outperforming multiple baselines, including a Temporal Convolutional Network (TCN), BiLSTM, and a vanilla Transformer. While not ready for deployment, these results demonstrate a promising path towards automated referee assistance in foil fencing and new opportunities for AI applications, such as coaching in the field of fencing.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "91",
        "title": "Trace Is In Sentences: Unbiased Lightweight ChatGPT-Generated Text Detector",
        "author": [
            "Mo Mu",
            "Dianqiao Lei",
            "Chang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18535",
        "abstract": "The widespread adoption of ChatGPT has raised concerns about its misuse, highlighting the need for robust detection of AI-generated text. Current word-level detectors are vulnerable to paraphrasing or simple prompts (PSP), suffer from biases induced by ChatGPT's word-level patterns (CWP) and training data content, degrade on modified text, and often require large models or online LLM interaction. To tackle these issues, we introduce a novel task to detect both original and PSP-modified AI-generated texts, and propose a lightweight framework that classifies texts based on their internal structure, which remains invariant under word-level changes. Our approach encodes sentence embeddings from pre-trained language models and models their relationships via attention. We employ contrastive learning to mitigate embedding biases from autoregressive generation and incorporate a causal graph with counterfactual methods to isolate structural features from topic-related biases. Experiments on two curated datasets, including abstract comparisons and revised life FAQs, validate the effectiveness of our method.",
        "tags": [
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "92",
        "title": "CCQA: Generating Question from Solution Can Improve Inference-Time Reasoning in SLMs",
        "author": [
            "Jin Young Kim",
            "Ji Won Yoon"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18536",
        "abstract": "Recently, inference-time reasoning strategies have further improved the accuracy of large language models (LLMs), but their effectiveness on smaller models remains unclear. Based on the observation that conventional approaches often fail to improve performance in this context, we propose \\textbf{C}ycle-\\textbf{C}onsistency in \\textbf{Q}uestion \\textbf{A}nswering (CCQA), a novel reasoning method that can be effectively applied to SLMs. Inspired by cycle consistency, CCQA generates a question from each reasoning path and answer, evaluates each by its similarity to the original question, and then selects the candidate solution with the highest similarity score as the final response. Since conventional SLMs struggle to generate accurate questions from their own reasoning paths and answers, we employ a lightweight Flan-T5 model specialized for question generation to support this process efficiently. From the experimental results, it is verified that CCQA consistently outperforms existing state-of-the-art (SOTA) methods across eight models on mathematical and commonsense reasoning benchmarks. Furthermore, our method establishes a new practical baseline for efficient reasoning in SLMs. Source code can be found at https://github.com/scai-research/ccqa_official.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "93",
        "title": "GeoRemover: Removing Objects and Their Causal Visual Artifacts",
        "author": [
            "Zixin Zhu",
            "Haoxiang Li",
            "Xuelu Feng",
            "He Wu",
            "Chunming Qiao",
            "Junsong Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18538",
        "abstract": "Towards intelligent image editing, object removal should eliminate both the target object and its causal visual artifacts, such as shadows and reflections. However, existing image appearance-based methods either follow strictly mask-aligned training and fail to remove these causal effects which are not explicitly masked, or adopt loosely mask-aligned strategies that lack controllability and may unintentionally over-erase other objects. We identify that these limitations stem from ignoring the causal relationship between an object's geometry presence and its visual effects. To address this limitation, we propose a geometry-aware two-stage framework that decouples object removal into (1) geometry removal and (2) appearance rendering. In the first stage, we remove the object directly from the geometry (e.g., depth) using strictly mask-aligned supervision, enabling structure-aware editing with strong geometric constraints. In the second stage, we render a photorealistic RGB image conditioned on the updated geometry, where causal visual effects are considered implicitly as a result of the modified 3D geometry. To guide learning in the geometry removal stage, we introduce a preference-driven objective based on positive and negative sample pairs, encouraging the model to remove objects as well as their causal visual artifacts while avoiding new structural insertions. Extensive experiments demonstrate that our method achieves state-of-the-art performance in removing both objects and their associated artifacts on two popular benchmarks. The code is available at https://github.com/buxiangzhiren/GeoRemover.",
        "tags": [
            "3D",
            "Image Editing"
        ]
    },
    {
        "id": "94",
        "title": "Symphony-MoE: Harmonizing Disparate Pre-trained Models into a Coherent Mixture-of-Experts",
        "author": [
            "Qi Wang",
            "Hanyang Peng",
            "Yue Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18542",
        "abstract": "Mixture-of-Experts (MoE) models enable scalable performance by activating large parameter sets sparsely, minimizing computational overhead. To circumvent the prohibitive cost of training MoEs from scratch, recent work employs upcycling, reusing a single pre-trained dense model by replicating its feed-forward network (FFN) layers into experts. However, this limits expert diversity, as all experts originate from a single pre-trained dense model. This paper addresses this limitation by constructing powerful MoE models using experts sourced from multiple identically-architected but disparate pre-trained models (e.g., Llama2-Chat and Code Llama). A key challenge lies in the fact that these source models occupy disparate, dissonant regions of the parameter space, making direct upcycling prone to severe performance degradation. To overcome this, we propose Symphony-MoE, a novel two-stage framework designed to harmonize these models into a single, coherent expert mixture. First, we establish this harmony in a training-free manner: we construct a shared backbone via a layer-aware fusion strategy and, crucially, alleviate parameter misalignment among experts using activation-based functional alignment. Subsequently, a single lightweight stage of router training coordinates the entire architecture. Experiments demonstrate that our method successfully integrates experts from heterogeneous sources, achieving an MoE model that significantly surpasses baselines in multi-domain tasks and out-of-distribution generalization.",
        "tags": [
            "LLaMA",
            "MoE"
        ]
    },
    {
        "id": "95",
        "title": "Accelerating Network Slice Placement with Multi-Agent Reinforcement Learning",
        "author": [
            "Ioannis Panitsas",
            "Tolga O. Atalay",
            "Dragoslav Stojadinovic",
            "Angelos Stavrou",
            "Leandros Tassiulas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18545",
        "abstract": "Cellular networks are increasingly realized through software-based entities, with core functions deployed as Virtual Network Functions (VNFs) on Commercial-off-the-Shelf (COTS) hardware. Network slicing has emerged as a key enabler of 5G by providing logically isolated Quality of Service (QoS) guarantees for diverse applications. With the adoption of cloud-native infrastructures, the placement of network slices across heterogeneous multi-cloud environments poses new challenges due to variable resource capabilities and slice-specific requirements. This paper introduces a modular framework for autonomous and near-optimal VNF placement based on a disaggregated Multi-Agent Reinforcement Learning (MARL) approach. The framework incorporates real traffic profiles to estimate slice resource demands and employs a MARL-based scheduler to minimize deployment cost while meeting QoS constraints. Experimental evaluation on a multi-cloud testbed shows a 19x speed-up compared to combinatorial optimization, with deployment costs within 7.8% of the optimal. While the method incurs up to 2.42x more QoS violations under high load, the trade-off provides significantly faster decision-making and reduced computational complexity. These results suggest that MARL-based approaches offer a scalable and cost-efficient solution for real-time network slice placement in heterogeneous infrastructures.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "96",
        "title": "HadaSmileNet: Hadamard fusion of handcrafted and deep-learning features for enhancing facial emotion recognition of genuine smiles",
        "author": [
            "Mohammad Junayed Hasan",
            "Nabeel Mohammed",
            "Shafin Rahman",
            "Philipp Koehn"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18550",
        "abstract": "The distinction between genuine and posed emotions represents a fundamental pattern recognition challenge with significant implications for data mining applications in social sciences, healthcare, and human-computer interaction. While recent multi-task learning frameworks have shown promise in combining deep learning architectures with handcrafted D-Marker features for smile facial emotion recognition, these approaches exhibit computational inefficiencies due to auxiliary task supervision and complex loss balancing requirements. This paper introduces HadaSmileNet, a novel feature fusion framework that directly integrates transformer-based representations with physiologically grounded D-Markers through parameter-free multiplicative interactions. Through systematic evaluation of 15 fusion strategies, we demonstrate that Hadamard multiplicative fusion achieves optimal performance by enabling direct feature interactions while maintaining computational efficiency. The proposed approach establishes new state-of-the-art results for deep learning methods across four benchmark datasets: UvA-NEMO (88.7 percent, +0.8), MMI (99.7 percent), SPOS (98.5 percent, +0.7), and BBC (100 percent, +5.0). Comprehensive computational analysis reveals 26 percent parameter reduction and simplified training compared to multi-task alternatives, while feature visualization demonstrates enhanced discriminative power through direct domain knowledge integration. The framework's efficiency and effectiveness make it particularly suitable for practical deployment in multimedia data mining applications that require real-time affective computing capabilities.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "97",
        "title": "Global Minimizers of Sigmoid Contrastive Loss",
        "author": [
            "Kiril Bangachev",
            "Guy Bresler",
            "Iliyas Noman",
            "Yury Polyanskiy"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18552",
        "abstract": "The meta-task of obtaining and aligning representations through contrastive pretraining is steadily gaining importance since its introduction in CLIP and ALIGN. In this paper we theoretically explain the advantages of synchronizing with trainable inverse temperature and bias under the sigmoid loss, as implemented in the recent SigLIP and SigLIP2 models of Google DeepMind. Temperature and bias can drive the loss function to zero for a rich class of configurations that we call $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations. $(\\mathsf{m}, \\mathsf{b}_{\\mathsf{rel}})$-Constellations are a novel combinatorial object related to spherical codes and are parametrized by a margin $\\mathsf{m}$ and relative bias $\\mathsf{b}_{\\mathsf{rel}}$. We use our characterization of constellations to theoretically justify the success of SigLIP on retrieval, to explain the modality gap present in SigLIP, and to identify the necessary dimension for producing high-quality representations. Finally, we propose a reparameterization of the sigmoid loss with explicit relative bias, which improves training dynamics in experiments with synthetic data.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "98",
        "title": "LLMZ+: Contextual Prompt Whitelist Principles for Agentic LLMs",
        "author": [
            "Tom Pawelek",
            "Raj Patel",
            "Charlotte Crowell",
            "Noorbakhsh Amiri",
            "Sudip Mittal",
            "Shahram Rahimi",
            "Andy Perkins"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18557",
        "abstract": "Compared to traditional models, agentic AI represents a highly valuable target for potential attackers as they possess privileged access to data sources and API tools, which are traditionally not incorporated into classical agents. Unlike a typical software application residing in a Demilitarized Zone (DMZ), agentic LLMs consciously rely on nondeterministic behavior of the AI (only defining a final goal, leaving the path selection to LLM). This characteristic introduces substantial security risk to both operational security and information security. Most common existing defense mechanism rely on detection of malicious intent and preventing it from reaching the LLM agent, thus protecting against jailbreak attacks such as prompt injection. In this paper, we present an alternative approach, LLMZ+, which moves beyond traditional detection-based approaches by implementing prompt whitelisting. Through this method, only contextually appropriate and safe messages are permitted to interact with the agentic LLM. By leveraging the specificity of context, LLMZ+ guarantees that all exchanges between external users and the LLM conform to predefined use cases and operational boundaries. Our approach streamlines the security framework, enhances its long-term resilience, and reduces the resources required for sustaining LLM information security. Our empirical evaluation demonstrates that LLMZ+ provides strong resilience against the most common jailbreak prompts. At the same time, legitimate business communications are not disrupted, and authorized traffic flows seamlessly between users and the agentic LLM. We measure the effectiveness of approach using false positive and false negative rates, both of which can be reduced to 0 in our experimental setting.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "99",
        "title": "Event-guided 3D Gaussian Splatting for Dynamic Human and Scene Reconstruction",
        "author": [
            "Xiaoting Yin",
            "Hao Shi",
            "Kailun Yang",
            "Jiajun Zhai",
            "Shangwei Guo",
            "Lin Wang",
            "Kaiwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18566",
        "abstract": "Reconstructing dynamic humans together with static scenes from monocular videos remains difficult, especially under fast motion, where RGB frames suffer from motion blur. Event cameras exhibit distinct advantages, e.g., microsecond temporal resolution, making them a superior sensing choice for dynamic human reconstruction. Accordingly, we present a novel event-guided human-scene reconstruction framework that jointly models human and scene from a single monocular event camera via 3D Gaussian Splatting. Specifically, a unified set of 3D Gaussians carries a learnable semantic attribute; only Gaussians classified as human undergo deformation for animation, while scene Gaussians stay static. To combat blur, we propose an event-guided loss that matches simulated brightness changes between consecutive renderings with the event stream, improving local fidelity in fast-moving regions. Our approach removes the need for external human masks and simplifies managing separate Gaussian sets. On two benchmark datasets, ZJU-MoCap-Blur and MMHPSD-Blur, it delivers state-of-the-art human-scene reconstruction, with notable gains over strong baselines in PSNR/SSIM and reduced LPIPS, especially for high-speed subjects.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "100",
        "title": "Explore the Reinforcement Learning for the LLM based ASR and TTS system",
        "author": [
            "Changfeng Gao",
            "Yabin Li",
            "Keyu An",
            "Zhifu Gao",
            "Zhihao Du",
            "Han Zhao",
            "Xiangang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18569",
        "abstract": "In recent years, large language models (LLMs) have played an important role in automatic speech recognition (ASR) and text-to-speech (TTS) systems. While reinforcement learning (RL) has significantly enhanced LLM performance in text-based tasks, its application to ASR and TTS remains underexplored due to the complexity of training audio-based models. In this study, we propose a lightweight RL framework tailored for audio-based LLMs that can process audio inputs and generate audio outputs. Based on this framework, we evaluate the effectiveness of reinforcement learning on both ASR and TTS tasks. For the ASR task, we experiment with different rule-based reward functions within the Group Relative Policy Optimization (GRPO) framework and investigate the impact of RL data construction. For the TTS task, we compare GRPO with Differentiable Reward Optimization (DiffRO) and further combine the two approaches to achieve improved performance. Our experiments demonstrate that RL can significantly enhance the performance of both ASR and TTS systems, even with limited training data and a small number of optimization steps.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "101",
        "title": "Live-E2T: Real-time Threat Monitoring in Video via Deduplicated Event Reasoning and Chain-of-Thought",
        "author": [
            "Yuhan Wang",
            "Cheng Liu",
            "Zihan Zhao",
            "Weichao Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18571",
        "abstract": "Real-time threat monitoring identifies threatening behaviors in video streams and provides reasoning and assessment of threat events through explanatory text. However, prevailing methodologies, whether based on supervised learning or generative models, struggle to concurrently satisfy the demanding requirements of real-time performance and decision explainability. To bridge this gap, we introduce Live-E2T, a novel framework that unifies these two objectives through three synergistic mechanisms. First, we deconstruct video frames into structured Human-Object-Interaction-Place semantic tuples. This approach creates a compact, semantically focused representation, circumventing the information degradation common in conventional feature compression. Second, an efficient online event deduplication and updating mechanism is proposed to filter spatio-temporal redundancies, ensuring the system's real time responsiveness. Finally, we fine-tune a Large Language Model using a Chain-of-Thought strategy, endow it with the capability for transparent and logical reasoning over event sequences to produce coherent threat assessment reports. Extensive experiments on benchmark datasets, including XD-Violence and UCF-Crime, demonstrate that Live-E2T significantly outperforms state-of-the-art methods in terms of threat detection accuracy, real-time efficiency, and the crucial dimension of explainability.",
        "tags": [
            "CoT",
            "Detection"
        ]
    },
    {
        "id": "102",
        "title": "Interaction Topological Transformer for Multiscale Learning in Porous Materials",
        "author": [
            "Dong Chen",
            "Jian Liu",
            "Chun-Long Chen",
            "Guo-Wei Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18573",
        "abstract": "Porous materials exhibit vast structural diversity and support critical applications in gas storage, separations, and catalysis. However, predictive modeling remains challenging due to the multiscale nature of structure-property relationships, where performance is governed by both local chemical environments and global pore-network topology. These complexities, combined with sparse and unevenly distributed labeled data, hinder generalization across material families. We propose the Interaction Topological Transformer (ITT), a unified data-efficient framework that leverages novel interaction topology to capture materials information across multiple scales and multiple levels, including structural, elemental, atomic, and pairwise-elemental organization. ITT extracts scale-aware features that reflect both compositional and relational structure within complex porous frameworks, and integrates them through a built-in Transformer architecture that supports joint reasoning across scales. Trained using a two-stage strategy, i.e., self-supervised pretraining on 0.6 million unlabeled structures followed by supervised fine-tuning, ITT achieves state-of-the-art, accurate, and transferable predictions for adsorption, transport, and stability properties. This framework provides a principled and scalable path for learning-guided discovery in structurally and chemically diverse porous materials.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "103",
        "title": "The Ranking Blind Spot: Decision Hijacking in LLM-based Text Ranking",
        "author": [
            "Yaoyao Qian",
            "Yifan Zeng",
            "Yuchao Jiang",
            "Chelsi Jain",
            "Huazheng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18575",
        "abstract": "Large Language Models (LLMs) have demonstrated strong performance in information retrieval tasks like passage ranking. Our research examines how instruction-following capabilities in LLMs interact with multi-document comparison tasks, identifying what we term the \"Ranking Blind Spot\", a characteristic of LLM decision processes during comparative evaluation. We analyze how this ranking blind spot affects LLM evaluation systems through two approaches: Decision Objective Hijacking, which alters the evaluation goal in pairwise ranking systems, and Decision Criteria Hijacking, which modifies relevance standards across ranking schemes. These approaches demonstrate how content providers could potentially influence LLM-based ranking systems to affect document positioning. These attacks aim to force the LLM ranker to prefer a specific passage and rank it at the top. Malicious content providers can exploit this weakness, which helps them gain additional exposure by attacking the ranker. In our experiment, We empirically show that the proposed attacks are effective in various LLMs and can be generalized to multiple ranking schemes. We apply these attack to realistic examples to show their effectiveness. We also found stronger LLMs are more vulnerable to these attacks. Our code is available at: https://github.com/blindspotorg/RankingBlindSpot",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "104",
        "title": "LCMF: Lightweight Cross-Modality Mambaformer for Embodied Robotics VQA",
        "author": [
            "Zeyi Kang",
            "Liang He",
            "Yanxin Zhang",
            "Zuheng Ming",
            "Kaixing Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18576",
        "abstract": "Multimodal semantic learning plays a critical role in embodied intelligence, especially when robots perceive their surroundings, understand human instructions, and make intelligent decisions. However, the field faces technical challenges such as effective fusion of heterogeneous data and computational efficiency in resource-constrained environments. To address these challenges, this study proposes the lightweight LCMF cascaded attention framework, introducing a multi-level cross-modal parameter sharing mechanism into the Mamba module. By integrating the advantages of Cross-Attention and Selective parameter-sharing State Space Models (SSMs), the framework achieves efficient fusion of heterogeneous modalities and semantic complementary alignment. Experimental results show that LCMF surpasses existing multimodal baselines with an accuracy of 74.29% in VQA tasks and achieves competitive mid-tier performance within the distribution cluster of Large Language Model Agents (LLM Agents) in EQA video tasks. Its lightweight design achieves a 4.35-fold reduction in FLOPs relative to the average of comparable baselines while using only 166.51M parameters (image-text) and 219M parameters (video-text), providing an efficient solution for Human-Robot Interaction (HRI) applications in resource-constrained scenarios with strong multimodal decision generalization capabilities.",
        "tags": [
            "LLM",
            "Mamba",
            "Robotics",
            "SSMs"
        ]
    },
    {
        "id": "105",
        "title": "Prior-based Noisy Text Data Filtering: Fast and Strong Alternative For Perplexity",
        "author": [
            "Yeongbin Seo",
            "Gayoung Kim",
            "Jaehyung Kim",
            "Jinyoung Yeo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18577",
        "abstract": "As large language models (LLMs) are pretrained on massive web corpora, careful selection of data becomes essential to ensure effective and efficient learning. While perplexity (PPL)-based filtering has shown strong performance, it suffers from drawbacks: substantial time costs and inherent unreliability of the model when handling noisy or out-of-distribution samples. In this work, we propose a simple yet powerful alternative: a prior-based data filtering method that estimates token priors using corpus-level term frequency statistics, inspired by linguistic insights on word roles and lexical density. Our approach filters documents based on the mean and standard deviation of token priors, serving as a fast proxy to PPL while requiring no model inference. Despite its simplicity, the prior-based filter achieves the highest average performance across 20 downstream benchmarks, while reducing time cost by over 1000x compared to PPL-based filtering. We further demonstrate its applicability to symbolic languages such as code and math, and its dynamic adaptability to multilingual corpora without supervision",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "106",
        "title": "The Photographer Eye: Teaching Multimodal Large Language Models to See and Critique like Photographers",
        "author": [
            "Daiqing Qi",
            "Handong Zhao",
            "Jing Shi",
            "Simon Jenni",
            "Yifei Fan",
            "Franck Dernoncourt",
            "Scott Cohen",
            "Sheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18582",
        "abstract": "While editing directly from life, photographers have found it too difficult to see simultaneously both the blue and the sky. Photographer and curator, Szarkowski insightfully revealed one of the notable gaps between general and aesthetic visual understanding: while the former focuses on identifying the factual element in an image (sky), the latter transcends such object identification, viewing it instead as an aesthetic component--a pure color block (blue). Such fundamental distinctions between general (detection, localization, etc.) and aesthetic (color, lighting, composition, etc.) visual understanding present a significant challenge for Multimodal Large Language Models (MLLMs). Although some recent works have made initial explorations, they are often limited to general and basic aesthetic commonsense. As a result, they frequently fall short in real-world scenarios (Fig. 1), which require extensive expertise--including photographic techniques, photo pre/post-processing knowledge, and more, to provide a detailed analysis and description. To fundamentally enhance the aesthetics understanding of MLLMs, we first introduce a novel dataset, PhotoCritique, derived from extensive discussions among professional photographers and enthusiasts, and characterized by the large scale, expertise, and diversity. Then, to better learn visual aesthetics from PhotoCritique, we furthur propose a novel model, PhotoEye, featuring a languageguided multi-view vision fusion mechanism to understand image aesthetics from multiple perspectives. Finally, we present a novel benchmark, PhotoBench, a comprehensive and professional benchmark for aesthetic visual understanding. On existing benchmarks and PhotoBench, our model demonstrates clear advantages over existing models.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "107",
        "title": "DS-Diffusion: Data Style-Guided Diffusion Model for Time-Series Generation",
        "author": [
            "Mingchun Sun",
            "Rongqiang Zhao",
            "Jie Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18584",
        "abstract": "Diffusion models are the mainstream approach for time series generation tasks. However, existing diffusion models for time series generation require retraining the entire framework to introduce specific conditional guidance. There also exists a certain degree of distributional bias between the generated data and the real data, which leads to potential model biases in downstream tasks. Additionally, the complexity of diffusion models and the latent spaces leads to an uninterpretable inference process. To address these issues, we propose the data style-guided diffusion model (DS-Diffusion). In the DS-Diffusion, a diffusion framework based on style-guided kernels is developed to avoid retraining for specific conditions. The time-information based hierarchical denoising mechanism (THD) is developed to reduce the distributional bias between the generated data and the real data. Furthermore, the generated samples can clearly indicate the data style from which they originate. We conduct comprehensive evaluations using multiple public datasets to validate our approach. Experimental results show that, compared to the state-of-the-art model such as ImagenTime, the predictive score and the discriminative score decrease by 5.56% and 61.55%, respectively. The distributional bias between the generated data and the real data is further reduced, the inference process is also more interpretable. Moreover, by eliminating the need to retrain the diffusion model, the flexibility and adaptability of the model to specific conditions are also enhanced.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "108",
        "title": "TsqLoRA: Towards Sensitivity and Quality Low-Rank Adaptation for Efficient Fine-Tuning",
        "author": [
            "Yu Chen",
            "Yifei Han",
            "Long Zhang",
            "Yue Du",
            "Bin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18585",
        "abstract": "Fine-tuning large pre-trained models for downstream tasks has become a fundamental approach in natural language processing. Fully fine-tuning all model parameters is computationally expensive and memory-intensive, especially in resource-constrained environments. Existing parameter-efficient fine-tuning methods reduce the number of trainable parameters but typically overlook the varying sensitivity of different model layers and the importance of training data. In this work, we propose TsqLoRA, a novel method that integrates data-quality-driven selection with sensitivity-aware low-rank adaptation, consisted of two main components: a quality-aware sampling mechanism for selecting the most informative training data, and a dynamic rank allocation module that adjusts the rank of each layer based on its sensitivity to parameter updates. The experimental results demonstrate that TsqLoRA improves fine-tuning efficiency while maintaining or even improving performance on a variety of NLP tasks. Our code will be available at https://github.com/Benjamin-Ricky/TsqLoRA.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "109",
        "title": "VLN-Zero: Rapid Exploration and Cache-Enabled Neurosymbolic Vision-Language Planning for Zero-Shot Transfer in Robot Navigation",
        "author": [
            "Neel P. Bhatt",
            "Yunhao Yang",
            "Rohan Siva",
            "Pranay Samineni",
            "Daniel Milan",
            "Zhangyang Wang",
            "Ufuk Topcu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18592",
        "abstract": "Rapid adaptation in unseen environments is essential for scalable real-world autonomy, yet existing approaches rely on exhaustive exploration or rigid navigation policies that fail to generalize. We present VLN-Zero, a two-phase vision-language navigation framework that leverages vision-language models to efficiently construct symbolic scene graphs and enable zero-shot neurosymbolic navigation. In the exploration phase, structured prompts guide VLM-based search toward informative and diverse trajectories, yielding compact scene graph representations. In the deployment phase, a neurosymbolic planner reasons over the scene graph and environmental observations to generate executable plans, while a cache-enabled execution module accelerates adaptation by reusing previously computed task-location trajectories. By combining rapid exploration, symbolic reasoning, and cache-enabled execution, the proposed framework overcomes the computational inefficiency and poor generalization of prior vision-language navigation methods, enabling robust and scalable decision-making in unseen environments. VLN-Zero achieves 2x higher success rate compared to state-of-the-art zero-shot models, outperforms most fine-tuned baselines, and reaches goal locations in half the time with 55% fewer VLM calls on average compared to state-of-the-art models across diverse environments. Codebase, datasets, and videos for VLN-Zero are available at: https://vln-zero.github.io/.",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "110",
        "title": "Growing with Your Embodied Agent: A Human-in-the-Loop Lifelong Code Generation Framework for Long-Horizon Manipulation Skills",
        "author": [
            "Yuan Meng",
            "Zhenguo Sun",
            "Max Fest",
            "Xukun Li",
            "Zhenshan Bing",
            "Alois Knoll"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18597",
        "abstract": "Large language models (LLMs)-based code generation for robotic manipulation has recently shown promise by directly translating human instructions into executable code, but existing methods remain noisy, constrained by fixed primitives and limited context windows, and struggle with long-horizon tasks. While closed-loop feedback has been explored, corrected knowledge is often stored in improper formats, restricting generalization and causing catastrophic forgetting, which highlights the need for learning reusable skills. Moreover, approaches that rely solely on LLM guidance frequently fail in extremely long-horizon scenarios due to LLMs' limited reasoning capability in the robotic domain, where such issues are often straightforward for humans to identify. To address these challenges, we propose a human-in-the-loop framework that encodes corrections into reusable skills, supported by external memory and Retrieval-Augmented Generation with a hint mechanism for dynamic reuse. Experiments on Ravens, Franka Kitchen, and MetaWorld, as well as real-world settings, show that our framework achieves a 0.93 success rate (up to 27% higher than baselines) and a 42% efficiency improvement in correction rounds. It can robustly solve extremely long-horizon tasks such as \"build a house\", which requires planning over 20 primitives.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "111",
        "title": "Training-Free Multi-Style Fusion Through Reference-Based Adaptive Modulation",
        "author": [
            "Xu Liu",
            "Yibo Lu",
            "Xinxian Wang",
            "Xinyu Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18602",
        "abstract": "We propose Adaptive Multi-Style Fusion (AMSF), a reference-based training-free framework that enables controllable fusion of multiple reference styles in diffusion models. Most of the existing reference-based methods are limited by (a) acceptance of only one style image, thus prohibiting hybrid aesthetics and scalability to more styles, and (b) lack of a principled mechanism to balance several stylistic influences. AMSF mitigates these challenges by encoding all style images and textual hints with a semantic token decomposition module that is adaptively injected into every cross-attention layer of an frozen diffusion model. A similarity-aware re-weighting module then recalibrates, at each denoising step, the attention allocated to every style component, yielding balanced and user-controllable blends without any fine-tuning or external adapters. Both qualitative and quantitative evaluations show that AMSF produces multi-style fusion results that consistently outperform the state-of-the-art approaches, while its fusion design scales seamlessly to two or more styles. These capabilities position AMSF as a practical step toward expressive multi-style generation in diffusion models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "112",
        "title": "Reflect before Act: Proactive Error Correction in Language Models",
        "author": [
            "Qiuhai Zeng",
            "Sarvesh Rajkumar",
            "Di Wang",
            "Narendra Gyanchandani",
            "Wenbo Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18607",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in interactive decision-making tasks, but existing methods often struggle with error accumulation and lack robust self-correction mechanisms. We introduce \"Reflect before Act\" (REBACT), a novel approach that enhances LLM-based decision-making by introducing a critical reflect step prior to taking the next action. This approach allows for immediate error correction, ensuring smooth action path and adaptibity to environment feedback. We evaluate REBACT on three diverse interactive environments: ALFWorld, WebShop, and TextCraft. Our results demonstrate that REBACT significantly outperforms strong baselines, improving success rates by up to 24% on WebShop (achieving 61%), 6.72% on ALFWorld (achieving 98.51%), and 0.5% on TextCraft (achieving 99.5%) using Claude3.5-sonnet as the underlying LLM. Further analysis reveals that REBACT's performance improvements are achieved with only a few modification steps, demonstrating its computational efficiency.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "113",
        "title": "End-to-End Crop Row Navigation via LiDAR-Based Deep Reinforcement Learning",
        "author": [
            "Ana Luiza Mineiro",
            "Francisco Affonso",
            "Marcelo Becker"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18608",
        "abstract": "Reliable navigation in under-canopy agricultural environments remains a challenge due to GNSS unreliability, cluttered rows, and variable lighting. To address these limitations, we present an end-to-end learning-based navigation system that maps raw 3D LiDAR data directly to control commands using a deep reinforcement learning policy trained entirely in simulation. Our method includes a voxel-based downsampling strategy that reduces LiDAR input size by 95.83%, enabling efficient policy learning without relying on labeled datasets or manually designed control interfaces. The policy was validated in simulation, achieving a 100% success rate in straight-row plantations and showing a gradual decline in performance as row curvature increased, tested across varying sinusoidal frequencies and amplitudes.",
        "tags": [
            "3D",
            "RL"
        ]
    },
    {
        "id": "114",
        "title": "PIE: Perception and Interaction Enhanced End-to-End Motion Planning for Autonomous Driving",
        "author": [
            "Chengran Yuan",
            "Zijian Lu",
            "Zhanqi Zhang",
            "Yimin Zhao",
            "Zefan Huang",
            "Shuo Sun",
            "Jiawei Sun",
            "Jiahui Li",
            "Christina Dao Wen Lee",
            "Dongen Li",
            "Marcelo H. Ang Jr"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18609",
        "abstract": "End-to-end motion planning is promising for simplifying complex autonomous driving pipelines. However, challenges such as scene understanding and effective prediction for decision-making continue to present substantial obstacles to its large-scale deployment. In this paper, we present PIE, a pioneering framework that integrates advanced perception, reasoning, and intention modeling to dynamically capture interactions between the ego vehicle and surrounding agents. It incorporates a bidirectional Mamba fusion that addresses data compression losses in multimodal fusion of camera and LiDAR inputs, alongside a novel reasoning-enhanced decoder integrating Mamba and Mixture-of-Experts to facilitate scene-compliant anchor selection and optimize adaptive trajectory inference. PIE adopts an action-motion interaction module to effectively utilize state predictions of surrounding agents to refine ego planning. The proposed framework is thoroughly validated on the NAVSIM benchmark. PIE, without using any ensemble and data augmentation techniques, achieves an 88.9 PDM score and 85.6 EPDM score, surpassing the performance of prior state-of-the-art methods. Comprehensive quantitative and qualitative analyses demonstrate that PIE is capable of reliably generating feasible and high-quality ego trajectories.",
        "tags": [
            "Mamba",
            "MoE"
        ]
    },
    {
        "id": "115",
        "title": "SINGER: An Onboard Generalist Vision-Language Navigation Policy for Drones",
        "author": [
            "Maximilian Adang",
            "JunEn Low",
            "Ola Shorinwa",
            "Mac Schwager"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18610",
        "abstract": "Large vision-language models have driven remarkable progress in open-vocabulary robot policies, e.g., generalist robot manipulation policies, that enable robots to complete complex tasks specified in natural language. Despite these successes, open-vocabulary autonomous drone navigation remains an unsolved challenge due to the scarcity of large-scale demonstrations, real-time control demands of drones for stabilization, and lack of reliable external pose estimation modules. In this work, we present SINGER for language-guided autonomous drone navigation in the open world using only onboard sensing and compute. To train robust, open-vocabulary navigation policies, SINGER leverages three central components: (i) a photorealistic language-embedded flight simulator with minimal sim-to-real gap using Gaussian Splatting for efficient data generation, (ii) an RRT-inspired multi-trajectory generation expert for collision-free navigation demonstrations, and these are used to train (iii) a lightweight end-to-end visuomotor policy for real-time closed-loop control. Through extensive hardware flight experiments, we demonstrate superior zero-shot sim-to-real transfer of our policy to unseen environments and unseen language-conditioned goal objects. When trained on ~700k-1M observation action pairs of language conditioned visuomotor data and deployed on hardware, SINGER outperforms a velocity-controlled semantic guidance baseline by reaching the query 23.33% more on average, and maintains the query in the field of view 16.67% more on average, with 10% fewer collisions.",
        "tags": [
            "Gaussian Splatting",
            "Pose Estimation",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "116",
        "title": "Flow marching for a generative PDE foundation model",
        "author": [
            "Zituo Chen",
            "Sili Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18611",
        "abstract": "Pretraining on large-scale collections of PDE-governed spatiotemporal trajectories has recently shown promise for building generalizable models of dynamical systems. Yet most existing PDE foundation models rely on deterministic Transformer architectures, which lack generative flexibility for many science and engineering applications. We propose Flow Marching, an algorithm that bridges neural operator learning with flow matching motivated by an analysis of error accumulation in physical dynamical systems, and we build a generative PDE foundation model on top of it. By jointly sampling the noise level and the physical time step between adjacent states, the model learns a unified velocity field that transports a noisy current state toward its clean successor, reducing long-term rollout drift while enabling uncertainty-aware ensemble generations. Alongside this core algorithm, we introduce a Physics-Pretrained Variational Autoencoder (P2VAE) to embed physical states into a compact latent space, and an efficient Flow Marching Transformer (FMT) that combines a diffusion-forcing scheme with latent temporal pyramids, achieving up to 15x greater computational efficiency than full-length video diffusion models and thereby enabling large-scale pretraining at substantially reduced cost. We curate a corpus of ~2.5M trajectories across 12 distinct PDE families and train suites of P2VAEs and FMTs at multiple scales. On downstream evaluation, we benchmark on unseen Kolmogorov turbulence with few-shot adaptation, demonstrate long-term rollout stability over deterministic counterparts, and present uncertainty-stratified ensemble results, highlighting the importance of generative PDE foundation models for real-world applications.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "Transformer"
        ]
    },
    {
        "id": "117",
        "title": "Prompt-Guided Dual Latent Steering for Inversion Problems",
        "author": [
            "Yichen Wu",
            "Xu Liu",
            "Chenxuan Zhao",
            "Xinyu Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18619",
        "abstract": "Inverting corrupted images into the latent space of diffusion models is challenging. Current methods, which encode an image into a single latent vector, struggle to balance structural fidelity with semantic accuracy, leading to reconstructions with semantic drift, such as blurred details or incorrect attributes. To overcome this, we introduce Prompt-Guided Dual Latent Steering (PDLS), a novel, training-free framework built upon Rectified Flow models for their stable inversion paths. PDLS decomposes the inversion process into two complementary streams: a structural path to preserve source integrity and a semantic path guided by a prompt. We formulate this dual guidance as an optimal control problem and derive a closed-form solution via a Linear Quadratic Regulator (LQR). This controller dynamically steers the generative trajectory at each step, preventing semantic drift while ensuring the preservation of fine detail without costly, per-image optimization. Extensive experiments on FFHQ-1K and ImageNet-1K under various inversion tasks, including Gaussian deblurring, motion deblurring, super-resolution and freeform inpainting, demonstrate that PDLS produces reconstructions that are both more faithful to the original image and better aligned with the semantic information than single-latent baselines.",
        "tags": [
            "Deblurring",
            "Diffusion",
            "Inpainting",
            "Rectified Flow",
            "Super Resolution"
        ]
    },
    {
        "id": "118",
        "title": "Scalable Evaluation for Audio Identification via Synthetic Latent Fingerprint Generation",
        "author": [
            "Aditya Bhattacharjee",
            "Marco Pasini",
            "Emmanouil Benetos"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18620",
        "abstract": "The evaluation of audio fingerprinting at a realistic scale is limited by the scarcity of large public music databases. We present an audio-free approach that synthesises latent fingerprints which approximate the distribution of real fingerprints. Our method trains a Rectified Flow model on embeddings extracted by pre-trained neural audio fingerprinting systems. The synthetic fingerprints generated using our system act as realistic distractors and enable the simulation of retrieval performance at a large scale without requiring additional audio. We assess the fidelity of synthetic fingerprints by comparing the distributions to real data. We further benchmark the retrieval performances across multiple state-of-the-art audio fingerprinting frameworks by augmenting real reference databases with synthetic distractors, and show that the scaling trends obtained with synthetic distractors closely track those obtained with real distractors. Finally, we scale the synthetic distractor database to model retrieval performance for very large databases, providing a practical metric of system scalability that does not depend on access to audio corpora.",
        "tags": [
            "Rectified Flow"
        ]
    },
    {
        "id": "119",
        "title": "HyperAdapt: Simple High-Rank Adaptation",
        "author": [
            "Abel Gurung",
            "Joseph Campbell"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18629",
        "abstract": "Foundation models excel across diverse tasks, but adapting them to specialized applications often requires fine-tuning, an approach that is memory and compute-intensive. Parameter-efficient fine-tuning (PEFT) methods mitigate this by updating only a small subset of weights. In this paper, we introduce HyperAdapt, a parameter-efficient fine-tuning method that significantly reduces the number of trainable parameters compared to state-of-the-art methods like LoRA. Specifically, HyperAdapt adapts a pre-trained weight matrix by applying row- and column-wise scaling through diagonal matrices, thereby inducing a high-rank update while requiring only $n+m$ trainable parameters for an $n \\times m$ matrix. Theoretically, we establish an upper bound on the rank of HyperAdapt's updates, and empirically, we confirm that it consistently induces high-rank transformations across model layers. Experiments on GLUE, arithmetic reasoning, and commonsense reasoning benchmarks with models up to 14B parameters demonstrate that HyperAdapt matches or nearly matches the performance of full fine-tuning and state-of-the-art PEFT methods while using orders of magnitude fewer trainable parameters.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "120",
        "title": "Generalizable Domain Adaptation for Sim-and-Real Policy Co-Training",
        "author": [
            "Shuo Cheng",
            "Liqian Ma",
            "Zhenyang Chen",
            "Ajay Mandlekar",
            "Caelan Garrett",
            "Danfei Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18631",
        "abstract": "Behavior cloning has shown promise for robot manipulation, but real-world demonstrations are costly to acquire at scale. While simulated data offers a scalable alternative, particularly with advances in automated demonstration generation, transferring policies to the real world is hampered by various simulation and real domain gaps. In this work, we propose a unified sim-and-real co-training framework for learning generalizable manipulation policies that primarily leverages simulation and only requires a few real-world demonstrations. Central to our approach is learning a domain-invariant, task-relevant feature space. Our key insight is that aligning the joint distributions of observations and their corresponding actions across domains provides a richer signal than aligning observations (marginals) alone. We achieve this by embedding an Optimal Transport (OT)-inspired loss within the co-training framework, and extend this to an Unbalanced OT framework to handle the imbalance between abundant simulation data and limited real-world examples. We validate our method on challenging manipulation tasks, showing it can leverage abundant simulation data to achieve up to a 30% improvement in the real-world success rate and even generalize to scenarios seen only in simulation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "121",
        "title": "A Good Plan is Hard to Find: Aligning Models with Preferences is Misaligned with What Helps Users",
        "author": [
            "Nishant Balepur",
            "Matthew Shu",
            "Yoo Yeon Sung",
            "Seraphina Goldfarb-Tarrant",
            "Shi Feng",
            "Fumeng Yang",
            "Rachel Rudinger",
            "Jordan Lee Boyd-Graber"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18632",
        "abstract": "To assist users in complex tasks, LLMs generate plans: step-by-step instructions towards a goal. While alignment methods aim to ensure LLM plans are helpful, they train (RLHF) or evaluate (ChatbotArena) on what users prefer, assuming this reflects what helps them. We test this with Planorama: an interface where 126 users answer 300 multi-step questions with LLM plans. We get 4388 plan executions and 5584 comparisons to measure plan helpfulness (QA success) and user preferences on plans, and recreate the setup in agents and reward models to see if they simulate or prefer what helps users. We expose: 1) user/model preferences and agent success do not accurately predict which plans help users, so common alignment feedback can misalign with helpfulness; 2) this gap is not due to user-specific preferences, as users are similarly successful when using plans they prefer/disprefer; 3) surface-level cues like brevity and question similarity strongly link to preferences, but such biases fail to predict helpfulness. In all, we argue aligning helpful LLMs needs feedback from real user interactions, not just preferences of what looks helpful, so we discuss the plan NLP researchers can execute to solve this problem.",
        "tags": [
            "LLM",
            "RLHF"
        ]
    },
    {
        "id": "122",
        "title": "Understanding-in-Generation: Reinforcing Generative Capability of Unified Model via Infusing Understanding into Generation",
        "author": [
            "Yuanhuiyi Lyu",
            "Chi Kit Wong",
            "Chenfei Liao",
            "Lutao Jiang",
            "Xu Zheng",
            "Zexin Lu",
            "Linfeng Zhang",
            "Xuming Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18639",
        "abstract": "Recent works have made notable advancements in enhancing unified models for text-to-image generation through the Chain-of-Thought (CoT). However, these reasoning methods separate the processes of understanding and generation, which limits their ability to guide the reasoning of unified models in addressing the deficiencies of their generative capabilities. To this end, we propose a novel reasoning framework for unified models, Understanding-in-Generation (UiG), which harnesses the robust understanding capabilities of unified models to reinforce their performance in image generation. The core insight of our UiG is to integrate generative guidance by the strong understanding capabilities during the reasoning process, thereby mitigating the limitations of generative abilities. To achieve this, we introduce \"Image Editing\" as a bridge to infuse understanding into the generation process. Initially, we verify the generated image and incorporate the understanding of unified models into the editing instructions. Subsequently, we enhance the generated image step by step, gradually infusing the understanding into the generation process. Our UiG framework demonstrates a significant performance improvement in text-to-image generation over existing text-to-image reasoning methods, e.g., a 3.92% gain on the long prompt setting of the TIIF benchmark. The project code: https://github.com/QC-LY/UiG",
        "tags": [
            "CoT",
            "Image Editing",
            "Text-to-Image"
        ]
    },
    {
        "id": "123",
        "title": "BloomIntent: Automating Search Evaluation with LLM-Generated Fine-Grained User Intents",
        "author": [
            "Yoonseo Choi",
            "Eunhye Kim",
            "Hyunwoo Kim",
            "Donghyun Park",
            "Honggu Lee",
            "Jinyoung Kim",
            "Juho Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18641",
        "abstract": "If 100 people issue the same search query, they may have 100 different goals. While existing work on user-centric AI evaluation highlights the importance of aligning systems with fine-grained user intents, current search evaluation methods struggle to represent and assess this diversity. We introduce BloomIntent, a user-centric search evaluation method that uses user intents as the evaluation unit. BloomIntent first generates a set of plausible, fine-grained search intents grounded on taxonomies of user attributes and information-seeking intent types. Then, BloomIntent provides an automated evaluation of search results against each intent powered by large language models. To support practical analysis, BloomIntent clusters semantically similar intents and summarizes evaluation outcomes in a structured interface. With three technical evaluations, we showed that BloomIntent generated fine-grained, evaluable, and realistic intents and produced scalable assessments of intent-level satisfaction that achieved 72% agreement with expert evaluators. In a case study (N=4), we showed that BloomIntent supported search specialists in identifying intents for ambiguous queries, uncovering underserved user needs, and discovering actionable insights for improving search experiences. By shifting from query-level to intent-level evaluation, BloomIntent reimagines how search systems can be assessed -- not only for performance but for their ability to serve a multitude of user goals.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "124",
        "title": "Do You Need Proprioceptive States in Visuomotor Policies?",
        "author": [
            "Juntu Zhao",
            "Wenbo Lu",
            "Di Zhang",
            "Yufeng Liu",
            "Yushen Liang",
            "Tianluo Zhang",
            "Yifeng Cao",
            "Junyuan Xie",
            "Yingdong Hu",
            "Shengjie Wang",
            "Junliang Guo",
            "Dequan Wang",
            "Yang Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18644",
        "abstract": "Imitation-learning-based visuomotor policies have been widely used in robot manipulation, where both visual observations and proprioceptive states are typically adopted together for precise control. However, in this study, we find that this common practice makes the policy overly reliant on the proprioceptive state input, which causes overfitting to the training trajectories and results in poor spatial generalization. On the contrary, we propose the State-free Policy, removing the proprioceptive state input and predicting actions only conditioned on visual observations. The State-free Policy is built in the relative end-effector action space, and should ensure the full task-relevant visual observations, here provided by dual wide-angle wrist cameras. Empirical results demonstrate that the State-free policy achieves significantly stronger spatial generalization than the state-based policy: in real-world tasks such as pick-and-place, challenging shirt-folding, and complex whole-body manipulation, spanning multiple robot embodiments, the average success rate improves from 0\\% to 85\\% in height generalization and from 6\\% to 64\\% in horizontal generalization. Furthermore, they also show advantages in data efficiency and cross-embodiment adaptation, enhancing their practicality for real-world deployment.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "125",
        "title": "SPiDR: A Simple Approach for Zero-Shot Safety in Sim-to-Real Transfer",
        "author": [
            "Yarden As",
            "Chengrui Qu",
            "Benjamin Unger",
            "Dongho Kang",
            "Max van der Hart",
            "Laixi Shi",
            "Stelian Coros",
            "Adam Wierman",
            "Andreas Krause"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18648",
        "abstract": "Safety remains a major concern for deploying reinforcement learning (RL) in real-world applications. Simulators provide safe, scalable training environments, but the inevitable sim-to-real gap introduces additional safety concerns, as policies must satisfy constraints in real-world conditions that differ from simulation. To address this challenge, robust safe RL techniques offer principled methods, but are often incompatible with standard scalable training pipelines. In contrast, domain randomization, a simple and popular sim-to-real technique, stands out as a promising alternative, although it often results in unsafe behaviors in practice. We present SPiDR, short for Sim-to-real via Pessimistic Domain Randomization -- a scalable algorithm with provable guarantees for safe sim-to-real transfer. SPiDR uses domain randomization to incorporate the uncertainty about the sim-to-real gap into the safety constraints, making it versatile and highly compatible with existing training pipelines. Through extensive experiments on sim-to-sim benchmarks and two distinct real-world robotic platforms, we demonstrate that SPiDR effectively ensures safety despite the sim-to-real gap while maintaining strong performance.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "126",
        "title": "Analyzing Uncertainty of LLM-as-a-Judge: Interval Evaluations with Conformal Prediction",
        "author": [
            "Huanxin Sheng",
            "Xinyi Liu",
            "Hangfeng He",
            "Jieyu Zhao",
            "Jian Kang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18658",
        "abstract": "LLM-as-a-judge has become a promising paradigm for using large language models (LLMs) to evaluate natural language generation (NLG), but the uncertainty of its evaluation remains underexplored. This lack of reliability may limit its deployment in many applications. This work presents the first framework to analyze the uncertainty by offering a prediction interval of LLM-based scoring via conformal prediction. Conformal prediction constructs continuous prediction intervals from a single evaluation run, and we design an ordinal boundary adjustment for discrete rating tasks. We also suggest a midpoint-based score within the interval as a low-bias alternative to raw model score and weighted average. We perform extensive experiments and analysis, which show that conformal prediction can provide valid prediction interval with coverage guarantees. We also explore the usefulness of interval midpoint and judge reprompting for better judgment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "127",
        "title": "Agentic AutoSurvey: Let LLMs Survey LLMs",
        "author": [
            "Yixin Liu",
            "Yonghui Wu",
            "Denghui Zhang",
            "Lichao Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18661",
        "abstract": "The exponential growth of scientific literature poses unprecedented challenges for researchers attempting to synthesize knowledge across rapidly evolving fields. We present \\textbf{Agentic AutoSurvey}, a multi-agent framework for automated survey generation that addresses fundamental limitations in existing approaches. Our system employs four specialized agents (Paper Search Specialist, Topic Mining \\& Clustering, Academic Survey Writer, and Quality Evaluator) working in concert to generate comprehensive literature surveys with superior synthesis quality. Through experiments on six representative LLM research topics from COLM 2024 categories, we demonstrate that our multi-agent approach achieves significant improvements over existing baselines, scoring 8.18/10 compared to AutoSurvey's 4.77/10. The multi-agent architecture processes 75--443 papers per topic (847 total across six topics) while targeting high citation coverage (often $\\geq$80\\% on 75--100-paper sets; lower on very large sets such as RLHF) through specialized agent orchestration. Our 12-dimension evaluation captures organization, synthesis integration, and critical analysis beyond basic metrics. These findings demonstrate that multi-agent architectures represent a meaningful advancement for automated literature survey generation in rapidly evolving scientific domains.",
        "tags": [
            "LLM",
            "RLHF"
        ]
    },
    {
        "id": "128",
        "title": "TERAG: Token-Efficient Graph-Based Retrieval-Augmented Generation",
        "author": [
            "Qiao Xiao",
            "Hong Ting Tsang",
            "Jiaxin Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18667",
        "abstract": "Graph-based Retrieval-augmented generation (RAG) has become a widely studied approach for improving the reasoning, accuracy, and factuality of Large Language Models. However, many existing graph-based RAG systems overlook the high cost associated with LLM token usage during graph construction, hindering large-scale adoption. To address this, we propose TERAG, a simple yet effective framework designed to build informative graphs at a significantly lower cost. Inspired by HippoRAG, we incorporate Personalized PageRank (PPR) during the retrieval phase, and we achieve at least 80% of the accuracy of widely used graph-based RAG methods while consuming only 3%-11% of the output tokens.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "129",
        "title": "N2M: Bridging Navigation and Manipulation by Learning Pose Preference from Rollout",
        "author": [
            "Kaixin Chai",
            "Hyunjun Lee",
            "Joseph J. Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18671",
        "abstract": "In mobile manipulation, the manipulation policy has strong preferences for initial poses where it is executed. However, the navigation module focuses solely on reaching the task area, without considering which initial pose is preferable for downstream manipulation. To address this misalignment, we introduce N2M, a transition module that guides the robot to a preferable initial pose after reaching the task area, thereby substantially improving task success rates. N2M features five key advantages: (1) reliance solely on ego-centric observation without requiring global or historical information; (2) real-time adaptation to environmental changes; (3) reliable prediction with high viewpoint robustness; (4) broad applicability across diverse tasks, manipulation policies, and robot hardware; and (5) remarkable data efficiency and generalizability. We demonstrate the effectiveness of N2M through extensive simulation and real-world experiments. In the PnPCounterToCab task, N2M improves the averaged success rate from 3% with the reachability-based baseline to 54%. Furthermore, in the Toybox Handover task, N2M provides reliable predictions even in unseen environments with only 15 data samples, showing remarkable data efficiency and generalizability.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "130",
        "title": "NaviSense: A Multimodal Assistive Mobile application for Object Retrieval by Persons with Visual Impairment",
        "author": [
            "Ajay Narayanan Sridhar",
            "Fuli Qiao",
            "Nelson Daniel Troncoso Aldas",
            "Yanpei Shi",
            "Mehrdad Mahdavi",
            "Laurent Itti",
            "Vijaykrishnan Narayanan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18672",
        "abstract": "People with visual impairments often face significant challenges in locating and retrieving objects in their surroundings. Existing assistive technologies present a trade-off: systems that offer precise guidance typically require pre-scanning or support only fixed object categories, while those with open-world object recognition lack spatial feedback for reaching the object. To address this gap, we introduce 'NaviSense', a mobile assistive system that combines conversational AI, vision-language models, augmented reality (AR), and LiDAR to support open-world object detection with real-time audio-haptic guidance. Users specify objects via natural language and receive continuous spatial feedback to navigate toward the target without needing prior setup. Designed with insights from a formative study and evaluated with 12 blind and low-vision participants, NaviSense significantly reduced object retrieval time and was preferred over existing tools, demonstrating the value of integrating open-world perception with precise, accessible guidance.",
        "tags": [
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "131",
        "title": "3D Flow Diffusion Policy: Visuomotor Policy Learning via Generating Flow in 3D Space",
        "author": [
            "Sangjun Noh",
            "Dongwoo Nam",
            "Kangmin Kim",
            "Geonhyup Lee",
            "Yeonguk Yu",
            "Raeyoung Kang",
            "Kyoobin Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18676",
        "abstract": "Learning robust visuomotor policies that generalize across diverse objects and interaction dynamics remains a central challenge in robotic manipulation. Most existing approaches rely on direct observation-to-action mappings or compress perceptual inputs into global or object-centric features, which often overlook localized motion cues critical for precise and contact-rich manipulation. We present 3D Flow Diffusion Policy (3D FDP), a novel framework that leverages scene-level 3D flow as a structured intermediate representation to capture fine-grained local motion cues. Our approach predicts the temporal trajectories of sampled query points and conditions action generation on these interaction-aware flows, implemented jointly within a unified diffusion architecture. This design grounds manipulation in localized dynamics while enabling the policy to reason about broader scene-level consequences of actions. Extensive experiments on the MetaWorld benchmark show that 3D FDP achieves state-of-the-art performance across 50 tasks, particularly excelling on medium and hard settings. Beyond simulation, we validate our method on eight real-robot tasks, where it consistently outperforms prior baselines in contact-rich and non-prehensile scenarios. These results highlight 3D flow as a powerful structural prior for learning generalizable visuomotor policies, supporting the development of more robust and versatile robotic manipulation. Robot demonstrations, additional results, and code can be found at https://sites.google.com/view/3dfdp/home.",
        "tags": [
            "3D",
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "132",
        "title": "Harnessing Multimodal Large Language Models for Personalized Product Search with Query-aware Refinement",
        "author": [
            "Beibei Zhang",
            "Yanan Lu",
            "Ruobing Xie",
            "Zongyi Li",
            "Siyuan Xing",
            "Tongwei Ren",
            "Fen Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18682",
        "abstract": "Personalized product search (PPS) aims to retrieve products relevant to the given query considering user preferences within their purchase histories. Since large language models (LLM) exhibit impressive potential in content understanding and reasoning, current methods explore to leverage LLM to comprehend the complicated relationships among user, query and product to improve the search performance of PPS. Despite the progress, LLM-based PPS solutions merely take textual contents into consideration, neglecting multimodal contents which play a critical role for product search. Motivated by this, we propose a novel framework, HMPPS, for \\textbf{H}arnessing \\textbf{M}ultimodal large language models (MLLM) to deal with \\textbf{P}ersonalized \\textbf{P}roduct \\textbf{S}earch based on multimodal contents. Nevertheless, the redundancy and noise in PPS input stand for a great challenge to apply MLLM for PPS, which not only misleads MLLM to generate inaccurate search results but also increases the computation expense of MLLM. To deal with this problem, we additionally design two query-aware refinement modules for HMPPS: 1) a perspective-guided summarization module that generates refined product descriptions around core perspectives relevant to search query, reducing noise and redundancy within textual contents; and 2) a two-stage training paradigm that introduces search query for user history filtering based on multimodal representations, capturing precise user preferences and decreasing the inference cost. Extensive experiments are conducted on four public datasets to demonstrate the effectiveness of HMPPS. Furthermore, HMPPS is deployed on an online search system with billion-level daily active users and achieves an evident gain in A/B testing.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "133",
        "title": "LEAF-Mamba: Local Emphatic and Adaptive Fusion State Space Model for RGB-D Salient Object Detection",
        "author": [
            "Lanhu Wu",
            "Zilin Gao",
            "Hao Fei",
            "Mong-Li Lee",
            "Wynne Hsu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18683",
        "abstract": "RGB-D salient object detection (SOD) aims to identify the most conspicuous objects in a scene with the incorporation of depth cues. Existing methods mainly rely on CNNs, limited by the local receptive fields, or Vision Transformers that suffer from the cost of quadratic complexity, posing a challenge in balancing performance and computational efficiency. Recently, state space models (SSM), Mamba, have shown great potential for modeling long-range dependency with linear complexity. However, directly applying SSM to RGB-D SOD may lead to deficient local semantics as well as the inadequate cross-modality fusion. To address these issues, we propose a Local Emphatic and Adaptive Fusion state space model (LEAF-Mamba) that contains two novel components: 1) a local emphatic state space module (LE-SSM) to capture multi-scale local dependencies for both modalities. 2) an SSM-based adaptive fusion module (AFM) for complementary cross-modality interaction and reliable cross-modality integration. Extensive experiments demonstrate that the LEAF-Mamba consistently outperforms 16 state-of-the-art RGB-D SOD methods in both efficacy and efficiency. Moreover, our method can achieve excellent performance on the RGB-T SOD task, proving a powerful generalization ability.",
        "tags": [
            "Detection",
            "Mamba",
            "SSMs"
        ]
    },
    {
        "id": "134",
        "title": "Query-Centric Diffusion Policy for Generalizable Robotic Assembly",
        "author": [
            "Ziyi Xu",
            "Haohong Lin",
            "Shiqi Liu",
            "Ding Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18686",
        "abstract": "The robotic assembly task poses a key challenge in building generalist robots due to the intrinsic complexity of part interactions and the sensitivity to noise perturbations in contact-rich settings. The assembly agent is typically designed in a hierarchical manner: high-level multi-part reasoning and low-level precise control. However, implementing such a hierarchical policy is challenging in practice due to the mismatch between high-level skill queries and low-level execution. To address this, we propose the Query-centric Diffusion Policy (QDP), a hierarchical framework that bridges high-level planning and low-level control by utilizing queries comprising objects, contact points, and skill information. QDP introduces a query-centric mechanism that identifies task-relevant components and uses them to guide low-level policies, leveraging point cloud observations to improve the policy's robustness. We conduct comprehensive experiments on the FurnitureBench in both simulation and real-world settings, demonstrating improved performance in skill precision and long-horizon success rate. In the challenging insertion and screwing tasks, QDP improves the skill-wise success rate by over 50% compared to baselines without structured queries.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "135",
        "title": "An overview of neural architectures for self-supervised audio representation learning from masked spectrograms",
        "author": [
            "Sarthak Yadav",
            "Sergios Theodoridis",
            "Zheng-Hua Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18691",
        "abstract": "In recent years, self-supervised learning has amassed significant interest for training deep neural representations without labeled data. One such self-supervised learning approach is masked spectrogram modeling, where the objective is to learn semantically rich contextual representations by predicting removed or hidden portions of the input audio spectrogram. With the Transformer neural architecture at its core, masked spectrogram modeling has emerged as the prominent approach for learning general purpose audio representations, a.k.a. audio foundation models. Meanwhile, addressing the issues of the Transformer architecture, in particular the underlying Scaled Dot-product Attention operation, which scales quadratically with input sequence length, has led to renewed interest in recurrent sequence modeling approaches. Among them, Selective structured state space models (such as Mamba) and extended Long Short-Term Memory (xLSTM) are the two most promising approaches which have experienced widespread adoption. While the body of work on these two topics continues to grow, there is currently a lack of an adequate overview encompassing the intersection of these topics. In this paper, we present a comprehensive overview of the aforementioned research domains, covering masked spectrogram modeling and the previously mentioned neural sequence modeling architectures, Mamba and xLSTM. Further, we compare Transformers, Mamba and xLSTM based masked spectrogram models in a unified, reproducible framework on ten diverse downstream audio classification tasks, which will help interested readers to make informed decisions regarding suitability of the evaluated approaches to adjacent applications.",
        "tags": [
            "Mamba",
            "SSMs",
            "Transformer"
        ]
    },
    {
        "id": "136",
        "title": "Lightweight Vision Transformer with Window and Spatial Attention for Food Image Classification",
        "author": [
            "Xinle Gao",
            "Linghui Ye",
            "Zhiyong Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18692",
        "abstract": "With the rapid development of society and continuous advances in science and technology, the food industry increasingly demands higher production quality and efficiency. Food image classification plays a vital role in enabling automated quality control on production lines, supporting food safety supervision, and promoting intelligent agricultural production. However, this task faces challenges due to the large number of parameters and high computational complexity of Vision Transformer models. To address these issues, we propose a lightweight food image classification algorithm that integrates a Window Multi-Head Attention Mechanism (WMHAM) and a Spatial Attention Mechanism (SAM). The WMHAM reduces computational cost by capturing local and global contextual features through efficient window partitioning, while the SAM adaptively emphasizes key spatial regions to improve discriminative feature representation. Experiments conducted on the Food-101 and Vireo Food-172 datasets demonstrate that our model achieves accuracies of 95.24% and 94.33%, respectively, while significantly reducing parameters and FLOPs compared with baseline methods. These results confirm that the proposed approach achieves an effective balance between computational efficiency and classification performance, making it well-suited for deployment in resource-constrained environments.",
        "tags": [
            "SAM",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "137",
        "title": "AGSwap: Overcoming Category Boundaries in Object Fusion via Adaptive Group Swapping",
        "author": [
            "Zedong Zhang",
            "Ying Tai",
            "Jianjun Qian",
            "Jian Yang",
            "Jun Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18699",
        "abstract": "Fusing cross-category objects to a single coherent object has gained increasing attention in text-to-image (T2I) generation due to its broad applications in virtual reality, digital media, film, and gaming. However, existing methods often produce biased, visually chaotic, or semantically inconsistent results due to overlapping artifacts and poor integration. Moreover, progress in this field has been limited by the absence of a comprehensive benchmark dataset. To address these problems, we propose \\textbf{Adaptive Group Swapping (AGSwap)}, a simple yet highly effective approach comprising two key components: (1) Group-wise Embedding Swapping, which fuses semantic attributes from different concepts through feature manipulation, and (2) Adaptive Group Updating, a dynamic optimization mechanism guided by a balance evaluation score to ensure coherent synthesis. Additionally, we introduce \\textbf{Cross-category Object Fusion (COF)}, a large-scale, hierarchically structured dataset built upon ImageNet-1K and WordNet. COF includes 95 superclasses, each with 10 subclasses, enabling 451,250 unique fusion pairs. Extensive experiments demonstrate that AGSwap outperforms state-of-the-art compositional T2I methods, including GPT-Image-1 using simple and complex prompts.",
        "tags": [
            "GPT",
            "Text-to-Image"
        ]
    },
    {
        "id": "138",
        "title": "Enhancing Automatic Chord Recognition through LLM Chain-of-Thought Reasoning",
        "author": [
            "Chih-Cheng Chang",
            "Bo-Yu Chen",
            "Lu-Rong Chen",
            "Li Su"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18700",
        "abstract": "Music Information Retrieval (MIR) encompasses a broad range of computational techniques for analyzing and understanding musical content, with recent deep learning advances driving substantial improvements. Building upon these advances, this paper explores how large language models (LLMs) can serve as an integrative bridge to connect and integrate information from multiple MIR tools, with a focus on enhancing automatic chord recognition performance. We present a novel approach that positions text-based LLMs as intelligent coordinators that process and integrate outputs from diverse state-of-the-art MIR tools-including music source separation, key detection, chord recognition, and beat tracking. Our method converts audio-derived musical information into textual representations, enabling LLMs to perform reasoning and correction specifically for chord recognition tasks. We design a 5-stage chain-of-thought framework that allows GPT-4o to systematically analyze, compare, and refine chord recognition results by leveraging music-theoretical knowledge to integrate information across different MIR components. Experimental evaluation on three datasets demonstrates consistent improvements across multiple evaluation metrics, with overall accuracy gains of 1-2.77% on the MIREX metric. Our findings demonstrate that LLMs can effectively function as integrative bridges in MIR pipelines, opening new directions for multi-tool coordination in music information retrieval tasks.",
        "tags": [
            "CoT",
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "139",
        "title": "Autonomous Data Agents: A New Opportunity for Smart Data",
        "author": [
            "Yanjie Fu",
            "Dongjie Wang",
            "Wangyang Ying",
            "Xiangliang Zhang",
            "Huan Liu",
            "Jian Pei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18710",
        "abstract": "As data continues to grow in scale and complexity, preparing, transforming, and analyzing it remains labor-intensive, repetitive, and difficult to scale. Since data contains knowledge and AI learns knowledge from it, the alignment between AI and data is essential. However, data is often not structured in ways that are optimal for AI utilization. Moreover, an important question arises: how much knowledge can we pack into data through intensive data operations? Autonomous data agents (DataAgents), which integrate LLM reasoning with task decomposition, action reasoning and grounding, and tool calling, can autonomously interpret data task descriptions, decompose tasks into subtasks, reason over actions, ground actions into python code or tool calling, and execute operations. Unlike traditional data management and engineering tools, DataAgents dynamically plan workflows, call powerful tools, and adapt to diverse data tasks at scale. This report argues that DataAgents represent a paradigm shift toward autonomous data-to-knowledge systems. DataAgents are capable of handling collection, integration, preprocessing, selection, transformation, reweighing, augmentation, reprogramming, repairs, and retrieval. Through these capabilities, DataAgents transform complex and unstructured data into coherent and actionable knowledge. We first examine why the convergence of agentic AI and data-to-knowledge systems has emerged as a critical trend. We then define the concept of DataAgents and discuss their architectural design, training strategies, as well as the new skills and capabilities they enable. Finally, we call for concerted efforts to advance action workflow optimization, establish open datasets and benchmark ecosystems, safeguard privacy, balance efficiency with scalability, and develop trustworthy DataAgent guardrails to prevent malicious actions.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "140",
        "title": "MemOrb: A Plug-and-Play Verbal-Reinforcement Memory Layer for E-Commerce Customer Service",
        "author": [
            "Yizhe Huang",
            "Yang Liu",
            "Ruiyu Zhao",
            "Xiaolong Zhong",
            "Xingming Yue",
            "Ling Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18713",
        "abstract": "Large Language Model-based agents(LLM-based agents) are increasingly deployed in customer service, yet they often forget across sessions, repeat errors, and lack mechanisms for continual self-improvement. This makes them unreliable in dynamic settings where stability and consistency are critical. To better evaluate these properties, we emphasize two indicators: task success rate as a measure of overall effectiveness, and consistency metrics such as Pass$^k$ to capture reliability across multiple trials. To address the limitations of existing approaches, we propose MemOrb, a lightweight and plug-and-play verbal reinforcement memory layer that distills multi-turn interactions into compact strategy reflections. These reflections are stored in a shared memory bank and retrieved to guide decision-making, without requiring any fine-tuning. Experiments show that MemOrb significantly improves both success rate and stability, achieving up to a 63 percentage-point gain in multi-turn success rate and delivering more consistent performance across repeated trials. Our results demonstrate that structured reflection is a powerful mechanism for enhancing long-term reliability of frozen LLM agents in customer service scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "141",
        "title": "A Generalized Bisimulation Metric of State Similarity between Markov Decision Processes: From Theoretical Propositions to Applications",
        "author": [
            "Zhenyu Tao",
            "Wei Xu",
            "Xiaohu You"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18714",
        "abstract": "The bisimulation metric (BSM) is a powerful tool for computing state similarities within a Markov decision process (MDP), revealing that states closer in BSM have more similar optimal value functions. While BSM has been successfully utilized in reinforcement learning (RL) for tasks like state representation learning and policy exploration, its application to multiple-MDP scenarios, such as policy transfer, remains challenging. Prior work has attempted to generalize BSM to pairs of MDPs, but a lack of rigorous analysis of its mathematical properties has limited further theoretical progress. In this work, we formally establish a generalized bisimulation metric (GBSM) between pairs of MDPs, which is rigorously proven with the three fundamental properties: GBSM symmetry, inter-MDP triangle inequality, and the distance bound on identical state spaces. Leveraging these properties, we theoretically analyse policy transfer, state aggregation, and sampling-based estimation in MDPs, obtaining explicit bounds that are strictly tighter than those derived from the standard BSM. Additionally, GBSM provides a closed-form sample complexity for estimation, improving upon existing asymptotic results based on BSM. Numerical results validate our theoretical findings and demonstrate the effectiveness of GBSM in multi-MDP scenarios.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "142",
        "title": "What Makes You Unique? Attribute Prompt Composition for Object Re-Identification",
        "author": [
            "Yingquan Wang",
            "Pingping Zhang",
            "Chong Sun",
            "Dong Wang",
            "Huchuan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18715",
        "abstract": "Object Re-IDentification (ReID) aims to recognize individuals across non-overlapping camera views. While recent advances have achieved remarkable progress, most existing models are constrained to either single-domain or cross-domain scenarios, limiting their real-world applicability. Single-domain models tend to overfit to domain-specific features, whereas cross-domain models often rely on diverse normalization strategies that may inadvertently suppress identity-specific discriminative cues. To address these limitations, we propose an Attribute Prompt Composition (APC) framework, which exploits textual semantics to jointly enhance discrimination and generalization. Specifically, we design an Attribute Prompt Generator (APG) consisting of a Semantic Attribute Dictionary (SAD) and a Prompt Composition Module (PCM). SAD is an over-complete attribute dictionary to provide rich semantic descriptions, while PCM adaptively composes relevant attributes from SAD to generate discriminative attribute-aware features. In addition, motivated by the strong generalization ability of Vision-Language Models (VLM), we propose a Fast-Slow Training Strategy (FSTS) to balance ReID-specific discrimination and generalizable representation learning. Specifically, FSTS adopts a Fast Update Stream (FUS) to rapidly acquire ReID-specific discriminative knowledge and a Slow Update Stream (SUS) to retain the generalizable knowledge inherited from the pre-trained VLM. Through a mutual interaction, the framework effectively focuses on ReID-relevant features while mitigating overfitting. Extensive experiments on both conventional and Domain Generalized (DG) ReID datasets demonstrate that our framework surpasses state-of-the-art methods, exhibiting superior performances in terms of both discrimination and generalization. The source code is available at https://github.com/AWangYQ/APC.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "143",
        "title": "LLM-Enhanced Self-Evolving Reinforcement Learning for Multi-Step E-Commerce Payment Fraud Risk Detection",
        "author": [
            "Bo Qu",
            "Zhurong Wang",
            "Daisuke Yagi",
            "Zhen Xu",
            "Yang Zhao",
            "Yinan Shan",
            "Frank Zahradnik"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18719",
        "abstract": "This paper presents a novel approach to e-commerce payment fraud detection by integrating reinforcement learning (RL) with Large Language Models (LLMs). By framing transaction risk as a multi-step Markov Decision Process (MDP), RL optimizes risk detection across multiple payment stages. Crafting effective reward functions, essential for RL model success, typically requires significant human expertise due to the complexity and variability in design. LLMs, with their advanced reasoning and coding capabilities, are well-suited to refine these functions, offering improvements over traditional methods. Our approach leverages LLMs to iteratively enhance reward functions, achieving better fraud detection accuracy and demonstrating zero-shot capability. Experiments with real-world data confirm the effectiveness, robustness, and resilience of our LLM-enhanced RL framework through long-term evaluations, underscoring the potential of LLMs in advancing industrial RL applications.",
        "tags": [
            "Detection",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "144",
        "title": "Dual Iterative Learning Control for Multiple-Input Multiple-Output Dynamics with Validation in Robotic Systems",
        "author": [
            "Jan-Hendrik Ewering",
            "Alessandro Papa",
            "Simon F.G. Ehlers",
            "Thomas Seel",
            "Michael Meindl"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18723",
        "abstract": "Solving motion tasks autonomously and accurately is a core ability for intelligent real-world systems. To achieve genuine autonomy across multiple systems and tasks, key challenges include coping with unknown dynamics and overcoming the need for manual parameter tuning, which is especially crucial in complex Multiple-Input Multiple-Output (MIMO) systems.\nThis paper presents MIMO Dual Iterative Learning Control (DILC), a novel data-driven iterative learning scheme for simultaneous tracking control and model learning, without requiring any prior system knowledge or manual parameter tuning. The method is designed for repetitive MIMO systems and integrates seamlessly with established iterative learning control methods. We provide monotonic convergence conditions for both reference tracking error and model error in linear time-invariant systems.\nThe DILC scheme -- rapidly and autonomously -- solves various motion tasks in high-fidelity simulations of an industrial robot and in multiple nonlinear real-world MIMO systems, without requiring model knowledge or manually tuning the algorithm. In our experiments, many reference tracking tasks are solved within 10-20 trials, and even complex motions are learned in less than 100 iterations. We believe that, because of its rapid and autonomous learning capabilities, DILC has the potential to serve as an efficient building block within complex learning frameworks for intelligent real-world systems.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "145",
        "title": "MECap-R1: Emotion-aware Policy with Reinforcement Learning for Multimodal Emotion Captioning",
        "author": [
            "Haoqin Sun",
            "Chenyang Lyu",
            "Xiangyu Kong",
            "Shiwan Zhao",
            "Jiaming Zhou",
            "Hui Wang",
            "Aobo Kong",
            "Jinghua Zhao",
            "Longyue Wang",
            "Weihua Luo",
            "Kaifu Zhang",
            "Yong Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18729",
        "abstract": "Speech Emotion Captioning (SEC) has emerged as a notable research direction. The inherent complexity of emotional content in human speech makes it challenging for traditional discrete classification methods to provide an adequate representation. Consequently, utilizing natural language to describe speech emotions presents a novel avenue for more effectively capturing and expressing affect. In this paper, we propose MECap-R1, a pioneering emotion-aware policy with reinforcement learning for multimodal emotion captioning. By employing Group Relative Policy Optimization with emotion-aware reward (Emo-GRPO), the framework precisely captures the emotion and semantic features, thereby addressing the shortcomings of rigid rules in handling the dynamic and flexible nature of captions. Experimental results on the EmotionTalk dataset demonstrate that MECap-R1 performs well in generating emotion descriptions and achieves substantial gains in both accuracy and diversity.",
        "tags": [
            "GRPO",
            "RL"
        ]
    },
    {
        "id": "146",
        "title": "Knowledge Transfer from Interaction Learning",
        "author": [
            "Yilin Gao",
            "Kangyi Chen",
            "Zhongxing Peng",
            "Hengjie Lu",
            "Shugong Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18733",
        "abstract": "Current visual foundation models (VFMs) face a fundamental limitation in transferring knowledge from vision language models (VLMs), while VLMs excel at modeling cross-modal interactions through unified representation spaces, existing VFMs predominantly adopt result-oriented paradigms that neglect the underlying interaction processes. This representational discrepancy hinders effective knowledge transfer and limits generalization across diverse vision tasks. We propose Learning from Interactions (LFI), a cognitive-inspired framework that addresses this gap by explicitly modeling visual understanding as an interactive process. Our key insight is that capturing the dynamic interaction patterns encoded in pre-trained VLMs enables more faithful and efficient knowledge transfer to VFMs. The approach centers on two technical innovations, Interaction Queries, which maintain persistent relational structures across network layers, and interaction-based supervision, derived from the cross-modal attention mechanisms of VLMs. Comprehensive experiments demonstrate consistent improvements across multiple benchmarks, achieving 3.3 and 1.6mAP/2.4AP absolute gains on TinyImageNet classification and COCO detection/segmentation respectively, with minimal parameter overhead and faster convergence. The framework particularly excels in cross-domain settings, delivering 2.4 and 9.3 zero-shot improvements on PACS and VLCS. Human evaluations further confirm its cognitive alignment, outperforming result-oriented methods by 2.7 times in semantic consistency metrics.",
        "tags": [
            "Detection",
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "147",
        "title": "Learning Obstacle Avoidance using Double DQN for Quadcopter Navigation",
        "author": [
            "Nishant Doshi",
            "Amey Sutvani",
            "Sanket Gujar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18734",
        "abstract": "One of the challenges faced by Autonomous Aerial Vehicles is reliable navigation through urban environments. Factors like reduction in precision of Global Positioning System (GPS), narrow spaces and dynamically moving obstacles make the path planning of an aerial robot a complicated task. One of the skills required for the agent to effectively navigate through such an environment is to develop an ability to avoid collisions using information from onboard depth sensors. In this paper, we propose Reinforcement Learning of a virtual quadcopter robot agent equipped with a Depth Camera to navigate through a simulated urban environment.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "148",
        "title": "HyPSAM: Hybrid Prompt-driven Segment Anything Model for RGB-Thermal Salient Object Detection",
        "author": [
            "Ruichao Hou",
            "Xingyuan Li",
            "Tongwei Ren",
            "Dongming Zhou",
            "Gangshan Wu",
            "Jinde Cao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18738",
        "abstract": "RGB-thermal salient object detection (RGB-T SOD) aims to identify prominent objects by integrating complementary information from RGB and thermal modalities. However, learning the precise boundaries and complete objects remains challenging due to the intrinsic insufficient feature fusion and the extrinsic limitations of data scarcity. In this paper, we propose a novel hybrid prompt-driven segment anything model (HyPSAM), which leverages the zero-shot generalization capabilities of the segment anything model (SAM) for RGB-T SOD. Specifically, we first propose a dynamic fusion network (DFNet) that generates high-quality initial saliency maps as visual prompts. DFNet employs dynamic convolution and multi-branch decoding to facilitate adaptive cross-modality interaction, overcoming the limitations of fixed-parameter kernels and enhancing multi-modal feature representation. Moreover, we propose a plug-and-play refinement network (P2RNet), which serves as a general optimization strategy to guide SAM in refining saliency maps by using hybrid prompts. The text prompt ensures reliable modality input, while the mask and box prompts enable precise salient object localization. Extensive experiments on three public datasets demonstrate that our method achieves state-of-the-art performance. Notably, HyPSAM has remarkable versatility, seamlessly integrating with different RGB-T SOD methods to achieve significant performance gains, thereby highlighting the potential of prompt engineering in this field. The code and results of our method are available at: https://github.com/milotic233/HyPSAM.",
        "tags": [
            "Detection",
            "SAM"
        ]
    },
    {
        "id": "149",
        "title": "Global-Recent Semantic Reasoning on Dynamic Text-Attributed Graphs with Large Language Models",
        "author": [
            "Yunan Wang",
            "Jianxin Li",
            "Ziwei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18742",
        "abstract": "Dynamic Text-Attribute Graphs (DyTAGs), characterized by time-evolving graph interactions and associated text attributes, are prevalent in real-world applications. Existing methods, such as Graph Neural Networks (GNNs) and Large Language Models (LLMs), mostly focus on static TAGs. Extending these existing methods to DyTAGs is challenging as they largely neglect the recent-global temporal semantics: the recent semantic dependencies among interaction texts and the global semantic evolution of nodes over time. Furthermore, applying LLMs to the abundant and evolving text in DyTAGs faces efficiency issues. To tackle these challenges, we propose Dynamic Global-Recent Adaptive Semantic Processing (DyGRASP), a novel method that leverages LLMs and temporal GNNs to efficiently and effectively reason on DyTAGs. Specifically, we first design a node-centric implicit reasoning method together with a sliding window mechanism to efficiently capture recent temporal semantics. In addition, to capture global semantic dynamics of nodes, we leverage explicit reasoning with tailored prompts and an RNN-like chain structure to infer long-term semantics. Lastly, we intricately integrate the recent and global temporal semantics as well as the dynamic graph structural information using updating and merging layers. Extensive experiments on DyTAG benchmarks demonstrate DyGRASP's superiority, achieving up to 34% improvement in Hit@10 for destination node retrieval task. Besides, DyGRASP exhibits strong generalization across different temporal GNNs and LLMs.",
        "tags": [
            "LLM",
            "RNN"
        ]
    },
    {
        "id": "150",
        "title": "COLT: Enhancing Video Large Language Models with Continual Tool Usage",
        "author": [
            "Yuyang Liu",
            "Xinyuan Shi",
            "Bang Yang",
            "Peilin Zhou",
            "Jiahua Dong",
            "Long Chen",
            "Ian Reid",
            "Xiaondan Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18754",
        "abstract": "The success of Large Language Models (LLMs) has significantly propelled the research of video understanding. To harvest the benefits of well-trained expert models (i.e., tools), video LLMs prioritize the exploration of tool usage capabilities. Existing methods either prompt closed-source LLMs or employ the instruction tuning paradigm for tool-use fine-tuning. These methods, however, assume an established repository of fixed tools and struggle to generalize to real-world environments where tool data is perpetually evolving and streaming in. To this end, we propose to enhance open-source video LLMs with COntinuaL Tool usage (termed COLT), which automatically acquires tool-use ability in a successive tool stream without suffering 'catastrophic forgetting' of the past learned tools. Specifically, our COLT incorporates a learnable tool codebook as a tool-specific memory system. Then relevant tools are dynamically selected based on the similarity between user instruction and tool features within the codebook. To unleash the tool usage potential of video LLMs, we collect a video-centric tool-use instruction tuning dataset VideoToolBench. Extensive experiments on both previous video LLM benchmarks and the tool-use-specific VideoToolBench dataset demonstrate the state-of-the-art performance of our proposed COLT.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "151",
        "title": "MV-UMI: A Scalable Multi-View Interface for Cross-Embodiment Learning",
        "author": [
            "Omar Rayyan",
            "John Abanes",
            "Mahmoud Hafez",
            "Anthony Tzes",
            "Fares Abu-Dakka"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18757",
        "abstract": "Recent advances in imitation learning have shown great promise for developing robust robot manipulation policies from demonstrations. However, this promise is contingent on the availability of diverse, high-quality datasets, which are not only challenging and costly to collect but are often constrained to a specific robot embodiment. Portable handheld grippers have recently emerged as intuitive and scalable alternatives to traditional robotic teleoperation methods for data collection. However, their reliance solely on first-person view wrist-mounted cameras often creates limitations in capturing sufficient scene contexts. In this paper, we present MV-UMI (Multi-View Universal Manipulation Interface), a framework that integrates a third-person perspective with the egocentric camera to overcome this limitation. This integration mitigates domain shifts between human demonstration and robot deployment, preserving the cross-embodiment advantages of handheld data-collection devices. Our experimental results, including an ablation study, demonstrate that our MV-UMI framework improves performance in sub-tasks requiring broad scene understanding by approximately 47% across 3 tasks, confirming the effectiveness of our approach in expanding the range of feasible manipulation tasks that can be learned using handheld gripper systems, without compromising the cross-embodiment advantages inherent to such systems.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "152",
        "title": "FixingGS: Enhancing 3D Gaussian Splatting via Training-Free Score Distillation",
        "author": [
            "Zhaorui Wang",
            "Yi Gu",
            "Deming Zhou",
            "Renjing Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18759",
        "abstract": "Recently, 3D Gaussian Splatting (3DGS) has demonstrated remarkable success in 3D reconstruction and novel view synthesis. However, reconstructing 3D scenes from sparse viewpoints remains highly challenging due to insufficient visual information, which results in noticeable artifacts persisting across the 3D representation. To address this limitation, recent methods have resorted to generative priors to remove artifacts and complete missing content in under-constrained areas. Despite their effectiveness, these approaches struggle to ensure multi-view consistency, resulting in blurred structures and implausible details. In this work, we propose FixingGS, a training-free method that fully exploits the capabilities of the existing diffusion model for sparse-view 3DGS reconstruction enhancement. At the core of FixingGS is our distillation approach, which delivers more accurate and cross-view coherent diffusion priors, thereby enabling effective artifact removal and inpainting. In addition, we propose an adaptive progressive enhancement scheme that further refines reconstructions in under-constrained regions. Extensive experiments demonstrate that FixingGS surpasses existing state-of-the-art methods with superior visual quality and reconstruction performance. Our code will be released publicly.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Inpainting"
        ]
    },
    {
        "id": "153",
        "title": "Security smells in infrastructure as code: a taxonomy update beyond the seven sins",
        "author": [
            "Aicha War",
            "Serge L.B. Nikiema",
            "Jordan Samhi",
            "Jacques Klein",
            "Tegawende F. Bissyande"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18761",
        "abstract": "Infrastructure as Code (IaC) has become essential for modern software management, yet security flaws in IaC scripts can have severe consequences, as exemplified by the recurring exploits of Cloud Web Services. Prior work has recognized the need to build a precise taxonomy of security smells in IaC scripts as a first step towards developing approaches to improve IaC security. This first effort led to the unveiling of seven sins, limited by the focus on a single IaC tool as well as by the extensive, and potentially biased, manual effort that was required. We propose, in our work, to revisit this taxonomy: first, we extend the study of IaC security smells to a more diverse dataset with scripts associated with seven popular IaC tools, including Terraform, Ansible, Chef, Puppet, Pulumi, Saltstack, and Vagrant; second, we bring in some automation for the analysis by relying on an LLM. While we leverage LLMs for initial pattern processing, all taxonomic decisions underwent systematic human validation and reconciliation with established security standards. Our study yields a comprehensive taxonomy of 62 security smell categories, significantly expanding beyond the previously known seven. We demonstrate actionability by implementing new security checking rules within linters for seven popular IaC tools, often achieving 1.00 precision score. Our evolution study of security smells in GitHub projects reveals that these issues persist for extended periods, likely due to inadequate detection and mitigation tools. This work provides IaC practitioners with insights for addressing common security smells and systematically adopting DevSecOps practices to build safer infrastructure code.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "154",
        "title": "When Long Helps Short: How Context Length in Supervised Fine-tuning Affects Behavior of Large Language Models",
        "author": [
            "Yingming Zheng",
            "Hanqi Li",
            "Kai Yu",
            "Lu Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18762",
        "abstract": "Large language models (LLMs) have achieved impressive performance across natural language processing (NLP) tasks. As real-world applications increasingly demand longer context windows, continued pretraining and supervised fine-tuning (SFT) on long-context data has become a common approach. While the effects of data length in continued pretraining have been extensively studied, their implications for SFT remain unclear. In this work, we systematically investigate how SFT data length influences LLM behavior on short-context tasks. Counterintuitively, we find that long-context SFT improves short-context performance, contrary to the commonly observed degradation from long-context pretraining. To uncover the underlying mechanisms of this phenomenon, we first decouple and analyze two key components, Multi-Head Attention (MHA) and Feed-Forward Network (FFN), and show that both independently benefit from long-context SFT. We further study their interaction and reveal a knowledge preference bias: long-context SFT promotes contextual knowledge, while short-context SFT favors parametric knowledge, making exclusive reliance on long-context SFT suboptimal. Finally, we demonstrate that hybrid training mitigates this bias, offering explainable guidance for fine-tuning LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "155",
        "title": "Bi-VLM: Pushing Ultra-Low Precision Post-Training Quantization Boundaries in Vision-Language Models",
        "author": [
            "Xijun Wang",
            "Junyun Huang",
            "Rayyan Abdalla",
            "Chengyuan Zhang",
            "Ruiqi Xian",
            "Dinesh Manocha"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18763",
        "abstract": "We address the critical gap between the computational demands of vision-language models and the possible ultra-low-bit weight precision (bitwidth $\\leq2$ bits) we can use for higher efficiency. Our work is motivated by the substantial computational cost and memory requirements of VLMs, which restrict their applicability in hardware-constrained environments. We propose Bi-VLM, which separates model weights non-uniformly based on the Gaussian quantiles. Our formulation groups the model weights into outlier (salient) and multiple inlier (unsalient) subsets, ensuring that each subset contains a proportion of weights corresponding to its quantile in the distribution. We propose a saliency-aware hybrid quantization algorithm and use it to quantize weights by imposing different constraints on the scaler and binary matrices based on the saliency metric and compression objective. We have evaluated our approach on different VLMs. For the language model part of the VLM, our Bi-VLM outperforms the SOTA by 3%-47% on the visual question answering task in terms of four different benchmarks and three different models. For the overall VLM, our Bi-VLM outperforms the SOTA by 4%-45%. We also perform token pruning on the quantized models and observe that there is redundancy of image tokens 90% - 99% in the quantized models. This helps us to further prune the visual tokens to improve efficiency.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "156",
        "title": "Experience Scaling: Post-Deployment Evolution For Large Language Models",
        "author": [
            "Xingkun Yin",
            "Kaibin Huang",
            "Dong In Kim",
            "Hongyang Du"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18771",
        "abstract": "Scaling model size, training data, and compute power have driven advances in large language models (LLMs), but these approaches are reaching saturation as human-generated text is exhausted and further gains diminish. We propose experience scaling, a framework for continuous post-deployment evolution for LLMs through autonomous interaction with the environment and collaborative sharing of accumulated experience. The framework captures raw interactions, distills them into compact, reusable knowledge, and periodically refines stored content to preserve relevance and efficiency. We validate the framework in simulated real-world scenarios involving generalization to previously unseen but related tasks, repetitive queries, and over-saturated knowledge stores. Across all settings, experience scaling improves accuracy, sustains performance over time, and maintains gains when applied to novel situations. These results demonstrate that structured post-deployment learning can extend LLM capabilities beyond the limits of static human-generated data, offering a scalable path for continued intelligence progress.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "157",
        "title": "AECBench: A Hierarchical Benchmark for Knowledge Evaluation of Large Language Models in the AEC Field",
        "author": [
            "Chen Liang",
            "Zhaoqi Huang",
            "Haofen Wang",
            "Fu Chai",
            "Chunying Yu",
            "Huanhuan Wei",
            "Zhengjie Liu",
            "Yanpeng Li",
            "Hongjun Wang",
            "Ruifeng Luo",
            "Xianzhong Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18776",
        "abstract": "Large language models (LLMs), as a novel information technology, are seeing increasing adoption in the Architecture, Engineering, and Construction (AEC) field. They have shown their potential to streamline processes throughout the building lifecycle. However, the robustness and reliability of LLMs in such a specialized and safety-critical domain remain to be evaluated. To address this challenge, this paper establishes AECBench, a comprehensive benchmark designed to quantify the strengths and limitations of current LLMs in the AEC domain. The benchmark defines 23 representative tasks within a five-level cognition-oriented evaluation framework encompassing Knowledge Memorization, Understanding, Reasoning, Calculation, and Application. These tasks were derived from authentic AEC practice, with scope ranging from codes retrieval to specialized documents generation. Subsequently, a 4,800-question dataset encompassing diverse formats, including open-ended questions, was crafted primarily by engineers and validated through a two-round expert review. Furthermore, an LLM-as-a-Judge approach was introduced to provide a scalable and consistent methodology for evaluating complex, long-form responses leveraging expert-derived rubrics. Through the evaluation of nine LLMs, a clear performance decline across five cognitive levels was revealed. Despite demonstrating proficiency in foundational tasks at the Knowledge Memorization and Understanding levels, the models showed significant performance deficits, particularly in interpreting knowledge from tables in building codes, executing complex reasoning and calculation, and generating domain-specific documents. Consequently, this study lays the groundwork for future research and development aimed at the robust and reliable integration of LLMs into safety-critical engineering practices.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "158",
        "title": "VGGT-DP: Generalizable Robot Control via Vision Foundation Models",
        "author": [
            "Shijia Ge",
            "Yinxin Zhang",
            "Shuzhao Xie",
            "Weixiang Zhang",
            "Mingcai Zhou",
            "Zhi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18778",
        "abstract": "Visual imitation learning frameworks allow robots to learn manipulation skills from expert demonstrations. While existing approaches mainly focus on policy design, they often neglect the structure and capacity of visual encoders, limiting spatial understanding and generalization. Inspired by biological vision systems, which rely on both visual and proprioceptive cues for robust control, we propose VGGT-DP, a visuomotor policy framework that integrates geometric priors from a pretrained 3D perception model with proprioceptive feedback. We adopt the Visual Geometry Grounded Transformer (VGGT) as the visual encoder and introduce a proprioception-guided visual learning strategy to align perception with internal robot states, improving spatial grounding and closed-loop control. To reduce inference latency, we design a frame-wise token reuse mechanism that compacts multi-view tokens into an efficient spatial representation. We further apply random token pruning to enhance policy robustness and reduce overfitting. Experiments on challenging MetaWorld tasks show that VGGT-DP significantly outperforms strong baselines such as DP and DP3, particularly in precision-critical and long-horizon scenarios.",
        "tags": [
            "3D",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "159",
        "title": "Human-Interpretable Uncertainty Explanations for Point Cloud Registration",
        "author": [
            "Johannes A. Gaus",
            "Loris Schneider",
            "Yitian Shi",
            "Jongseok Lee",
            "Rania Rayyes",
            "Rudolph Triebel"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18786",
        "abstract": "In this paper, we address the point cloud registration problem, where well-known methods like ICP fail under uncertainty arising from sensor noise, pose-estimation errors, and partial overlap due to occlusion. We develop a novel approach, Gaussian Process Concept Attribution (GP-CA), which not only quantifies registration uncertainty but also explains it by attributing uncertainty to well-known sources of errors in registration problems. Our approach leverages active learning to discover new uncertainty sources in the wild by querying informative instances. We validate GP-CA on three publicly available datasets and in our real-world robot experiment. Extensive ablations substantiate our design choices. Our approach outperforms other state-of-the-art methods in terms of runtime, high sample-efficiency with active learning, and high accuracy. Our real-world experiment clearly demonstrates its applicability. Our video also demonstrates that GP-CA enables effective failure-recovery behaviors, yielding more robust robotic perception.",
        "tags": [
            "Pose Estimation",
            "Robotics"
        ]
    },
    {
        "id": "160",
        "title": "The AGNTCY Agent Directory Service: Architecture and Implementation",
        "author": [
            "Luca Muscariello",
            "Vijoy Pandey",
            "Ramiz Polic"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18787",
        "abstract": "The Agent Directory Service (ADS) is a distributed directory for the discovery of AI agent capabilities, metadata, and provenance. It leverages content-addressed storage, hierarchical taxonomies, and cryptographic signing to enable efficient, verifiable, and multi-dimensional discovery across heterogeneous Multi-Agent Systems (MAS). Built on the Open Agentic Schema Framework (OASF), ADS decouples capability indexing from content location through a two-level mapping realized over a Kademlia-based Distributed Hash Table (DHT). It reuses mature OCI / ORAS infrastructure for artifact distribution, integrates Sigstore for provenance, and supports schema-driven extensibility for emerging agent modalities (LLM prompt agents, MCP servers, A2A-enabled components). This paper formalizes the architectural model, describes storage and discovery layers, explains security and performance properties, and positions ADS within the broader landscape of emerging agent registry and interoperability initiatives.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "161",
        "title": "Detection of security smells in IaC scripts through semantics-aware code and language processing",
        "author": [
            "Aicha War",
            "Adnan A. Rawass",
            "Abdoul K. Kabore",
            "Jordan Samhi",
            "Jacques Klein",
            "Tegawende F. Bissyande"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18790",
        "abstract": "Infrastructure as Code (IaC) automates the provisioning and management of IT infrastructure through scripts and tools, streamlining software deployment. Prior studies have shown that IaC scripts often contain recurring security misconfigurations, and several detection and mitigation approaches have been proposed. Most of these rely on static analysis, using statistical code representations or Machine Learning (ML) classifiers to distinguish insecure configurations from safe code.\nIn this work, we introduce a novel approach that enhances static analysis with semantic understanding by jointly leveraging natural language and code representations. Our method builds on two complementary ML models: CodeBERT, to capture semantics across code and text, and LongFormer, to represent long IaC scripts without losing contextual information. We evaluate our approach on misconfiguration datasets from two widely used IaC tools, Ansible and Puppet. To validate its effectiveness, we conduct two ablation studies (removing code text from the natural language input and truncating scripts to reduce context) and compare against four large language models (LLMs) and prior work. Results show that semantic enrichment substantially improves detection, raising precision and recall from 0.46 and 0.79 to 0.92 and 0.88 on Ansible, and from 0.55 and 0.97 to 0.87 and 0.75 on Puppet, respectively.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "162",
        "title": "Beyond the Leaderboard: Understanding Performance Disparities in Large Language Models via Model Diffing",
        "author": [
            "Sabri Boughorbel",
            "Fahim Dalvi",
            "Nadir Durrani",
            "Majd Hawasly"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18792",
        "abstract": "As fine-tuning becomes the dominant paradigm for improving large language models (LLMs), understanding what changes during this process is increasingly important. Traditional benchmarking often fails to explain why one model outperforms another. In this work, we use model diffing, a mechanistic interpretability approach, to analyze the specific capability differences between Gemma-2-9b-it and a SimPO-enhanced variant. Using crosscoders, we identify and categorize latent representations that differentiate the two models. We find that SimPO acquired latent concepts predominantly enhance safety mechanisms (+32.8%), multilingual capabilities (+43.8%), and instruction-following (+151.7%), while its additional training also reduces emphasis on model self-reference (-44.1%) and hallucination management (-68.5%). Our analysis shows that model diffing can yield fine-grained insights beyond leaderboard metrics, attributing performance gaps to concrete mechanistic capabilities. This approach offers a transparent and targeted framework for comparing LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "163",
        "title": "Application Management in C-ITS: Orchestrating Demand-Driven Deployments and Reconfigurations",
        "author": [
            "Lukas Zanger",
            "Bastian Lampe",
            "Lennart Reiher",
            "Lutz Eckstein"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18793",
        "abstract": "Vehicles are becoming increasingly automated and interconnected, enabling the formation of cooperative intelligent transport systems (C-ITS) and the use of offboard services. As a result, cloud-native techniques, such as microservices and container orchestration, play an increasingly important role in their operation. However, orchestrating applications in a large-scale C-ITS poses unique challenges due to the dynamic nature of the environment and the need for efficient resource utilization. In this paper, we present a demand-driven application management approach that leverages cloud-native techniques - specifically Kubernetes - to address these challenges. Taking into account the demands originating from different entities within the C-ITS, the approach enables the automation of processes, such as deployment, reconfiguration, update, upgrade, and scaling of microservices. Executing these processes on demand can, for example, reduce computing resource consumption and network traffic. A demand may include a request for provisioning an external supporting service, such as a collective environment model. The approach handles changing and new demands by dynamically reconciling them through our proposed application management framework built on Kubernetes and the Robot Operating System (ROS 2). We demonstrate the operation of our framework in the C-ITS use case of collective environment perception and make the source code of the prototypical framework publicly available at https://github.com/ika-rwth-aachen/application_manager .",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "164",
        "title": "SR-Eval: Evaluating LLMs on Code Generation under Stepwise Requirement Refinement",
        "author": [
            "Zexun Zhan",
            "Shuzheng Gao",
            "Ruida Hu",
            "Cuiyun Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18808",
        "abstract": "Large language models (LLMs) have achieved remarkable progress in code generation. However, existing benchmarks mainly formalize the task as a static, single-turn problem, overlooking the stepwise requirement changes and iterative workflows in real-world software development. This mismatch limits the understanding of how well LLMs can support real-world development workflows. Constructing such iterative benchmarks is challenging due to the lack of public interaction traces and the difficulty of creating discriminative, turn-specific test cases.\nTo bridge this gap, we present SR-Eval, a benchmark specifically designed to assess LLMs on iterative code generation under Stepwise requirements Refinement. SR-Eval spans both function-level and repository-level tasks in Python and Java, enabling fine-grained and progressive evaluation across evolving requirements. The construction of SR-Eval follows a carefully designed pipeline that first leverages a multi-agent-based requirement generation method to simulate the development process and recover the multi-round interaction process from final requirements, then employs a semantic-aware discriminative test case generation component to ensure discriminative and consistent evaluation at each turn. SR-Eval comprises 443 multi-turn tasks and 1,857 questions at both function and repository levels. Using SR-Eval, we evaluate 11 representative LLMs with three prompting strategies that simulate different usage patterns. Results show that iterative code generation under stepwise requirement refinement remains highly challenging: the best-performing model achieves only 22.67% completion rate on function-level tasks and 20.00% on repository-level tasks. We further observe that prompting strategies substantially influence performance, highlighting the need for the development of advanced methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "165",
        "title": "Training-Free Data Assimilation with GenCast",
        "author": [
            "Thomas Savary",
            "FranÃ§ois Rozet",
            "Gilles Louppe"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18811",
        "abstract": "Data assimilation is widely used in many disciplines such as meteorology, oceanography, and robotics to estimate the state of a dynamical system from noisy observations. In this work, we propose a lightweight and general method to perform data assimilation using diffusion models pre-trained for emulating dynamical systems. Our method builds on particle filters, a class of data assimilation algorithms, and does not require any further training. As a guiding example throughout this work, we illustrate our methodology on GenCast, a diffusion-based model that generates global ensemble weather forecasts.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "166",
        "title": "MAPEX: A Multi-Agent Pipeline for Keyphrase Extraction",
        "author": [
            "Liting Zhang",
            "Shiwan Zhao",
            "Aobo Kong",
            "Qicheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18813",
        "abstract": "Keyphrase extraction is a fundamental task in natural language processing. However, existing unsupervised prompt-based methods for Large Language Models (LLMs) often rely on single-stage inference pipelines with uniform prompting, regardless of document length or LLM backbone. Such one-size-fits-all designs hinder the full exploitation of LLMs' reasoning and generation capabilities, especially given the complexity of keyphrase extraction across diverse scenarios. To address these challenges, we propose MAPEX, the first framework that introduces multi-agent collaboration into keyphrase extraction. MAPEX coordinates LLM-based agents through modules for expert recruitment, candidate extraction, topic guidance, knowledge augmentation, and post-processing. A dual-path strategy dynamically adapts to document length: knowledge-driven extraction for short texts and topic-guided extraction for long texts. Extensive experiments on six benchmark datasets across three different LLMs demonstrate its strong generalization and universality, outperforming the state-of-the-art unsupervised method by 2.44\\% and standard LLM baselines by 4.01\\% in F1@5 on average. Code is available at https://github.com/NKU-LITI/MAPEX.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "167",
        "title": "Pay More Attention To Audio: Mitigating Imbalance of Cross-Modal Attention in Large Audio Language Models",
        "author": [
            "Junyu Wang",
            "Ziyang Ma",
            "Zhengding Luo",
            "Tianrui Wang",
            "Meng Ge",
            "Xiaobao Wang",
            "Longbiao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18816",
        "abstract": "Large Audio-Language Models (LALMs) often suffer from audio-textual attention imbalance, prioritizing text over acoustic information, particularly in the multi-modal fusion layers of the Transformer architecture. This bias hinders their ability to fully utilize acoustic cues, causing suboptimal performance on audio reasoning tasks. To mitigate this, we propose \\textbf{MATA}, a novel training-free method that dynamically pushes LALMs to pay \\textbf{M}ore \\textbf{A}ttention \\textbf{T}o \\textbf{A}udio tokens within the self-attention mechanism. Specifically, MATA intervenes post raw attention scoring, targeting only the last token in intermediate layers without introducing additional parameters or computational overhead. Experiments on the MMAU and MMAR benchmarks confirm MATA's effectiveness, with consistent performance gains. Notably, on MMAR, MATA enables an open-source model to surpass the proprietary Gemini 2.0 Flash for the first time. Our work provides an efficient solution to mitigate attention bias and opens a new research direction for enhancing the audio-processing capabilities of multi-modal models.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "168",
        "title": "Hyper-Bagel: A Unified Acceleration Framework for Multimodal Understanding and Generation",
        "author": [
            "Yanzuo Lu",
            "Xin Xia",
            "Manlin Zhang",
            "Huafeng Kuang",
            "Jianbin Zheng",
            "Yuxi Ren",
            "Xuefeng Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18824",
        "abstract": "Unified multimodal models have recently attracted considerable attention for their remarkable abilities in jointly understanding and generating diverse content. However, as contexts integrate increasingly numerous interleaved multimodal tokens, the iterative processes of diffusion denoising and autoregressive decoding impose significant computational overhead. To address this, we propose Hyper-Bagel, a unified acceleration framework designed to simultaneously speed up both multimodal understanding and generation tasks. Our approach uses a divide-and-conquer strategy, employing speculative decoding for next-token prediction and a multi-stage distillation process for diffusion denoising. The framework delivers substantial performance gains, achieving over a 2x speedup in multimodal understanding. For generative tasks, our resulting lossless 6-NFE model yields a 16.67x speedup in text-to-image generation and a 22x speedup in image editing, all while preserving the high-quality output of the original model. We further develop a highly efficient 1-NFE model that enables near real-time interactive editing and generation. By combining advanced adversarial distillation with human feedback learning, this model achieves ultimate cost-effectiveness and responsiveness, making complex multimodal interactions seamless and instantaneous.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "Text-to-Image"
        ]
    },
    {
        "id": "169",
        "title": "DexSkin: High-Coverage Conformable Robotic Skin for Learning Contact-Rich Manipulation",
        "author": [
            "Suzannah Wistreich",
            "Baiyu Shi",
            "Stephen Tian",
            "Samuel Clarke",
            "Michael Nath",
            "Chengyi Xu",
            "Zhenan Bao",
            "Jiajun Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18830",
        "abstract": "Human skin provides a rich tactile sensing stream, localizing intentional and unintentional contact events over a large and contoured region. Replicating these tactile sensing capabilities for dexterous robotic manipulation systems remains a longstanding challenge. In this work, we take a step towards this goal by introducing DexSkin. DexSkin is a soft, conformable capacitive electronic skin that enables sensitive, localized, and calibratable tactile sensing, and can be tailored to varying geometries. We demonstrate its efficacy for learning downstream robotic manipulation by sensorizing a pair of parallel jaw gripper fingers, providing tactile coverage across almost the entire finger surfaces. We empirically evaluate DexSkin's capabilities in learning challenging manipulation tasks that require sensing coverage across the entire surface of the fingers, such as reorienting objects in hand and wrapping elastic bands around boxes, in a learning-from-demonstration framework. We then show that, critically for data-driven approaches, DexSkin can be calibrated to enable model transfer across sensor instances, and demonstrate its applicability to online reinforcement learning on real robots. Our results highlight DexSkin's suitability and practicality for learning real-world, contact-rich manipulation. Please see our project webpage for videos and visualizations: https://dex-skin.github.io/.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "170",
        "title": "Text Slider: Efficient and Plug-and-Play Continuous Concept Control for Image/Video Synthesis via LoRA Adapters",
        "author": [
            "Pin-Yen Chiu",
            "I-Sheng Fang",
            "Jun-Cheng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18831",
        "abstract": "Recent advances in diffusion models have significantly improved image and video synthesis. In addition, several concept control methods have been proposed to enable fine-grained, continuous, and flexible control over free-form text prompts. However, these methods not only require intensive training time and GPU memory usage to learn the sliders or embeddings but also need to be retrained for different diffusion backbones, limiting their scalability and adaptability. To address these limitations, we introduce Text Slider, a lightweight, efficient and plug-and-play framework that identifies low-rank directions within a pre-trained text encoder, enabling continuous control of visual concepts while significantly reducing training time, GPU memory consumption, and the number of trainable parameters. Furthermore, Text Slider supports multi-concept composition and continuous control, enabling fine-grained and flexible manipulation in both image and video synthesis. We show that Text Slider enables smooth and continuous modulation of specific attributes while preserving the original spatial layout and structure of the input. Text Slider achieves significantly better efficiency: 5$\\times$ faster training than Concept Slider and 47$\\times$ faster than Attribute Control, while reducing GPU memory usage by nearly 2$\\times$ and 4$\\times$, respectively.",
        "tags": [
            "Diffusion",
            "LoRA"
        ]
    },
    {
        "id": "171",
        "title": "Bounded PCTL Model Checking of Large Language Model Outputs",
        "author": [
            "Dennis Gross",
            "Helge Spieker",
            "Arnaud Gotlieb"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18836",
        "abstract": "In this paper, we introduce LLMCHECKER, a model-checking-based verification method to verify the probabilistic computation tree logic (PCTL) properties of an LLM text generation process. We empirically show that only a limited number of tokens are typically chosen during text generation, which are not always the same. This insight drives the creation of $\\alpha$-$k$-bounded text generation, narrowing the focus to the $\\alpha$ maximal cumulative probability on the top-$k$ tokens at every step of the text generation process. Our verification method considers an initial string and the subsequent top-$k$ tokens while accommodating diverse text quantification methods, such as evaluating text quality and biases. The threshold $\\alpha$ further reduces the selected tokens, only choosing those that exceed or meet it in cumulative probability. LLMCHECKER then allows us to formally verify the PCTL properties of $\\alpha$-$k$-bounded LLMs. We demonstrate the applicability of our method in several LLMs, including Llama, Gemma, Mistral, Genstruct, and BERT. To our knowledge, this is the first time PCTL-based model checking has been used to check the consistency of the LLM text generation process.",
        "tags": [
            "BERT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "172",
        "title": "Benchmarking Vision-Language and Multimodal Large Language Models in Zero-shot and Few-shot Scenarios: A study on Christian Iconography",
        "author": [
            "Gianmarco Spinaci",
            "Lukas Klic",
            "Giovanni Colavizza"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18839",
        "abstract": "This study evaluates the capabilities of Multimodal Large Language Models (LLMs) and Vision Language Models (VLMs) in the task of single-label classification of Christian Iconography. The goal was to assess whether general-purpose VLMs (CLIP and SigLIP) and LLMs, such as GPT-4o and Gemini 2.5, can interpret the Iconography, typically addressed by supervised classifiers, and evaluate their performance. Two research questions guided the analysis: (RQ1) How do multimodal LLMs perform on image classification of Christian saints? And (RQ2), how does performance vary when enriching input with contextual information or few-shot exemplars? We conducted a benchmarking study using three datasets supporting Iconclass natively: ArtDL, ICONCLASS, and Wikidata, filtered to include the top 10 most frequent classes. Models were tested under three conditions: (1) classification using class labels, (2) classification with Iconclass descriptions, and (3) few-shot learning with five exemplars. Results were compared against ResNet50 baselines fine-tuned on the same datasets. The findings show that Gemini-2.5 Pro and GPT-4o outperformed the ResNet50 baselines. Accuracy dropped significantly on the Wikidata dataset, where Siglip reached the highest accuracy score, suggesting model sensitivity to image size and metadata alignment. Enriching prompts with class descriptions generally improved zero-shot performance, while few-shot learning produced lower results, with only occasional and minimal increments in accuracy. We conclude that general-purpose multimodal LLMs are capable of classification in visually complex cultural heritage domains. These results support the application of LLMs as metadata curation tools in digital humanities workflows, suggesting future research on prompt optimization and the expansion of the study to other classification strategies and models.",
        "tags": [
            "CLIP",
            "GPT",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "173",
        "title": "Are Smaller Open-Weight LLMs Closing the Gap to Proprietary Models for Biomedical Question Answering?",
        "author": [
            "Damian Stachura",
            "Joanna Konieczna",
            "Artur Nowak"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18843",
        "abstract": "Open-weight versions of large language models (LLMs) are rapidly advancing, with state-of-the-art models like DeepSeek-V3 now performing comparably to proprietary LLMs. This progression raises the question of whether small open-weight LLMs are capable of effectively replacing larger closed-source models. We are particularly interested in the context of biomedical question-answering, a domain we explored by participating in Task 13B Phase B of the BioASQ challenge. In this work, we compare several open-weight models against top-performing systems such as GPT-4o, GPT-4.1, Claude 3.5 Sonnet, and Claude 3.7 Sonnet. To enhance question answering capabilities, we use various techniques including retrieving the most relevant snippets based on embedding distance, in-context learning, and structured outputs. For certain submissions, we utilize ensemble approaches to leverage the diverse outputs generated by different models for exact-answer questions. Our results demonstrate that open-weight LLMs are comparable to proprietary ones. In some instances, open-weight LLMs even surpassed their closed counterparts, particularly when ensembling strategies were applied. All code is publicly available at https://github.com/evidenceprime/BioASQ-13b.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "174",
        "title": "Failure Makes the Agent Stronger: Enhancing Accuracy through Structured Reflection for Reliable Tool Interactions",
        "author": [
            "Junhao Su",
            "Yuanliang Wan",
            "Junwei Yang",
            "Hengyu Shi",
            "Tianyang Han",
            "Junfeng Luo",
            "Yurui Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18847",
        "abstract": "Tool-augmented large language models (LLMs) are usually trained with supervised imitation or coarse-grained reinforcement learning that optimizes single tool calls. Current self-reflection practices rely on heuristic prompts or one-way reasoning: the model is urged to 'think more' instead of learning error diagnosis and repair. This is fragile in multi-turn interactions; after a failure the model often repeats the same mistake. We propose structured reflection, which turns the path from error to repair into an explicit, controllable, and trainable action. The agent produces a short yet precise reflection: it diagnoses the failure using evidence from the previous step and then proposes a correct, executable follow-up call. For training we combine DAPO and GSPO objectives with a reward scheme tailored to tool use, optimizing the stepwise strategy Reflect, then Call, then Final. To evaluate, we introduce Tool-Reflection-Bench, a lightweight benchmark that programmatically checks structural validity, executability, parameter correctness, and result consistency. Tasks are built as mini trajectories of erroneous call, reflection, and corrected call, with disjoint train and test splits. Experiments on BFCL v3 and Tool-Reflection-Bench show large gains in multi-turn tool-call success and error recovery, and a reduction of redundant calls. These results indicate that making reflection explicit and optimizing it directly improves the reliability of tool interaction and offers a reproducible path for agents to learn from failure.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "175",
        "title": "MAPO: Mixed Advantage Policy Optimization",
        "author": [
            "Wenke Huang",
            "Quan Zhang",
            "Yiyang Fang",
            "Jian Liang",
            "Xuankun Rong",
            "Huanjin Yao",
            "Guancheng Wan",
            "Ke Liang",
            "Wenwen He",
            "Mingjun Li",
            "Leszek Rutkowski",
            "Mang Ye",
            "Bo Du",
            "Dacheng Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18849",
        "abstract": "Recent advances in reinforcement learning for foundation models, such as Group Relative Policy Optimization (GRPO), have significantly improved the performance of foundation models on reasoning tasks. Notably, the advantage function serves as a central mechanism in GRPO for ranking the trajectory importance. However, existing explorations encounter both advantage reversion and advantage mirror problems, which hinder the reasonable advantage allocation across different query samples. In this work, we propose an easy but effective GRPO strategy, Mixed Advantage Policy Optimization (MAPO). We reveal that the trajectory appears with different certainty and propose the advantage percent deviation for samples with high-certainty trajectories. Furthermore, we dynamically reweight the advantage function for samples with varying trajectory certainty, thereby adaptively configuring the advantage function to account for sample-specific characteristics. Comparison with related state-of-the-art methods, along with ablation studies on different advantage variants, validates the effectiveness of our approach.",
        "tags": [
            "GRPO",
            "RL"
        ]
    },
    {
        "id": "176",
        "title": "NGRPO: Negative-enhanced Group Relative Policy Optimization",
        "author": [
            "Gongrui Nan",
            "Siye Chen",
            "Jing Huang",
            "Mengyu Lu",
            "Dexun Wang",
            "Chunmei Xie",
            "Weiqi Xiong",
            "Xianzhou Zeng",
            "Qixuan Zhou",
            "Yadong Li",
            "Xingzhong Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18851",
        "abstract": "RLVR has enhanced the reasoning capabilities of Large Language Models (LLMs) across various tasks. However, GRPO, a representative RLVR algorithm, suffers from a critical limitation: when all responses within a group are either entirely correct or entirely incorrect, the model fails to learn from these homogeneous responses. This is particularly problematic for homogeneously incorrect groups, where GRPO's advantage function yields a value of zero, leading to null gradients and the loss of valuable learning signals. To overcome this issue, we propose NGRPO (Negative-enhanced Group Relative Policy Optimization), an algorithm designed to convert homogeneous errors into robust learning signals. First, NGRPO introduces Advantage Calibration. This mechanism hypothesizes the existence of a virtual maximum-reward sample during advantage calculation, thereby altering the mean and variance of rewards within a group and ensuring that the advantages for homogeneously incorrect samples are no longer zero. Second, NGRPO employs Asymmetric Clipping, which relaxes the update magnitude for positive samples while imposing stricter constraints on that of negative samples. This serves to stabilize the exploration pressure introduced by the advantage calibration. Our experiments on Qwen2.5-Math-7B demonstrate that NGRPO significantly outperforms baselines such as PPO, GRPO, DAPO, and PSR-NSR on mathematical benchmarks including MATH500, AMC23, and AIME2025. These results validate NGRPO's ability to learn from homogeneous errors, leading to stable and substantial improvements in mathematical reasoning. Our code is available at https://github.com/nangongrui-ngr/NGRPO.",
        "tags": [
            "GRPO",
            "LLM",
            "PPO"
        ]
    },
    {
        "id": "177",
        "title": "Multi-Hierarchical Feature Detection for Large Language Model Generated Text",
        "author": [
            "Luyan Zhang",
            "Xinyu Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18862",
        "abstract": "With the rapid advancement of large language model technology, there is growing interest in whether multi-feature approaches can significantly improve AI text detection beyond what single neural models achieve. While intuition suggests that combining semantic, syntactic, and statistical features should provide complementary signals, this assumption has not been rigorously tested with modern LLM-generated text. This paper provides a systematic empirical investigation of multi-hierarchical feature integration for AI text detection, specifically testing whether the computational overhead of combining multiple feature types is justified by performance gains. We implement MHFD (Multi-Hierarchical Feature Detection), integrating DeBERTa-based semantic analysis, syntactic parsing, and statistical probability features through adaptive fusion. Our investigation reveals important negative results: despite theoretical expectations, multi-feature integration provides minimal benefits (0.4-0.5% improvement) while incurring substantial computational costs (4.2x overhead), suggesting that modern neural language models may already capture most relevant detection signals efficiently. Experimental results on multiple benchmark datasets demonstrate that the MHFD method achieves 89.7% accuracy in in-domain detection and maintains 84.2% stable performance in cross-domain detection, showing modest improvements of 0.4-2.6% over existing methods.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "178",
        "title": "Conf-Profile: A Confidence-Driven Reasoning Paradigm for Label-Free User Profiling",
        "author": [
            "Yingxin Li",
            "Jianbo Zhao",
            "Xueyu Ren",
            "Jie Tang",
            "Wangjie You",
            "Xu Chen",
            "Kan Zhou",
            "Chao Feng",
            "Jiao Ran",
            "Yuan Meng",
            "Zhi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18864",
        "abstract": "User profiling, as a core technique for user understanding, aims to infer structural attributes from user information. Large Language Models (LLMs) provide a promising avenue for user profiling, yet the progress is hindered by the lack of comprehensive benchmarks. To bridge this gap, we propose ProfileBench, an industrial benchmark derived from a real-world video platform, encompassing heterogeneous user data and a well-structured profiling taxonomy. However, the profiling task remains challenging due to the difficulty of collecting large-scale ground-truth labels, and the heterogeneous and noisy user information can compromise the reliability of LLMs. To approach label-free and reliable user profiling, we propose a Confidence-driven Profile reasoning framework Conf-Profile, featuring a two-stage paradigm. We first synthesize high-quality labels by leveraging advanced LLMs with confidence hints, followed by confidence-weighted voting for accuracy improvement and confidence calibration for a balanced distribution. The multiple profile results, rationales, and confidence scores are aggregated and distilled into a lightweight LLM. We further enhance the reasoning ability via confidence-guided unsupervised reinforcement learning, which exploits confidence for difficulty filtering, quasi-ground truth voting, and reward weighting. Experimental results demonstrate that Conf-Profile delivers substantial performance through the two-stage training, improving F1 by 13.97 on Qwen3-8B.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "179",
        "title": "Bi-VLA: Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation",
        "author": [
            "Masato Kobayashi",
            "Thanpimon Buamanee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18865",
        "abstract": "We propose Bilateral Control-Based Imitation Learning via Vision-Language Fusion for Action Generation (Bi-VLA), a novel framework that extends bilateral control-based imitation learning to handle more than one task within a single model. Conventional bilateral control methods exploit joint angle, velocity, torque, and vision for precise manipulation but require task-specific models, limiting their generality. Bi-VLA overcomes this limitation by utilizing robot joint angle, velocity, and torque data from leader-follower bilateral control with visual features and natural language instructions through SigLIP and FiLM-based fusion. We validated Bi-VLA on two task types: one requiring supplementary language cues and another distinguishable solely by vision. Real-robot experiments showed that Bi-VLA successfully interprets vision-language combinations and improves task success rates compared to conventional bilateral control-based imitation learning. Our Bi-VLA addresses the single-task limitation of prior bilateral approaches and provides empirical evidence that combining vision and language significantly enhances versatility. Experimental results validate the effectiveness of Bi-VLA in real-world tasks. For additional material, please visit the website: https://mertcookimg.github.io/bi-vla/",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "180",
        "title": "Memory in Large Language Models: Mechanisms, Evaluation and Evolution",
        "author": [
            "Dianxing Zhang",
            "Wendong Li",
            "Kani Song",
            "Jiaye Lu",
            "Gang Li",
            "Liuchun Yang",
            "Sheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18868",
        "abstract": "Under a unified operational definition, we define LLM memory as a persistent state written during pretraining, finetuning, or inference that can later be addressed and that stably influences outputs. We propose a four-part taxonomy (parametric, contextual, external, procedural/episodic) and a memory quadruple (location, persistence, write/access path, controllability). We link mechanism, evaluation, and governance via the chain write -> read -> inhibit/update. To avoid distorted comparisons across heterogeneous setups, we adopt a three-setting protocol (parametric only, offline retrieval, online retrieval) that decouples capability from information availability on the same data and timeline. On this basis we build a layered evaluation: parametric (closed-book recall, edit differential, memorization/privacy), contextual (position curves and the mid-sequence drop), external (answer correctness vs snippet attribution/faithfulness), and procedural/episodic (cross-session consistency and timeline replay, E MARS+). The framework integrates temporal governance and leakage auditing (freshness hits, outdated answers, refusal slices) and uncertainty reporting via inter-rater agreement plus paired tests with multiple-comparison correction. For updating and forgetting, we present DMM Gov: coordinating DAPT/TAPT, PEFT, model editing (ROME, MEND, MEMIT, SERAC), and RAG to form an auditable loop covering admission thresholds, rollout, monitoring, rollback, and change audits, with specs for timeliness, conflict handling, and long-horizon consistency. Finally, we give four testable propositions: minimum identifiability; a minimal evaluation card; causally constrained editing with verifiable forgetting; and when retrieval with small-window replay outperforms ultra-long-context reading. This yields a reproducible, comparable, and governable coordinate system for research and deployment.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "181",
        "title": "When Ads Become Profiles: Large-Scale Audit of Algorithmic Biases and LLM Profiling Risks",
        "author": [
            "Baiyu Chen",
            "Benjamin Tag",
            "Hao Xue",
            "Daniel Angus",
            "Flora Salim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18874",
        "abstract": "Automated ad targeting on social media is opaque, creating risks of exploitation and invisibility to external scrutiny. Users may be steered toward harmful content while independent auditing of these processes remains blocked. Large Language Models (LLMs) raise a new concern: the potential to reverse-engineer sensitive user attributes from exposure alone. We introduce a multi-stage auditing framework to investigate these risks. First, a large-scale audit of over 435,000 ad impressions delivered to 891 Australian Facebook users reveals algorithmic biases, including disproportionate Gambling and Politics ads shown to socioeconomically vulnerable and politically aligned groups. Second, a multimodal LLM can reconstruct users' demographic profiles from ad streams, outperforming census-based baselines and matching or exceeding human performance. Our results provide the first empirical evidence that ad streams constitute rich digital footprints for public AI inference, highlighting urgent privacy risks and the need for content-level auditing and governance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "182",
        "title": "LongCat-Flash-Thinking Technical Report",
        "author": [
            "Meituan LongCat Team",
            "Anchun Gui",
            "Bei Li",
            "Bingyang Tao",
            "Bole Zhou",
            "Borun Chen",
            "Chao Zhang",
            "Chao Zhang",
            "Chengcheng Han",
            "Chenhui Yang",
            "Chi Zhang",
            "Chong Peng",
            "Chuyu Zhang",
            "Cong Chen",
            "Fengcun Li",
            "Gang Xu",
            "Guoyuan Lin",
            "Hao Jiang",
            "Hao Liang",
            "Haomin Fu",
            "Haoxiang Ma",
            "Hong Liu",
            "Hongyan Hao",
            "Hongyin Tang",
            "Hongyu Zang",
            "Hongzhi Ni",
            "Hui Su",
            "Jiahao Liu",
            "Jiahuan Li",
            "Jialin Liu",
            "Jianfei Zhang",
            "Jianhao Xu",
            "Jianing Wang",
            "Jiaqi Sun",
            "Jiaqi Zhang",
            "Jiarong Shi",
            "Jiawei Yang",
            "Jingang Wang",
            "Jinrui Ding",
            "Jun Kuang",
            "Jun Xu",
            "Ke He",
            "Kefeng Zhang",
            "Keheng Wang",
            "Keqing He",
            "Li Wei",
            "Liang Shi",
            "Lin Qiu",
            "Lingbin Kong",
            "Lingchuan Liu",
            "Linsen Guo",
            "Longfei An",
            "Mai Xia",
            "Meng Zhou",
            "Mengshen Zhu",
            "Peng Pei",
            "Pengcheng Jia",
            "Qi Gu",
            "Qi Guo",
            "Qiong Huang",
            "Quan Chen",
            "Quanchi Weng",
            "Rongxiang Weng",
            "Ruichen Shao",
            "Rumei Li",
            "Shanglin Lei",
            "Shuai Du",
            "Shuaikang Liu",
            "Shuang Zhou",
            "Shuhao Hu",
            "Siyu Xu",
            "Songshan Gong",
            "Tao Liang",
            "Tianhao Hu",
            "Wei He",
            "Wei Shi",
            "Wei Wang",
            "Wei Wu",
            "Wei Zhuo",
            "Weifeng Tang",
            "Wenjie Shi",
            "Wenlong Zhu",
            "Xi Su",
            "Xiangcheng Liu",
            "Xiangyu Xi",
            "Xiangzhou Huang",
            "Xiao Liu",
            "Xiaochen Jiang",
            "Xiaowei Shi",
            "Xiaowen Shi",
            "Xiaoyu Li",
            "Xin Chen",
            "Xinyue Zhao",
            "Xuan Huang",
            "Xuemiao Zhang",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Yajie Zhang",
            "Yang Chen",
            "Yang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18883",
        "abstract": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
        "tags": [
            "CoT",
            "MoE",
            "RL"
        ]
    },
    {
        "id": "183",
        "title": "Confidential LLM Inference: Performance and Cost Across CPU and GPU TEEs",
        "author": [
            "Marcin Chrapek",
            "Marcin Copik",
            "Etienne Mettaz",
            "Torsten Hoefler"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18886",
        "abstract": "Large Language Models (LLMs) are increasingly deployed on converged Cloud and High-Performance Computing (HPC) infrastructure. However, as LLMs handle confidential inputs and are fine-tuned on costly, proprietary datasets, their heightened security requirements slow adoption in privacy-sensitive sectors such as healthcare and finance. We investigate methods to address this gap and propose Trusted Execution Environments (TEEs) as a solution for securing end-to-end LLM inference. We validate their practicality by evaluating these compute-intensive workloads entirely within CPU and GPU TEEs. On the CPU side, we conduct an in-depth study running full Llama2 inference pipelines (7B, 13B, 70B) inside Intel's TDX and SGX, accelerated by Advanced Matrix Extensions (AMX). We derive 12 insights, including that across various data types, batch sizes, and input lengths, CPU TEEs impose under 10% throughput and 20% latency overheads, further reduced by AMX. We run LLM inference on NVIDIA H100 Confidential Compute GPUs, contextualizing our CPU findings and observing throughput penalties of 4-8% that diminish as batch and input sizes grow. By comparing performance, cost, and security trade-offs, we show how CPU TEEs can be more cost-effective or secure than their GPU counterparts. To our knowledge, our work is the first to comprehensively demonstrate the performance and practicality of modern TEEs across both CPUs and GPUs for enabling confidential LLMs (cLLMs).",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "184",
        "title": "Attack for Defense: Adversarial Agents for Point Prompt Optimization Empowering Segment Anything Model",
        "author": [
            "Xueyu Liu",
            "Xiaoyi Zhang",
            "Guangze Shi",
            "Meilin Liu",
            "Yexin Lai",
            "Yongfei Wu",
            "Mingqiang Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18891",
        "abstract": "Prompt quality plays a critical role in the performance of the Segment Anything Model (SAM), yet existing approaches often rely on heuristic or manually crafted prompts, limiting scalability and generalization. In this paper, we propose Point Prompt Defender, an adversarial reinforcement learning framework that adopts an attack-for-defense paradigm to automatically optimize point prompts. We construct a task-agnostic point prompt environment by representing image patches as nodes in a dual-space graph, where edges encode both physical and semantic distances. Within this environment, an attacker agent learns to activate a subset of prompts that maximally degrade SAM's segmentation performance, while a defender agent learns to suppress these disruptive prompts and restore accuracy. Both agents are trained using Deep Q-Networks with a reward signal based on segmentation quality variation. During inference, only the defender is deployed to refine arbitrary coarse prompt sets, enabling enhanced SAM segmentation performance across diverse tasks without retraining. Extensive experiments show that Point Prompt Defender effectively improves SAM's robustness and generalization, establishing a flexible, interpretable, and plug-and-play framework for prompt-based segmentation.",
        "tags": [
            "RL",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "185",
        "title": "DeblurSplat: SfM-free 3D Gaussian Splatting with Event Camera for Robust Deblurring",
        "author": [
            "Pengteng Li",
            "Yunfan Lu",
            "Pinhao Song",
            "Weiyu Guo",
            "Huizai Yao",
            "F. Richard Yu",
            "Hui Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18898",
        "abstract": "In this paper, we propose the first Structure-from-Motion (SfM)-free deblurring 3D Gaussian Splatting method via event camera, dubbed DeblurSplat. We address the motion-deblurring problem in two ways. First, we leverage the pretrained capability of the dense stereo module (DUSt3R) to directly obtain accurate initial point clouds from blurred images. Without calculating camera poses as an intermediate result, we avoid the cumulative errors transfer from inaccurate camera poses to the initial point clouds' positions. Second, we introduce the event stream into the deblur pipeline for its high sensitivity to dynamic change. By decoding the latent sharp images from the event stream and blurred images, we can provide a fine-grained supervision signal for scene reconstruction optimization. Extensive experiments across a range of scenes demonstrate that DeblurSplat not only excels in generating high-fidelity novel views but also achieves significant rendering efficiency compared to the SOTAs in deblur 3D-GS.",
        "tags": [
            "3D",
            "Deblurring",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "186",
        "title": "Extractive Fact Decomposition for Interpretable Natural Language Inference in one Forward Pass",
        "author": [
            "Nicholas PopoviÄ",
            "Michael FÃ¤rber"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18901",
        "abstract": "Recent works in Natural Language Inference (NLI) and related tasks, such as automated fact-checking, employ atomic fact decomposition to enhance interpretability and robustness. For this, existing methods rely on resource-intensive generative large language models (LLMs) to perform decomposition. We propose JEDI, an encoder-only architecture that jointly performs extractive atomic fact decomposition and interpretable inference without requiring generative models during inference. To facilitate training, we produce a large corpus of synthetic rationales covering multiple NLI benchmarks. Experimental results demonstrate that JEDI achieves competitive accuracy in distribution and significantly improves robustness out of distribution and in adversarial settings over models based solely on extractive rationale supervision. Our findings show that interpretability and robust generalization in NLI can be realized using encoder-only architectures and synthetic rationales. Code and data available at https://jedi.nicpopovic.com",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "187",
        "title": "How Far are VLMs from Visual Spatial Intelligence? A Benchmark-Driven Perspective",
        "author": [
            "Songsong Yu",
            "Yuxin Chen",
            "Hao Ju",
            "Lianjie Jia",
            "Fuxi Zhang",
            "Shaofei Huang",
            "Yuhan Wu",
            "Rundi Cui",
            "Binghao Ran",
            "Zaibin Zhang",
            "Zhedong Zheng",
            "Zhipeng Zhang",
            "Yifan Wang",
            "Lin Song",
            "Lijun Wang",
            "Yanwei Li",
            "Ying Shan",
            "Huchuan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18905",
        "abstract": "Visual Spatial Reasoning (VSR) is a core human cognitive ability and a critical requirement for advancing embodied intelligence and autonomous systems. Despite recent progress in Vision-Language Models (VLMs), achieving human-level VSR remains highly challenging due to the complexity of representing and reasoning over three-dimensional space. In this paper, we present a systematic investigation of VSR in VLMs, encompassing a review of existing methodologies across input modalities, model architectures, training strategies, and reasoning mechanisms. Furthermore, we categorize spatial intelligence into three levels of capability, ie, basic perception, spatial understanding, spatial planning, and curate SIBench, a spatial intelligence benchmark encompassing nearly 20 open-source datasets across 23 task settings. Experiments with state-of-the-art VLMs reveal a pronounced gap between perception and reasoning, as models show competence in basic perceptual tasks but consistently underperform in understanding and planning tasks, particularly in numerical estimation, multi-view reasoning, temporal dynamics, and spatial imagination. These findings underscore the substantial challenges that remain in achieving spatial intelligence, while providing both a systematic roadmap and a comprehensive benchmark to drive future research in the field. The related resources of this study are accessible at https://sibench.github.io/Awesome-Visual-Spatial-Reasoning/.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "188",
        "title": "Frequency-Domain Decomposition and Recomposition for Robust Audio-Visual Segmentation",
        "author": [
            "Yunzhe Shen",
            "Kai Peng",
            "Leiye Liu",
            "Wei Ji",
            "Jingjing Li",
            "Miao Zhang",
            "Yongri Piao",
            "Huchuan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18912",
        "abstract": "Audio-visual segmentation (AVS) plays a critical role in multimodal machine learning by effectively integrating audio and visual cues to precisely segment objects or regions within visual scenes. Recent AVS methods have demonstrated significant improvements. However, they overlook the inherent frequency-domain contradictions between audio and visual modalities--the pervasively interfering noise in audio high-frequency signals vs. the structurally rich details in visual high-frequency signals. Ignoring these differences can result in suboptimal performance. In this paper, we rethink the AVS task from a deeper perspective by reformulating AVS task as a frequency-domain decomposition and recomposition problem. To this end, we introduce a novel Frequency-Aware Audio-Visual Segmentation (FAVS) framework consisting of two key modules: Frequency-Domain Enhanced Decomposer (FDED) module and Synergistic Cross-Modal Consistency (SCMC) module. FDED module employs a residual-based iterative frequency decomposition to discriminate modality-specific semantics and structural features, and SCMC module leverages a mixture-of-experts architecture to reinforce semantic consistency and modality-specific feature preservation through dynamic expert routing. Extensive experiments demonstrate that our FAVS framework achieves state-of-the-art performance on three benchmark datasets, and abundant qualitative visualizations further verify the effectiveness of the proposed FDED and SCMC modules. The code will be released as open source upon acceptance of the paper.",
        "tags": [
            "MoE",
            "Segmentation"
        ]
    },
    {
        "id": "189",
        "title": "LiDAR Point Cloud Image-based Generation Using Denoising Diffusion Probabilistic Models",
        "author": [
            "Amirhesam Aghanouri",
            "Cristina Olaverri-Monreal"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18917",
        "abstract": "Autonomous vehicles (AVs) are expected to revolutionize transportation by improving efficiency and safety. Their success relies on 3D vision systems that effectively sense the environment and detect traffic agents. Among sensors AVs use to create a comprehensive view of surroundings, LiDAR provides high-resolution depth data enabling accurate object detection, safe navigation, and collision avoidance. However, collecting real-world LiDAR data is time-consuming and often affected by noise and sparsity due to adverse weather or sensor limitations. This work applies a denoising diffusion probabilistic model (DDPM), enhanced with novel noise scheduling and time-step embedding techniques to generate high-quality synthetic data for augmentation, thereby improving performance across a range of computer vision tasks, particularly in AV perception. These modifications impact the denoising process and the model's temporal awareness, allowing it to produce more realistic point clouds based on the projection. The proposed method was extensively evaluated under various configurations using the IAMCV and KITTI-360 datasets, with four performance metrics compared against state-of-the-art (SOTA) methods. The results demonstrate the model's superior performance over most existing baselines and its effectiveness in mitigating the effects of noisy and sparse LiDAR data, producing diverse point clouds with rich spatial relationships and structural detail.",
        "tags": [
            "3D",
            "DDPM",
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "190",
        "title": "SynapFlow: A Modular Framework Towards Large-Scale Analysis of Dendritic Spines",
        "author": [
            "Pamela Osuna-Vargas",
            "Altug Kamacioglu",
            "Dominik F. Aschauer",
            "Petros E. Vlachos",
            "Sercan Alipek",
            "Jochen Triesch",
            "Simon Rumpel",
            "Matthias Kaschube"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18926",
        "abstract": "Dendritic spines are key structural components of excitatory synapses in the brain. Given the size of dendritic spines provides a proxy for synaptic efficacy, their detection and tracking across time is important for studies of the neural basis of learning and memory. Despite their relevance, large-scale analyses of the structural dynamics of dendritic spines in 3D+time microscopy data remain challenging and labor-intense. Here, we present a modular machine learning-based pipeline designed to automate the detection, time-tracking, and feature extraction of dendritic spines in volumes chronically recorded with two-photon microscopy. Our approach tackles the challenges posed by biological data by combining a transformer-based detection module, a depth-tracking component that integrates spatial features, a time-tracking module to associate 3D spines across time by leveraging spatial consistency, and a feature extraction unit that quantifies biologically relevant spine properties. We validate our method on open-source labeled spine data, and on two complementary annotated datasets that we publish alongside this work: one for detection and depth-tracking, and one for time-tracking, which, to the best of our knowledge, is the first data of this kind. To encourage future research, we release our data, code, and pre-trained weights at https://github.com/pamelaosuna/SynapFlow, establishing a baseline for scalable, end-to-end analysis of dendritic spine dynamics.",
        "tags": [
            "3D",
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "191",
        "title": "Tackling GNARLy Problems: Graph Neural Algorithmic Reasoning Reimagined through Reinforcement Learning",
        "author": [
            "Alex Schutz",
            "Victor-Alexandru Darvariu",
            "Efimia Panagiotaki",
            "Bruno Lacerda",
            "Nick Hawes"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18930",
        "abstract": "Neural Algorithmic Reasoning (NAR) is a paradigm that trains neural networks to execute classic algorithms by supervised learning. Despite its successes, important limitations remain: inability to construct valid solutions without post-processing and to reason about multiple correct ones, poor performance on combinatorial NP-hard problems, and inapplicability to problems for which strong algorithms are not yet known. To address these limitations, we reframe the problem of learning algorithm trajectories as a Markov Decision Process, which imposes structure on the solution construction procedure and unlocks the powerful tools of imitation and reinforcement learning (RL). We propose the GNARL framework, encompassing the methodology to translate problem formulations from NAR to RL and a learning architecture suitable for a wide range of graph-based problems. We achieve very high graph accuracy results on several CLRS-30 problems, performance matching or exceeding much narrower NAR approaches for NP-hard problems and, remarkably, applicability even when lacking an expert algorithm.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "192",
        "title": "Generic Adversarial Smart Contract Detection with Semantics and Uncertainty-Aware LLM",
        "author": [
            "Yating Liu",
            "Xing Su",
            "Hao Wu",
            "Sijin Li",
            "Yuxi Cheng",
            "Fengyuan Xu",
            "Sheng Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18934",
        "abstract": "Adversarial smart contracts, mostly on EVM-compatible chains like Ethereum and BSC, are deployed as EVM bytecode to exploit vulnerable smart contracts typically for financial gains. Detecting such malicious contracts at the time of deployment is an important proactive strategy preventing loss from victim contracts. It offers a better cost-benefit than detecting vulnerabilities on diverse potential victims. However, existing works are not generic with limited detection types and effectiveness due to imbalanced samples, while the emerging LLM technologies, which show its potentials in generalization, have two key problems impeding its application in this task: hard digestion of compiled-code inputs, especially those with task-specific logic, and hard assessment of LLMs' certainty in their binary answers, i.e., yes-or-no answers. Therefore, we propose a generic adversarial smart contracts detection framework FinDet, which leverages LLMs with two enhancements addressing above two problems. FinDet takes as input only the EVM-bytecode contracts and identifies adversarial ones among them with high balanced accuracy. The first enhancement extracts concise semantic intentions and high-level behavioral logic from the low-level bytecode inputs, unleashing the LLM reasoning capability restricted by the task input. The second enhancement probes and measures the LLM uncertainty to its multi-round answering to the same query, improving the LLM answering robustness for binary classifications required by the task output. Our comprehensive evaluation shows that FinDet achieves a BAC of 0.9223 and a TPR of 0.8950, significantly outperforming existing baselines. It remains robust under challenging conditions including unseen attack patterns, low-data settings, and feature obfuscation. FinDet detects all 5 public and 20+ unreported adversarial contracts in a 10-day real-world test, confirmed manually.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "193",
        "title": "Lang2Morph: Language-Driven Morphological Design of Robotic Hands",
        "author": [
            "Yanyuan Qiao",
            "Kieran Gilday",
            "Yutong Xie",
            "Josie Hughes"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18937",
        "abstract": "Designing robotic hand morphologies for diverse manipulation tasks requires balancing dexterity, manufacturability, and task-specific functionality. While open-source frameworks and parametric tools support reproducible design, they still rely on expert heuristics and manual tuning. Automated methods using optimization are often compute-intensive, simulation-dependent, and rarely target dexterous hands. Large language models (LLMs), with their broad knowledge of human-object interactions and strong generative capabilities, offer a promising alternative for zero-shot design reasoning. In this paper, we present Lang2Morph, a language-driven pipeline for robotic hand design. It uses LLMs to translate natural-language task descriptions into symbolic structures and OPH-compatible parameters, enabling 3D-printable task-specific morphologies. The pipeline consists of: (i) Morphology Design, which maps tasks into semantic tags, structural grammars, and OPH-compatible parameters; and (ii) Selection and Refinement, which evaluates design candidates based on semantic alignment and size compatibility, and optionally applies LLM-guided refinement when needed. We evaluate Lang2Morph across varied tasks, and results show that our approach can generate diverse, task-relevant morphologies. To our knowledge, this is the first attempt to develop an LLM-based framework for task-conditioned robotic hand design.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "194",
        "title": "No Labels Needed: Zero-Shot Image Classification with Collaborative Self-Learning",
        "author": [
            "Matheus VinÃ­cius Todescato",
            "Joel LuÃ­s Carbonera"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18938",
        "abstract": "While deep learning, including Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs), has significantly advanced classification performance, its typical reliance on extensive annotated datasets presents a major obstacle in many practical scenarios where such data is scarce. Vision-language models (VLMs) and transfer learning with pre-trained visual models appear as promising techniques to deal with this problem. This paper proposes a novel zero-shot image classification framework that combines a VLM and a pre-trained visual model within a self-learning cycle. Requiring only the set of class names and no labeled training data, our method utilizes a confidence-based pseudo-labeling strategy to train a lightweight classifier directly on the test data, enabling dynamic adaptation. The VLM identifies high-confidence samples, and the pre-trained visual model enhances their visual representations. These enhanced features then iteratively train the classifier, allowing the system to capture complementary semantic and visual cues without supervision. Notably, our approach avoids VLM fine-tuning and the use of large language models, relying on the visual-only model to reduce the dependence on semantic representation. Experimental evaluations on ten diverse datasets demonstrate that our approach outperforms the baseline zero-shot method.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "195",
        "title": "Data Efficient Adaptation in Large Language Models via Continuous Low-Rank Fine-Tuning",
        "author": [
            "Xiao Han",
            "Zimo Zhao",
            "Wanyu Wang",
            "Maolin Wang",
            "Zitao Liu",
            "Yi Chang",
            "Xiangyu Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18942",
        "abstract": "Recent advancements in Large Language Models (LLMs) have emphasized the critical role of fine-tuning (FT) techniques in adapting LLMs to specific tasks, especially when retraining from scratch is computationally infeasible. Fine-tuning enables LLMs to leverage task- or domain-specific data, producing models that more effectively meet the requirements of targeted applications. However, con- ventional FT approaches often suffer from catastrophic forgetting and suboptimal data efficiency, limiting their real-world applicability. To address these challenges, this paper proposes DEAL, a novel framework that integrates Low-Rank Adapta- tion (LoRA) with a continuous fine-tuning strategy. By incorporating knowledge retention and adaptive parameter update modules, the framework mitigates the lim- itations of existing FT methods while maintaining efficiency in privacy-preserving settings. Experiments on 15 diverse datasets show that DEAL consistently outper- forms baseline methods, yielding substantial gains in task accuracy and resource efficiency. These findings demonstrate the potential of our approach to advance continual adaptation in LLMs by enhancing task performance while improving resource efficiency.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "196",
        "title": "One-shot Embroidery Customization via Contrastive LoRA Modulation",
        "author": [
            "Jun Ma",
            "Qian He",
            "Gaofeng He",
            "Huang Chen",
            "Chen Liu",
            "Xiaogang Jin",
            "Huamin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18948",
        "abstract": "Diffusion models have significantly advanced image manipulation techniques, and their ability to generate photorealistic images is beginning to transform retail workflows, particularly in presale visualization. Beyond artistic style transfer, the capability to perform fine-grained visual feature transfer is becoming increasingly important. Embroidery is a textile art form characterized by intricate interplay of diverse stitch patterns and material properties, which poses unique challenges for existing style transfer methods. To explore the customization for such fine-grained features, we propose a novel contrastive learning framework that disentangles fine-grained style and content features with a single reference image, building on the classic concept of image analogy. We first construct an image pair to define the target style, and then adopt a similarity metric based on the decoupled representations of pretrained diffusion models for style-content separation. Subsequently, we propose a two-stage contrastive LoRA modulation technique to capture fine-grained style features. In the first stage, we iteratively update the whole LoRA and the selected style blocks to initially separate style from content. In the second stage, we design a contrastive learning strategy to further decouple style and content through self-knowledge distillation. Finally, we build an inference pipeline to handle image or text inputs with only the style blocks. To evaluate our method on fine-grained style transfer, we build a benchmark for embroidery customization. Our approach surpasses prior methods on this task and further demonstrates strong generalization to three additional domains: artistic style transfer, sketch colorization, and appearance transfer.",
        "tags": [
            "Diffusion",
            "LoRA",
            "Style Transfer"
        ]
    },
    {
        "id": "197",
        "title": "Towards Robust LiDAR Localization: Deep Learning-based Uncertainty Estimation",
        "author": [
            "Minoo Dolatabadi",
            "Fardin Ayar",
            "Ehsan Javanmardi",
            "Manabu Tsukada",
            "Mahdi Javanmardi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18954",
        "abstract": "LiDAR-based localization and SLAM often rely on iterative matching algorithms, particularly the Iterative Closest Point (ICP) algorithm, to align sensor data with pre-existing maps or previous scans. However, ICP is prone to errors in featureless environments and dynamic scenes, leading to inaccurate pose estimation. Accurately predicting the uncertainty associated with ICP is crucial for robust state estimation but remains challenging, as existing approaches often rely on handcrafted models or simplified assumptions. Moreover, a few deep learning-based methods for localizability estimation either depend on a pre-built map, which may not always be available, or provide a binary classification of localizable versus non-localizable, which fails to properly model uncertainty. In this work, we propose a data-driven framework that leverages deep learning to estimate the registration error covariance of ICP before matching, even in the absence of a reference map. By associating each LiDAR scan with a reliable 6-DoF error covariance estimate, our method enables seamless integration of ICP within Kalman filtering, enhancing localization accuracy and robustness. Extensive experiments on the KITTI dataset demonstrate the effectiveness of our approach, showing that it accurately predicts covariance and, when applied to localization using a pre-built map or SLAM, reduces localization errors and improves robustness.",
        "tags": [
            "Pose Estimation",
            "SLAM"
        ]
    },
    {
        "id": "198",
        "title": "Seeing Through Reflections: Advancing 3D Scene Reconstruction in Mirror-Containing Environments with Gaussian Splatting",
        "author": [
            "Zijing Guo",
            "Yunyang Zhao",
            "Lin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18956",
        "abstract": "Mirror-containing environments pose unique challenges for 3D reconstruction and novel view synthesis (NVS), as reflective surfaces introduce view-dependent distortions and inconsistencies. While cutting-edge methods such as Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) excel in typical scenes, their performance deteriorates in the presence of mirrors. Existing solutions mainly focus on handling mirror surfaces through symmetry mapping but often overlook the rich information carried by mirror reflections. These reflections offer complementary perspectives that can fill in absent details and significantly enhance reconstruction quality. To advance 3D reconstruction in mirror-rich environments, we present MirrorScene3D, a comprehensive dataset featuring diverse indoor scenes, 1256 high-quality images, and annotated mirror masks, providing a benchmark for evaluating reconstruction methods in reflective settings. Building on this, we propose ReflectiveGS, an extension of 3D Gaussian Splatting that utilizes mirror reflections as complementary viewpoints rather than simple symmetry artifacts, enhancing scene geometry and recovering absent details. Experiments on MirrorScene3D show that ReflectiveGaussian outperforms existing methods in SSIM, PSNR, LPIPS, and training speed, setting a new benchmark for 3D reconstruction in mirror-rich environments.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "199",
        "title": "TD3-Sched: Learning to Orchestrate Container-based Cloud-Edge Resources via Distributed Reinforcement Learning",
        "author": [
            "Shengye Song",
            "Minxian Xu",
            "Kan Hu",
            "Wenxia Guo",
            "Kejiang Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18957",
        "abstract": "Resource scheduling in cloud-edge systems is challenging as edge nodes run latency-sensitive workloads under tight resource constraints, while existing centralized schedulers can suffer from performance bottlenecks and user experience degradation. To address the issues of distributed decisions in cloud-edge environments, we present TD3-Sched, a distributed reinforcement learning (DRL) scheduler based on Twin Delayed Deep Deterministic Policy Gradient (TD3) for continuous control of CPU and memory allocation, which can achieve optimized decisions for resource provisioning under dynamic workloads. On a realistic cloud-edge testbed with SockShop application and Alibaba traces, TD3-Sched achieves reductions of 17.9% to 38.6% in latency under same loads compared with other reinforcement-learning and rule-based baselines, and 16% to 31.6% under high loads. TD3-Sched also shows superior Service Level Objective (SLO) compliance with only 0.47% violations. These results indicate faster convergence, lower latency, and more stable performance while preserving service quality in container-based cloud-edge environment compared with the baselines.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "200",
        "title": "Benchmarking PDF Accessibility Evaluation A Dataset and Framework for Assessing Automated and LLM-Based Approaches for Accessibility Testing",
        "author": [
            "Anukriti Kumar",
            "Tanushree Padath",
            "Lucy Lu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18965",
        "abstract": "PDFs remain the dominant format for scholarly communication, despite significant accessibility challenges for blind and low-vision users. While various tools attempt to evaluate PDF accessibility, there is no standardized methodology to evaluate how different accessibility assessment approaches perform. Our work addresses this critical gap by introducing a novel benchmark dataset of scholarly PDFs with expert-validated accessibility annotations across seven criteria (alternative text quality, logical reading order, semantic tagging, table structure, functional hyperlinks, color contrast, and font readability), and a four-category evaluation framework with standardized labels (Passed, Failed, Not Present, Cannot Tell) to systematically assess accessibility evaluation approaches. Using our evaluation framework, we explore whether large language models (LLMs) are capable of supporting automated accessibility evaluation. We benchmark five LLMs, which demonstrate varying capabilities in correctly assessing different accessibility criteria, with GPT-4-Turbo achieving the highest overall accuracy (0.85). However, all models struggled in correctly categorizing documents with Not Present and Cannot Tell accessibility labels, particularly for alt text quality assessment. Our qualitative comparison with standard automated checkers reveals complementary strengths: rule-based tools excel at technical verification, while LLMs better evaluate semantic appropriateness and contextual relevance. Based on our findings, we propose a hybrid approach that would combine automated checkers, LLM evaluation, and human assessment as a future strategy for PDF accessibility evaluation.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "201",
        "title": "Otters: An Energy-Efficient SpikingTransformer via Optical Time-to-First-Spike Encoding",
        "author": [
            "Zhanglu Yan",
            "Jiayi Mao",
            "Qianhui Liu",
            "Fanfan Li",
            "Gang Pan",
            "Tao Luo",
            "Bowen Zhu",
            "Weng-Fai Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18968",
        "abstract": "Spiking neural networks (SNNs) promise high energy efficiency, particularly with time-to-first-spike (TTFS) encoding, which maximizes sparsity by emitting at most one spike per neuron. However, such energy advantage is often unrealized because inference requires evaluating a temporal decay function and subsequent multiplication with the synaptic weights. This paper challenges this costly approach by repurposing a physical hardware `bug', namely, the natural signal decay in optoelectronic devices, as the core computation of TTFS. We fabricated a custom indium oxide optoelectronic synapse, showing how its natural physical decay directly implements the required temporal function. By treating the device's analog output as the fused product of the synaptic weight and temporal decay, optoelectronic synaptic TTFS (named Otters) eliminates these expensive digital operations. To use the Otters paradigm in complex architectures like the transformer, which are challenging to train directly due to the sparsity issue, we introduce a novel quantized neural network-to-SNN conversion algorithm. This complete hardware-software co-design enables our model to achieve state-of-the-art accuracy across seven GLUE benchmark datasets and demonstrates a 1.77$\\times$ improvement in energy efficiency over previous leading SNNs, based on a comprehensive analysis of compute, data movement, and memory access costs using energy measurements from a commercial 22nm process. Our work thus establishes a new paradigm for energy-efficient SNNs, translating fundamental device physics directly into powerful computational primitives. All codes and data are open source.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "202",
        "title": "LLM-based Agents Suffer from Hallucinations: A Survey of Taxonomy, Methods, and Directions",
        "author": [
            "Xixun Lin",
            "Yucheng Ning",
            "Jingwen Zhang",
            "Yan Dong",
            "Yilong Liu",
            "Yongxuan Wu",
            "Xiaohua Qi",
            "Nan Sun",
            "Yanmin Shang",
            "Pengfei Cao",
            "Lixin Zou",
            "Xu Chen",
            "Chuan Zhou",
            "Jia Wu",
            "Shirui Pan",
            "Bin Wang",
            "Yanan Cao",
            "Kai Chen",
            "Songlin Hu",
            "Li Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18970",
        "abstract": "Driven by the rapid advancements of Large Language Models (LLMs), LLM-based agents have emerged as powerful intelligent systems capable of human-like cognition, reasoning, and interaction. These agents are increasingly being deployed across diverse real-world applications, including student education, scientific research, and financial analysis. However, despite their remarkable potential, LLM-based agents remain vulnerable to hallucination issues, which can result in erroneous task execution and undermine the reliability of the overall system design. Addressing this critical challenge requires a deep understanding and a systematic consolidation of recent advances on LLM-based agents. To this end, we present the first comprehensive survey of hallucinations in LLM-based agents. By carefully analyzing the complete workflow of agents, we propose a new taxonomy that identifies different types of agent hallucinations occurring at different stages. Furthermore, we conduct an in-depth examination of eighteen triggering causes underlying the emergence of agent hallucinations. Through a detailed review of a large number of existing studies, we summarize approaches for hallucination mitigation and detection, and highlight promising directions for future research. We hope this survey will inspire further efforts toward addressing hallucinations in LLM-based agents, ultimately contributing to the development of more robust and reliable agent systems.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "203",
        "title": "Prompt-DAS: Annotation-Efficient Prompt Learning for Domain Adaptive Semantic Segmentation of Electron Microscopy Images",
        "author": [
            "Jiabao Chen",
            "Shan Xiong",
            "Jialin Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18973",
        "abstract": "Domain adaptive segmentation (DAS) of numerous organelle instances from large-scale electron microscopy (EM) is a promising way to enable annotation-efficient learning. Inspired by SAM, we propose a promptable multitask framework, namely Prompt-DAS, which is flexible enough to utilize any number of point prompts during the adaptation training stage and testing stage. Thus, with varying prompt configurations, Prompt-DAS can perform unsupervised domain adaptation (UDA) and weakly supervised domain adaptation (WDA), as well as interactive segmentation during testing. Unlike the foundation model SAM, which necessitates a prompt for each individual object instance, Prompt-DAS is only trained on a small dataset and can utilize full points on all instances, sparse points on partial instances, or even no points at all, facilitated by the incorporation of an auxiliary center-point detection task. Moreover, a novel prompt-guided contrastive learning is proposed to enhance discriminative feature learning. Comprehensive experiments conducted on challenging benchmarks demonstrate the effectiveness of the proposed approach over existing UDA, WDA, and SAM-based approaches.",
        "tags": [
            "Detection",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "204",
        "title": "Category-Level Object Shape and Pose Estimation in Less Than a Millisecond",
        "author": [
            "Lorenzo Shaikewitz",
            "Tim Nguyen",
            "Luca Carlone"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18979",
        "abstract": "Object shape and pose estimation is a foundational robotics problem, supporting tasks from manipulation to scene understanding and navigation. We present a fast local solver for shape and pose estimation which requires only category-level object priors and admits an efficient certificate of global optimality. Given an RGB-D image of an object, we use a learned front-end to detect sparse, category-level semantic keypoints on the target object. We represent the target object's unknown shape using a linear active shape model and pose a maximum a posteriori optimization problem to solve for position, orientation, and shape simultaneously. Expressed in unit quaternions, this problem admits first-order optimality conditions in the form of an eigenvalue problem with eigenvector nonlinearities. Our primary contribution is to solve this problem efficiently with self-consistent field iteration, which only requires computing a 4-by-4 matrix and finding its minimum eigenvalue-vector pair at each iterate. Solving a linear system for the corresponding Lagrange multipliers gives a simple global optimality certificate. One iteration of our solver runs in about 100 microseconds, enabling fast outlier rejection. We test our method on synthetic data and a variety of real-world settings, including two public datasets and a drone tracking scenario. Code is released at https://github.com/MIT-SPARK/Fast-ShapeAndPose.",
        "tags": [
            "Pose Estimation",
            "Robotics"
        ]
    },
    {
        "id": "205",
        "title": "Simulating Online Social Media Conversations on Controversial Topics Using AI Agents Calibrated on Real-World Data",
        "author": [
            "Elisa Composta",
            "Nicolo' Fontana",
            "Francesco Corso",
            "Francesco Pierri"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18985",
        "abstract": "Online social networks offer a valuable lens to analyze both individual and collective phenomena. Researchers often use simulators to explore controlled scenarios, and the integration of Large Language Models (LLMs) makes these simulations more realistic by enabling agents to understand and generate natural language content. In this work, we investigate the behavior of LLM-based agents in a simulated microblogging social network. We initialize agents with realistic profiles calibrated on real-world online conversations from the 2022 Italian political election and extend an existing simulator by introducing mechanisms for opinion modeling. We examine how LLM agents simulate online conversations, interact with others, and evolve their opinions under different scenarios. Our results show that LLM agents generate coherent content, form connections, and build a realistic social network structure. However, their generated content displays less heterogeneity in tone and toxicity compared to real data. We also find that LLM-based opinion dynamics evolve over time in ways similar to traditional mathematical models. Varying parameter configurations produces no significant changes, indicating that simulations require more careful cognitive modeling at initialization to replicate human behavior more faithfully. Overall, we demonstrate the potential of LLMs for simulating user behavior in social environments, while also identifying key challenges in capturing heterogeneity and complex dynamics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "206",
        "title": "CR-Net: Scaling Parameter-Efficient Training with Cross-Layer Low-Rank Structure",
        "author": [
            "Boao Kong",
            "Junzhu Liang",
            "Yuxi Liu",
            "Renjia Deng",
            "Kun Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18993",
        "abstract": "Low-rank architectures have become increasingly important for efficient large language model (LLM) pre-training, providing substantial reductions in both parameter complexity and memory/computational demands. Despite these advantages, current low-rank methods face three critical shortcomings: (1) compromised model performance, (2) considerable computational overhead, and (3) limited activation memory savings. To address these limitations, we propose Cross-layer Low-Rank residual Network (CR-Net), an innovative parameter-efficient framework inspired by our discovery that inter-layer activation residuals possess low-rank properties. CR-Net implements this insight through a dual-path architecture that efficiently reconstructs layer activations by combining previous-layer outputs with their low-rank differences, thereby maintaining high-rank information with minimal parameters. We further develop a specialized activation recomputation strategy tailored for CR-Net that dramatically reduces memory requirements. Extensive pre-training experiments across model scales from 60M to 7B parameters demonstrate that CR-Net consistently outperforms state-of-the-art low-rank frameworks while requiring fewer computational resources and less memory.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "207",
        "title": "VIR-Bench: Evaluating Geospatial and Temporal Understanding of MLLMs via Travel Video Itinerary Reconstruction",
        "author": [
            "Hao Wang",
            "Eiki Murata",
            "Lingfang Zhang",
            "Ayako Sato",
            "So Fukuda",
            "Ziqi Yin",
            "Wentao Hu",
            "Keisuke Nakao",
            "Yusuke Nakamura",
            "Sebastian Zwirner",
            "Yi-Chia Chen",
            "Hiroyuki Otomo",
            "Hiroki Ouchi",
            "Daisuke Kawahara"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19002",
        "abstract": "Recent advances in multimodal large language models (MLLMs) have significantly enhanced video understanding capabilities, opening new possibilities for practical applications. Yet current video benchmarks focus largely on indoor scenes or short-range outdoor activities, leaving the challenges associated with long-distance travel largely unexplored. Mastering extended geospatial-temporal trajectories is critical for next-generation MLLMs, underpinning real-world tasks such as embodied-AI planning and navigation. To bridge this gap, we present VIR-Bench, a novel benchmark consisting of 200 travel videos that frames itinerary reconstruction as a challenging task designed to evaluate and push forward MLLMs' geospatial-temporal intelligence. Experimental results reveal that state-of-the-art MLLMs, including proprietary ones, struggle to achieve high scores, underscoring the difficulty of handling videos that span extended spatial and temporal scales. Moreover, we conduct an in-depth case study in which we develop a prototype travel-planning agent that leverages the insights gained from VIR-Bench. The agent's markedly improved itinerary recommendations verify that our evaluation protocol not only benchmarks models effectively but also translates into concrete performance gains in user-facing applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "208",
        "title": "Unveiling Chain of Step Reasoning for Vision-Language Models with Fine-grained Rewards",
        "author": [
            "Honghao Chen",
            "Xingzhou Lou",
            "Xiaokun Feng",
            "Kaiqi Huang",
            "Xinlong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19003",
        "abstract": "Chain of thought reasoning has demonstrated remarkable success in large language models, yet its adaptation to vision-language reasoning remains an open challenge with unclear best practices. Existing attempts typically employ reasoning chains at a coarse-grained level, which struggles to perform fine-grained structured reasoning and, more importantly, are difficult to evaluate the reward and quality of intermediate reasoning. In this work, we delve into chain of step reasoning for vision-language models, enabling assessing reasoning step quality accurately and leading to effective reinforcement learning and inference-time scaling with fine-grained rewards. We present a simple, effective, and fully transparent framework, including the step-level reasoning data, process reward model (PRM), and reinforcement learning training. With the proposed approaches, our models set strong baselines with consistent improvements on challenging vision-language benchmarks. More importantly, we conduct a thorough empirical analysis and ablation study, unveiling the impact of each component and several intriguing properties of inference-time scaling. We believe this paper serves as a baseline for vision-language models and offers insights into more complex multimodal reasoning. Our dataset, PRM, and code will be available at https://github.com/baaivision/CoS.",
        "tags": [
            "CoT",
            "LLM",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "209",
        "title": "Pure Vision Language Action (VLA) Models: A Comprehensive Survey",
        "author": [
            "Dapeng Zhang",
            "Jin Sun",
            "Chenghui Hu",
            "Xiaoyan Wu",
            "Zhenlong Yuan",
            "Rui Zhou",
            "Fei Shen",
            "Qingguo Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19012",
        "abstract": "The emergence of Vision Language Action (VLA) models marks a paradigm shift from traditional policy-based control to generalized robotics, reframing Vision Language Models (VLMs) from passive sequence generators into active agents for manipulation and decision-making in complex, dynamic environments. This survey delves into advanced VLA methods, aiming to provide a clear taxonomy and a systematic, comprehensive review of existing research. It presents a comprehensive analysis of VLA applications across different scenarios and classifies VLA approaches into several paradigms: autoregression-based, diffusion-based, reinforcement-based, hybrid, and specialized methods; while examining their motivations, core strategies, and implementations in detail. In addition, foundational datasets, benchmarks, and simulation platforms are introduced. Building on the current VLA landscape, the review further proposes perspectives on key challenges and future directions to advance research in VLA models and generalizable robotics. By synthesizing insights from over three hundred recent studies, this survey maps the contours of this rapidly evolving field and highlights the opportunities and challenges that will shape the development of scalable, general-purpose VLA methods.",
        "tags": [
            "Diffusion",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "210",
        "title": "Fully Learnable Neural Reward Machines",
        "author": [
            "Hazem Dewidar",
            "Elena Umili"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19017",
        "abstract": "Non-Markovian Reinforcement Learning (RL) tasks present significant challenges, as agents must reason over entire trajectories of state-action pairs to make optimal decisions. A common strategy to address this is through symbolic formalisms, such as Linear Temporal Logic (LTL) or automata, which provide a structured way to express temporally extended objectives. However, these approaches often rely on restrictive assumptions -- such as the availability of a predefined Symbol Grounding (SG) function mapping raw observations to high-level symbolic representations, or prior knowledge of the temporal task. In this work, we propose a fully learnable version of Neural Reward Machines (NRM), which can learn both the SG function and the automaton end-to-end, removing any reliance on prior knowledge. Our approach is therefore as easily applicable as classic deep RL (DRL) approaches, while being far more explainable, because of the finite and compact nature of automata. Furthermore, we show that by integrating Fully Learnable Reward Machines (FLNRM) with DRL, our method outperforms previous approaches based on Recurrent Neural Networks (RNNs).",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "211",
        "title": "OmniBridge: Unified Multimodal Understanding, Generation, and Retrieval via Latent Space Alignment",
        "author": [
            "Teng Xiao",
            "Zuchao Li",
            "Lefei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19018",
        "abstract": "Recent advances in multimodal large language models (LLMs) have led to significant progress in understanding, generation, and retrieval tasks. However, current solutions often treat these tasks in isolation or require training LLMs from scratch, resulting in high computational costs and limited generalization across modalities. In this work, we present OmniBridge, a unified and modular multimodal framework that supports vision-language understanding, generation, and retrieval within a unified architecture. OmniBridge adopts a language-centric design that reuses pretrained LLMs and introduces a lightweight bidirectional latent alignment module. To address the challenge of task interference, we propose a two-stage decoupled training strategy: supervised fine-tuning and latent space alignment for aligning LLM behavior with multimodal reasoning, and semantic-guided diffusion training to align cross-modal latent spaces via learnable query embeddings. Extensive experiments across a wide range of benchmarks demonstrate that OmniBridge achieves competitive or state-of-the-art performance in all three tasks. Moreover, our results highlight the effectiveness of latent space alignment for unifying multimodal modeling under a shared representation space. Code and models are released at https://github.com/xiao-xt/OmniBridge.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "212",
        "title": "Reduced-Order Model-Guided Reinforcement Learning for Demonstration-Free Humanoid Locomotion",
        "author": [
            "Shuai Liu",
            "Meng Cheng Lau"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19023",
        "abstract": "We introduce Reduced-Order Model-Guided Reinforcement Learning (ROM-GRL), a two-stage reinforcement learning framework for humanoid walking that requires no motion capture data or elaborate reward shaping. In the first stage, a compact 4-DOF (four-degree-of-freedom) reduced-order model (ROM) is trained via Proximal Policy Optimization. This generates energy-efficient gait templates. In the second stage, those dynamically consistent trajectories guide a full-body policy trained with Soft Actor--Critic augmented by an adversarial discriminator, ensuring the student's five-dimensional gait feature distribution matches the ROM's demonstrations. Experiments at 1 meter-per-second and 4 meter-per-second show that ROM-GRL produces stable, symmetric gaits with substantially lower tracking error than a pure-reward baseline. By distilling lightweight ROM guidance into high-dimensional policies, ROM-GRL bridges the gap between reward-only and imitation-based locomotion methods, enabling versatile, naturalistic humanoid behaviors without any human demonstrations.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "213",
        "title": "Weakly Supervised Food Image Segmentation using Vision Transformers and Segment Anything Model",
        "author": [
            "Ioannis Sarafis",
            "Alexandros Papadopoulos",
            "Anastasios Delopoulos"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19028",
        "abstract": "In this paper, we propose a weakly supervised semantic segmentation approach for food images which takes advantage of the zero-shot capabilities and promptability of the Segment Anything Model (SAM) along with the attention mechanisms of Vision Transformers (ViTs). Specifically, we use class activation maps (CAMs) from ViTs to generate prompts for SAM, resulting in masks suitable for food image segmentation. The ViT model, a Swin Transformer, is trained exclusively using image-level annotations, eliminating the need for pixel-level annotations during training. Additionally, to enhance the quality of the SAM-generated masks, we examine the use of image preprocessing techniques in combination with single-mask and multi-mask SAM generation strategies. The methodology is evaluated on the FoodSeg103 dataset, generating an average of 2.4 masks per image (excluding background), and achieving an mIoU of 0.54 for the multi-mask scenario. We envision the proposed approach as a tool to accelerate food image annotation tasks or as an integrated component in food and nutrition tracking applications.",
        "tags": [
            "SAM",
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "214",
        "title": "Charting a Decade of Computational Linguistics in Italy: The CLiC-it Corpus",
        "author": [
            "Chiara Alzetta",
            "Serena Auriemma",
            "Alessandro Bondielli",
            "Luca Dini",
            "Chiara Fazzone",
            "Alessio Miaschi",
            "Martina Miliani",
            "Marta Sartor"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19033",
        "abstract": "Over the past decade, Computational Linguistics (CL) and Natural Language Processing (NLP) have evolved rapidly, especially with the advent of Transformer-based Large Language Models (LLMs). This shift has transformed research goals and priorities, from Lexical and Semantic Resources to Language Modelling and Multimodality. In this study, we track the research trends of the Italian CL and NLP community through an analysis of the contributions to CLiC-it, arguably the leading Italian conference in the field. We compile the proceedings from the first 10 editions of the CLiC-it conference (from 2014 to 2024) into the CLiC-it Corpus, providing a comprehensive analysis of both its metadata, including author provenance, gender, affiliations, and more, as well as the content of the papers themselves, which address various topics. Our goal is to provide the Italian and international research communities with valuable insights into emerging trends and key developments over time, supporting informed decisions and future directions in the field.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "215",
        "title": "Position: Human-Robot Interaction in Embodied Intelligence Demands a Shift From Static Privacy Controls to Dynamic Learning",
        "author": [
            "Shuning Zhang",
            "Hong Jia",
            "Simin Li",
            "Ting Dang",
            "Yongquan `Owen' Hu",
            "Xin Yi",
            "Hewu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19041",
        "abstract": "The reasoning capabilities of embodied agents introduce a critical, under-explored inferential privacy challenge, where the risk of an agent generate sensitive conclusions from ambient data. This capability creates a fundamental tension between an agent's utility and user privacy, rendering traditional static controls ineffective. To address this, this position paper proposes a framework that reframes privacy as a dynamic learning problem grounded in theory of Contextual Integrity (CI). Our approach enables agents to proactively learn and adapt to individual privacy norms through interaction, outlining a research agenda to develop embodied agents that are both capable and function as trustworthy safeguards of user privacy.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "216",
        "title": "ManipForce: Force-Guided Policy Learning with Frequency-Aware Representation for Contact-Rich Manipulation",
        "author": [
            "Geonhyup Lee",
            "Yeongjin Lee",
            "Kangmin Kim",
            "Seongju Lee",
            "Sangjun Noh",
            "Seunghyeok Back",
            "Kyoobin Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19047",
        "abstract": "Contact-rich manipulation tasks such as precision assembly require precise control of interaction forces, yet existing imitation learning methods rely mainly on vision-only demonstrations. We propose ManipForce, a handheld system designed to capture high-frequency force-torque (F/T) and RGB data during natural human demonstrations for contact-rich manipulation. Building on these demonstrations, we introduce the Frequency-Aware Multimodal Transformer (FMT). FMT encodes asynchronous RGB and F/T signals using frequency- and modality-aware embeddings and fuses them via bi-directional cross-attention within a transformer diffusion policy. Through extensive experiments on six real-world contact-rich manipulation tasks - such as gear assembly, box flipping, and battery insertion - FMT trained on ManipForce demonstrations achieves robust performance with an average success rate of 83% across all tasks, substantially outperforming RGB-only baselines. Ablation and sampling-frequency analyses further confirm that incorporating high-frequency F/T data and cross-modal integration improves policy performance, especially in tasks demanding high precision and stable contact.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "217",
        "title": "A DyL-Unet framework based on dynamic learning for Temporally Consistent Echocardiographic Segmentation",
        "author": [
            "Jierui Qu",
            "Jianchun Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19052",
        "abstract": "Accurate segmentation of cardiac anatomy in echocardiography is essential for cardiovascular diagnosis and treatment. Yet echocardiography is prone to deformation and speckle noise, causing frame-to-frame segmentation jitter. Even with high accuracy in single-frame segmentation, temporal instability can weaken functional estimates and impair clinical interpretability. To address these issues, we propose DyL-UNet, a dynamic learning-based temporal consistency U-Net segmentation architecture designed to achieve temporally stable and precise echocardiographic segmentation. The framework constructs an Echo-Dynamics Graph (EDG) through dynamic learning to extract dynamic information from videos. DyL-UNet incorporates multiple Swin-Transformer-based encoder-decoder branches for processing single-frame images. It further introduces Cardiac Phase-Dynamics Attention (CPDA) at the skip connections, which uses EDG-encoded dynamic features and cardiac-phase cues to enforce temporal consistency during segmentation. Extensive experiments on the CAMUS and EchoNet-Dynamic datasets demonstrate that DyL-UNet maintains segmentation accuracy comparable to existing methods while achieving superior temporal consistency, providing a reliable solution for automated clinical echocardiography.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "218",
        "title": "RELATE: Relation Extraction in Biomedical Abstracts with LLMs and Ontology Constraints",
        "author": [
            "Olawumi Olasunkanmi",
            "Mathew Satursky",
            "Hong Yi",
            "Chris Bizon",
            "Harlin Lee",
            "Stanley Ahalt"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19057",
        "abstract": "Biomedical knowledge graphs (KGs) are vital for drug discovery and clinical decision support but remain incomplete. Large language models (LLMs) excel at extracting biomedical relations, yet their outputs lack standardization and alignment with ontologies, limiting KG integration. We introduce RELATE, a three-stage pipeline that maps LLM-extracted relations to standardized ontology predicates using ChemProt and the Biolink Model. The pipeline includes: (1) ontology preprocessing with predicate embeddings, (2) similarity-based retrieval enhanced with SapBERT, and (3) LLM-based reranking with explicit negation handling. This approach transforms relation extraction from free-text outputs to structured, ontology-constrained representations. On the ChemProt benchmark, RELATE achieves 52% exact match and 94% accuracy@10, and in 2,400 HEAL Project abstracts, it effectively rejects irrelevant associations (0.4%) and identifies negated assertions. RELATE captures nuanced biomedical relationships while ensuring quality for KG augmentation. By combining vector search with contextual LLM reasoning, RELATE provides a scalable, semantically accurate framework for converting unstructured biomedical literature into standardized KGs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "219",
        "title": "ColorBlindnessEval: Can Vision-Language Models Pass Color Blindness Tests?",
        "author": [
            "Zijian Ling",
            "Han Zhang",
            "Yazhuo Zhou",
            "Jiahao Cui"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19070",
        "abstract": "This paper presents ColorBlindnessEval, a novel benchmark designed to evaluate the robustness of Vision-Language Models (VLMs) in visually adversarial scenarios inspired by the Ishihara color blindness test. Our dataset comprises 500 Ishihara-like images featuring numbers from 0 to 99 with varying color combinations, challenging VLMs to accurately recognize numerical information embedded in complex visual patterns. We assess 9 VLMs using Yes/No and open-ended prompts and compare their performance with human participants. Our experiments reveal limitations in the models' ability to interpret numbers in adversarial contexts, highlighting prevalent hallucination issues. These findings underscore the need to improve the robustness of VLMs in complex visual environments. ColorBlindnessEval serves as a valuable tool for benchmarking and improving the reliability of VLMs in real-world applications where accuracy is critical.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "220",
        "title": "WaveletGaussian: Wavelet-domain Diffusion for Sparse-view 3D Gaussian Object Reconstruction",
        "author": [
            "Hung Nguyen",
            "Runfa Li",
            "An Le",
            "Truong Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19073",
        "abstract": "3D Gaussian Splatting (3DGS) has become a powerful representation for image-based object reconstruction, yet its performance drops sharply in sparse-view settings. Prior works address this limitation by employing diffusion models to repair corrupted renders, subsequently using them as pseudo ground truths for later optimization. While effective, such approaches incur heavy computation from the diffusion fine-tuning and repair steps. We present WaveletGaussian, a framework for more efficient sparse-view 3D Gaussian object reconstruction. Our key idea is to shift diffusion into the wavelet domain: diffusion is applied only to the low-resolution LL subband, while high-frequency subbands are refined with a lightweight network. We further propose an efficient online random masking strategy to curate training pairs for diffusion fine-tuning, replacing the commonly used, but inefficient, leave-one-out strategy. Experiments across two benchmark datasets, Mip-NeRF 360 and OmniObject3D, show WaveletGaussian achieves competitive rendering quality while substantially reducing training time.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "221",
        "title": "Code Driven Planning with Domain-Adaptive Critic",
        "author": [
            "Zikang Tian",
            "Shaohui Peng",
            "Du Huang",
            "Jiaming Guo",
            "Ruizhi Chen",
            "Rui Zhang",
            "Xishan Zhang",
            "Yuxuan Guo",
            "Zidong Du",
            "Qi Guo",
            "Ling Li",
            "Yewen Pu",
            "Xing Hu",
            "Yunji Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19077",
        "abstract": "Large Language Models (LLMs) have been widely adopted as task planners for AI agents in sequential decision-making problems, leveraging their extensive world knowledge. However, the gap between their general knowledge and environment-specific requirements often leads to inaccurate plans. To address this, existing approaches rely on frequent LLM queries to iteratively refine plans based on immediate environmental feedback, which incurs substantial query costs. However, this refinement is typically guided by short-term environmental feedback, limiting LLMs from developing plans aligned with long-term rewards. We propose Code Driven Planning with Domain-Adaptive Critic (CoPiC). Instead of relying on frequent queries, CoPiC employs LLMs to generate a diverse set of high-level planning programs, which iteratively produce and refine candidate plans. A trained domain-adaptive critic then evaluates these candidates and selects the one most aligned with long-term rewards for execution. Using high-level planning programs as planner and domain-adaptive critic as estimator, CoPiC improves planning while significantly reducing query costs. Results in ALFWorld, NetHack, and StarCraft II Unit Building show that CoPiC outperforms advanced LLM-based baselines, AdaPlanner and Reflexion, achieving an average (1) 23.33% improvement in success rate and (2) 91.27% reduction in query costs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "222",
        "title": "Diffusion Bridge Variational Inference for Deep Gaussian Processes",
        "author": [
            "Jian Xu",
            "Qibin Zhao",
            "John Paisley",
            "Delu Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19078",
        "abstract": "Deep Gaussian processes (DGPs) enable expressive hierarchical Bayesian modeling but pose substantial challenges for posterior inference, especially over inducing variables. Denoising diffusion variational inference (DDVI) addresses this by modeling the posterior as a time-reversed diffusion from a simple Gaussian prior. However, DDVI's fixed unconditional starting distribution remains far from the complex true posterior, resulting in inefficient inference trajectories and slow convergence. In this work, we propose Diffusion Bridge Variational Inference (DBVI), a principled extension of DDVI that initiates the reverse diffusion from a learnable, data-dependent initial distribution. This initialization is parameterized via an amortized neural network and progressively adapted using gradients from the ELBO objective, reducing the posterior gap and improving sample efficiency. To enable scalable amortization, we design the network to operate on the inducing inputs, which serve as structured, low-dimensional summaries of the dataset and naturally align with the inducing variables' shape. DBVI retains the mathematical elegance of DDVI, including Girsanov-based ELBOs and reverse-time SDEs,while reinterpreting the prior via a Doob-bridged diffusion process. We derive a tractable training objective under this formulation and implement DBVI for scalable inference in large-scale DGPs. Across regression, classification, and image reconstruction tasks, DBVI consistently outperforms DDVI and other variational baselines in predictive accuracy, convergence speed, and posterior quality.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "223",
        "title": "World4RL: Diffusion World Models for Policy Refinement with Reinforcement Learning for Robotic Manipulation",
        "author": [
            "Zhennan Jiang",
            "Kai Liu",
            "Yuxin Qin",
            "Shuai Tian",
            "Yupeng Zheng",
            "Mingcai Zhou",
            "Chao Yu",
            "Haoran Li",
            "Dongbin Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19080",
        "abstract": "Robotic manipulation policies are commonly initialized through imitation learning, but their performance is limited by the scarcity and narrow coverage of expert data. Reinforcement learning can refine polices to alleviate this limitation, yet real-robot training is costly and unsafe, while training in simulators suffers from the sim-to-real gap. Recent advances in generative models have demonstrated remarkable capabilities in real-world simulation, with diffusion models in particular excelling at generation. This raises the question of how diffusion model-based world models can be combined to enhance pre-trained policies in robotic manipulation. In this work, we propose World4RL, a framework that employs diffusion-based world models as high-fidelity simulators to refine pre-trained policies entirely in imagined environments for robotic manipulation. Unlike prior works that primarily employ world models for planning, our framework enables direct end-to-end policy optimization. World4RL is designed around two principles: pre-training a diffusion world model that captures diverse dynamics on multi-task datasets and refining policies entirely within a frozen world model to avoid online real-world interactions. We further design a two-hot action encoding scheme tailored for robotic manipulation and adopt diffusion backbones to improve modeling fidelity. Extensive simulation and real-world experiments demonstrate that World4RL provides high-fidelity environment modeling and enables consistent policy refinement, yielding significantly higher success rates compared to imitation learning and other baselines. More visualization results are available at https://world4rl.github.io/.",
        "tags": [
            "Diffusion",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "224",
        "title": "A Mega-Study of Digital Twins Reveals Strengths, Weaknesses and Opportunities for Further Improvement",
        "author": [
            "Tiany Peng",
            "George Gui",
            "Daniel J. Merlau",
            "Grace Jiarui Fan",
            "Malek Ben Sliman",
            "Melanie Brucks",
            "Eric J. Johnson",
            "Vicki Morwitz",
            "Abdullah Althenayyan",
            "Silvia Bellezza",
            "Dante Donati",
            "Hortense Fong",
            "Elizabeth Friedman",
            "Ariana Guevara",
            "Mohamed Hussein",
            "Kinshuk Jerath",
            "Bruce Kogut",
            "Kristen Lane",
            "Hannah Li",
            "Patryk Perkowski",
            "Oded Netzer",
            "Olivier Toubia"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19088",
        "abstract": "Do \"digital twins\" capture individual responses in surveys and experiments? We run 19 pre-registered studies on a national U.S. panel and their LLM-powered digital twins (constructed based on previously-collected extensive individual-level data) and compare twin and human answers across 164 outcomes. The correlation between twin and human answers is modest (approximately 0.2 on average) and twin responses are less variable than human responses. While constructing digital twins based on rich individual-level data improves our ability to capture heterogeneity across participants and predict relative differences between them, it does not substantially improve our ability to predict the exact answers given by specific participants or enhance predictions of population means. Twin performance varies by domain and is higher among more educated, higher-income, and ideologically moderate participants. These results suggest current digital twins can capture some degree of relative differences but are unreliable for individual-level predictions and sample mean and variance estimation, underscoring the need for careful validation before use. Our data and code are publicly available for researchers and practitioners interested in optimizing digital twin pipelines.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "225",
        "title": "Pathways of Thoughts: Multi-Directional Thinking for Long-form Personalized Question Answering",
        "author": [
            "Alireza Salemi",
            "Cheng Li",
            "Mingyang Zhang",
            "Qiaozhu Mei",
            "Zhuowan Li",
            "Spurthi Amba Hombaiah",
            "Weize Kong",
            "Tao Chen",
            "Hamed Zamani",
            "Michael Bendersky"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19094",
        "abstract": "Personalization is essential for adapting question answering (QA) systems to user-specific information needs, thereby improving both accuracy and user satisfaction. However, personalized QA remains relatively underexplored due to challenges such as inferring preferences from long, noisy, and implicit contexts, and generating responses that are simultaneously correct, contextually appropriate, and aligned with user expectations and background knowledge. To address these challenges, we propose Pathways of Thoughts (PoT), an inference-stage method that applies to any large language model (LLM) without requiring task-specific fine-tuning. The approach models the reasoning of an LLM as an iterative decision process, where the model dynamically selects among cognitive operations such as reasoning, revision, personalization, and clarification. This enables exploration of multiple reasoning trajectories, producing diverse candidate responses that capture different perspectives. PoT then aggregates and reweights these candidates according to inferred user preferences, yielding a final personalized response that benefits from the complementary strengths of diverse reasoning paths. Experiments on the LaMP-QA benchmark for personalized QA show that PoT consistently outperforms competitive baselines, achieving up to a 13.1% relative improvement. Human evaluation corroborates these results, with annotators preferring outputs from PoT in 66% of cases and reporting ties in only 15% of cases.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "226",
        "title": "Investigating Traffic Accident Detection Using Multimodal Large Language Models",
        "author": [
            "Ilhan Skender",
            "Kailin Tong",
            "Selim Solmaz",
            "Daniel Watzenig"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19096",
        "abstract": "Traffic safety remains a critical global concern, with timely and accurate accident detection essential for hazard reduction and rapid emergency response. Infrastructure-based vision sensors offer scalable and efficient solutions for continuous real-time monitoring, facilitating automated detection of acci- dents directly from captured images. This research investigates the zero-shot capabilities of multimodal large language models (MLLMs) for detecting and describing traffic accidents using images from infrastructure cameras, thus minimizing reliance on extensive labeled datasets. Main contributions include: (1) Evaluation of MLLMs using the simulated DeepAccident dataset from CARLA, explicitly addressing the scarcity of diverse, realistic, infrastructure-based accident data through controlled simulations; (2) Comparative performance analysis between Gemini 1.5 and 2.0, Gemma 3 and Pixtral models in acci- dent identification and descriptive capabilities without prior fine-tuning; and (3) Integration of advanced visual analytics, specifically YOLO for object detection, Deep SORT for multi- object tracking, and Segment Anything (SAM) for instance segmentation, into enhanced prompts to improve model accuracy and explainability. Key numerical results show Pixtral as the top performer with an F1-score of 0.71 and 83% recall, while Gemini models gained precision with enhanced prompts (e.g., Gemini 1.5 rose to 90%) but suffered notable F1 and recall losses. Gemma 3 offered the most balanced performance with minimal metric fluctuation. These findings demonstrate the substantial potential of integrating MLLMs with advanced visual analytics techniques, enhancing their applicability in real-world automated traffic monitoring systems.",
        "tags": [
            "Detection",
            "LLM",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "227",
        "title": "FUNCanon: Learning Pose-Aware Action Primitives via Functional Object Canonicalization for Generalizable Robotic Manipulation",
        "author": [
            "Hongli Xu",
            "Lei Zhang",
            "Xiaoyue Hu",
            "Boyang Zhong",
            "Kaixin Bai",
            "ZoltÃ¡n-Csaba MÃ¡rton",
            "Zhenshan Bing",
            "Zhaopeng Chen",
            "Alois Christian Knoll",
            "Jianwei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19102",
        "abstract": "General-purpose robotic skills from end-to-end demonstrations often leads to task-specific policies that fail to generalize beyond the training distribution. Therefore, we introduce FunCanon, a framework that converts long-horizon manipulation tasks into sequences of action chunks, each defined by an actor, verb, and object. These chunks focus policy learning on the actions themselves, rather than isolated tasks, enabling compositionality and reuse. To make policies pose-aware and category-general, we perform functional object canonicalization for functional alignment and automatic manipulation trajectory transfer, mapping objects into shared functional frames using affordance cues from large vision language models. An object centric and action centric diffusion policy FuncDiffuser trained on this aligned data naturally respects object affordances and poses, simplifying learning and improving generalization ability. Experiments on simulated and real-world benchmarks demonstrate category-level generalization, cross-task behavior reuse, and robust sim2real deployment, showing that functional canonicalization provides a strong inductive bias for scalable imitation learning in complex manipulation domains. Details of the demo and supplemental material are available on our project website https://sites.google.com/view/funcanon.",
        "tags": [
            "Diffusion",
            "VLM"
        ]
    },
    {
        "id": "228",
        "title": "DRO-REBEL: Distributionally Robust Relative-Reward Regression for Fast and Efficient LLM Alignment",
        "author": [
            "Sharan Sahu",
            "Martin T. Wells"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19104",
        "abstract": "Reinforcement learning with human feedback (RLHF) has become crucial for aligning Large Language Models (LLMs) with human intent. However, existing offline RLHF approaches suffer from overoptimization, where models overfit to reward misspecification and drift from preferred behaviors observed during training. We introduce DRO-REBEL, a unified family of robust REBEL updates with type-$p$ Wasserstein, KL, and $\\chi^2$ ambiguity sets. Using Fenchel duality, each update reduces to a simple relative-reward regression, preserving scalability and avoiding PPO-style clipping or auxiliary value networks. Under standard linear-reward and log-linear policy classes with a data-coverage condition, we establish $O(n^{-1/4})$ estimation bounds with tighter constants than prior DRO-DPO approaches, and recover the minimax-optimal $O(n^{-1/2})$ rate via a localized Rademacher complexity analysis. The same analysis closes the gap for Wasserstein-DPO and KL-DPO, showing both also attain optimal parametric rates. We derive practical SGD algorithms for all three divergences: gradient regularization (Wasserstein), importance weighting (KL), and a fast 1-D dual solve ($\\chi^2$). Experiments on Emotion Alignment, the large-scale ArmoRM multi-objective benchmark, and HH-Alignment demonstrate strong worst-case robustness across unseen preference mixtures, model sizes, and data scales, with $\\chi^2$-REBEL showing consistently strong empirical performance. A controlled radius--coverage study validates a no-free-lunch trade-off: radii shrinking faster than empirical divergence concentration rates achieve minimax-optimal parametric rates but forfeit coverage, while coverage-guaranteeing radii incur $O(n^{-1/4})$ rates.",
        "tags": [
            "DPO",
            "LLM",
            "PPO",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "229",
        "title": "Spectral Signature Mapping from RGB Imagery for Terrain-Aware Navigation",
        "author": [
            "Sarvesh Prajapati",
            "Ananya Trivedi",
            "Nathaniel Hanson",
            "Bruce Maxwell",
            "Taskin Padir"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19105",
        "abstract": "Successful navigation in outdoor environments requires accurate prediction of the physical interactions between the robot and the terrain. To this end, several methods rely on geometric or semantic labels to classify traversable surfaces. However, such labels cannot distinguish visually similar surfaces that differ in material properties. Spectral sensors enable inference of material composition from surface reflectance measured across multiple wavelength bands. Although spectral sensing is gaining traction in robotics, widespread deployment remains constrained by the need for custom hardware integration, high sensor costs, and compute-intensive processing pipelines. In this paper, we present RGB Image to Spectral Signature Neural Network (RS-Net), a deep neural network designed to bridge the gap between the accessibility of RGB sensing and the rich material information provided by spectral data. RS-Net predicts spectral signatures from RGB patches, which we map to terrain labels and friction coefficients. The resulting terrain classifications are integrated into a sampling-based motion planner for a wheeled robot operating in outdoor environments. Likewise, the friction estimates are incorporated into a contact-force-based MPC for a quadruped robot navigating slippery surfaces. Thus, we introduce a framework that learns the task-relevant physical property once during training and thereafter relies solely on RGB sensing at test time. The code is available at https://github.com/prajapatisarvesh/RS-Net.",
        "tags": [
            "MPC",
            "Robotics"
        ]
    },
    {
        "id": "230",
        "title": "Human-Annotated NER Dataset for the Kyrgyz Language",
        "author": [
            "Timur Turatali",
            "Anton Alekseev",
            "Gulira Jumalieva",
            "Gulnara Kabaeva",
            "Sergey Nikolenko"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19109",
        "abstract": "We introduce KyrgyzNER, the first manually annotated named entity recognition dataset for the Kyrgyz language. Comprising 1,499 news articles from the http://24.KG news portal, the dataset contains 10,900 sentences and 39,075 entity mentions across 27 named entity classes. We show our annotation scheme, discuss the challenges encountered in the annotation process, and present the descriptive statistics. We also evaluate several named entity recognition models, including traditional sequence labeling approaches based on conditional random fields and state-of-the-art multilingual transformer-based models fine-tuned on our dataset. While all models show difficulties with rare entity categories, models such as the multilingual RoBERTa variant pretrained on a large corpus across many languages achieve a promising balance between precision and recall. These findings emphasize both the challenges and opportunities of using multilingual pretrained models for processing languages with limited resources. Although the multilingual RoBERTa model performed best, other multilingual models yielded comparable results. This suggests that future work exploring more granular annotation schemes may offer deeper insights for Kyrgyz language processing pipelines evaluation.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "231",
        "title": "A Fast Initialization Method for Neural Network Controllers: A Case Study of Image-based Visual Servoing Control for the multicopter Interception",
        "author": [
            "Chenxu Ke",
            "Congling Tian",
            "Kaichen Xu",
            "Ye Li",
            "Lingcong Bao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19110",
        "abstract": "Reinforcement learning-based controller design methods often require substantial data in the initial training phase. Moreover, the training process tends to exhibit strong randomness and slow convergence. It often requires considerable time or high computational resources. Another class of learning-based method incorporates Lyapunov stability theory to obtain a control policy with stability guarantees. However, these methods generally require an initially stable neural network control policy at the beginning of training. Evidently, a stable neural network controller can not only serve as an initial policy for reinforcement learning, allowing the training to focus on improving controller performance, but also act as an initial state for learning-based Lyapunov control methods. Although stable controllers can be designed using traditional control theory, designers still need to have a great deal of control design knowledge to address increasingly complicated control problems. The proposed neural network rapid initialization method in this paper achieves the initial training of the neural network control policy by constructing datasets that conform to the stability conditions based on the system model. Furthermore, using the image-based visual servoing control for multicopter interception as a case study, simulations and experiments were conducted to validate the effectiveness and practical performance of the proposed method. In the experiment, the trained control policy attains a final interception velocity of 15 m/s.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "232",
        "title": "Track-On2: Enhancing Online Point Tracking with Memory",
        "author": [
            "GÃ¶rkay Aydemir",
            "Weidi Xie",
            "Fatma GÃ¼ney"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19115",
        "abstract": "In this paper, we consider the problem of long-term point tracking, which requires consistent identification of points across video frames under significant appearance changes, motion, and occlusion. We target the online setting, i.e. tracking points frame-by-frame, making it suitable for real-time and streaming applications. We extend our prior model Track-On into Track-On2, a simple and efficient transformer-based model for online long-term tracking. Track-On2 improves both performance and efficiency through architectural refinements, more effective use of memory, and improved synthetic training strategies. Unlike prior approaches that rely on full-sequence access or iterative updates, our model processes frames causally and maintains temporal coherence via a memory mechanism, which is key to handling drift and occlusions without requiring future frames. At inference, we perform coarse patch-level classification followed by refinement. Beyond architecture, we systematically study synthetic training setups and their impact on memory behavior, showing how they shape temporal robustness over long sequences. Through comprehensive experiments, Track-On2 achieves state-of-the-art results across five synthetic and real-world benchmarks, surpassing prior online trackers and even strong offline methods that exploit bidirectional context. These results highlight the effectiveness of causal, memory-based architectures trained purely on synthetic data as scalable solutions for real-world point tracking. Project page: https://kuis-ai.github.io/track_on2",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "233",
        "title": "LLM-based Vulnerability Discovery through the Lens of Code Metrics",
        "author": [
            "Felix Weissberg",
            "Lukas Pirch",
            "Erik Imgrund",
            "Jonas MÃ¶ller",
            "Thorsten Eisenhofer",
            "Konrad Rieck"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19117",
        "abstract": "Large language models (LLMs) excel in many tasks of software engineering, yet progress in leveraging them for vulnerability discovery has stalled in recent years. To understand this phenomenon, we investigate LLMs through the lens of classic code metrics. Surprisingly, we find that a classifier trained solely on these metrics performs on par with state-of-the-art LLMs for vulnerability discovery. A root-cause analysis reveals a strong correlation and a causal effect between LLMs and code metrics: When the value of a metric is changed, LLM predictions tend to shift by a corresponding magnitude. This dependency suggests that LLMs operate at a similarly shallow level as code metrics, limiting their ability to grasp complex patterns and fully realize their potential in vulnerability discovery. Based on these findings, we derive recommendations on how research should more effectively address this challenge.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "234",
        "title": "Analysis on distribution and clustering of weight",
        "author": [
            "Chunming Ye",
            "Wenquan Tian",
            "Yalan Gao",
            "Songzhou Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19122",
        "abstract": "The study on architecture and parameter characteristics remains the hot topic in the research of large language models. In this paper we concern with the characteristics of weight which are used to analyze the correlations and differences between models. Two kinds of vectors-standard deviation vector and clustering vector-are proposed to describe features of models. In the first case, the weights are assumed to follow normal distribution. The standard deviation values of projection matrices are normalized to form Standard-Deviation Vector, representing the distribution characteristics of models. In the second case, the singular values from each weight projection matrix are extracted and grouped by K-Means algorithm. The grouped data with the same type matrix are combined as Clustering Vector to represent the correlation characteristics of models' weights. The study reveals that these two vectors can effectively distinguish between different models and clearly show the similarities among models of the same family. Moreover, after conducting LoRA fine-tuning with different datasets and models, it is found that the distribution of weights represented by standard deviation vector is directly influenced by the dataset, but the correlations between different weights represented by clustering vector remain unaffected and maintain a high consistency with the pre-trained model.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "235",
        "title": "Context-Aware Hierarchical Taxonomy Generation for Scientific Papers via LLM-Guided Multi-Aspect Clustering",
        "author": [
            "Kun Zhu",
            "Lizi Liao",
            "Yuxuan Gu",
            "Lei Huang",
            "Xiaocheng Feng",
            "Bing Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19125",
        "abstract": "The rapid growth of scientific literature demands efficient methods to organize and synthesize research findings. Existing taxonomy construction methods, leveraging unsupervised clustering or direct prompting of large language models (LLMs), often lack coherence and granularity. We propose a novel context-aware hierarchical taxonomy generation framework that integrates LLM-guided multi-aspect encoding with dynamic clustering. Our method leverages LLMs to identify key aspects of each paper (e.g., methodology, dataset, evaluation) and generates aspect-specific paper summaries, which are then encoded and clustered along each aspect to form a coherent hierarchy. In addition, we introduce a new evaluation benchmark of 156 expert-crafted taxonomies encompassing 11.6k papers, providing the first naturally annotated dataset for this task. Experimental results demonstrate that our method significantly outperforms prior approaches, achieving state-of-the-art performance in taxonomy coherence, granularity, and interpretability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "236",
        "title": "PipelineRL: Faster On-policy Reinforcement Learning for Long Sequence Generatio",
        "author": [
            "Alexandre PichÃ©",
            "Ehsan Kamaloo",
            "Rafael Pardinas",
            "Dzmitry Bahdanau"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19128",
        "abstract": "Reinforcement Learning (RL) is increasingly utilized to enhance the reasoning capabilities of Large Language Models (LLMs). However, effectively scaling these RL methods presents significant challenges, primarily due to the difficulty in maintaining high AI accelerator utilization without generating stale, off-policy data that harms common RL algorithms. This paper introduces PipelineRL, an approach designed to achieve a superior trade-off between hardware efficiency and data on-policyness for LLM training. PipelineRL employs concurrent asynchronous data generation and model training, distinguished by the novel in-flight weight updates. This mechanism allows the LLM generation engine to receive updated model weights with minimal interruption during the generation of token sequences, thereby maximizing both the accelerator utilization and the freshness of training data. Experiments conducted on long-form reasoning tasks using 128 H100 GPUs demonstrate that PipelineRL achieves approximately $\\sim 2x$ faster learning compared to conventional RL baselines while maintaining highly on-policy training data. A scalable and modular open-source implementation of PipelineRL is also released as a key contribution.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "237",
        "title": "On the Soundness and Consistency of LLM Agents for Executing Test Cases Written in Natural Language",
        "author": [
            "SÃ©bastien Salva",
            "Redha Taguelmimt"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19136",
        "abstract": "The use of natural language (NL) test cases for validating graphical user interface (GUI) applications is emerging as a promising direction to manually written executable test scripts, which are costly to develop and difficult to maintain. Recent advances in large language models (LLMs) have opened the possibility of the direct execution of NL test cases by LLM agents. This paper investigates this direction, focusing on the impact on NL test case unsoundness and on test case execution consistency. NL test cases are inherently unsound, as they may yield false failures due to ambiguous instructions or unpredictable agent behaviour. Furthermore, repeated executions of the same NL test case may lead to inconsistent outcomes, undermining test reliability. To address these challenges, we propose an algorithm for executing NL test cases with guardrail mechanisms and specialised agents that dynamically verify the correct execution of each test step. We introduce measures to evaluate the capabilities of LLMs in test execution and one measure to quantify execution consistency. We propose a definition of weak unsoundness to characterise contexts in which NL test case execution remains acceptable, with respect to the industrial quality levels Six Sigma. Our experimental evaluation with eight publicly available LLMs, ranging from 3B to 70B parameters, demonstrates both the potential and current limitations of current LLM agents for GUI testing. Our experiments show that Meta Llama 3.1 70B demonstrates acceptable capabilities in NL test case execution with high execution consistency (above the level 3-sigma). We provide prototype tools, test suites, and results.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "238",
        "title": "2D implementation of Kinetic-diffusion Monte Carlo in Eiron",
        "author": [
            "Oskar Lappi",
            "Emil LÃ¸vbak",
            "Thijs Steel",
            "Giovanni Samaey"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19140",
        "abstract": "Particle-based kinetic Monte Carlo simulations of neutral particles is one of the major computational bottlenecks in tokamak scrape-off layer simulations. This computational cost comes from the need to resolve individual collision events in high-collisional regimes. However, in such regimes, one can approximate the high-collisional kinetic dynamics with computationally cheaper diffusion. Asymptotic-preserving schemes make use of this limit to perform simulations in these regimes, without a blow-up in computational cost as incurred by standard kinetic approaches. One such scheme is Kinetic-diffusion Monte Carlo. In this paper, we present a first extension of this scheme to the two-dimensional setting and its implementation in the Eiron particle code. We then demonstrate that this implementation produces a significant speedup over kinetic simulations in high-collisional cases.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "239",
        "title": "BiGraspFormer: End-to-End Bimanual Grasp Transformer",
        "author": [
            "Kangmin Kim",
            "Seunghyeok Back",
            "Geonhyup Lee",
            "Sangbeom Lee",
            "Sangjun Noh",
            "Kyoobin Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19142",
        "abstract": "Bimanual grasping is essential for robots to handle large and complex objects. However, existing methods either focus solely on single-arm grasping or employ separate grasp generation and bimanual evaluation stages, leading to coordination problems including collision risks and unbalanced force distribution. To address these limitations, we propose BiGraspFormer, a unified end-to-end transformer framework that directly generates coordinated bimanual grasps from object point clouds. Our key idea is the Single-Guided Bimanual (SGB) strategy, which first generates diverse single grasp candidates using a transformer decoder, then leverages their learned features through specialized attention mechanisms to jointly predict bimanual poses and quality scores. This conditioning strategy reduces the complexity of the 12-DoF search space while ensuring coordinated bimanual manipulation. Comprehensive simulation experiments and real-world validation demonstrate that BiGraspFormer consistently outperforms existing methods while maintaining efficient inference speed (<0.05s), confirming the effectiveness of our framework. Code and supplementary materials are available at https://sites.google.com/bigraspformer",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "240",
        "title": "Anecdoctoring: Automated Red-Teaming Across Language and Place",
        "author": [
            "Alejandro Cuevas",
            "Saloni Dash",
            "Bharat Kumar Nayak",
            "Dan Vann",
            "Madeleine I. G. Daepp"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19143",
        "abstract": "Disinformation is among the top risks of generative artificial intelligence (AI) misuse. Global adoption of generative AI necessitates red-teaming evaluations (i.e., systematic adversarial probing) that are robust across diverse languages and cultures, but red-teaming datasets are commonly US- and English-centric. To address this gap, we propose \"anecdoctoring\", a novel red-teaming approach that automatically generates adversarial prompts across languages and cultures. We collect misinformation claims from fact-checking websites in three languages (English, Spanish, and Hindi) and two geographies (US and India). We then cluster individual claims into broader narratives and characterize the resulting clusters with knowledge graphs, with which we augment an attacker LLM. Our method produces higher attack success rates and offers interpretability benefits relative to few-shot prompting. Results underscore the need for disinformation mitigations that scale globally and are grounded in real-world adversarial misuse.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "241",
        "title": "LLMs as verification oracles for Solidity",
        "author": [
            "Massimo Bartoletti",
            "Enrico Lipparini",
            "Livio Pompianu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19153",
        "abstract": "Ensuring the correctness of smart contracts is critical, as even subtle flaws can lead to severe financial losses. While bug detection tools able to spot common vulnerability patterns can serve as a first line of defense, most real-world exploits and losses stem from errors in the contract business logic. Formal verification tools such as SolCMC and the Certora Prover address this challenge, but their impact remains limited by steep learning curves and restricted specification languages. Recent works have begun to explore the use of large language models (LLMs) for security-related tasks such as vulnerability detection and test generation. Yet, a fundamental question remains open: can LLMs serve as verification oracles, capable of reasoning about arbitrary contract-specific properties? In this paper, we provide the first systematic evaluation of GPT-5, a state-of-the-art reasoning LLM, in this role. We benchmark its performance on a large dataset of verification tasks, compare its outputs against those of established formal verification tools, and assess its practical effectiveness in real-world auditing scenarios. Our study combines quantitative metrics with qualitative analysis, and shows that recent reasoning-oriented LLMs can be surprisingly effective as verification oracles, suggesting a new frontier in the convergence of AI and formal methods for secure smart contract development and auditing.",
        "tags": [
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "242",
        "title": "Efficient Reinforcement Learning by Reducing Forgetting with Elephant Activation Functions",
        "author": [
            "Qingfeng Lan",
            "Gautham Vasan",
            "A. Rupam Mahmood"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19159",
        "abstract": "Catastrophic forgetting has remained a significant challenge for efficient reinforcement learning for decades (Ring 1994, Rivest and Precup 2003). While recent works have proposed effective methods to mitigate this issue, they mainly focus on the algorithmic side. Meanwhile, we do not fully understand what architectural properties of neural networks lead to catastrophic forgetting. This study aims to fill this gap by studying the role of activation functions in the training dynamics of neural networks and their impact on catastrophic forgetting in reinforcement learning setup. Our study reveals that, besides sparse representations, the gradient sparsity of activation functions also plays an important role in reducing forgetting. Based on this insight, we propose a new class of activation functions, elephant activation functions, that can generate both sparse outputs and sparse gradients. We show that by simply replacing classical activation functions with elephant activation functions in the neural networks of value-based algorithms, we can significantly improve the resilience of neural networks to catastrophic forgetting, thus making reinforcement learning more sample-efficient and memory-efficient.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "243",
        "title": "Circuit Complexity From Physical Constraints: Scaling Limitations of Attention",
        "author": [
            "Benjamin Prada",
            "Ankur Mali"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19161",
        "abstract": "We argue that the standard circuit complexity measures derived from $NC, AC, TC$ provide limited practical information and are now insufficient to further differentiate model expressivity. To address these new limitations, we define a novel notion of local uniformity and a family of circuit complexity classes $RC(\\cdot)$ that capture the fundamental constraints of scaling physical circuits. Through the lens of $RC(\\cdot)$, we show that attention mechanisms with $\\omega(n^{3/2})$ runtime cannot scale to accommodate the entropy of increasingly complex datasets. Our results simultaneously provide a methodology for defining meaningful bounds on transformer expressivity and naturally expose the restricted viability of attention.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "244",
        "title": "A Multimodal Stochastic Planning Approach for Navigation and Multi-Robot Coordination",
        "author": [
            "Mark Gonzales",
            "Ethan Oh",
            "Joseph Moore"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19168",
        "abstract": "In this paper, we present a receding-horizon, sampling-based planner capable of reasoning over multimodal policy distributions. By using the cross-entropy method to optimize a multimodal policy under a common cost function, our approach increases robustness against local minima and promotes effective exploration of the solution space. We show that our approach naturally extends to multi-robot collision-free planning, enables agents to share diverse candidate policies to avoid deadlocks, and allows teams to minimize a global objective without incurring the computational complexity of centralized optimization. Numerical simulations demonstrate that employing multiple modes significantly improves success rates in trap environments and in multi-robot collision avoidance. Hardware experiments further validate the approach's real-time feasibility and practical performance.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "245",
        "title": "Soft Tokens, Hard Truths",
        "author": [
            "Natasha Butt",
            "Ariel Kwiatkowski",
            "Ismail Labiad",
            "Julia Kempe",
            "Yann Ollivier"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19170",
        "abstract": "The use of continuous instead of discrete tokens during the Chain-of-Thought (CoT) phase of reasoning LLMs has garnered attention recently, based on the intuition that a continuous mixture of discrete tokens could simulate a superposition of several reasoning paths simultaneously. Theoretical results have formally proven that continuous tokens have much greater expressivity and can solve specific problems more efficiently. However, practical use of continuous tokens has been limited by strong training difficulties: previous works either just use continuous tokens at inference time on a pre-trained discrete-token model, or must distill the continuous CoT from ground-truth discrete CoTs and face computational costs that limit the CoT to very few tokens.\nThis is the first work introducing a scalable method to learn continuous CoTs via reinforcement learning (RL), without distilling from reference discrete CoTs. We use \"soft\" tokens: mixtures of tokens together with noise on the input embedding to provide RL exploration. Computational overhead is minimal, enabling us to learn continuous CoTs with hundreds of tokens. On math reasoning benchmarks with Llama and Qwen models up to 8B, training with continuous CoTs match discrete-token CoTs for pass@1 and surpass them for pass@32, showing greater CoT diversity. In systematic comparisons, the best-performing scenario is to train with continuous CoT tokens then use discrete tokens for inference, meaning the \"soft\" models can be deployed in a standard way. Finally, we show continuous CoT RL training better preserves the predictions of the base model on out-of-domain tasks, thus providing a softer touch to the base model.",
        "tags": [
            "CoT",
            "LLM",
            "LLaMA",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": "246",
        "title": "A noise-robust Monte Carlo method for electric field calculations in EMC3",
        "author": [
            "William De Deyn",
            "Ruben De Wolf",
            "Vince Maes",
            "Giovanni Samaey"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19178",
        "abstract": "One of the main codes to analyze and optimize stellarator configurations is the EMC3 code, which implements a state-of-the-art 3D Monte Carlo plasma edge transport code. However, so far, a self-consistent treatment of the E x B drift is absent. This plasma drift is known to significantly impact the particle and heat distribution in the plasma edge. It is desirable to incorporate this drift into EMC3 to improve the predictive capabilities of the code. The calculation of the E x B drift requires the approximation of the electric field E, which is proportional to the gradient of the electric potential $ \\varphi $. In previous work, the gradient was calculated with a least squares method based on a finite difference approximation of the electric potential. However, due to the stochastic nature of EMC3, the output plasma fields computed by the code are inherently noisy. The finite difference method further amplifies the noise, with the amplification growing as the grid size decreases. We continue from, which introduced a new noise-robust method for 1D derivatives. We extend the noise-robust method to 2D and apply it to the electric potential. We show that a PDE can be derived that describes the evolution of the electric field in case of a uniform diffusion coefficient. This PDE allows us to approximate the electric field directly with a Monte Carlo simulation, thus avoiding the need for a finite difference approximation. We illustrate the accuracy of the method and the noise robustness with a test case.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "247",
        "title": "The 1st Solution for MOSEv2 Challenge 2025: Long-term and Concept-aware Video Segmentation via SeC",
        "author": [
            "Mingqi Gao",
            "Jingkun Chen",
            "Yunqi Miao",
            "Gengshen Wu",
            "Zhijin Qin",
            "Jungong Han"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19183",
        "abstract": "This technical report explores the MOSEv2 track of the LSVOS Challenge, which targets complex semi-supervised video object segmentation. By analysing and adapting SeC, an enhanced SAM-2 framework, we conduct a detailed study of its long-term memory and concept-aware memory, showing that long-term memory preserves temporal continuity under occlusion and reappearance, while concept-aware memory supplies semantic priors that suppress distractors; together, these traits directly benefit several MOSEv2's core challenges. Our solution achieves a JF score of 39.89% on the test set, ranking 1st in the MOSEv2 track of the LSVOS Challenge.",
        "tags": [
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "248",
        "title": "Unveiling the Role of Learning Rate Schedules via Functional Scaling Laws",
        "author": [
            "Binghui Li",
            "Fengling Chen",
            "Zixun Huang",
            "Lean Wang",
            "Lei Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19189",
        "abstract": "Scaling laws have played a cornerstone role in guiding the training of large language models (LLMs). However, most existing works on scaling laws primarily focus on the final-step loss, overlooking the loss dynamics during the training process and, crucially, the impact of learning rate schedule (LRS). In this paper, we aim to bridge this gap by studying a teacher-student kernel regression setup trained via online stochastic gradient descent (SGD). Leveraging a novel intrinsic time viewpoint and stochastic differential equation (SDE) modeling of SGD, we introduce the Functional Scaling Law (FSL), which characterizes the evolution of population risk during the training process for general LRSs. Remarkably, the impact of the LRSs is captured through an explicit convolution-type functional term, making their effects fully tractable. To illustrate the utility of FSL, we analyze three widely used LRSs -- constant, exponential decay, and warmup-stable-decay (WSD) -- under both data-limited and compute-limited regimes. We provide theoretical justification for widely adopted empirical practices in LLMs pre-training such as (i) higher-capacity models are more data- and compute-efficient; (ii) learning rate decay can improve training efficiency; (iii) WSD-like schedules can outperform direct-decay schedules. Lastly, we explore the practical relevance of FSL as a surrogate model for fitting, predicting and optimizing the loss curves in LLM pre-training, with experiments conducted across model sizes ranging from 0.1B to 1B parameters. We hope our FSL framework can deepen the understanding of LLM pre-training dynamics and provide insights for improving large-scale model training.",
        "tags": [
            "LLM",
            "SDE"
        ]
    },
    {
        "id": "249",
        "title": "Reading Images Like Texts: Sequential Image Understanding in Vision-Language Models",
        "author": [
            "Yueyan Li",
            "Chenggong Zhao",
            "Zeyuan Zang",
            "Caixia Yuan",
            "Xiaojie Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19191",
        "abstract": "Vision-Language Models (VLMs) have demonstrated remarkable performance across a variety of real-world tasks. However, existing VLMs typically process visual information by serializing images, a method that diverges significantly from the parallel nature of human vision. Moreover, their opaque internal mechanisms hinder both deeper understanding and architectural innovation. Inspired by the dual-stream hypothesis of human vision, which distinguishes the \"what\" and \"where\" pathways, we deconstruct the visual processing in VLMs into object recognition and spatial perception for separate study. For object recognition, we convert images into text token maps and find that the model's perception of image content unfolds as a two-stage process from shallow to deep layers, beginning with attribute recognition and culminating in semantic disambiguation. For spatial perception, we theoretically derive and empirically verify the geometric structure underlying the positional representation in VLMs. Based on these findings, we introduce an instruction-agnostic token compression algorithm based on a plug-and-play visual decoder to improve decoding efficiency, and a RoPE scaling technique to enhance spatial reasoning. Through rigorous experiments, our work validates these analyses, offering a deeper understanding of VLM internals and providing clear principles for designing more capable future architectures.",
        "tags": [
            "RoPE",
            "VLM"
        ]
    },
    {
        "id": "250",
        "title": "Online Process Reward Leanring for Agentic Reinforcement Learning",
        "author": [
            "Xiaoqian Liu",
            "Ke Wang",
            "Yuchuan Wu",
            "Fei Huang",
            "Yongbin Li",
            "Junge Zhang",
            "Jianbin Jiao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19199",
        "abstract": "Large language models (LLMs) are increasingly trained with reinforcement learning (RL) as autonomous agents that reason and act over long horizons in interactive environments.\nHowever, sparse and sometimes unverifiable rewards make temporal credit assignment extremely challenging.\nRecent work attempts to integrate process supervision into agent learning but suffers from biased annotation, reward hacking, high-variance from overly fine-grained signals or failtures when state overlap is rare.\nWe therefore introduce Online Process Reward Learning (OPRL), a general credit-assignment strategy for agentic RL that integrates seamlessly with standard on-policy algorithms without relying on additional rollouts or explicit step labels.\nIn OPRL, we optimize an implicit process reward model (PRM) alternately with the agent's policy to transform trajectory preferences into implicit step rewards through a trajectory-based DPO objective.\nThese step rewards are then used to compute step-level advantages, which are combined with episode-level advantages from outcome rewards for policy update, creating a self-reinforcing loop.\nTheoretical findings guarantee that the learned step rewards are consistent with trajectory preferences and act as potential-based shaping rewards, providing bounded gradients to stabilize training.\nEmpirically, we evaluate OPRL on three distinct agent benmarks, including WebShop and VisualSokoban, as well as open-ended social interactions with unverfiable rewards in SOTOPIA.\nCrucially, OPRL shows superior performance over frontier LLMs and strong RL baselines across domains, achieving state-of-the-art results with higher sample-efficiency and lower variance during training.\nFurther analysis also demonstrates the efficient exploration by OPRL using fewer actions, underscoring its potential for agentic learning in real-world scenarios.",
        "tags": [
            "DPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "251",
        "title": "Vision-Free Retrieval: Rethinking Multimodal Search with Textual Scene Descriptions",
        "author": [
            "Ioanna Ntinou",
            "Alexandros Xenos",
            "Yassine Ouali",
            "Adrian Bulat",
            "Georgios Tzimiropoulos"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19203",
        "abstract": "Contrastively-trained Vision-Language Models (VLMs), such as CLIP, have become the standard approach for learning discriminative vision-language representations. However, these models often exhibit shallow language understanding, manifesting bag-of-words behaviour. These limitations are reinforced by their dual-encoder design, which induces a modality gap. Additionally, the reliance on vast web-collected data corpora for training makes the process computationally expensive and introduces significant privacy concerns. To address these limitations, in this work, we challenge the necessity of vision encoders for retrieval tasks by introducing a vision-free, single-encoder retrieval pipeline. Departing from the traditional text-to-image retrieval paradigm, we migrate to a text-to-text paradigm with the assistance of VLLM-generated structured image descriptions. We demonstrate that this paradigm shift has significant advantages, including a substantial reduction of the modality gap, improved compositionality, and better performance on short and long caption queries, all attainable with only a few hours of calibration on two GPUs. Additionally, substituting raw images with textual descriptions introduces a more privacy-friendly alternative for retrieval. To further assess generalisation and address some of the shortcomings of prior compositionality benchmarks, we release two benchmarks derived from Flickr30k and COCO, containing diverse compositional queries made of short captions, which we coin subFlickr and subCOCO. Our vision-free retriever matches and often surpasses traditional multimodal models. Importantly, our approach achieves state-of-the-art zero-shot performance on multiple retrieval and compositionality benchmarks, with models as small as 0.3B parameters. Code is available at: https://github.com/IoannaNti/LexiCLIP",
        "tags": [
            "CLIP",
            "Text-to-Image",
            "VLM"
        ]
    },
    {
        "id": "252",
        "title": "Long Story Short: Disentangling Compositionality and Long-Caption Understanding in VLMs",
        "author": [
            "Israfel Salazar",
            "Desmond Elliott",
            "Yova Kementchedjhieva"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19207",
        "abstract": "Contrastive vision-language models (VLMs) have made significant progress in binding visual and textual information, but understanding long, dense captions remains an open challenge. We hypothesize that compositionality, the capacity to reason about object-attribute bindings and inter-object relationships, is key to understanding longer captions. In this paper, we investigate the interaction between compositionality and long-caption understanding, asking whether training for one property enhances the other. We train and evaluate a range of models that target each of these capabilities. Our results reveal a bidirectional relationship: compositional training improves performance on long-caption retrieval, and training on long captions promotes compositionality. However, these gains are sensitive to data quality and model design. We find that training on poorly structured captions, or with limited parameter updates, fails to support generalization. Likewise, strategies that aim at retaining general alignment, such as freezing positional embeddings, do not improve compositional understanding. Overall, we find that compositional understanding and long-caption understanding are intertwined capabilities that can be jointly learned through training on dense, grounded descriptions. Despite these challenges, we show that models trained on high-quality, long-caption data can achieve strong performance in both tasks, offering practical guidance for improving VLM generalization.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "253",
        "title": "Enabling Plant Phenotyping in Weedy Environments using Multi-Modal Imagery via Synthetic and Generated Training Data",
        "author": [
            "Earl Ranario",
            "Ismael Mayanja",
            "Heesup Yun",
            "Brian N. Bailey",
            "J. Mason Earles"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19208",
        "abstract": "Accurate plant segmentation in thermal imagery remains a significant challenge for high throughput field phenotyping, particularly in outdoor environments where low contrast between plants and weeds and frequent occlusions hinder performance. To address this, we present a framework that leverages synthetic RGB imagery, a limited set of real annotations, and GAN-based cross-modality alignment to enhance semantic segmentation in thermal images. We trained models on 1,128 synthetic images containing complex mixtures of crop and weed plants in order to generate image segmentation masks for crop and weed plants. We additionally evaluated the benefit of integrating as few as five real, manually segmented field images within the training process using various sampling strategies. When combining all the synthetic images with a few labeled real images, we observed a maximum relative improvement of 22% for the weed class and 17% for the plant class compared to the full real-data baseline. Cross-modal alignment was enabled by translating RGB to thermal using CycleGAN-turbo, allowing robust template matching without calibration. Results demonstrated that combining synthetic data with limited manual annotations and cross-domain translation via generative models can significantly boost segmentation performance in complex field environments for multi-model imagery.",
        "tags": [
            "GAN",
            "Segmentation"
        ]
    },
    {
        "id": "254",
        "title": "A Knowledge Graph and a Tripartite Evaluation Framework Make Retrieval-Augmented Generation Scalable and Transparent",
        "author": [
            "Olalekan K. Akindele",
            "Bhupesh Kumar Mishra",
            "Kenneth Y. Wertheim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19209",
        "abstract": "Large Language Models (LLMs) have significantly enhanced conversational Artificial Intelligence(AI) chatbots; however, domain-specific accuracy and the avoidance of factual inconsistencies remain pressing challenges, particularly for large datasets. Designing an effective chatbot with appropriate methods and evaluating its effectiveness is among the challenges in this domain. This study presents a Retrieval Augmented Generation (RAG) chatbot that harnesses a knowledge graph and vector search retrieval to deliver precise, context-rich responses in an exemplary use case from over high-volume engineering project-related emails, thereby minimising the need for document chunking. A central innovation of this work is the introduction of RAG Evaluation (RAG-Eval), a novel chain-of-thought LLM-based tripartite evaluation framework specifically developed to assess RAG applications. This framework operates in parallel with the chatbot, jointly assessing the user's query, the retrieved document, and the generated response, enabling a holistic evaluation across multiple quality metrics like query relevance, factual accuracy, coverage, coherence and fluency. The resulting scoring system is provided directly to users as a confidence score (1 to 100%), enabling quick identification of possible misaligned or incomplete answers. This proposed approach promotes transparency and rapid verification by incorporating metadata email IDs, timestamps into responses. Experimental comparisons against BERTScore and G-EVAL for summarisation evaluation tasks confirm its effectiveness, and empirical analysis also shows RAG-Eval reliably detects factual gaps and query mismatches, thereby fostering trust in high demand, data centric environments. These findings highlight a scalable path for developing accurate, user-verifiable chatbots that bridge the gap between high-level conversational fluency and factual accuracy.",
        "tags": [
            "CoT",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "255",
        "title": "Steering Multimodal Large Language Models Decoding for Context-Aware Safety",
        "author": [
            "Zheyuan Liu",
            "Zhangchen Xu",
            "Guangyao Dou",
            "Xiangchi Yuan",
            "Zhaoxuan Tan",
            "Radha Poovendran",
            "Meng Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19212",
        "abstract": "Multimodal Large Language Models (MLLMs) are increasingly deployed in real-world applications, yet their ability to make context-aware safety decisions remains limited. Existing methods often fail to balance oversensitivity (unjustified refusals of benign queries) and undersensitivity (missed detection of visually grounded risks), leaving a persistent gap in safety alignment. To address this issue, we introduce Safety-aware Contrastive Decoding (SafeCoDe), a lightweight and model-agnostic decoding framework that dynamically adjusts token generation based on multimodal context. SafeCoDe operates in two stages: (1) a contrastive decoding mechanism that highlights tokens sensitive to visual context by contrasting real and Gaussian-noised images, and (2) a global-aware token modulation strategy that integrates scene-level reasoning with token-level adjustment to adapt refusals according to the predicted safety verdict. Extensive experiments across diverse MLLM architectures and safety benchmarks, covering undersensitivity, oversensitivity, and general safety evaluations, show that SafeCoDe consistently improves context-sensitive refusal behaviors while preserving model helpfulness.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "256",
        "title": "Video Killed the Energy Budget: Characterizing the Latency and Power Regimes of Open Text-to-Video Models",
        "author": [
            "Julien Delavande",
            "Regis Pierrard",
            "Sasha Luccioni"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19222",
        "abstract": "Recent advances in text-to-video (T2V) generation have enabled the creation of high-fidelity, temporally coherent clips from natural language prompts. Yet these systems come with significant computational costs, and their energy demands remain poorly understood. In this paper, we present a systematic study of the latency and energy consumption of state-of-the-art open-source T2V models. We first develop a compute-bound analytical model that predicts scaling laws with respect to spatial resolution, temporal length, and denoising steps. We then validate these predictions through fine-grained experiments on WAN2.1-T2V, showing quadratic growth with spatial and temporal dimensions, and linear scaling with the number of denoising steps. Finally, we extend our analysis to six diverse T2V models, comparing their runtime and energy profiles under default settings. Our results provide both a benchmark reference and practical insights for designing and deploying more sustainable generative video systems.",
        "tags": [
            "Text-to-Video"
        ]
    },
    {
        "id": "257",
        "title": "MsFIN: Multi-scale Feature Interaction Network for Traffic Accident Anticipation",
        "author": [
            "Tongshuai Wu",
            "Chao Lu",
            "Ze Song",
            "Yunlong Lin",
            "Sizhe Fan",
            "Xuemei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19227",
        "abstract": "With the widespread deployment of dashcams and advancements in computer vision, developing accident prediction models from the dashcam perspective has become critical for proactive safety interventions. However, two key challenges persist: modeling feature-level interactions among traffic participants (often occluded in dashcam views) and capturing complex, asynchronous multi-temporal behavioral cues preceding accidents. To deal with these two challenges, a Multi-scale Feature Interaction Network (MsFIN) is proposed for early-stage accident anticipation from dashcam videos. MsFIN has three layers for multi-scale feature aggregation, temporal feature processing and multi-scale feature post fusion, respectively. For multi-scale feature aggregation, a Multi-scale Module is designed to extract scene representations at short-term, mid-term and long-term temporal scales. Meanwhile, the Transformer architecture is leveraged to facilitate comprehensive feature interactions. Temporal feature processing captures the sequential evolution of scene and object features under causal constraints. In the multi-scale feature post fusion stage, the network fuses scene and object features across multiple temporal scales to generate a comprehensive risk representation. Experiments on DAD and DADA datasets show that MsFIN significantly outperforms state-of-the-art models with single-scale feature extraction in both prediction correctness and earliness. Ablation studies validate the effectiveness of each module in MsFIN, highlighting how the network achieves superior performance through multi-scale feature fusion and contextual interaction modeling.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "258",
        "title": "CompLLM: Compression for Long Context Q&A",
        "author": [
            "Gabriele Berton",
            "Jayakrishnan Unnikrishnan",
            "Son Tran",
            "Mubarak Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19228",
        "abstract": "Large Language Models (LLMs) face significant computational challenges when processing long contexts due to the quadratic complexity of self-attention. While soft context compression methods, which map input text to smaller latent representations, have shown promise, their real-world adoption is limited. Existing techniques typically compress the context as a single unit, which leads to quadratic compression complexity and an inability to reuse computations across queries with overlapping contexts. In this work, we introduce CompLLM, a soft compression technique designed for practical deployment. Instead of processing the context holistically, CompLLM divides it into segments and compresses each one independently. This simple design choice yields three critical properties: efficiency, as the compression step scales linearly with the context length; scalability, enabling models trained on short sequences (e.g., 1k tokens) to generalize to contexts of 100k tokens; and reusability, allowing compressed segments to be cached and reused across different queries. Our experiments show that with a 2x compression rate, at high context lengths CompLLM speeds up Time To First Token (TTFT) by up to 4x and reduces the KV cache size by 50%. Furthermore, CompLLM achieves performance comparable to that obtained with the uncompressed context, and even surpasses it on very long sequences, demonstrating its effectiveness and practical utility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "259",
        "title": "DevFD: Developmental Face Forgery Detection by Learning Shared and Orthogonal LoRA Subspaces",
        "author": [
            "Tianshuo Zhang",
            "Li Gao",
            "Siran Peng",
            "Xiangyu Zhu",
            "Zhen Lei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19230",
        "abstract": "The rise of realistic digital face generation and manipulation poses significant social risks. The primary challenge lies in the rapid and diverse evolution of generation techniques, which often outstrip the detection capabilities of existing models. To defend against the ever-evolving new types of forgery, we need to enable our model to quickly adapt to new domains with limited computation and data while avoiding forgetting previously learned forgery types. In this work, we posit that genuine facial samples are abundant and relatively stable in acquisition methods, while forgery faces continuously evolve with the iteration of manipulation techniques. Given the practical infeasibility of exhaustively collecting all forgery variants, we frame face forgery detection as a continual learning problem and allow the model to develop as new forgery types emerge. Specifically, we employ a Developmental Mixture of Experts (MoE) architecture that uses LoRA models as its individual experts. These experts are organized into two groups: a Real-LoRA to learn and refine knowledge of real faces, and multiple Fake-LoRAs to capture incremental information from different forgery types. To prevent catastrophic forgetting, we ensure that the learning direction of Fake-LoRAs is orthogonal to the established subspace. Moreover, we integrate orthogonal gradients into the orthogonal loss of Fake-LoRAs, preventing gradient interference throughout the training process of each task. Experimental results under both the datasets and manipulation types incremental protocols demonstrate the effectiveness of our method.",
        "tags": [
            "Detection",
            "LoRA",
            "MoE"
        ]
    },
    {
        "id": "260",
        "title": "Stability and Generalization of Adversarial Diffusion Training",
        "author": [
            "Hesam Hosseini",
            "Ying Cao",
            "Ali H. Sayed"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19234",
        "abstract": "Algorithmic stability is an established tool for analyzing generalization. While adversarial training enhances model robustness, it often suffers from robust overfitting and an enlarged generalization gap. Although recent work has established the convergence of adversarial training in decentralized networks, its generalization properties remain unexplored. This work presents a stability-based generalization analysis of adversarial training under the diffusion strategy for convex losses. We derive a bound showing that the generalization error grows with both the adversarial perturbation strength and the number of training steps, a finding consistent with single-agent case but novel for decentralized settings. Numerical experiments on logistic regression validate these theoretical predictions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "261",
        "title": "AgentInit: Initializing LLM-based Multi-Agent Systems via Diversity and Expertise Orchestration for Effective and Efficient Collaboration",
        "author": [
            "Chunhao Tian",
            "Yutong Wang",
            "Xuebo Liu",
            "Zhexuan Wang",
            "Liang Ding",
            "Miao Zhang",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19236",
        "abstract": "Proper initialization is crucial for any system, particularly in multi-agent systems (MAS), where it plays a pivotal role in determining both the system's efficiency and effectiveness. However, existing MAS initialization methods do not fully account for the collaborative needs of the generated agents in subsequent stages. Inspired by the principles of effective team composition, we propose AgentInit, which aims to optimize the structure of agent teams. Specifically, in addition to multi-round interactions and reflections between agents during agent generation, AgentInit incorporates a Natural Language to Format mechanism to ensure consistency and standardization. Balanced team selection strategies using Pareto principles are subsequently applied to jointly consider agent team diversity and task relevance to promote effective and efficient collaboration and enhance overall system performance. Experiments show that AgentInit consistently outperforms state-of-the-art initialization methods and pre-defined strategies across various frameworks and tasks, achieving an overall performance improvement of up to 1.2 and 1.6, respectively, while also significantly reducing token consumption. Further analysis confirms its strong transferability to similar tasks and verifies the effectiveness of its key components, demonstrating its capability and adaptability as a reliable MAS initialization method. Source code and models are available at https://github.com/1737423697/AgentInit.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "262",
        "title": "Lavida-O: Elastic Masked Diffusion Models for Unified Multimodal Understanding and Generation",
        "author": [
            "Shufan Li",
            "Jiuxiang Gu",
            "Kangning Liu",
            "Zhe Lin",
            "Zijun Wei",
            "Aditya Grover",
            "Jason Kuen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19244",
        "abstract": "We proposed Lavida-O, a unified multi-modal Masked Diffusion Model (MDM) capable of image understanding and generation tasks. Unlike existing multimodal diffsion language models such as MMaDa and Muddit which only support simple image-level understanding tasks and low-resolution image generation, Lavida-O exhibits many new capabilities such as object grounding, image-editing, and high-resolution (1024px) image synthesis. It is also the first unified MDM that uses its understanding capabilities to improve image generation and editing results through planning and iterative self-reflection. To allow effective and efficient training and sampling, Lavida-O ntroduces many novel techniques such as Elastic Mixture-of-Transformer architecture, universal text conditioning, and stratified sampling. \\ours~achieves state-of-the-art performance on a wide range of benchmarks such as RefCOCO object grounding, GenEval text-to-image generation, and ImgEdit image editing, outperforming existing autoregressive and continuous diffusion models such as Qwen2.5-VL and FluxKontext-dev, while offering considerable speedup at inference.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "263",
        "title": "Reinforcement Learning on Pre-Training Data",
        "author": [
            "Siheng Li",
            "Kejiao Li",
            "Zenan Xu",
            "Guanhua Huang",
            "Evander Yang",
            "Kun Li",
            "Haoyuan Wu",
            "Jiajia Wu",
            "Zihao Zheng",
            "Chenchen Zhang",
            "Kun Shi",
            "Kyrierl Deng",
            "Qi Yi",
            "Ruibin Xiong",
            "Tingqiang Xu",
            "Yuhao Jiang",
            "Jianfeng Yan",
            "Yuyuan Zeng",
            "Guanghui Xu",
            "Jinbao Xue",
            "Zhijiang Xu",
            "Zheng Fang",
            "Shuai Li",
            "Qibin Liu",
            "Xiaoxue Li",
            "Zhuoyu Li",
            "Yangyu Tao",
            "Fei Gao",
            "Cheng Jiang",
            "Bo Chao Wang",
            "Kai Liu",
            "Jianchen Zhu",
            "Wai Lam",
            "Wayyt Wang",
            "Bo Zhou",
            "Di Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19249",
        "abstract": "The growing disparity between the exponential scaling of computational resources and the finite growth of high-quality text data now constrains conventional scaling approaches for large language models (LLMs). To address this challenge, we introduce Reinforcement Learning on Pre-Training data (RLPT), a new training-time scaling paradigm for optimizing LLMs. In contrast to prior approaches that scale training primarily through supervised learning, RLPT enables the policy to autonomously explore meaningful trajectories to learn from pre-training data and improve its capability through reinforcement learning (RL). While existing RL strategies such as reinforcement learning from human feedback (RLHF) and reinforcement learning with verifiable rewards (RLVR) rely on human annotation for reward construction, RLPT eliminates this dependency by deriving reward signals directly from pre-training data. Specifically, it adopts a next-segment reasoning objective, rewarding the policy for accurately predicting subsequent text segments conditioned on the preceding context. This formulation allows RL to be scaled on pre-training data, encouraging the exploration of richer trajectories across broader contexts and thereby fostering more generalizable reasoning skills. Extensive experiments on both general-domain and mathematical reasoning benchmarks across multiple models validate the effectiveness of RLPT. For example, when applied to Qwen3-4B-Base, RLPT yields absolute improvements of $3.0$, $5.1$, $8.1$, $6.0$, $6.6$, and $5.3$ on MMLU, MMLU-Pro, GPQA-Diamond, KOR-Bench, AIME24, and AIME25, respectively. The results further demonstrate favorable scaling behavior, suggesting strong potential for continued gains with more compute. In addition, RLPT provides a solid foundation, extending the reasoning boundaries of LLMs and enhancing RLVR performance.",
        "tags": [
            "LLM",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "264",
        "title": "Adversarially-Refined VQ-GAN with Dense Motion Tokenization for Spatio-Temporal Heatmaps",
        "author": [
            "Gabriel Maldonado",
            "Narges Rashvand",
            "Armin Danesh Pazho",
            "Ghazal Alinezhad Noghre",
            "Vinit Katariya",
            "Hamed Tabkhi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19252",
        "abstract": "Continuous human motion understanding remains a core challenge in computer vision due to its high dimensionality and inherent redundancy. Efficient compression and representation are crucial for analyzing complex motion dynamics. In this work, we introduce an adversarially-refined VQ-GAN framework with dense motion tokenization for compressing spatio-temporal heatmaps while preserving the fine-grained traces of human motion. Our approach combines dense motion tokenization with adversarial refinement, which eliminates reconstruction artifacts like motion smearing and temporal misalignment observed in non-adversarial baselines. Our experiments on the CMU Panoptic dataset provide conclusive evidence of our method's superiority, outperforming the dVAE baseline by 9.31% SSIM and reducing temporal instability by 37.1%. Furthermore, our dense tokenization strategy enables a novel analysis of motion complexity, revealing that 2D motion can be optimally represented with a compact 128-token vocabulary, while 3D motion's complexity demands a much larger 1024-token codebook for faithful reconstruction. These results establish practical deployment feasibility across diverse motion analysis applications. The code base for this work is available at https://github.com/TeCSAR-UNCC/Pose-Quantization.",
        "tags": [
            "3D",
            "GAN"
        ]
    },
    {
        "id": "265",
        "title": "Reconstruction of a potential parameter in time-fractional diffusion problems via a Kohn--Vogelius type functional: Theoretical aspects",
        "author": [
            "Hamza Kahlaoui",
            "Mourad Hrizi",
            "Abdessamad Oulmelk",
            "Xiangcheng Zheng",
            "Ahmed Hendy"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19260",
        "abstract": "Of concern is the problem of reconstructing a space-dependent potential from boundary observations in the Caputo time-fractional diffusion equation, utilizing a stable and robust recovery method. We develop an algorithm to minimize the Kohn-Vogelius (KV) cost function, which measures the difference between the solutions of two excitations. The inverse potential problem is recast into an optimization problem, where the objective is to minimize a Kohn-Vogelius-type functional within a set of admissible potentials. We establish the well-posedness of this optimization problem by proving the existence and uniqueness of a minimizer and demonstrating its stability with respect to perturbations in the boundary data. Furthermore, we analyze the FrÃ©chet differentiability of the KV functional and prove the Lipschitz continuity of its gradient. These theoretical results enable the development of a convergent conjugate gradient algorithm for numerical reconstruction. The effectiveness and robustness of the proposed method are confirmed through several numerical examples in both one and two dimensions, including cases with noisy data.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "266",
        "title": "Cross-Cultural Transfer of Commonsense Reasoning in LLMs: Evidence from the Arab World",
        "author": [
            "Saeed Almheiri",
            "Rania Hossam",
            "Mena Attia",
            "Chenxi Wang",
            "Preslav Nakov",
            "Timothy Baldwin",
            "Fajri Koto"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19265",
        "abstract": "Large language models (LLMs) often reflect Western-centric biases, limiting their effectiveness in diverse cultural contexts. Although some work has explored cultural alignment, the potential for cross-cultural transfer, using alignment in one culture to improve performance in others, remains underexplored. This paper investigates cross-cultural transfer of commonsense reasoning in the Arab world, where linguistic and historical similarities coexist with local cultural differences. Using a culturally grounded commonsense reasoning dataset covering 13 Arab countries, we evaluate lightweight alignment methods such as in-context learning and demonstration-based reinforcement (DITTO), alongside baselines like supervised fine-tuning and direct preference optimization. Our results show that merely 12 culture-specific examples from one country can improve performance in others by 10\\% on average, within multilingual models. In addition, we demonstrate that out-of-culture demonstrations from Indonesia and US contexts can match or surpass in-culture alignment for MCQ reasoning, highlighting cultural commonsense transferability beyond the Arab world. These findings demonstrate that efficient cross-cultural alignment is possible and offer a promising approach to adapt LLMs to low-resource cultural settings.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "267",
        "title": "Extracting Conceptual Spaces from LLMs Using Prototype Embeddings",
        "author": [
            "Nitesh Kumar",
            "Usashi Chatterjee",
            "Steven Schockaert"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19269",
        "abstract": "Conceptual spaces represent entities and concepts using cognitively meaningful dimensions, typically referring to perceptual features. Such representations are widely used in cognitive science and have the potential to serve as a cornerstone for explainable AI. Unfortunately, they have proven notoriously difficult to learn, although recent LLMs appear to capture the required perceptual features to a remarkable extent. Nonetheless, practical methods for extracting the corresponding conceptual spaces are currently still lacking. While various methods exist for extracting embeddings from LLMs, extracting conceptual spaces also requires us to encode the underlying features. In this paper, we propose a strategy in which features (e.g. sweetness) are encoded by embedding the description of a corresponding prototype (e.g. a very sweet food). To improve this strategy, we fine-tune the LLM to align the prototype embeddings with the corresponding conceptual space dimensions. Our empirical analysis finds this approach to be highly effective.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "268",
        "title": "DRISHTIKON: A Multimodal Multilingual Benchmark for Testing Language Models' Understanding on Indian Culture",
        "author": [
            "Arijit Maji",
            "Raghvendra Kumar",
            "Akash Ghosh",
            "Anushka",
            "Nemil Shah",
            "Abhilekh Borah",
            "Vanshika Shah",
            "Nishant Mishra",
            "Sriparna Saha"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19274",
        "abstract": "We introduce DRISHTIKON, a first-of-its-kind multimodal and multilingual benchmark centered exclusively on Indian culture, designed to evaluate the cultural understanding of generative AI systems. Unlike existing benchmarks with a generic or global scope, DRISHTIKON offers deep, fine-grained coverage across India's diverse regions, spanning 15 languages, covering all states and union territories, and incorporating over 64,000 aligned text-image pairs. The dataset captures rich cultural themes including festivals, attire, cuisines, art forms, and historical heritage amongst many more. We evaluate a wide range of vision-language models (VLMs), including open-source small and large models, proprietary systems, reasoning-specialized VLMs, and Indic-focused models, across zero-shot and chain-of-thought settings. Our results expose key limitations in current models' ability to reason over culturally grounded, multimodal inputs, particularly for low-resource languages and less-documented traditions. DRISHTIKON fills a vital gap in inclusive AI research, offering a robust testbed to advance culturally aware, multimodally competent language technologies.",
        "tags": [
            "CoT",
            "VLM"
        ]
    },
    {
        "id": "269",
        "title": "What Characterizes Effective Reasoning? Revisiting Length, Review, and Structure of CoT",
        "author": [
            "Yunzhen Feng",
            "Julia Kempe",
            "Cheng Zhang",
            "Parag Jain",
            "Anthony Hartshorn"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19284",
        "abstract": "Large reasoning models (LRMs) spend substantial test-time compute on long chain-of-thought (CoT) traces, but what *characterizes* an effective CoT remains unclear. While prior work reports gains from lengthening CoTs and increasing review (revisiting earlier steps) via appended *wait* tokens, recent studies suggest that shorter thinking can outperform longer traces. We therefore conduct a systematic evaluation across ten LRMs on math and scientific reasoning. Contrary to the \"longer-is-better\" narrative, we find that both naive CoT lengthening and increased review are associated with *lower* accuracy.\nAs CoT unfolds step by step, token-level metrics can conflate verbosity with process quality. We introduce a graph view of CoT to extract structure and identify a single statistic-the *Failed-Step Fraction (FSF)*, the fraction of steps in abandoned branches-that consistently outpredicts length and review ratio for correctness across models. To probe causality, we design two interventions. First, we rank candidate CoTs by each metric at test time, where FSF yields the largest pass@1 gains; second, we edit CoTs to remove failed branches, which significantly improves accuracy, indicating that failed branches bias subsequent reasoning. Taken together, these results characterize effective CoTs as those that *fail less* and support *structure-aware* test-time scaling over indiscriminately generating long CoT.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "270",
        "title": "SOE: Sample-Efficient Robot Policy Self-Improvement via On-Manifold Exploration",
        "author": [
            "Yang Jin",
            "Jun Lv",
            "Han Xue",
            "Wendi Chen",
            "Chuan Wen",
            "Cewu Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19292",
        "abstract": "Intelligent agents progress by continually refining their capabilities through actively exploring environments. Yet robot policies often lack sufficient exploration capability due to action mode collapse. Existing methods that encourage exploration typically rely on random perturbations, which are unsafe and induce unstable, erratic behaviors, thereby limiting their effectiveness. We propose Self-Improvement via On-Manifold Exploration (SOE), a framework that enhances policy exploration and improvement in robotic manipulation. SOE learns a compact latent representation of task-relevant factors and constrains exploration to the manifold of valid actions, ensuring safety, diversity, and effectiveness. It can be seamlessly integrated with arbitrary policy models as a plug-in module, augmenting exploration without degrading the base policy performance. Moreover, the structured latent space enables human-guided exploration, further improving efficiency and controllability. Extensive experiments in both simulation and real-world tasks demonstrate that SOE consistently outperforms prior methods, achieving higher task success rates, smoother and safer exploration, and superior sample efficiency. These results establish on-manifold exploration as a principled approach to sample-efficient policy self-improvement. Project website: https://ericjin2002.github.io/SOE",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "271",
        "title": "Lyra: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation",
        "author": [
            "Sherwin Bahmani",
            "Tianchang Shen",
            "Jiawei Ren",
            "Jiahui Huang",
            "Yifeng Jiang",
            "Haithem Turki",
            "Andrea Tagliasacchi",
            "David B. Lindell",
            "Zan Gojcic",
            "Sanja Fidler",
            "Huan Ling",
            "Jun Gao",
            "Xuanchi Ren"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19296",
        "abstract": "The ability to generate virtual environments is crucial for applications ranging from gaming to physical AI domains such as robotics, autonomous driving, and industrial AI. Current learning-based 3D reconstruction methods rely on the availability of captured real-world multi-view data, which is not always readily available. Recent advancements in video diffusion models have shown remarkable imagination capabilities, yet their 2D nature limits the applications to simulation where a robot needs to navigate and interact with the environment. In this paper, we propose a self-distillation framework that aims to distill the implicit 3D knowledge in the video diffusion models into an explicit 3D Gaussian Splatting (3DGS) representation, eliminating the need for multi-view training data. Specifically, we augment the typical RGB decoder with a 3DGS decoder, which is supervised by the output of the RGB decoder. In this approach, the 3DGS decoder can be purely trained with synthetic data generated by video diffusion models. At inference time, our model can synthesize 3D scenes from either a text prompt or a single image for real-time rendering. Our framework further extends to dynamic 3D scene generation from a monocular input video. Experimental results show that our framework achieves state-of-the-art performance in static and dynamic 3D scene generation.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Robotics"
        ]
    },
    {
        "id": "272",
        "title": "VolSplat: Rethinking Feed-Forward 3D Gaussian Splatting with Voxel-Aligned Prediction",
        "author": [
            "Weijie Wang",
            "Yeqing Chen",
            "Zeyu Zhang",
            "Hengyu Liu",
            "Haoxiao Wang",
            "Zhiyuan Feng",
            "Wenkang Qin",
            "Zheng Zhu",
            "Donny Y. Chen",
            "Bohan Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19297",
        "abstract": "Feed-forward 3D Gaussian Splatting (3DGS) has emerged as a highly effective solution for novel view synthesis. Existing methods predominantly rely on a pixel-aligned Gaussian prediction paradigm, where each 2D pixel is mapped to a 3D Gaussian. We rethink this widely adopted formulation and identify several inherent limitations: it renders the reconstructed 3D models heavily dependent on the number of input views, leads to view-biased density distributions, and introduces alignment errors, particularly when source views contain occlusions or low texture. To address these challenges, we introduce VolSplat, a new multi-view feed-forward paradigm that replaces pixel alignment with voxel-aligned Gaussians. By directly predicting Gaussians from a predicted 3D voxel grid, it overcomes pixel alignment's reliance on error-prone 2D feature matching, ensuring robust multi-view consistency. Furthermore, it enables adaptive control over Gaussian density based on 3D scene complexity, yielding more faithful Gaussian point clouds, improved geometric consistency, and enhanced novel-view rendering quality. Experiments on widely used benchmarks including RealEstate10K and ScanNet demonstrate that VolSplat achieves state-of-the-art performance while producing more plausible and view-consistent Gaussian reconstructions. In addition to superior results, our approach establishes a more scalable framework for feed-forward 3D reconstruction with denser and more robust representations, paving the way for further research in wider communities. The video results, code and trained models are available on our project page: https://lhmd.top/volsplat.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "273",
        "title": "CAR-Flow: Condition-Aware Reparameterization Aligns Source and Target for Better Flow Matching",
        "author": [
            "Chen Chen",
            "Pengsheng Guo",
            "Liangchen Song",
            "Jiasen Lu",
            "Rui Qian",
            "Xinze Wang",
            "Tsu-Jui Fu",
            "Wei Liu",
            "Yinfei Yang",
            "Alex Schwing"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19300",
        "abstract": "Conditional generative modeling aims to learn a conditional data distribution from samples containing data-condition pairs. For this, diffusion and flow-based methods have attained compelling results. These methods use a learned (flow) model to transport an initial standard Gaussian noise that ignores the condition to the conditional data distribution. The model is hence required to learn both mass transport and conditional injection. To ease the demand on the model, we propose Condition-Aware Reparameterization for Flow Matching (CAR-Flow) -- a lightweight, learned shift that conditions the source, the target, or both distributions. By relocating these distributions, CAR-Flow shortens the probability path the model must learn, leading to faster training in practice. On low-dimensional synthetic data, we visualize and quantify the effects of CAR. On higher-dimensional natural image data (ImageNet-256), equipping SiT-XL/2 with CAR-Flow reduces FID from 2.07 to 1.68, while introducing less than 0.6% additional parameters.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "274",
        "title": "Residual Off-Policy RL for Finetuning Behavior Cloning Policies",
        "author": [
            "Lars Ankile",
            "Zhenyu Jiang",
            "Rocky Duan",
            "Guanya Shi",
            "Pieter Abbeel",
            "Anusha Nagabandi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19301",
        "abstract": "Recent advances in behavior cloning (BC) have enabled impressive visuomotor control policies. However, these approaches are limited by the quality of human demonstrations, the manual effort required for data collection, and the diminishing returns from increasing offline data. In comparison, reinforcement learning (RL) trains an agent through autonomous interaction with the environment and has shown remarkable success in various domains. Still, training RL policies directly on real-world robots remains challenging due to sample inefficiency, safety concerns, and the difficulty of learning from sparse rewards for long-horizon tasks, especially for high-degree-of-freedom (DoF) systems. We present a recipe that combines the benefits of BC and RL through a residual learning framework. Our approach leverages BC policies as black-box bases and learns lightweight per-step residual corrections via sample-efficient off-policy RL. We demonstrate that our method requires only sparse binary reward signals and can effectively improve manipulation policies on high-degree-of-freedom (DoF) systems in both simulation and the real world. In particular, we demonstrate, to the best of our knowledge, the first successful real-world RL training on a humanoid robot with dexterous hands. Our results demonstrate state-of-the-art performance in various vision-based tasks, pointing towards a practical pathway for deploying RL in the real world. Project website: https://residual-offpolicy-rl.github.io",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "275",
        "title": "End-Cut Preference in Survival Trees",
        "author": [
            "Xiaogang Su"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18477",
        "abstract": "The end-cut preference (ECP) problem, referring to the tendency to favor split points near the boundaries of a feature's range, is a well-known issue in CART (Breiman et al., 1984). ECP may induce highly imbalanced and biased splits, obscure weak signals, and lead to tree structures that are both unstable and difficult to interpret. For survival trees, we show that ECP also arises when using greedy search to select the optimal cutoff point by maximizing the log-rank test statistic. To address this issue, we propose a smooth sigmoid surrogate (SSS) approach, in which the hard-threshold indicator function is replaced by a smooth sigmoid function. We further demonstrate, both theoretically and through numerical illustrations, that SSS provides an effective remedy for mitigating or avoiding ECP.",
        "tags": [
            "SSS"
        ]
    },
    {
        "id": "276",
        "title": "No Verifiable Reward for Prosody: Toward Preference-Guided Prosody Learning in TTS",
        "author": [
            "Seungyoun Shin",
            "Dongha Ahn",
            "Jiwoo Kim",
            "Sungwook Jeon"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18531",
        "abstract": "Recent work reports gains in neural text-to-speech (TTS) with Group Relative Policy Optimization (GRPO). However, in the absence of a verifiable reward for \\textit{prosody}, GRPO trained on transcription-oriented signals (CER/NLL) lowers error rates yet collapses prosody into monotone, unnatural speech; adding speaker-similarity further destabilizes training and degrades CER. We address this with an \\textit{iterative Direct Preference Optimization (DPO)} scheme that uses only a few hundred human-labeled preference pairs per round to directly optimize prosodic naturalness while regularizing to the current model. On \\textbf{KoCC-TTS}, a curated dataset of authentic Korean call center interactions capturing task-oriented dialogues, our method attains the highest human preference (ELO) with competitive CER, outperforming GRPO and strong commercial baselines. These results suggest that when prosody cannot be rewarded automatically, \\textit{human preference optimization} offers a practical and data-efficient path to natural and robust TTS. The demo page is available at \\href{https://tts.ch.dev}",
        "tags": [
            "DPO",
            "GRPO"
        ]
    },
    {
        "id": "277",
        "title": "HarmoniFuse: A Component-Selective and Prompt-Adaptive Framework for Multi-Task Speech Language Modeling",
        "author": [
            "Yuke Si",
            "Runyan Yang",
            "Yingying Gao",
            "Junlan Feng",
            "Chao Deng",
            "Shilei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18570",
        "abstract": "Recent advances in large language models have facilitated the development of unified speech language models (SLMs) capable of supporting multiple speech tasks within a shared architecture. However, tasks such as automatic speech recognition (ASR) and speech emotion recognition (SER) rely on distinct types of information: ASR primarily depends on linguistic content, whereas SER requires the integration of both linguistic and paralinguistic cues. Existing multitask SLMs typically adopt naive parameter sharing or prompt-based conditioning without explicitly modeling the differences in information composition required by each task. Such designs risk task interference and performance degradation, especially under limited data conditions. To address these limitations, we propose HarmoniFuse, a component-selective and prompt-adaptive framework for multi-task speech language modeling. HarmoniFuse is designed to harmonize heterogeneous task demands by selecting and fusing task-relevant components of speech representations. Specifically, it integrates a gated speech encoder to extract task-specific acoustic features and a prompt-adaptive dynamic fusion module to aggregate transformer layers based on task characteristics. In addition, a batch-interleaved training strategy enables leveraging separate ASR and SER datasets without requiring joint annotation. Experimental results demonstrate that HarmoniFuse improves both ASR and SER performance, offering a scalable and robust solution for multitask speech understanding under realistic data constraints.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "278",
        "title": "SynSonic: Augmenting Sound Event Detection through Text-to-Audio Diffusion ControlNet and Effective Sample Filtering",
        "author": [
            "Jiarui Hai",
            "Mounya Elhilali"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18603",
        "abstract": "Data synthesis and augmentation are essential for Sound Event Detection (SED) due to the scarcity of temporally labeled data. While augmentation methods like SpecAugment and Mix-up can enhance model performance, they remain constrained by the diversity of existing samples. Recent generative models offer new opportunities, yet their direct application to SED is challenging due to the lack of precise temporal annotations and the risk of introducing noise through unreliable filtering. To address these challenges and enable generative-based augmentation for SED, we propose SynSonic, a data augmentation method tailored for this task. SynSonic leverages text-to-audio diffusion models guided by an energy-envelope ControlNet to generate temporally coherent sound events. A joint score filtering strategy with dual classifiers ensures sample quality, and we explore its practical integration into training pipelines. Experimental results show that SynSonic improves Polyphonic Sound Detection Scores (PSDS1 and PSDS2), enhancing both temporal localization and sound class discrimination.",
        "tags": [
            "ControlNet",
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "279",
        "title": "FlexSED: Towards Open-Vocabulary Sound Event Detection",
        "author": [
            "Jiarui Hai",
            "Helin Wang",
            "Weizhe Guo",
            "Mounya Elhilali"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18606",
        "abstract": "Despite recent progress in large-scale sound event detection (SED) systems capable of handling hundreds of sound classes, existing multi-class classification frameworks remain fundamentally limited. They cannot process free-text sound queries, which enable more flexible and user-friendly interaction, and they lack zero-shot capabilities and offer poor few-shot adaptability. Although text-query-based separation methods have been explored, they primarily focus on source separation and are ill-suited for SED tasks that require precise temporal localization and efficient detection across large and diverse sound vocabularies. In this paper, we propose FlexSED, an open-vocabulary sound event detection system. FlexSED builds on a pretrained audio SSL model and the CLAP text encoder, introducing an encoder-decoder composition and an adaptive fusion strategy to enable effective continuous training from pretrained weights. To ensure robust supervision, it also employs large language models (LLMs) to assist in event query selection during training, addressing challenges related to missing labels. As a result, FlexSED achieves superior performance compared to vanilla SED models on AudioSet-Strong, while demonstrating strong zero-shot and few-shot capabilities. We release the code and pretrained models to support future research and applications based on FlexSED.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "280",
        "title": "Scalable bayesian shadow tomography for quantum property estimation with set transformers",
        "author": [
            "Hyunho Cha",
            "Wonjung Kim",
            "Jungwoo Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18674",
        "abstract": "A scalable Bayesian machine learning framework is introduced for estimating scalar properties of an unknown quantum state from measurement data, which bypasses full density matrix reconstruction. This work is the first to integrate the classical shadows protocol with a permutation-invariant set transformer architecture, enabling the approach to predict and correct bias in existing estimators to approximate the true Bayesian posterior mean. Measurement outcomes are encoded as fixed-dimensional feature vectors, and the network outputs a residual correction to a baseline estimator. Scalability to large quantum systems is ensured by the polynomial dependence of input size on system size and number of measurements. On Greenberger-Horne-Zeilinger state fidelity and second-order RÃ©nyi entropy estimation tasks -- using random Pauli and random Clifford measurements -- this Bayesian estimator always achieves lower mean squared error than classical shadows alone, with more than a 99\\% reduction in the few copy regime.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "281",
        "title": "Complexity of Activity Patterns in a Bio-Inspired Hopfield-Type Network in Different Topologies",
        "author": [
            "Marco Cafiso",
            "Paolo Paradisi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18758",
        "abstract": "Neural network models capable of storing memory have been extensively studied in computer science and computational neuroscience. The Hopfield network is a prototypical example of a model designed for associative, or content-addressable, memory and has been analyzed in many forms. Further, ideas and methods from complex network theory have been incorporated into artificial neural networks and learning, emphasizing their structural properties. Nevertheless, the temporal dynamics also play a vital role in biological neural networks, whose temporal structure is a crucial feature to examine. Biological neural networks display complex intermittency and, thus, can be studied through the lens of the temporal complexity (TC) theory. The TC approach look at the metastability of self-organized states, characterized by a power-law decay in the inter-event time distribution and in the total activity distribution or a scaling behavior in the corresponding event-driven diffusion processes. In this study, we present a temporal complexity (TC) analysis of a biologically-inspired Hopfield-type neural network model. We conducted a comparative assessment between scale-free and random network topologies, with particular emphasis on their global activation patterns. Our parametric analysis revealed comparable dynamical behaviors across both neural network architectures. Furthermore, our investigation into temporal complexity characteristics uncovered that seemingly distinct dynamical patterns exhibit similar temporal complexity behaviors. In particular, similar power-law decay in the activity distribution and similar complexity levels are observed in both topologies, but with a much reduced noise in the scale-free topology. Notably, most of the complex dynamical profiles were consistently observed in scale-free network configurations, thus confirming the crucial role of hubs in neural network dynamics.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "282",
        "title": "On the Convergence of Policy Mirror Descent with Temporal Difference Evaluation",
        "author": [
            "Jiacai Liu",
            "Wenye Li",
            "Ke Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.18822",
        "abstract": "Policy mirror descent (PMD) is a general policy optimization framework in reinforcement learning, which can cover a wide range of typical policy optimization methods by specifying different mirror maps. Existing analysis of PMD requires exact or approximate evaluation (for example unbiased estimation via Monte Carlo simulation) of action values solely based on policy. In this paper, we consider policy mirror descent with temporal difference evaluation (TD-PMD). It is shown that, given the access to exact policy evaluations, the dimension-free $O(1/T)$ sublinear convergence still holds for TD-PMD with any constant step size and any initialization. In order to achieve this result, new monotonicity and shift invariance arguments have been developed. The dimension free $\\gamma$-rate linear convergence of TD-PMD is also established provided the step size is selected adaptively. For the two common instances of TD-PMD (i.e., TD-PQA and TD-NPG), it is further shown that they enjoy the convergence in the policy domain. Additionally, we investigate TD-PMD in the inexact setting and give the sample complexity for it to achieve the last iterate $\\varepsilon$-optimality under a generative model, which improves the last iterate sample complexity for PMD over the dependence on $1/(1-\\gamma)$.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "283",
        "title": "HD-PPT: Hierarchical Decoding of Content- and Prompt-Preference Tokens for Instruction-based TTS",
        "author": [
            "Sihang Nie",
            "Xiaofen Xing",
            "Jingyuan Xing",
            "Baiji Liu",
            "Xiangmin Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19001",
        "abstract": "Large Language Model (LLM)-based Text-to-Speech (TTS) models have already reached a high degree of naturalness. However, the precision control of TTS inference is still challenging. Although instruction-based Text-to-Speech (Instruct-TTS) models are proposed, these models still lack fine-grained control due to the modality gap between single-level text instructions and multilevel speech tokens. To address this limitation, we propose HD-PPT, a framework that transforms speech synthesis into a structured, hierarchical task. To enable fine-grained control, we introduce a novel speech codec to extract distinct prompt-preference and content-preference tokens from the complex speech tokens, supervised by automatic speech recognition (ASR) and cross-lingual audio-text pre-training (CLAP) objectives. To bridge the modality gap of these tokens, we propose a hierarchical decoding strategy, where the LLM generates tokens in a structured order: first semantic, then fine-grained style, and finally complete acoustic representation. Extensive experiments demonstrate that this hierarchical paradigm significantly improves instruction adherence and achieves state-of-the-art naturalness, validating our approach for precise and controllable speech synthesis. Audio samples are available at https://xxh333.github.io/.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "284",
        "title": "Training Flow Matching Models with Reliable Labels via Self-Purification",
        "author": [
            "Hyeongju Kim",
            "Yechan Yu",
            "June Young Yi",
            "Juheon Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19091",
        "abstract": "Training datasets are inherently imperfect, often containing mislabeled samples due to human annotation errors, limitations of tagging models, and other sources of noise. Such label contamination can significantly degrade the performance of a trained model. In this work, we introduce Self-Purifying Flow Matching (SPFM), a principled approach to filtering unreliable data within the flow-matching framework. SPFM identifies suspicious data using the model itself during the training process, bypassing the need for pretrained models or additional modules. Our experiments demonstrate that models trained with SPFM generate samples that accurately adhere to the specified conditioning, even when trained on noisy labels. Furthermore, we validate the robustness of SPFM on the TITW dataset, which consists of in-the-wild speech data, achieving performance that surpasses existing baselines.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "285",
        "title": "CayleyPy Growth: Efficient growth computations and hundreds of new conjectures on Cayley graphs (Brief version)",
        "author": [
            "A. Chervov",
            "D. Fedoriaka",
            "E. Konstantinova",
            "A. Naumov",
            "I. Kiselev",
            "A. Sheveleva",
            "I. Koltsov",
            "S. Lytkin",
            "A. Smolensky",
            "A. Soibelman",
            "F. Levkovich-Maslyuk",
            "R. Grimov",
            "D. Volovich",
            "A. Isakov",
            "A. Kostin",
            "M. Litvinov",
            "N. Vilkin-Krom",
            "A. Bidzhiev",
            "A. Krasnyi",
            "M. Evseev",
            "E. Geraseva",
            "L. Grunwald",
            "S. Galkin",
            "E. Koldunov",
            "S. Diner",
            "A. Chevychelov",
            "E. Kudasheva",
            "A. Sychev",
            "A. Kravchenko",
            "Z. Kogan",
            "A. Natyrova",
            "L. Shishina",
            "L. Cheldieva",
            "V. Zamkovoy",
            "D. Kovalenko",
            "O. Papulov",
            "S. Kudashev",
            "D. Shiltsov",
            "R. Turtayev",
            "O. Nikitina",
            "D. Mamayeva",
            "S. Nikolenko",
            "M. Obozov",
            "A. Titarenko",
            "A. Dolgorukova",
            "A. Aparnev",
            "O. Debeaupuis",
            "S. Alami C.",
            "H. Isambert"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19162",
        "abstract": "This is the third paper of the CayleyPy project applying artificial intelligence to problems in group theory. We announce the first public release of CayleyPy, an open source Python library for computations with Cayley and Schreier graphs. Compared with systems such as GAP and Sage, CayleyPy handles much larger graphs and performs several orders of magnitude faster.\nUsing CayleyPy we obtained about 200 new conjectures on Cayley and Schreier graphs, focused on diameters and growth. For many Cayley graphs of symmetric groups Sn we observe quasi polynomial diameter formulas: a small set of quadratic or linear polynomials indexed by n mod s. We conjecture that this is a general phenomenon, giving efficient diameter computation despite the problem being NP hard. We propose a refinement of the Babai type conjecture on diameters of Sn: n^2/2 + 4n upper bounds in the undirected case, compared to previous O(n^2) bounds. We also provide explicit generator families, related to involutions in a square with whiskers pattern, conjectured to maximize the diameter; search confirms this for all n up to 15. We further conjecture an answer to a question posed by V M Glushkov in 1968 on directed Cayley graphs generated by a cyclic shift and a transposition.\nFor nilpotent groups we conjecture an improvement of J S Ellenberg's results on upper unitriangular matrices over Z/pZ, showing linear dependence of diameter on p. Moreover.\nSome conjectures are LLM friendly, naturally stated as sorting problems verifiable by algorithms or Python code. To benchmark path finding we created more than 10 Kaggle datasets. CayleyPy works with arbitrary permutation or matrix groups and includes over 100 predefined generators. Our growth computation code outperforms GAP and Sage up to 1000 times in speed and size.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "286",
        "title": "Improving Test-Time Performance of RVQ-based Neural Codecs",
        "author": [
            "Hyeongju Kim",
            "Junhyeok Lee",
            "Jacob Morton",
            "Juheon Lee",
            "Jinhyeok Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19186",
        "abstract": "The residual vector quantization (RVQ) technique plays a central role in recent advances in neural audio codecs. These models effectively synthesize high-fidelity audio from a limited number of codes due to the hierarchical structure among quantization levels. In this paper, we propose an encoding algorithm to further enhance the synthesis quality of RVQ-based neural codecs at test-time. Firstly, we point out the suboptimal nature of quantized vectors generated by conventional methods. We demonstrate that quantization error can be mitigated by selecting a different set of codes. Subsequently, we present our encoding algorithm, designed to identify a set of discrete codes that achieve a lower quantization error. We then apply the proposed method to pre-trained models and evaluate its efficacy using diverse metrics. Our experimental findings validate that our method not only reduces quantization errors, but also improves synthesis quality.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "287",
        "title": "A Gradient Flow Approach to Solving Inverse Problems with Latent Diffusion Models",
        "author": [
            "Tim Y. J. Wang",
            "O. Deniz Akyildiz"
        ],
        "pdf": "https://arxiv.org/pdf/2509.19276",
        "abstract": "Solving ill-posed inverse problems requires powerful and flexible priors. We propose leveraging pretrained latent diffusion models for this task through a new training-free approach, termed Diffusion-regularized Wasserstein Gradient Flow (DWGF). Specifically, we formulate the posterior sampling problem as a regularized Wasserstein gradient flow of the Kullback-Leibler divergence in the latent space. We demonstrate the performance of our method on standard benchmarks using StableDiffusion (Rombach et al., 2022) as the prior.",
        "tags": [
            "Diffusion"
        ]
    }
]