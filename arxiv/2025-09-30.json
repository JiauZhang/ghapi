[
    {
        "id": "1",
        "title": "SPELUNKER: Item Similarity Search Using Large Language Models and Custom K-Nearest Neighbors",
        "author": [
            "Ana Rodrigues",
            "JoÃ£o Mata",
            "Rui Rego"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21323",
        "abstract": "This paper presents a hybrid system for intuitive item similarity search that combines a Large Language Model (LLM) with a custom K-Nearest Neighbors (KNN) algorithm. Unlike black-box dense vector systems, this architecture provides superior interpretability by first using an LLM to convert natural language queries into structured, attribute-based searches. This structured query then serves as input to a custom KNN algorithm with a BallTree search strategy, which uses a heterogeneous distance metric to preserve distinct data types. Our evaluation, conducted on a dataset of 500 wine reviews, demonstrates the system's effectiveness. The LLM achieved an F1-score of 0.9779 in information extraction, while also demonstrating high fidelity with a Jaro string similarity of 0.9321. When we augmented the KNN algorithm with LLM-based re-ranking, we observed a statistically significant improvement in recall (p=0.013), indicating the LLM's ability to identify and promote relevant items that align with nuanced user intent. This approach effectively bridges the gap between human language and machine-understandable item representations, offering a transparent and nuanced search capability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "2",
        "title": "PIR-RAG: A System for Private Information Retrieval in Retrieval-Augmented Generation",
        "author": [
            "Baiqiang Wang",
            "Qian Lou",
            "Mengxin Zheng",
            "Dongfang Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21325",
        "abstract": "Retrieval-Augmented Generation (RAG) has become a foundational component of modern AI systems, yet it introduces significant privacy risks by exposing user queries to service providers. To address this, we introduce PIR-RAG, a practical system for privacy-preserving RAG. PIR-RAG employs a novel architecture that uses coarse-grained semantic clustering to prune the search space, combined with a fast, lattice-based Private Information Retrieval (PIR) protocol. This design allows for the efficient retrieval of entire document clusters, uniquely optimizing for the end-to-end RAG workflow where full document content is required. Our comprehensive evaluation against strong baseline architectures, including graph-based PIR and Tiptoe-style private scoring, demonstrates PIR-RAG's scalability and its superior performance in terms of \"RAG-Ready Latency\"-the true end-to-end time required to securely fetch content for an LLM. Our work establishes PIR-RAG as a viable and highly efficient solution for privacy in large-scale AI systems.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "3",
        "title": "HetaRAG: Hybrid Deep Retrieval-Augmented Generation across Heterogeneous Data Stores",
        "author": [
            "Guohang Yan",
            "Yue Zhang",
            "Pinlong Cai",
            "Ding Wang",
            "Song Mao",
            "Hongwei Zhang",
            "Yaoze Zhang",
            "Hairong Zhang",
            "Xinyu Cai",
            "Botian Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21336",
        "abstract": "Retrieval-augmented generation (RAG) has become a dominant paradigm for mitigating knowledge hallucination and staleness in large language models (LLMs) while preserving data security. By retrieving relevant evidence from private, domain-specific corpora and injecting it into carefully engineered prompts, RAG delivers trustworthy responses without the prohibitive cost of fine-tuning. Traditional retrieval-augmented generation (RAG) systems are text-only and often rely on a single storage backend, most commonly a vector database. In practice, this monolithic design suffers from unavoidable trade-offs: vector search captures semantic similarity yet loses global context; knowledge graphs excel at relational precision but struggle with recall; full-text indexes are fast and exact yet semantically blind; and relational engines such as MySQL provide strong transactional guarantees but no semantic understanding. We argue that these heterogeneous retrieval paradigms are complementary, and propose a principled fusion scheme to orchestrate them synergistically, mitigating the weaknesses of any single modality. In this work we introduce HetaRAG, a hybrid, deep-retrieval augmented generation framework that orchestrates cross-modal evidence from heterogeneous data stores. We plan to design a system that unifies vector indices, knowledge graphs, full-text engines, and structured databases into a single retrieval plane, dynamically routing and fusing evidence to maximize recall, precision, and contextual fidelity. To achieve this design goal, we carried out preliminary explorations and constructed an initial RAG pipeline; this technical report provides a brief overview. The partial code is available at https://github.com/KnowledgeXLab/HetaRAG.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "4",
        "title": "From Embeddings to Equations: Genetic-Programming Surrogates for Interpretable Transformer Classification",
        "author": [
            "Mohammad Sadegh Khorshidi",
            "Navid Yazdanjue",
            "Hassan Gharoun",
            "Mohammad Reza Nikoo",
            "Fang Chen",
            "Amir H. Gandomi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21341",
        "abstract": "We study symbolic surrogate modeling of frozen Transformer embeddings to obtain compact, auditable classifiers with calibrated probabilities. For five benchmarks (SST2G, 20NG, MNIST, CIFAR10, MSC17), embeddings from ModernBERT, DINOv2, and SigLIP are partitioned on the training set into disjoint, information-preserving views via semantic-preserving feature partitioning (SPFP). A cooperative multi-population genetic program (MEGP) then learns additive, closed-form logit programs over these views. Across 30 runs per dataset we report F1, AUC, log-loss, Brier, expected calibration error (ECE), and symbolic complexity; a canonical model is chosen by a one-standard-error rule on validation F1 with a parsimony tie-break. Temperature scaling fitted on validation yields substantial ECE reductions on test. The resulting surrogates achieve strong discrimination (up to F1 around 0.99 on MNIST, CIFAR10, MSC17; around 0.95 on SST2G), while 20NG remains most challenging. We provide reliability diagrams, dimension usage and overlap statistics, contribution-based importances, and global effect profiles (PDP and ALE), demonstrating faithful, cross-modal explanations grounded in explicit programs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "5",
        "title": "Towards mitigating information leakage when evaluating safety monitors",
        "author": [
            "Gerard Boxo",
            "Aman Neelappa",
            "Shivam Raval"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21344",
        "abstract": "White box monitors that analyze model internals offer promising advantages for detecting potentially harmful behaviors in large language models, including lower computational costs and integration into layered defense http://systems.However, training and evaluating these monitors requires response exemplars that exhibit the target behaviors, typically elicited through prompting or fine-tuning. This presents a challenge when the information used to elicit behaviors inevitably leaks into the data that monitors ingest, inflating their effectiveness. We present a systematic framework for evaluating a monitor's performance in terms of its ability to detect genuine model behavior rather than superficial elicitation artifacts. Furthermore, we propose three novel strategies to evaluate the monitor: content filtering (removing deception-related text from inputs), score filtering (aggregating only over task-relevant tokens), and prompt distilled fine-tuned model organisms (models trained to exhibit deceptive behavior without explicit prompting). Using deception detection as a representative case study, we identify two forms of leakage that inflate monitor performance: elicitation leakage from prompts that explicitly request harmful behavior, and reasoning leakage from models that verbalize their deceptive actions. Through experiments on multiple deception benchmarks, we apply our proposed mitigation strategies and measure performance retention. Our evaluation of the monitors reveal three crucial findings: (1) Content filtering is a good mitigation strategy that allows for a smooth removal of elicitation signal and can decrease probe AUROC by 30\\% (2) Score filtering was found to reduce AUROC by 15\\% but is not as straightforward to attribute to (3) A finetuned model organism improves monitor evaluations but reduces their performance by upto 40\\%, even when re-trained.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "KV-Efficient VLA: A Method of Speed up Vision Language Model with RNN-Gated Chunked KV Cache",
        "author": [
            "Wanshun Xu",
            "Long Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21354",
        "abstract": "Vision-Language-Action (VLA) models promise unified robotic perception and control, yet their scalability is constrained by the quadratic cost of attention and the unbounded growth of key-value (KV) memory during long-horizon inference. While recent methods improve generalization through scaling backbone architectures, they often neglect the inference inefficiencies critical to real-time deployment. In this work, we present KV-Efficient VLA, a model-agnostic memory compression framework that addresses these limitations by introducing a lightweight, training-friendly mechanism to selectively retain high-utility context. Our method partitions the KV cache into fixed size chunks and employs a recurrent gating module to summarize and filter historical context according to learned utility scores. This design preserves recent fine-grained detail while aggressively pruning stale, low-relevance memory, all while maintaining causality. Theoretically, KV-Efficient VLA yields up to 1.21x inference speedup and 36% KV memory reduction, with minimal impact on task success. Our method integrates seamlessly into existing autoregressive and hybrid VLA stacks, enabling scalable inference without modifying training pipelines or downstream control logic.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "7",
        "title": "Influence Guided Context Selection for Effective Retrieval-Augmented Generation",
        "author": [
            "Jiale Deng",
            "Yanyan Shen",
            "Ziyuan Pei",
            "Youmin Chen",
            "Linpeng Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21359",
        "abstract": "Retrieval-Augmented Generation (RAG) addresses large language model (LLM) hallucinations by grounding responses in external knowledge, but its effectiveness is compromised by poor-quality retrieved contexts containing irrelevant or noisy information. While existing approaches attempt to improve performance through context selection based on predefined context quality assessment metrics, they show limited gains over standard RAG. We attribute this limitation to their failure in holistically utilizing available information (query, context list, and generator) for comprehensive quality assessment. Inspired by recent advances in data selection, we reconceptualize context quality assessment as an inference-time data valuation problem and introduce the Contextual Influence Value (CI value). This novel metric quantifies context quality by measuring the performance degradation when removing each context from the list, effectively integrating query-aware relevance, list-aware uniqueness, and generator-aware alignment. Moreover, CI value eliminates complex selection hyperparameter tuning by simply retaining contexts with positive CI values. To address practical challenges of label dependency and computational overhead, we develop a parameterized surrogate model for CI value prediction during inference. The model employs a hierarchical architecture that captures both local query-context relevance and global inter-context interactions, trained through oracle CI value supervision and end-to-end generator feedback. Extensive experiments across 8 NLP tasks and multiple LLMs demonstrate that our context selection method significantly outperforms state-of-the-art baselines, effectively filtering poor-quality contexts while preserving critical information. Code is available at https://github.com/SJTU-DMTai/RAG-CSM.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "8",
        "title": "Multimodal Prompt Decoupling Attack on the Safety Filters in Text-to-Image Models",
        "author": [
            "Xingkai Peng",
            "Jun Jiang",
            "Meng Tong",
            "Shuai Li",
            "Weiming Zhang",
            "Nenghai Yu",
            "Kejiang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21360",
        "abstract": "Text-to-image (T2I) models have been widely applied in generating high-fidelity images across various domains. However, these models may also be abused to produce Not-Safe-for-Work (NSFW) content via jailbreak attacks. Existing jailbreak methods primarily manipulate the textual prompt, leaving potential vulnerabilities in image-based inputs largely unexplored. Moreover, text-based methods face challenges in bypassing the model's safety filters. In response to these limitations, we propose the Multimodal Prompt Decoupling Attack (MPDA), which utilizes image modality to separate the harmful semantic components of the original unsafe prompt. MPDA follows three core steps: firstly, a large language model (LLM) decouples unsafe prompts into pseudo-safe prompts and harmful prompts. The former are seemingly harmless sub-prompts that can bypass filters, while the latter are sub-prompts with unsafe semantics that trigger filters. Subsequently, the LLM rewrites the harmful prompts into natural adversarial prompts to bypass safety filters, which guide the T2I model to modify the base image into an NSFW output. Finally, to ensure semantic consistency between the generated NSFW images and the original unsafe prompts, the visual language model generates image captions, providing a new pathway to guide the LLM in iterative rewriting and refining the generated content.",
        "tags": [
            "LLM",
            "Text-to-Image"
        ]
    },
    {
        "id": "9",
        "title": "Context Is What You Need: The Maximum Effective Context Window for Real World Limits of LLMs",
        "author": [
            "Norman Paulsen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21361",
        "abstract": "Large language model (LLM) providers boast big numbers for maximum context window sizes. To test the real world use of context windows, we 1) define a concept of maximum effective context window, 2) formulate a testing method of a context window's effectiveness over various sizes and problem types, and 3) create a standardized way to compare model efficacy for increasingly larger context window sizes to find the point of failure. We collected hundreds of thousands of data points across several models and found significant differences between reported Maximum Context Window (MCW) size and Maximum Effective Context Window (MECW) size. Our findings show that the MECW is, not only, drastically different from the MCW but also shifts based on the problem type. A few top of the line models in our test group failed with as little as 100 tokens in context; most had severe degradation in accuracy by 1000 tokens in context. All models fell far short of their Maximum Context Window by as much as 99 percent. Our data reveals the Maximum Effective Context Window shifts based on the type of problem provided, offering clear and actionable insights into how to improve model accuracy and decrease model hallucination rates.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "10",
        "title": "MAJORScore: A Novel Metric for Evaluating Multimodal Relevance via Joint Representation",
        "author": [
            "Zhicheng Du",
            "Qingyang Shi",
            "Jiasheng Lu",
            "Yingshan Liang",
            "Xinyu Zhang",
            "Yiran Wang",
            "Peiwu Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21365",
        "abstract": "The multimodal relevance metric is usually borrowed from the embedding ability of pretrained contrastive learning models for bimodal data, which is used to evaluate the correlation between cross-modal data (e.g., CLIP). However, the commonly used evaluation metrics are only suitable for the associated analysis between two modalities, which greatly limits the evaluation of multimodal similarity. Herein, we propose MAJORScore, a brand-new evaluation metric for the relevance of multiple modalities (N modalities, N>=3) via multimodal joint representation for the first time. The ability of multimodal joint representation to integrate multiple modalities into the same latent space can accurately represent different modalities at one scale, providing support for fair relevance scoring. Extensive experiments have shown that MAJORScore increases by 26.03%-64.29% for consistent modality and decreases by 13.28%-20.54% for inconsistence compared to existing methods. MAJORScore serves as a more reliable metric for evaluating similarity on large-scale multimodal datasets and multimodal model performance evaluation.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "11",
        "title": "Design and Implementation of a Secure RAG-Enhanced AI Chatbot for Smart Tourism Customer Service: Defending Against Prompt Injection Attacks -- A Case Study of Hsinchu, Taiwan",
        "author": [
            "Yu-Kai Shih",
            "You-Kai Kang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21367",
        "abstract": "As smart tourism evolves, AI-powered chatbots have become indispensable for delivering personalized, real-time assistance to travelers while promoting sustainability and efficiency. However, these systems are increasingly vulnerable to prompt injection attacks, where adversaries manipulate inputs to elicit unintended behaviors such as leaking sensitive information or generating harmful content. This paper presents a case study on the design and implementation of a secure retrieval-augmented generation (RAG) chatbot for Hsinchu smart tourism services. The system integrates RAG with API function calls, multi-layered linguistic analysis, and guardrails against injections, achieving high contextual awareness and security. Key features include a tiered response strategy, RAG-driven knowledge grounding, and intent decomposition across lexical, semantic, and pragmatic levels. Defense mechanisms include system norms, gatekeepers for intent judgment, and reverse RAG text to prioritize verified data. We also benchmark a GPT-5 variant (released 2025-08-07) to assess inherent robustness. Evaluations with 674 adversarial prompts and 223 benign queries show over 95% accuracy on benign tasks and substantial detection of injection attacks. GPT-5 blocked about 85% of attacks, showing progress yet highlighting the need for layered defenses. Findings emphasize contributions to sustainable tourism, multilingual accessibility, and ethical AI deployment. This work offers a practical framework for deploying secure chatbots in smart tourism and contributes to resilient, trustworthy AI applications.",
        "tags": [
            "Detection",
            "GPT",
            "RAG"
        ]
    },
    {
        "id": "12",
        "title": "Language-in-the-Loop Culvert Inspection on the Erie Canal",
        "author": [
            "Yashom Dighe",
            "Yash Turkar",
            "Karthik Dantu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21370",
        "abstract": "Culverts on canals such as the Erie Canal, built originally in 1825, require frequent inspections to ensure safe operation. Human inspection of culverts is challenging due to age, geometry, poor illumination, weather, and lack of easy access. We introduce VISION, an end-to-end, language-in-the-loop autonomy system that couples a web-scale vision-language model (VLM) with constrained viewpoint planning for autonomous inspection of culverts. Brief prompts to the VLM solicit open-vocabulary ROI proposals with rationales and confidences, stereo depth is fused to recover scale, and a planner -- aware of culvert constraints -- commands repositioning moves to capture targeted close-ups. Deployed on a quadruped in a culvert under the Erie Canal, VISION closes the see, decide, move, re-image loop on-board and produces high-resolution images for detailed reporting without domain-specific fine-tuning. In an external evaluation by New York Canal Corporation personnel, initial ROI proposals achieved 61.4\\% agreement with subject-matter experts, and final post-re-imaging assessments reached 80\\%, indicating that VISION converts tentative hypotheses into grounded, expert-aligned findings.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "13",
        "title": "Automated Prompt Generation for Creative and Counterfactual Text-to-image Synthesis",
        "author": [
            "Aleksa Jelaca",
            "Ying Jiao",
            "Chang Tian",
            "Marie-Francine Moens"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21375",
        "abstract": "Text-to-image generation has advanced rapidly with large-scale multimodal training, yet fine-grained controllability remains a critical challenge. Counterfactual controllability, defined as the capacity to deliberately generate images that contradict common-sense patterns, remains a major challenge but plays a crucial role in enabling creativity and exploratory applications. In this work, we address this gap with a focus on counterfactual size (e.g., generating a tiny walrus beside a giant button) and propose an automatic prompt engineering framework that adapts base prompts into revised prompts for counterfactual images. The framework comprises three components: an image evaluator that guides dataset construction by identifying successful image generations, a supervised prompt rewriter that produces revised prompts, and a DPO-trained ranker that selects the optimal revised prompt. We construct the first counterfactual size text-image dataset and enhance the image evaluator by extending Grounded SAM with refinements, achieving a 114 percent improvement over its backbone. Experiments demonstrate that our method outperforms state-of-the-art baselines and ChatGPT-4o, establishing a foundation for future research on counterfactual controllability.",
        "tags": [
            "DPO",
            "GPT",
            "SAM",
            "Text-to-Image"
        ]
    },
    {
        "id": "14",
        "title": "Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation",
        "author": [
            "Yinfeng Yu",
            "Hailong Zhang",
            "Meiling Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21377",
        "abstract": "Audiovisual embodied navigation enables robots to locate audio sources by dynamically integrating visual observations from onboard sensors with the auditory signals emitted by the target. The core challenge lies in effectively leveraging multimodal cues to guide navigation. While prior works have explored basic fusion of visual and audio data, they often overlook deeper perceptual context. To address this, we propose the Dynamic Multi-Target Fusion for Efficient Audio-Visual Navigation (DMTF-AVN). Our approach uses a multi-target architecture coupled with a refined Transformer mechanism to filter and selectively fuse cross-modal information. Extensive experiments on the Replica and Matterport3D datasets demonstrate that DMTF-AVN achieves state-of-the-art performance, outperforming existing methods in success rate (SR), path efficiency (SPL), and scene adaptation (SNA). Furthermore, the model exhibits strong scalability and generalizability, paving the way for advanced multimodal fusion strategies in robotic navigation. The code and videos are available at\nhttps://github.com/zzzmmm-svg/DMTF.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "15",
        "title": "SAEmnesia: Erasing Concepts in Diffusion Models with Sparse Autoencoders",
        "author": [
            "Enrico Cassano",
            "Riccardo Renzulli",
            "Marco Nurisso",
            "Mirko Zaffaroni",
            "Alan Perotti",
            "Marco Grangetto"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21379",
        "abstract": "Effective concept unlearning in text-to-image diffusion models requires precise localization of concept representations within the model's latent space. While sparse autoencoders successfully reduce neuron polysemanticity (i.e., multiple concepts per neuron) compared to the original network, individual concept representations can still be distributed across multiple latent features, requiring extensive search procedures for concept unlearning. We introduce SAEmnesia, a supervised sparse autoencoder training method that promotes one-to-one concept-neuron mappings through systematic concept labeling, mitigating feature splitting and promoting feature centralization. Our approach learns specialized neurons with significantly stronger concept associations compared to unsupervised baselines. The only computational overhead introduced by SAEmnesia is limited to cross-entropy computation during training. At inference time, this interpretable representation reduces hyperparameter search by 96.67% with respect to current approaches. On the UnlearnCanvas benchmark, SAEmnesia achieves a 9.22% improvement over the state-of-the-art. In sequential unlearning tasks, we demonstrate superior scalability with a 28.4% improvement in unlearning accuracy for 9-object removal.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "16",
        "title": "MIXRAG : Mixture-of-Experts Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering",
        "author": [
            "Lihui Liu",
            "Carl J. Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21391",
        "abstract": "Large Language Models (LLMs) have achieved impressive performance across a wide range of applications. However, they often suffer from hallucinations in knowledge-intensive domains due to their reliance on static pretraining corpora. To address this limitation, Retrieval-Augmented Generation (RAG) enhances LLMs by incorporating external knowledge sources during inference. Among these sources, textual graphs provide structured and semantically rich information that supports more precise and interpretable reasoning. This has led to growing interest in graph-based RAG systems. Despite their potential, most existing approaches rely on a single retriever to identify relevant subgraphs, which limits their ability to capture the diverse aspects of complex queries. Moreover, these systems often struggle to accurately judge the relevance of retrieved content, making them prone to distraction by irrelevant noise. To address these challenges, in this paper, we propose MIXRAG, a Mixture-of-Experts Graph-RAG framework that introduces multiple specialized graph retrievers and a dynamic routing controller to better handle diverse query intents. Each retriever is trained to focus on a specific aspect of graph semantics, such as entities, relations, or subgraph topology. A Mixture-of-Experts module adaptively selects and fuses relevant retrievers based on the input query. To reduce noise in the retrieved information, we introduce a query-aware GraphEncoder that carefully analyzes relationships within the retrieved subgraphs, highlighting the most relevant parts while down-weighting unnecessary noise. Empirical results demonstrate that our method achieves state-of-the-art performance and consistently outperforms various baselines. MIXRAG is effective across a wide range of graph-based tasks in different domains. The code will be released upon paper acceptance.",
        "tags": [
            "LLM",
            "MoE",
            "RAG"
        ]
    },
    {
        "id": "17",
        "title": "Impact of Loss Weight and Model Complexity on Physics-Informed Neural Networks for Computational Fluid Dynamics",
        "author": [
            "Yi En Chou",
            "Te Hsin Liu",
            "Chao An Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21393",
        "abstract": "Physics Informed Neural Networks offer a mesh free framework for solving PDEs but are highly sensitive to loss weight selection. We propose two dimensional analysis based weighting schemes, one based on quantifiable terms, and another also incorporating unquantifiable terms for more balanced training. Benchmarks on heat conduction, convection diffusion, and lid driven cavity flows show that the second scheme consistently improves stability and accuracy over equal weighting. Notably, in high Peclet number convection diffusion, where traditional solvers fail, PINNs with our scheme achieve stable, accurate predictions, highlighting their robustness and generalizability in CFD problems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "18",
        "title": "Large AI Model-Enabled Generative Semantic Communications for Image Transmission",
        "author": [
            "Qiyu Ma",
            "Wanli Ni",
            "Zhijin Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21394",
        "abstract": "The rapid development of generative artificial intelligence (AI) has introduced significant opportunities for enhancing the efficiency and accuracy of image transmission within semantic communication systems. Despite these advancements, existing methodologies often neglect the difference in importance of different regions of the image, potentially compromising the reconstruction quality of visually critical content. To address this issue, we introduce an innovative generative semantic communication system that refines semantic granularity by segmenting images into key and non-key regions. Key regions, which contain essential visual information, are processed using an image oriented semantic encoder, while non-key regions are efficiently compressed through an image-to-text modeling approach. Additionally, to mitigate the substantial storage and computational demands posed by large AI models, the proposed system employs a lightweight deployment strategy incorporating model quantization and low-rank adaptation fine-tuning techniques, significantly boosting resource utilization without sacrificing performance. Simulation results demonstrate that the proposed system outperforms traditional methods in terms of both semantic fidelity and visual quality, thereby affirming its effectiveness for image transmission tasks.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "19",
        "title": "SafeSteer: Adaptive Subspace Steering for Efficient Jailbreak Defense in Vision-Language Models",
        "author": [
            "Xiyu Zeng",
            "Siyuan Liang",
            "Liming Lu",
            "Haotian Zhu",
            "Enguang Liu",
            "Jisheng Dang",
            "Yongbin Zhou",
            "Shuchao Pang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21400",
        "abstract": "As the capabilities of Vision Language Models (VLMs) continue to improve, they are increasingly targeted by jailbreak attacks. Existing defense methods face two major limitations: (1) they struggle to ensure safety without compromising the model's utility; and (2) many defense mechanisms significantly reduce the model's inference efficiency. To address these challenges, we propose SafeSteer, a lightweight, inference-time steering framework that effectively defends against diverse jailbreak attacks without modifying model weights. At the core of SafeSteer is the innovative use of Singular Value Decomposition to construct a low-dimensional \"safety subspace.\" By projecting and reconstructing the raw steering vector into this subspace during inference, SafeSteer adaptively removes harmful generation signals while preserving the model's ability to handle benign inputs. The entire process is executed in a single inference pass, introducing negligible overhead. Extensive experiments show that SafeSteer reduces the attack success rate by over 60% and improves accuracy on normal tasks by 1-2%, without introducing significant inference latency. These results demonstrate that robust and practical jailbreak defense can be achieved through simple, efficient inference-time control.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "20",
        "title": "JaiLIP: Jailbreaking Vision-Language Models via Loss Guided Image Perturbation",
        "author": [
            "Md Jueal Mia",
            "M. Hadi Amini"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21401",
        "abstract": "Vision-Language Models (VLMs) have remarkable abilities in generating multimodal reasoning tasks. However, potential misuse or safety alignment concerns of VLMs have increased significantly due to different categories of attack vectors. Among various attack vectors, recent studies have demonstrated that image-based perturbations are particularly effective in generating harmful outputs. In the literature, many existing techniques have been proposed to jailbreak VLMs, leading to unstable performance and visible perturbations. In this study, we propose Jailbreaking with Loss-guided Image Perturbation (JaiLIP), a jailbreaking attack in the image space that minimizes a joint objective combining the mean squared error (MSE) loss between clean and adversarial image with the models harmful-output loss. We evaluate our proposed method on VLMs using standard toxicity metrics from Perspective API and Detoxify. Experimental results demonstrate that our method generates highly effective and imperceptible adversarial images, outperforming existing methods in producing toxicity. Moreover, we have evaluated our method in the transportation domain to demonstrate the attacks practicality beyond toxic text generation in specific domain. Our findings emphasize the practical challenges of image-based jailbreak attacks and the need for efficient defense mechanisms for VLMs.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "21",
        "title": "How Large Language Models Need Symbolism",
        "author": [
            "Xiaotie Deng",
            "Hanyu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21404",
        "abstract": "We argue that AI's future requires more than scaling. To unlock genuine discovery, large language models need a compass: human-crafted symbols to guide their powerful but blind intuition.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "22",
        "title": "Null-Space Filtering for Data-Free Continual Model Merging: Preserving Transparency, Promoting Fidelity",
        "author": [
            "Zihuan Qiu",
            "Lei Wang",
            "Yang Cao",
            "Runtong Zhang",
            "Bing Su",
            "Yi Xu",
            "Fanman Meng",
            "Linfeng Xu",
            "Qingbo Wu",
            "Hongliang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21413",
        "abstract": "Data-free continual model merging (DFCMM) aims to fuse independently fine-tuned models into a single backbone that evolves with incoming tasks without accessing task data. This paper formulate two fundamental desiderata for DFCMM: transparency, avoiding interference with earlier tasks, and fidelity, adapting faithfully to each new task. This poses a challenge that existing approaches fail to address: how to bridge data-level desiderata with parameter-space optimization to ensure transparency and fidelity in the absence of task data. To this end, we propose NUFILT (NUll-space FILTering), a data-free framework that directly links these desiderata to optimization. Our key observation is that task vectors approximately align with representation subspaces, providing structural surrogates for enforcing transparency and fidelity. Accordingly, we design a null-space projector that preserves prior responses by filtering out overlapping components of new task vectors, thereby ensuring transparency, and a lightweight LoRA adapter that injects complementary task-specific signals, enabling fidelity in adapting to new tasks. The adapter is trained with a projection-based surrogate loss to retain consistency with previous knowledge while introducing novel directions. This joint filtering-adaptation process allows the backbone to absorb new knowledge while retaining existing behaviors, and the updates are finally fused back in a layer-wise linear fashion without extra parameters or inference cost. Theoretically, we establish approximate subspace alignment guarantees that justify null-space filtering. Empirically, NUFILT achieves state-of-the-art performance with minimal forgetting on both vision and NLP benchmarks, improving average accuracy by 4-7% over OPCM and WUDI-Merging, while narrowing the gap to fine-tuning and reducing computation overhead.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "23",
        "title": "QuadGPT: Native Quadrilateral Mesh Generation with Autoregressive Models",
        "author": [
            "Jian Liu",
            "Chunshi Wang",
            "Song Guo",
            "Haohan Weng",
            "Zhen Zhou",
            "Zhiqi Li",
            "Jiaao Yu",
            "Yiling Zhu",
            "Jing Xu",
            "Biwen Lei",
            "Zhuo Chen",
            "Chunchao Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21420",
        "abstract": "The generation of quadrilateral-dominant meshes is a cornerstone of professional 3D content creation. However, existing generative models generate quad meshes by first generating triangle meshes and then merging triangles into quadrilaterals with some specific rules, which typically produces quad meshes with poor topology. In this paper, we introduce QuadGPT, the first autoregressive framework for generating quadrilateral meshes in an end-to-end manner. QuadGPT formulates this as a sequence prediction paradigm, distinguished by two key innovations: a unified tokenization method to handle mixed topologies of triangles and quadrilaterals, and a specialized Reinforcement Learning fine-tuning method tDPO for better generation quality. Extensive experiments demonstrate that QuadGPT significantly surpasses previous triangle-to-quad conversion pipelines in both geometric accuracy and topological quality. Our work establishes a new benchmark for native quad-mesh generation and showcases the power of combining large-scale autoregressive models with topology-aware RL refinement for creating structured 3D assets.",
        "tags": [
            "3D",
            "RL"
        ]
    },
    {
        "id": "24",
        "title": "Extracting Conceptual Knowledge to Locate Software Issues",
        "author": [
            "Ying Wang",
            "Wenjun Mao",
            "Chong Wang",
            "Zhenhao Zhou",
            "Yicheng Zhou",
            "Wenyun Zhao",
            "Yiling Lou",
            "Xin Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21427",
        "abstract": "Issue localization, which identifies faulty code elements such as files or functions, is critical for effective bug fixing. While recent LLM-based and LLM-agent-based approaches improve accuracy, they struggle in large-scale repositories due to concern mixing, where relevant logic is buried in large functions, and concern scattering, where related logic is dispersed across files.\nTo address these challenges, we propose RepoLens, a novel approach that abstracts and leverages conceptual knowledge from code repositories. RepoLens decomposes fine-grained functionalities and recomposes them into high-level concerns, semantically coherent clusters of functionalities that guide LLMs. It operates in two stages: an offline stage that extracts and enriches conceptual knowledge into a repository-wide knowledge base, and an online stage that retrieves issue-specific terms, clusters and ranks concerns by relevance, and integrates them into localization workflows via minimally intrusive prompt enhancements. We evaluate RepoLens on SWE-Lancer-Loc, a benchmark of 216 tasks derived from SWE-Lancer. RepoLens consistently improves three state-of-the-art tools, namely AgentLess, OpenHands, and mini-SWE-agent, achieving average gains of over 22% in Hit@k and 46% in Recall@k for file- and function-level localization. It generalizes across models (GPT-4o, GPT-4o-mini, GPT-4.1) with Hit@1 and Recall@10 gains up to 504% and 376%, respectively. Ablation studies and manual evaluation confirm the effectiveness and reliability of the constructed concerns.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "25",
        "title": "DyME: Dynamic Multi-Concept Erasure in Diffusion Models with Bi-Level Orthogonal LoRA Adaptation",
        "author": [
            "Jiaqi Liu",
            "Lan Zhang",
            "Xiaoyong Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21433",
        "abstract": "Text-to-image diffusion models (DMs) inadvertently reproduce copyrighted styles and protected visual concepts, raising legal and ethical concerns. Concept erasure has emerged as a safeguard, aiming to selectively suppress such concepts through fine-tuning. However, existing methods do not scale to practical settings where providers must erase multiple and possibly conflicting concepts. The core bottleneck is their reliance on static erasure: a single checkpoint is fine-tuned to remove all target concepts, regardless of the actual erasure needs at inference. This rigid design mismatches real-world usage, where requests vary per generation, leading to degraded erasure success and reduced fidelity for non-target content. We propose DyME, an on-demand erasure framework that trains lightweight, concept-specific LoRA adapters and dynamically composes only those needed at inference. This modular design enables flexible multi-concept erasure, but naive composition causes interference among adapters, especially when many or semantically related concepts are suppressed. To overcome this, we introduce bi-level orthogonality constraints at both the feature and parameter levels, disentangling representation shifts and enforcing orthogonal adapter subspaces. We further develop ErasureBench-H, a new hierarchical benchmark with brand-series-character structure, enabling principled evaluation across semantic granularities and erasure set sizes. Experiments on ErasureBench-H and standard datasets (e.g., CIFAR-100, Imagenette) demonstrate that DyME consistently outperforms state-of-the-art baselines, achieving higher multi-concept erasure fidelity with minimal collateral degradation.",
        "tags": [
            "Diffusion",
            "LoRA",
            "Text-to-Image"
        ]
    },
    {
        "id": "26",
        "title": "One Model, Many Morals: Uncovering Cross-Linguistic Misalignments in Computational Moral Reasoning",
        "author": [
            "Sualeha Farid",
            "Jayden Lin",
            "Zean Chen",
            "Shivani Kumar",
            "David Jurgens"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21443",
        "abstract": "Large Language Models (LLMs) are increasingly deployed in multilingual and multicultural environments where moral reasoning is essential for generating ethically appropriate responses. Yet, the dominant pretraining of LLMs on English-language data raises critical concerns about their ability to generalize judgments across diverse linguistic and cultural contexts. In this work, we systematically investigate how language mediates moral decision-making in LLMs. We translate two established moral reasoning benchmarks into five culturally and typologically diverse languages, enabling multilingual zero-shot evaluation. Our analysis reveals significant inconsistencies in LLMs' moral judgments across languages, often reflecting cultural misalignment. Through a combination of carefully constructed research questions, we uncover the underlying drivers of these disparities, ranging from disagreements to reasoning strategies employed by LLMs. Finally, through a case study, we link the role of pretraining data in shaping an LLM's moral compass. Through this work, we distill our insights into a structured typology of moral reasoning errors that calls for more culturally-aware AI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "27",
        "title": "Developing a Mono-Actuated Compliant GeoGami Robot",
        "author": [
            "Archie Webster",
            "Lee Skull",
            "Seyed Amir Tafrishi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21445",
        "abstract": "This paper presents the design of a new soft-rigid robotic platform, \"GeoGami\". We leverage origami surface capabilities to achieve shape contraction and to support locomotion with underactuated forms. A key challenge is that origami surfaces have high degrees of freedom and typically require many actuators; we address repeatability by integrating surface compliance. We propose a mono-actuated GeoGami mobile platform that combines origami surface compliance with a geometric compliant skeleton, enabling the robot to transform and locomote using a single actuator. We demonstrate the robot, develop a stiffness model, and describe the central gearbox mechanism. We also analyze alternative cable-driven actuation methods for the skeleton to enable surface transformation. Finally, we evaluate the GeoGami platform for capabilities, including shape transformation and rolling. This platform opens new capabilities for robots that change shape to access different environments and that use shape transformation for locomotion.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "28",
        "title": "Forecasting Seismic Waveforms: A Deep Learning Approach for Einstein Telescope",
        "author": [
            "Waleed Esmail",
            "Alexander Kappes",
            "Stuart Russell",
            "Christine Thomas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21446",
        "abstract": "We introduce \\textit{SeismoGPT}, a transformer-based model for forecasting three-component seismic waveforms in the context of future gravitational wave detectors like the Einstein Telescope. The model is trained in an autoregressive setting and can operate on both single-station and array-based inputs. By learning temporal and spatial dependencies directly from waveform data, SeismoGPT captures realistic ground motion patterns and provides accurate short-term forecasts. Our results show that the model performs well within the immediate prediction window and gradually degrades further ahead, as expected in autoregressive systems. This approach lays the groundwork for data-driven seismic forecasting that could support Newtonian noise mitigation and real-time observatory control.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "29",
        "title": "LLM-Based Support for Diabetes Diagnosis: Opportunities, Scenarios, and Challenges with GPT-5",
        "author": [
            "Gaurav Kumar Gupta",
            "Nirajan Acharya",
            "Pranal Pande"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21450",
        "abstract": "Diabetes mellitus is a major global health challenge, affecting over half a billion adults worldwide with prevalence projected to rise. Although the American Diabetes Association (ADA) provides clear diagnostic thresholds, early recognition remains difficult due to vague symptoms, borderline laboratory values, gestational complexity, and the demands of long-term monitoring. Advances in large language models (LLMs) offer opportunities to enhance decision support through structured, interpretable, and patient-friendly outputs. This study evaluates GPT-5, the latest generative pre-trained transformer, using a simulation framework built entirely on synthetic cases aligned with ADA Standards of Care 2025 and inspired by public datasets including NHANES, Pima Indians, EyePACS, and MIMIC-IV. Five representative scenarios were tested: symptom recognition, laboratory interpretation, gestational diabetes screening, remote monitoring, and multimodal complication detection. For each, GPT-5 classified cases, generated clinical rationales, produced patient explanations, and output structured JSON summaries. Results showed strong alignment with ADA-defined criteria, suggesting GPT-5 may function as a dual-purpose tool for clinicians and patients, while underscoring the importance of reproducible evaluation frameworks for responsibly assessing LLMs in healthcare.",
        "tags": [
            "Detection",
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "30",
        "title": "VideoJudge: Bootstrapping Enables Scalable Supervision of MLLM-as-a-Judge for Video Understanding",
        "author": [
            "Abdul Waheed",
            "Zhen Wu",
            "Dareen Alharthi",
            "Seungone Kim",
            "Bhiksha Raj"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21451",
        "abstract": "Precisely evaluating video understanding models remains challenging: commonly used metrics such as BLEU, ROUGE, and BERTScore fail to capture the fineness of human judgment, while obtaining such judgments through manual evaluation is costly. Recent work has explored using large language models (LLMs) or multimodal LLMs (MLLMs) as evaluators, but their extension to video understanding remains relatively unexplored. In this work, we introduce VideoJudge, a 3B and 7B-sized MLLM judge specialized to evaluate outputs from video understanding models (\\textit{i.e.}, text responses conditioned on videos). To train VideoJudge, our recipe builds on the interplay between a generator and an evaluator: the generator is prompted to produce responses conditioned on a target rating, and responses not matching the evaluator's rating are discarded. Across three out of four meta-evaluation benchmarks, VideoJudge-7B outperforms larger MLLM judge baselines such as Qwen2.5-VL (32B and 72B). Notably, we find that LLM judges (Qwen3) models perform worse than MLLM judges (Qwen2.5-VL) and long chain-of-thought reasoning does not improve performance, indicating that providing video inputs is crucial for evaluation of video understanding tasks.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "31",
        "title": "A State-of-the-Art SQL Reasoning Model using RLVR",
        "author": [
            "Alnur Ali",
            "Ashutosh Baheti",
            "Jonathan Chang",
            "Ta-Chung Chi",
            "Brandon Cui",
            "Andrew Drozdov",
            "Jonathan Frankle",
            "Abhay Gupta",
            "Pallavi Koppol",
            "Sean Kulinski",
            "Jonathan Li",
            "Dipendra Misra",
            "Krista Opsahl-Ong",
            "Jose Javier Gonzalez Ortiz",
            "Matei Zaharia",
            "Yue Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21459",
        "abstract": "Developing custom reasoning models via Reinforcement Learning (RL) that can incorporate organization-specific knowledge has great potential to address problems faced by enterprise customers. In many of these problems, the reward function is verifiable, a setting termed RL with Verifiable Rewards (RLVR). We apply RLVR to a popular data science benchmark called BIRD that measures the ability of an AI agent to convert a natural language query for a database to SQL executions. We apply a simple and general-purpose training recipe involving careful prompt and model selection, a warm-up stage using our offline RL approach called TAO, followed by rigorous online RLVR training. With no additional training data beyond the BIRD training set and no use of proprietary models, our very first submission to the BIRD leaderboard reached state-of-the-art accuracy on the private test set: 73.56% without self-consistency and 75.68% with self-consistency. In the latter case, our model also required fewer generations than the second-best approach. While BIRD is only a proxy task, the simplicity of our framework makes it broadly applicable to enterprise domains such as business intelligence, data science, and coding.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "32",
        "title": "Residual Vector Quantization For Communication-Efficient Multi-Agent Perception",
        "author": [
            "Dereje Shenkut",
            "B.V.K Vijaya Kumar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21464",
        "abstract": "Multi-agent collaborative perception (CP) improves scene understanding by sharing information across connected agents such as autonomous vehicles, unmanned aerial vehicles, and robots. Communication bandwidth, however, constrains scalability. We present ReVQom, a learned feature codec that preserves spatial identity while compressing intermediate features. ReVQom is an end-to-end method that compresses feature dimensions via a simple bottleneck network followed by multi-stage residual vector quantization (RVQ). This allows only per-pixel code indices to be transmitted, reducing payloads from 8192 bits per pixel (bpp) of uncompressed 32-bit float features to 6-30 bpp per agent with minimal accuracy loss. On DAIR-V2X real-world CP dataset, ReVQom achieves 273x compression at 30 bpp to 1365x compression at 6 bpp. At 18 bpp (455x), ReVQom matches or outperforms raw-feature CP, and at 6-12 bpp it enables ultra-low-bandwidth operation with graceful degradation. ReVQom allows efficient and accurate multi-agent collaborative perception with a step toward practical V2X deployment.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "33",
        "title": "Gender Stereotypes in Professional Roles Among Saudis: An Analytical Study of AI-Generated Images Using Language Models",
        "author": [
            "Khaloud S. AlKhalifah",
            "Malak Mashaabi",
            "Hend Al-Khalifa"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21466",
        "abstract": "This study investigates the extent to which contemporary Text-to-Image artificial intelligence (AI) models perpetuate gender stereotypes and cultural inaccuracies when generating depictions of professionals in Saudi Arabia. We analyzed 1,006 images produced by ImageFX, DALL-E V3, and Grok for 56 diverse Saudi professions using neutral prompts. Two trained Saudi annotators evaluated each image on five dimensions: perceived gender, clothing and appearance, background and setting, activities and interactions, and age. A third senior researcher adjudicated whenever the two primary raters disagreed, yielding 10,100 individual judgements. The results reveal a strong gender imbalance, with ImageFX outputs being 85\\% male, Grok 86.6\\% male, and DALL-E V3 96\\% male, indicating that DALL-E V3 exhibited the strongest overall gender stereotyping. This imbalance was most evident in leadership and technical roles. Moreover, cultural inaccuracies in clothing, settings, and depicted activities were frequently observed across all three models. Counter-stereotypical images often arise from cultural misinterpretations rather than genuinely progressive portrayals. We conclude that current models mirror societal biases embedded in their training data, generated by humans, offering only a limited reflection of the Saudi labour market's gender dynamics and cultural nuances. These findings underscore the urgent need for more diverse training data, fairer algorithms, and culturally sensitive evaluation frameworks to ensure equitable and authentic visual outputs.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "34",
        "title": "Score-based Idempotent Distillation of Diffusion Models",
        "author": [
            "Shehtab Zaman",
            "Chengyan Liu",
            "Kenneth Chiu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21470",
        "abstract": "Idempotent generative networks (IGNs) are a new line of generative models based on idempotent mapping to a target manifold. IGNs support both single-and multi-step generation, allowing for a flexible trade-off between computational cost and sample quality. But similar to Generative Adversarial Networks (GANs), conventional IGNs require adversarial training and are prone to training instabilities and mode collapse. Diffusion and score-based models are popular approaches to generative modeling that iteratively transport samples from one distribution, usually a Gaussian, to a target data distribution. These models have gained popularity due to their stable training dynamics and high-fidelity generation quality. However, this stability and quality come at the cost of high computational cost, as the data must be transported incrementally along the entire trajectory. New sampling methods, model distillation, and consistency models have been developed to reduce the sampling cost and even perform one-shot sampling from diffusion models. In this work, we unite diffusion and IGNs by distilling idempotent models from diffusion model scores, called SIGN. Our proposed method is highly stable and does not require adversarial losses. We provide a theoretical analysis of our proposed score-based training methods and empirically show that IGNs can be effectively distilled from a pre-trained diffusion model, enabling faster inference than iterative score-based models. SIGNs can perform multi-step sampling, allowing users to trade off quality for efficiency. These models operate directly on the source domain; they can project corrupted or alternate distributions back onto the target manifold, enabling zero-shot editing of inputs. We validate our models on multiple image datasets, achieving state-of-the-art results for idempotent models on the CIFAR and CelebA datasets.",
        "tags": [
            "Consistency Models",
            "Diffusion"
        ]
    },
    {
        "id": "35",
        "title": "Are Hallucinations Bad Estimations?",
        "author": [
            "Hude Liu",
            "Jerry Yao-Chieh Hu",
            "Jennifer Yuntong Zhang",
            "Zhao Song",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21473",
        "abstract": "We formalize hallucinations in generative models as failures to link an estimate to any plausible cause. Under this interpretation, we show that even loss-minimizing optimal estimators still hallucinate. We confirm this with a general high probability lower bound on hallucinate rate for generic data distributions. This reframes hallucination as structural misalignment between loss minimization and human-acceptable outputs, and hence estimation errors induced by miscalibration. Experiments on coin aggregation, open-ended QA, and text-to-image support our theory.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "36",
        "title": "d2: Improved Techniques for Training Reasoning Diffusion Language Models",
        "author": [
            "Guanghan Wang",
            "Yair Schiff",
            "Gilad Turok",
            "Volodymyr Kuleshov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21474",
        "abstract": "While diffusion language models (DLMs) have achieved competitive performance in text generation, improving their reasoning ability with reinforcement learning remains an active research area. Here, we introduce d2, a reasoning framework tailored for masked DLMs. Central to our framework is a new policy gradient algorithm that relies on properties of masking to accurately estimate the likelihoods of sampling trajectories. Our estimators trade off computation for approximation accuracy in an analytically tractable manner, and are particularly effective for DLMs that support any-order likelihood estimation. We characterize and study this property in popular DLMs and show that it is key for efficient diffusion-based reasoning. Empirically, d2 significantly improves over previous diffusion reasoning frameworks using only RL (without relying on supervised fine-tuning), and sets a new state-of-the-art performance for DLMs on logical reasoning tasks (Countdown and Sudoku) and math reasoning benchmarks (GSM8K and MATH500).",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "37",
        "title": "Learning to Reason with Mixture of Tokens",
        "author": [
            "Adit Jain",
            "Brendan Rappazzo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21482",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has become a leading approach for improving large language model (LLM) reasoning capabilities. Most current methods follow variants of Group Relative Policy Optimization, which samples multiple reasoning completions, scores them relative to each other, and adjusts the policy accordingly. However, these approaches invariably sample discrete tokens at each reasoning step, discarding the rich distributional information in the model's probability distribution over candidate tokens. While preserving and utilizing this distributional information has proven beneficial in non-RL settings, current RLVR methods seem to be unnecessarily constraining the reasoning search space by not using this information. To address this limitation, we investigate mixture-of-token generation (MoT-G) in RLVR. We present a unified framework that generalizes existing MoT-G approaches, including existing training-free methods that construct mixture embeddings as weighted sums over token embeddings, and extend RLVR to operate directly in this continuous mixture space for generating chain-of-thought. Evaluating two MoT-G variants on Reasoning-Gym, a suite of reasoning-intensive language tasks, we find that MoT--G methods achieve substantial improvements (5--35 \\% gains on 7 out of 10 tasks) compared to standard decoding with the Qwen2.5-1.5B model, while reaching comparable accuracy with half the number of trajectories, suggesting improved training efficiency. Through comprehensive hidden-state and token-level analyses, we provide evidence that MoT--G's benefits may stem from its ability to maintain higher hidden-state entropy throughout the reasoning process and promote exploration in token space.",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "38",
        "title": "Reasoning-Enhanced Domain-Adaptive Pretraining of Multimodal Large Language Models for Short Video Content Moderation",
        "author": [
            "Zixuan Wang",
            "Yu Sun",
            "Hongwei Wang",
            "Baoyu Jing",
            "Xiang Shen",
            "Xin Dong",
            "Zhuolin Hao",
            "Hongyu Xiong",
            "Yang Song"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21486",
        "abstract": "Short video platforms are evolving rapidly, making the identification of inappropriate content increasingly critical. Existing approaches typically train separate and small classification models for each type of issue, which requires extensive human-labeled data and lacks cross-issue generalization. We propose a reasoning-enhanced multimodal large language model (MLLM) pretraining paradigm for unified inappropriate content detection. To address the distribution gap between short video content and the original pretraining data of MLLMs, as well as the complex issue definitions, we introduce three targeted pretraining tasks: (1) \\textit{Caption}, to enhance the MLLM's perception of video details; (2) \\textit{Visual Question Answering (VQA)}, to deepen the MLLM's understanding of issue definitions and annotation guidelines; (3) \\textit{Chain-of-Thought (CoT)}, to enhance the MLLM's reasoning capability. Experimental results show that our pretraining approach significantly improves the MLLM's performance in both zero-shot and supervised fine-tuning (SFT) settings. In addition, our pretrained model demonstrates strong generalization capabilities to emergent, previously unseen issues.",
        "tags": [
            "CoT",
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "39",
        "title": "Dual-Head Reasoning Distillation: Improving Classifier Accuracy with Train-Time-Only Reasoning",
        "author": [
            "Jillian Xu",
            "Dylan Zhou",
            "Vinay Shukla",
            "Yang Yang",
            "Junrui Ruan",
            "Shuhuai Lin",
            "Wenfei Zou",
            "Yinxiao Liu",
            "Karthik Lakshmanan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21487",
        "abstract": "Chain-of-Thought (CoT) prompting often improves classification accuracy, but it introduces a significant throughput penalty with rationale generation (Wei et al., 2022; Cheng and Van Durme, 2024). To resolve this trade-off, we introduce Dual-Head Reasoning Distillation (DHRD), a simple training method for decoder-only language models (LMs) that adds (i) a pooled classification head used during training and inference and (ii) a reasoning head supervised by teacher rationales used only in training. We train with a loss function that is a weighted sum of label cross-entropy and token-level LM loss over input-plus-rationale sequences. On seven SuperGLUE tasks, DHRD yields relative gains of 0.65-5.47% over pooled baselines, with notably larger gains on entailment/causal tasks. Since we disable the reasoning head at test time, inference throughput matches pooled classifiers and exceeds CoT decoding on the same backbones by 96-142 times in QPS.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "40",
        "title": "Sci2Pol: Evaluating and Fine-tuning LLMs on Scientific-to-Policy Brief Generation",
        "author": [
            "Weimin Wu",
            "Alexander C. Furnas",
            "Eddie Yang",
            "Gefei Liu",
            "Akhil Pandey Akella",
            "Xuefeng Song",
            "Dashun Wang",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21493",
        "abstract": "We propose Sci2Pol-Bench and Sci2Pol-Corpus, the first benchmark and training dataset for evaluating and fine-tuning large language models (LLMs) on policy brief generation from a scientific paper. We build Sci2Pol-Bench on a five-stage taxonomy to mirror the human writing process: (i) Autocompletion, (ii) Understanding, (iii) Summarization, (iv) Generation, and (v) Verification. It features 18 tasks in multiple-choice and open-ended formats. Specifically, for the Generation stage, we show that BERTScore and ROUGE scores fail to capture the quality of brief writing, and introduce a new LLM-based evaluation metric aligned with expert judgement. Using this benchmark, we evaluate 13 leading open-source and commercial LLMs to uncover key limitations. To improve LLM performance on brief writing, we curate the Sci2Pol-Corpus for fine-tuning. We start by linking each cited scientific paper to its corresponding policy document, drawn from 5.6 million policy records. This produces 140,000 candidate pairs. We then employ an LLM-as-a-judge to filter high-quality examples, followed by in-context polishing using three expert-written samples as references. This process yields a final set of 639 new pairs. Finally, we fine-tune three models on Sci2Pol-Corpus: LLaMA-3.1-8B, Gemma-12B, and Gemma-27B. Fine-tuning leads to consistent performance improvements across Sci2Pol-Bench. Notably, after fine-tuning, Gemma-27B surpasses the much larger GPT-4o and DeepSeek-V3 (671B). These demonstrate the effectiveness of our corpus in bridging the gap between science and policy.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "41",
        "title": "SlimDiff: Training-Free, Activation-Guided Hands-free Slimming of Diffusion Models",
        "author": [
            "Arani Roy",
            "Shristi Das Biswas",
            "Kaushik Roy"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21498",
        "abstract": "Diffusion models (DMs), lauded for their generative performance, are computationally prohibitive due to their billion-scale parameters and iterative denoising dynamics. Existing efficiency techniques, such as quantization, timestep reduction, or pruning, offer savings in compute, memory, or runtime but are strictly bottlenecked by reliance on fine-tuning or retraining to recover performance. In this work, we introduce SlimDiff, an automated activation-informed structural compression framework that reduces both attention and feedforward dimensionalities in DMs, while being entirely gradient-free. SlimDiff reframes DM compression as a spectral approximation task, where activation covariances across denoising timesteps define low-rank subspaces that guide dynamic pruning under a fixed compression budget. This activation-aware formulation mitigates error accumulation across timesteps by applying module-wise decompositions over functional weight groups: query--key interactions, value--output couplings, and feedforward projections, rather than isolated matrix factorizations, while adaptively allocating sparsity across modules to respect the non-uniform geometry of diffusion trajectories. SlimDiff achieves up to 35\\% acceleration and $\\sim$100M parameter reduction over baselines, with generation quality on par with uncompressed models without any backpropagation. Crucially, our approach requires only about 500 calibration samples, over 70$\\times$ fewer than prior methods. To our knowledge, this is the first closed-form, activation-guided structural compression of DMs that is entirely training-free, providing both theoretical clarity and practical efficiency.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "42",
        "title": "On Code-Induced Reasoning in LLMs",
        "author": [
            "Abdul Waheed",
            "Zhen Wu",
            "Carolyn RosÃ©",
            "Daphne Ippolito"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21499",
        "abstract": "Code data has been shown to enhance the reasoning capabilities of large language models (LLMs), but it remains unclear which aspects of code are most responsible. We investigate this question with a systematic, data-centric framework. We construct parallel instruction datasets in ten programming languages and apply controlled perturbations that selectively disrupt structural or semantic properties of code. We then finetune LLMs from five model families and eight scales on each variant and evaluate their performance on natural language, math, and code tasks. Across 3,331 experiments, our results show that LLMs are more vulnerable to structural perturbations than semantic ones, particularly on math and code tasks. Appropriate abstractions like pseudocode and flowcharts can be as effective as code, while encoding the same information with fewer tokens without adhering to original syntax can often retain or even improve performance. Remarkably, even corrupted code with misleading signals remains competitive when surface-level regularities persist. Finally, syntactic styles also shape task-specific gains with Python favoring natural language reasoning and lower-level languages such as Java and Rust favoring math. Through our systematic framework, we aim to provide insight into how different properties of code influence reasoning and inform the design of training data for enhancing LLM reasoning capabilities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "43",
        "title": "Chasing the Tail: Effective Rubric-based Reward Modeling for Large Language Model Post-Training",
        "author": [
            "Junkai Zhang",
            "Zihao Wang",
            "Lin Gui",
            "Swarnashree Mysore Sathyendra",
            "Jaehwan Jeong",
            "Victor Veitch",
            "Wei Wang",
            "Yunzhong He",
            "Bing Liu",
            "Lifeng Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21500",
        "abstract": "Reinforcement fine-tuning (RFT) often suffers from \\emph{reward over-optimization}, where a policy model hacks the reward signals to achieve high scores while producing low-quality outputs. Our theoretical analysis shows that the key lies in reward misspecification at the high-reward tail: the inability to reliably distinguish Excellent responses from merely Great ones. This motivate us to focus on the high-reward region. However, such tail examples are scarce under the base LLM. While off-policy exemplars (e.g. from stronger models or rewrites) are easier to obtain, naively training on them yields a misspecified reward for the policy we aim to align. To address this, we study rubric-based rewards. By design, rubrics can leverage off-policy examples while remaining insensitive to their artifacts. To elicit rubrics that capture the high-reward tail, we highlight the importance of distinguishing among great and diverse responses, and introduce a workflow to implement this idea. We empirically demonstrate that rubric-based rewards substantially mitigate reward over-optimization and deliver effective LLM post-training improvements. Our code can be accessed at https://github.com/Jun-Kai-Zhang/rubrics.git .",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "44",
        "title": "LLM Agent Meets Agentic AI: Can LLM Agents Simulate Customers to Evaluate Agentic-AI-based Shopping Assistants?",
        "author": [
            "Lu Sun",
            "Shihan Fu",
            "Bingsheng Yao",
            "Yuxuan Lu",
            "Wenbo Li",
            "Hansu Gu",
            "Jiri Gesi",
            "Jing Huang",
            "Chen Luo",
            "Dakuo Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21501",
        "abstract": "Agentic AI is emerging, capable of executing tasks through natural language, such as Copilot for coding or Amazon Rufus for shopping. Evaluating these systems is challenging, as their rapid evolution outpaces traditional human evaluation. Researchers have proposed LLM Agents to simulate participants as digital twins, but it remains unclear to what extent a digital twin can represent a specific customer in multi-turn interaction with an agentic AI system. In this paper, we recruited 40 human participants to shop with Amazon Rufus, collected their personas, interaction traces, and UX feedback, and then created digital twins to repeat the task. Pairwise comparison of human and digital-twin traces shows that while agents often explored more diverse choices, their action patterns aligned with humans and yielded similar design feedback. This study is the first to quantify how closely LLM agents can mirror human multi-turn interaction with an agentic AI system, highlighting their potential for scalable evaluation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "45",
        "title": "QuantMind: A Context-Engineering Based Knowledge Framework for Quantitative Finance",
        "author": [
            "Haoxue Wang",
            "Keli Wen",
            "Yuante Li",
            "Qiancheng Qu",
            "Xiangxu Mu",
            "Xinjie Shen",
            "Jiaqi Gao",
            "Chenyang Chang",
            "Chuhan Xie",
            "San Yu Cheung",
            "Zhuoyuan Hu",
            "Xinyu Wang",
            "Sirui Bi",
            "Bi'an Du"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21507",
        "abstract": "Quantitative research increasingly relies on unstructured financial content such as filings, earnings calls, and research notes, yet existing LLM and RAG pipelines struggle with point-in-time correctness, evidence attribution, and integration into research workflows. To tackle this, We present QuantMind, an intelligent knowledge extraction and retrieval framework tailored to quantitative finance. QuantMind adopts a two-stage architecture: (i) a knowledge extraction stage that transforms heterogeneous documents into structured knowledge through multi-modal parsing of text, tables, and formulas, adaptive summarization for scalability, and domain-specific tagging for fine-grained indexing; and (ii) an intelligent retrieval stage that integrates semantic search with flexible strategies, multi-hop reasoning across sources, and knowledge-aware generation for auditable outputs. A controlled user study demonstrates that QuantMind improves both factual accuracy and user experience compared to unaided reading and generic AI assistance, underscoring the value of structured, domain-specific context engineering for finance.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "46",
        "title": "DistillKac: Few-Step Image Generation via Damped Wave Equations",
        "author": [
            "Weiqiao Han",
            "Chenlin Meng",
            "Christopher D. Manning",
            "Stefano Ermon"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21513",
        "abstract": "We present DistillKac, a fast image generator that uses the damped wave equation and its stochastic Kac representation to move probability mass at finite speed. In contrast to diffusion models whose reverse time velocities can become stiff and implicitly allow unbounded propagation speed, Kac dynamics enforce finite speed transport and yield globally bounded kinetic energy. Building on this structure, we introduce classifier-free guidance in velocity space that preserves square integrability under mild conditions. We then propose endpoint only distillation that trains a student to match a frozen teacher over long intervals. We prove a stability result that promotes supervision at the endpoints to closeness along the entire path. Experiments demonstrate DistillKac delivers high quality samples with very few function evaluations while retaining the numerical stability benefits of finite speed probability flows.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "47",
        "title": "Shortcut Flow Matching for Speech Enhancement: Step-Invariant flows via single stage training",
        "author": [
            "Naisong Zhou",
            "Saisamarth Rajesh Phaye",
            "Milos Cernak",
            "Tijana Stojkovic",
            "Andy Pearce",
            "Andrea Cavallaro",
            "Andy Harper"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21522",
        "abstract": "Diffusion-based generative models have achieved state-of-the-art performance for perceptual quality in speech enhancement (SE). However, their iterative nature requires numerous Neural Function Evaluations (NFEs), posing a challenge for real-time applications. On the contrary, flow matching offers a more efficient alternative by learning a direct vector field, enabling high-quality synthesis in just a few steps using deterministic ordinary differential equation~(ODE) solvers. We thus introduce Shortcut Flow Matching for Speech Enhancement (SFMSE), a novel approach that trains a single, step-invariant model. By conditioning the velocity field on the target time step during a one-stage training process, SFMSE can perform single, few, or multi-step denoising without any architectural changes or fine-tuning. Our results demonstrate that a single-step SFMSE inference achieves a real-time factor (RTF) of 0.013 on a consumer GPU while delivering perceptual quality comparable to a strong diffusion baseline requiring 60 NFEs. This work also provides an empirical analysis of the role of stochasticity in training and inference, bridging the gap between high-quality generative SE and low-latency constraints.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "ODE"
        ]
    },
    {
        "id": "48",
        "title": "DroneFL: Federated Learning for Multi-UAV Visual Target Tracking",
        "author": [
            "Xiaofan Yu",
            "Yuwei Wu",
            "Katherine Mao",
            "Ye Tian",
            "Vijay Kumar",
            "Tajana Rosing"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21523",
        "abstract": "Multi-robot target tracking is a fundamental problem that requires coordinated monitoring of dynamic entities in applications such as precision agriculture, environmental monitoring, disaster response, and security surveillance. While Federated Learning (FL) has the potential to enhance learning across multiple robots without centralized data aggregation, its use in multi-Unmanned Aerial Vehicle (UAV) target tracking remains largely underexplored. Key challenges include limited onboard computational resources, significant data heterogeneity in FL due to varying targets and the fields of view, and the need for tight coupling between trajectory prediction and multi-robot planning. In this paper, we introduce DroneFL, the first federated learning framework specifically designed for efficient multi-UAV target tracking. We design a lightweight local model to predict target trajectories from sensor inputs, using a frozen YOLO backbone and a shallow transformer for efficient onboard training. The updated models are periodically aggregated in the cloud for global knowledge sharing. To alleviate the data heterogeneity that hinders FL convergence, DroneFL introduces a position-invariant model architecture with altitude-based adaptive instance normalization. Finally, we fuse predictions from multiple UAVs in the cloud and generate optimal trajectories that balance target prediction accuracy and overall tracking performance. Our results show that DroneFL reduces prediction error by 6%-83% and tracking distance by 0.4%-4.6% compared to a distributed non-FL framework. In terms of efficiency, DroneFL runs in real time on a Raspberry Pi 5 and has on average just 1.56 KBps data rate to the cloud.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "49",
        "title": "Preemptive Detection and Steering of LLM Misalignment via Latent Reachability",
        "author": [
            "Sathwik Karnik",
            "Somil Bansal"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21528",
        "abstract": "Large language models (LLMs) are now ubiquitous in everyday tools, raising urgent safety concerns about their tendency to generate harmful content. The dominant safety approach -- reinforcement learning from human feedback (RLHF) -- effectively shapes model behavior during training but offers no safeguards at inference time, where unsafe continuations may still arise. We propose BRT-Align, a reachability-based framework that brings control-theoretic safety tools to LLM inference. BRT-Align models autoregressive generation as a dynamical system in latent space and learn a safety value function via backward reachability, estimating the worst-case evolution of a trajectory. This enables two complementary mechanisms: (1) a runtime monitor that forecasts unsafe completions several tokens in advance, and (2) a least-restrictive steering filter that minimally perturbs latent states to redirect generation away from unsafe regions. Experiments across multiple LLMs and toxicity benchmarks demonstrate that BRT-Align provides more accurate and earlier detection of unsafe continuations than baselines. Moreover, for LLM safety alignment, BRT-Align substantially reduces unsafe generations while preserving sentence diversity and coherence. Qualitative results further highlight emergent alignment properties: BRT-Align consistently produces responses that are less violent, less profane, less offensive, and less politically biased. Together, these findings demonstrate that reachability analysis provides a principled and practical foundation for inference-time LLM safety.",
        "tags": [
            "Detection",
            "LLM",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "50",
        "title": "A circuit for predicting hierarchical structure in-context in Large Language Models",
        "author": [
            "Tankred Saanum",
            "Can Demircan",
            "Samuel J. Gershman",
            "Eric Schulz"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21534",
        "abstract": "Large Language Models (LLMs) excel at in-context learning, the ability to use information provided as context to improve prediction of future tokens. Induction heads have been argued to play a crucial role for in-context learning in Transformer Language Models. These attention heads make a token attend to successors of past occurrences of the same token in the input. This basic mechanism supports LLMs' ability to copy and predict repeating patterns. However, it is unclear if this same mechanism can support in-context learning of more complex repetitive patterns with hierarchical structure. Natural language is teeming with such cases: The article \"the\" in English usually prefaces multiple nouns in a text. When predicting which token succeeds a particular instance of \"the\", we need to integrate further contextual cues from the text to predict the correct noun. If induction heads naively attend to all past instances of successor tokens of \"the\" in a context-independent manner, they cannot support this level of contextual information integration. In this study, we design a synthetic in-context learning task, where tokens are repeated with hierarchical dependencies. Here, attending uniformly to all successor tokens is not sufficient to accurately predict future tokens. Evaluating a range of LLMs on these token sequences and natural language analogues, we find adaptive induction heads that support prediction by learning what to attend to in-context. Next, we investigate how induction heads themselves learn in-context. We find evidence that learning is supported by attention heads that uncover a set of latent contexts, determining the different token transition relationships. Overall, we not only show that LLMs have induction heads that learn, but offer a complete mechanistic account of how LLMs learn to predict higher-order repetitive patterns in-context.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "51",
        "title": "ControlHair: Physically-based Video Diffusion for Controllable Dynamic Hair Rendering",
        "author": [
            "Weikai Lin",
            "Haoxiang Li",
            "Yuhao Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21541",
        "abstract": "Hair simulation and rendering are challenging due to complex strand dynamics, diverse material properties, and intricate light-hair interactions. Recent video diffusion models can generate high-quality videos, but they lack fine-grained control over hair dynamics. We present ControlHair, a hybrid framework that integrates a physics simulator with conditional video diffusion to enable controllable dynamic hair rendering. ControlHair adopts a three-stage pipeline: it first encodes physics parameters (e.g., hair stiffness, wind) into per-frame geometry using a simulator, then extracts per-frame control signals, and finally feeds control signals into a video diffusion model to generate videos with desired hair dynamics. This cascaded design decouples physics reasoning from video generation, supports diverse physics, and makes training the video diffusion model easy. Trained on a curated 10K video dataset, ControlHair outperforms text- and pose-conditioned baselines, delivering precisely controlled hair dynamics. We further demonstrate three use cases of ControlHair: dynamic hairstyle try-on, bullet-time effects, and cinemagraphic. ControlHair introduces the first physics-informed video diffusion framework for controllable dynamics. We provide a teaser video and experimental results on our website.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "52",
        "title": "Plan2Evolve: LLM Self-Evolution for Improved Planning Capability via Automated Domain Generation",
        "author": [
            "Jinbang Huang",
            "Zhiyuan Li",
            "Zhanguang Zhang",
            "Xingyue Quan",
            "Jianye Hao",
            "Yingxue Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21543",
        "abstract": "Large Language Models (LLMs) have recently shown strong potential in robotic task planning, particularly through automatic planning domain generation that integrates symbolic search. Prior approaches, however, have largely treated these domains as search utilities, with limited attention to their potential as scalable sources of reasoning data. At the same time, progress in reasoning LLMs has been driven by chain-of-thought (CoT) supervision, whose application in robotics remains dependent on costly, human-curated datasets. We propose Plan2Evolve, an LLM self-evolving framework in which the base model generates planning domains that serve as engines for producing symbolic problem-plan pairs as reasoning traces. These pairs are then transformed into extended CoT trajectories by the same model through natural-language explanations, thereby explicitly aligning symbolic planning structures with natural language reasoning. The resulting data extend beyond the model's intrinsic planning capacity, enabling model fine-tuning that yields a planning-enhanced LLM with improved planning success, stronger cross-task generalization, and reduced inference costs.",
        "tags": [
            "CoT",
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "53",
        "title": "Evidence for Limited Metacognition in LLMs",
        "author": [
            "Christopher Ackerman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21545",
        "abstract": "The possibility of LLM self-awareness and even sentience is gaining increasing public attention and has major safety and policy implications, but the science of measuring them is still in a nascent state. Here we introduce a novel methodology for quantitatively evaluating metacognitive abilities in LLMs. Taking inspiration from research on metacognition in nonhuman animals, our approach eschews model self-reports and instead tests to what degree models can strategically deploy knowledge of internal states. Using two experimental paradigms, we demonstrate that frontier LLMs introduced since early 2024 show increasingly strong evidence of certain metacognitive abilities, specifically the ability to assess and utilize their own confidence in their ability to answer factual and reasoning questions correctly and the ability to anticipate what answers they would give and utilize that information appropriately. We buttress these behavioral findings with an analysis of the token probabilities returned by the models, which suggests the presence of an upstream internal signal that could provide the basis for metacognition. We further find that these abilities 1) are limited in resolution, 2) emerge in context-dependent manners, and 3) seem to be qualitatively different from those of humans. We also report intriguing differences across models of similar capabilities, suggesting that LLM post-training may have a role in developing metacognitive abilities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "54",
        "title": "Correct Reasoning Paths Visit Shared Decision Pivots",
        "author": [
            "Dongkyu Cho",
            "Amy B.Z. Zhang",
            "Bilel Fehri",
            "Sheng Wang",
            "Rumi Chunara",
            "Rui Song",
            "Hengrui Cai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21549",
        "abstract": "Chain-of-thought (CoT) reasoning exposes the intermediate thinking process of large language models (LLMs), yet verifying those traces at scale remains unsolved. In response, we introduce the idea of decision pivots-minimal, verifiable checkpoints that any correct reasoning path must visit. We hypothesize that correct reasoning, though stylistically diverse, converge on the same pivot set, while incorrect ones violate at least one pivot. Leveraging this property, we propose a self-training pipeline that (i) samples diverse reasoning paths and mines shared decision pivots, (ii) compresses each trace into pivot-focused short-path reasoning using an auxiliary verifier, and (iii) post-trains the model using its self-generated outputs. The proposed method aligns reasoning without ground truth reasoning data or external metrics. Experiments on standard benchmarks such as LogiQA, MedQA, and MATH500 show the effectiveness of our method.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "55",
        "title": "Learning GUI Grounding with Spatial Reasoning from Visual Feedback",
        "author": [
            "Yu Zhao",
            "Wei-Ning Chen",
            "Huseyin Atahan Inan",
            "Samuel Kessler",
            "Lu Wang",
            "Lukas Wutschitz",
            "Fangkai Yang",
            "Chaoyun Zhang",
            "Pasquale Minervini",
            "Saravan Rajmohan",
            "Robert Sim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21552",
        "abstract": "Graphical User Interface (GUI) grounding is commonly framed as a coordinate prediction task -- given a natural language instruction, generate on-screen coordinates for actions such as clicks and keystrokes. However, recent Vision Language Models (VLMs) often fail to predict accurate numeric coordinates when processing high-resolution GUI images with complex layouts. To address this issue, we reframe GUI grounding as an \\emph{interactive search task}, where the VLM generates actions to move a cursor in the GUI to locate UI elements. At each step, the model determines the target object, evaluates the spatial relations between the cursor and the target, and moves the cursor closer to the target conditioned on the movement history. In this interactive process, the rendered cursor provides visual feedback to help the model align its predictions with the corresponding on-screen locations. We train our GUI grounding model, GUI-Cursor, using multi-step online reinforcement learning with a dense trajectory-based reward function. Our experimental results show that GUI-Cursor, based on Qwen2.5-VL-7B, improves the GUI grounding accuracy and achieves state-of-the-art results on ScreenSpot-v2 ($88.8\\% \\rightarrow 93.9\\%$) and ScreenSpot-Pro ($26.8\\% \\rightarrow 56.5\\%$). Moreover, we observe that GUI-Cursor learns to solve the problem within two steps for 95\\% of instances and can adaptively conduct more steps on more difficult examples.",
        "tags": [
            "RL",
            "VLM"
        ]
    },
    {
        "id": "56",
        "title": "Generation-Time vs. Post-hoc Citation: A Holistic Evaluation of LLM Attribution",
        "author": [
            "Yash Saxena",
            "Raviteja Bommireddy",
            "Ankur Padia",
            "Manas Gaur"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21557",
        "abstract": "Trustworthy Large Language Models (LLMs) must cite human-verifiable sources in high-stakes domains such as healthcare, law, academia, and finance, where even small errors can have severe consequences. Practitioners and researchers face a choice: let models generate citations during decoding, or let models draft answers first and then attach appropriate citations. To clarify this choice, we introduce two paradigms: Generation-Time Citation (G-Cite), which produces the answer and citations in one pass, and Post-hoc Citation (P-Cite), which adds or verifies citations after drafting. We conduct a comprehensive evaluation from zero-shot to advanced retrieval-augmented methods across four popular attribution datasets and provide evidence-based recommendations that weigh trade-offs across use cases. Our results show a consistent trade-off between coverage and citation correctness, with retrieval as the main driver of attribution quality in both paradigms. P-Cite methods achieve high coverage with competitive correctness and moderate latency, whereas G-Cite methods prioritize precision at the cost of coverage and speed. We recommend a retrieval-centric, P-Cite-first approach for high-stakes applications, reserving G-Cite for precision-critical settings such as strict claim verification. Our codes and human evaluation results are available at https://anonymous.4open.science/r/Citation_Paradigms-BBB5/",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "57",
        "title": "X-CoT: Explainable Text-to-Video Retrieval via LLM-based Chain-of-Thought Reasoning",
        "author": [
            "Prasanna Reddy Pulakurthi",
            "Jiamian Wang",
            "Majid Rabbani",
            "Sohail Dianat",
            "Raghuveer Rao",
            "Zhiqiang Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21559",
        "abstract": "Prevalent text-to-video retrieval systems mainly adopt embedding models for feature extraction and compute cosine similarities for ranking. However, this design presents two limitations. Low-quality text-video data pairs could compromise the retrieval, yet are hard to identify and examine. Cosine similarity alone provides no explanation for the ranking results, limiting the interpretability. We ask that can we interpret the ranking results, so as to assess the retrieval models and examine the text-video data? This work proposes X-CoT, an explainable retrieval framework upon LLM CoT reasoning in place of the embedding model-based similarity ranking. We first expand the existing benchmarks with additional video annotations to support semantic understanding and reduce data bias. We also devise a retrieval CoT consisting of pairwise comparison steps, yielding detailed reasoning and complete ranking. X-CoT empirically improves the retrieval performance and produces detailed rationales. It also facilitates the model behavior and data quality analysis. Code and data are available at: https://github.com/PrasannaPulakurthi/X-CoT.",
        "tags": [
            "CoT",
            "LLM",
            "Text-to-Video"
        ]
    },
    {
        "id": "58",
        "title": "No Alignment Needed for Generation: Learning Linearly Separable Representations in Diffusion Models",
        "author": [
            "Junno Yun",
            "YaÅar Utku AlÃ§alar",
            "Mehmet AkÃ§akaya"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21565",
        "abstract": "Efficient training strategies for large-scale diffusion models have recently emphasized the importance of improving discriminative feature representations in these models. A central line of work in this direction is representation alignment with features obtained from powerful external encoders, which improves the representation quality as assessed through linear probing. Alignment-based approaches show promise but depend on large pretrained encoders, which are computationally expensive to obtain. In this work, we propose an alternative regularization for training, based on promoting the Linear SEParability (LSEP) of intermediate layer representations. LSEP eliminates the need for an auxiliary encoder and representation alignment, while incorporating linear probing directly into the network's learning dynamics rather than treating it as a simple post-hoc evaluation tool. Our results demonstrate substantial improvements in both training efficiency and generation quality on flow-based transformer architectures such as SiTs, achieving an FID of 1.46 on $256 \\times 256$ ImageNet dataset.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "59",
        "title": "Autonomous UAV-Quadruped Docking in Complex Terrains via Active Posture Alignment and Constraint-Aware Control",
        "author": [
            "HaoZhe Xu",
            "Cheng Cheng",
            "HongRui Sang",
            "Zhipeng Wang",
            "Qiyong He",
            "Xiuxian Li",
            "Bin He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21571",
        "abstract": "Autonomous docking between Unmanned Aerial Vehicles (UAVs) and ground robots is essential for heterogeneous systems, yet most existing approaches target wheeled platforms whose limited mobility constrains exploration in complex terrains. Quadruped robots offer superior adaptability but undergo frequent posture variations, making it difficult to provide a stable landing surface for UAVs. To address these challenges, we propose an autonomous UAV-quadruped docking framework for GPS-denied environments. On the quadruped side, a Hybrid Internal Model with Horizontal Alignment (HIM-HA), learned via deep reinforcement learning, actively stabilizes the torso to provide a level platform. On the UAV side, a three-phase strategy is adopted, consisting of long-range acquisition with a median-filtered YOLOv8 detector, close-range tracking with a constraint-aware controller that integrates a Nonsingular Fast Terminal Sliding Mode Controller (NFTSMC) and a logarithmic Barrier Function (BF) to guarantee finite-time error convergence under field-of-view (FOV) constraints, and terminal descent guided by a Safety Period (SP) mechanism that jointly verifies tracking accuracy and platform stability. The proposed framework is validated in both simulation and real-world scenarios, successfully achieving docking on outdoor staircases higher than 17 cm and rough slopes steeper than 30 degrees. Supplementary materials and videos are available at: https://uav-quadruped-docking.github.io.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "60",
        "title": "X-Streamer: Unified Human World Modeling with Audiovisual Interaction",
        "author": [
            "You Xie",
            "Tianpei Gu",
            "Zenan Li",
            "Chenxu Zhang",
            "Guoxian Song",
            "Xiaochen Zhao",
            "Chao Liang",
            "Jianwen Jiang",
            "Hongyi Xu",
            "Linjie Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21574",
        "abstract": "We introduce X-Streamer, an end-to-end multimodal human world modeling framework for building digital human agents capable of infinite interactions across text, speech, and video within a single unified architecture. Starting from a single portrait, X-Streamer enables real-time, open-ended video calls driven by streaming multimodal inputs. At its core is a Thinker-Actor dual-transformer architecture that unifies multimodal understanding and generation, turning a static portrait into persistent and intelligent audiovisual interactions. The Thinker module perceives and reasons over streaming user inputs, while its hidden states are translated by the Actor into synchronized multimodal streams in real time. Concretely, the Thinker leverages a pretrained large language-speech model, while the Actor employs a chunk-wise autoregressive diffusion model that cross-attends to the Thinker's hidden states to produce time-aligned multimodal responses with interleaved discrete text and audio tokens and continuous video latents. To ensure long-horizon stability, we design inter- and intra-chunk attentions with time-aligned multimodal positional embeddings for fine-grained cross-modality alignment and context retention, further reinforced by chunk-wise diffusion forcing and global identity referencing. X-Streamer runs in real time on two A100 GPUs, sustaining hours-long consistent video chat experiences from arbitrary portraits and paving the way toward unified world modeling of interactive digital humans.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "61",
        "title": "Vision Language Models Cannot Plan, but Can They Formalize?",
        "author": [
            "Muyu He",
            "Yuxi Zheng",
            "Yuchen Liu",
            "Zijian An",
            "Bill Cai",
            "Jiani Huang",
            "Lifeng Zhou",
            "Feng Liu",
            "Ziyang Li",
            "Li Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21576",
        "abstract": "The advancement of vision language models (VLMs) has empowered embodied agents to accomplish simple multimodal planning tasks, but not long-horizon ones requiring long sequences of actions. In text-only simulations, long-horizon planning has seen significant improvement brought by repositioning the role of LLMs. Instead of directly generating action sequences, LLMs translate the planning domain and problem into a formal planning language like the Planning Domain Definition Language (PDDL), which can call a formal solver to derive the plan in a verifiable manner. In multimodal environments, research on VLM-as-formalizer remains scarce, usually involving gross simplifications such as predefined object vocabulary or overly similar few-shot examples. In this work, we present a suite of five VLM-as-formalizer pipelines that tackle one-shot, open-vocabulary, and multimodal PDDL formalization. We evaluate those on an existing benchmark while presenting another two that for the first time account for planning with authentic, multi-view, and low-quality images. We conclude that VLM-as-formalizer greatly outperforms end-to-end plan generation. We reveal the bottleneck to be vision rather than language, as VLMs often fail to capture an exhaustive set of necessary object relations. While generating intermediate, textual representations such as captions or scene graphs partially compensate for the performance, their inconsistent gain leaves headroom for future research directions on multimodal planning formalization.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "62",
        "title": "\"Be My Cheese?\": Assessing Cultural Nuance in Multilingual LLM Translations",
        "author": [
            "Madison Van Doren",
            "Cory Holland"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21577",
        "abstract": "This pilot study explores the localisation capabilities of state-of-the-art multilingual AI models when translating figurative language, such as idioms and puns, from English into a diverse range of global languages. It expands on existing LLM translation research and industry benchmarks, which emphasise grammatical accuracy and token-level correctness, by focusing on cultural appropriateness and overall localisation quality - critical factors for real-world applications like marketing and e-commerce.\nTo investigate these challenges, this project evaluated a sample of 87 LLM-generated translations of e-commerce marketing emails across 24 regional dialects of 20 languages. Human reviewers fluent in each target language provided quantitative ratings and qualitative feedback on faithfulness to the original's tone, meaning, and intended audience. Findings suggest that, while leading models generally produce grammatically correct translations, culturally nuanced language remains a clear area for improvement, often requiring substantial human refinement. Notably, even high-resource global languages, despite topping industry benchmark leaderboards, frequently mistranslated figurative expressions and wordplay.\nThis work challenges the assumption that data volume is the most reliable predictor of machine translation quality and introduces cultural appropriateness as a key determinant of multilingual LLM performance - an area currently underexplored in existing academic and industry benchmarks. As a proof of concept, this pilot highlights limitations of current multilingual AI systems for real-world localisation use cases. Results of this pilot support the opportunity for expanded research at greater scale to deliver generalisable insights and inform deployment of reliable machine translation workflows in culturally diverse contexts.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "63",
        "title": "What Happens Next? Anticipating Future Motion by Generating Point Trajectories",
        "author": [
            "Gabrijel Boduljak",
            "Laurynas Karazija",
            "Iro Laina",
            "Christian Rupprecht",
            "Andrea Vedaldi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21592",
        "abstract": "We consider the problem of forecasting motion from a single image, i.e., predicting how objects in the world are likely to move, without the ability to observe other parameters such as the object velocities or the forces applied to them. We formulate this task as conditional generation of dense trajectory grids with a model that closely follows the architecture of modern video generators but outputs motion trajectories instead of pixels. This approach captures scene-wide dynamics and uncertainty, yielding more accurate and diverse predictions than prior regressors and generators. We extensively evaluate our method on simulated data, demonstrate its effectiveness on downstream applications such as robotics, and show promising accuracy on real-world intuitive physics datasets. Although recent state-of-the-art video generators are often regarded as world models, we show that they struggle with forecasting motion from a single image, even in simple physical scenarios such as falling blocks or mechanical object interactions, despite fine-tuning on such data. We show that this limitation arises from the overhead of generating pixels rather than directly modeling motion.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "64",
        "title": "GeoEvolve: Automating Geospatial Model Discovery via Multi-Agent Large Language Models",
        "author": [
            "Peng Luo",
            "Xiayin Lou",
            "Yu Zheng",
            "Zhuo Zheng",
            "Stefano Ermon"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21593",
        "abstract": "Geospatial modeling provides critical solutions for pressing global challenges such as sustainability and climate change. Existing large language model (LLM)-based algorithm discovery frameworks, such as AlphaEvolve, excel at evolving generic code but lack the domain knowledge and multi-step reasoning required for complex geospatial problems. We introduce GeoEvolve, a multi-agent LLM framework that couples evolutionary search with geospatial domain knowledge to automatically design and refine geospatial algorithms. GeoEvolve operates in two nested loops: an inner loop leverages a code evolver to generate and mutate candidate solutions, while an outer agentic controller evaluates global elites and queries a GeoKnowRAG module -- a structured geospatial knowledge base that injects theoretical priors from geography. This knowledge-guided evolution steers the search toward theoretically meaningful and computationally efficient algorithms. We evaluate GeoEvolve on two fundamental and classical tasks: spatial interpolation (kriging) and spatial uncertainty quantification (geospatial conformal prediction). Across these benchmarks, GeoEvolve automatically improves and discovers new algorithms, incorporating geospatial theory on top of classical models. It reduces spatial interpolation error (RMSE) by 13-21% and enhances uncertainty estimation performance by 17\\%. Ablation studies confirm that domain-guided retrieval is essential for stable, high-quality evolution. These results demonstrate that GeoEvolve provides a scalable path toward automated, knowledge-driven geospatial modeling, opening new opportunities for trustworthy and efficient AI-for-Science discovery.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "65",
        "title": "Message passing for epidemiological interventions on networks with loops",
        "author": [
            "Erik Weis",
            "Laurent HÃ©bert-Dufresne",
            "Jean-Gabriel Young"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21596",
        "abstract": "Spreading models capture key dynamics on networks, such as cascading failures in economic systems, (mis)information diffusion, and pathogen transmission. Here, we focus on design intervention problems -- for example, designing optimal vaccination rollouts or wastewater surveillance systems -- which can be solved by comparing outcomes under various counterfactuals. A leading approach to computing these outcomes is message passing, which allows for the rapid and direct computation of the marginal probabilities for each node. However, despite its efficiency, classical message passing tends to overestimate outbreak sizes on real-world networks, leading to incorrect predictions and, thus, interventions. Here, we improve these estimates by using the neighborhood message passing (NMP) framework for the epidemiological calculations. We evaluate the quality of the improved algorithm and demonstrate how it can be used to test possible solutions to three intervention design problems: influence maximization, optimal vaccination, and sentinel surveillance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "66",
        "title": "Real-Time Indoor Object SLAM with LLM-Enhanced Priors",
        "author": [
            "Yang Jiao",
            "Yiding Qiu",
            "Henrik I. Christensen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21602",
        "abstract": "Object-level Simultaneous Localization and Mapping (SLAM), which incorporates semantic information for high-level scene understanding, faces challenges of under-constrained optimization due to sparse observations. Prior work has introduced additional constraints using commonsense knowledge, but obtaining such priors has traditionally been labor-intensive and lacks generalizability across diverse object categories. We address this limitation by leveraging large language models (LLMs) to provide commonsense knowledge of object geometric attributes, specifically size and orientation, as prior factors in a graph-based SLAM framework. These priors are particularly beneficial during the initial phase when object observations are limited. We implement a complete pipeline integrating these priors, achieving robust data association on sparse object-level features and enabling real-time object SLAM. Our system, evaluated on the TUM RGB-D and 3RScan datasets, improves mapping accuracy by 36.8\\% over the latest baseline. Additionally, we present real-world experiments in the supplementary video, demonstrating its real-time performance.",
        "tags": [
            "LLM",
            "SLAM"
        ]
    },
    {
        "id": "67",
        "title": "VLCE: A Knowledge-Enhanced Framework for Image Description in Disaster Assessment",
        "author": [
            "Md. Mahfuzur Rahman",
            "Kishor Datta Gupta",
            "Marufa Kamal",
            "Fahad Rahman",
            "Sunzida Siddique",
            "Ahmed Rafi Hasan",
            "Mohd Ariful Haque",
            "Roy George"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21609",
        "abstract": "Immediate damage assessment is essential after natural catastrophes; yet, conventional hand evaluation techniques are sluggish and perilous. Although satellite and unmanned aerial vehicle (UAV) photos offer extensive perspectives of impacted regions, current computer vision methodologies generally yield just classification labels or segmentation masks, so constraining their capacity to deliver a thorough situational comprehension. We introduce the Vision Language Caption Enhancer (VLCE), a multimodal system designed to produce comprehensive, contextually-informed explanations of disaster imagery. VLCE employs a dual-architecture approach: a CNN-LSTM model with a ResNet50 backbone pretrained on EuroSat satellite imagery for the xBD dataset, and a Vision Transformer (ViT) model pretrained on UAV pictures for the RescueNet dataset. Both systems utilize external semantic knowledge from ConceptNet and WordNet to expand vocabulary coverage and improve description accuracy. We assess VLCE in comparison to leading vision-language models (LLaVA and QwenVL) utilizing CLIPScore for semantic alignment and InfoMetIC for caption informativeness. Experimental findings indicate that VLCE markedly surpasses baseline models, attaining a maximum of 95.33% on InfoMetIC while preserving competitive semantic alignment. Our dual-architecture system demonstrates significant potential for improving disaster damage assessment by automating the production of actionable, information-dense descriptions from satellite and drone photos.",
        "tags": [
            "LLaVA",
            "Segmentation",
            "Transformer",
            "VLM",
            "ViT"
        ]
    },
    {
        "id": "68",
        "title": "Multi-Objective Reinforcement Learning for Large Language Model Optimization: Visionary Perspective",
        "author": [
            "Lingxiao Kong",
            "Cong Yang",
            "Oya Deniz Beyan",
            "Zeyd Boukhers"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21613",
        "abstract": "Multi-Objective Reinforcement Learning (MORL) presents significant challenges and opportunities for optimizing multiple objectives in Large Language Models (LLMs). We introduce a MORL taxonomy and examine the advantages and limitations of various MORL methods when applied to LLM optimization, identifying the need for efficient and flexible approaches that accommodate personalization functionality and inherent complexities in LLMs and RL. We propose a vision for a MORL benchmarking framework that addresses the effects of different methods on diverse objective relationships. As future research directions, we focus on meta-policy MORL development that can improve efficiency and flexibility through its bi-level learning paradigm, highlighting key research questions and potential solutions for improving LLM performance.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "69",
        "title": "PreLoRA: Hybrid Pre-training of Vision Transformers with Full Training and Low-Rank Adapters",
        "author": [
            "Krishu K Thapa",
            "Reet Barik",
            "Krishna Teja Chitty-Venkata",
            "Murali Emani",
            "Venkatram Vishwanath"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21619",
        "abstract": "Training large models ranging from millions to billions of parameters is highly resource-intensive, requiring significant time, compute, and memory. It is observed that most of the learning (higher change in weights) takes place in the earlier stage of the training loop. These changes stabilize as training continues, enabling them to be captured by matrices of a low intrinsic rank. Therefore, we propose an approach to identify such states of partial convergence and dynamically switch from full parameter training to Low-Rank Adaptation (LoRA) on the ViT-Large model. We introduce a flexible approach that leverages user-defined hyperparameters to determine the switching point and assign a rank specific to each module layer based on its level of convergence. Experimental results show that this approach preserves model accuracy while reducing the number of trainable parameters to 10% of its original size, resulting in a 3x improvement in throughput, and a 1.5x reduction in average training time per epoch while also reducing GPU memory consumption by 20%",
        "tags": [
            "LoRA",
            "ViT"
        ]
    },
    {
        "id": "70",
        "title": "OjaKV: Context-Aware Online Low-Rank KV Cache Compression with Oja's Rule",
        "author": [
            "Yuxuan Zhu",
            "David H. Yang",
            "Mohammad Mohammadi Amiri",
            "Keerthiram Murugesan",
            "Tejaswini Pedapati",
            "Pin-Yu Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21623",
        "abstract": "The expanding long-context capabilities of large language models are constrained by a significant memory bottleneck: the key-value (KV) cache required for autoregressive generation. This bottleneck is substantial; for instance, a Llama-3.1-8B model processing a 32K-token prompt at a batch size of 4 requires approximately 16GB for its KV cache, a size exceeding the model's weights. While KV-cache compression via low-rank projection is a promising direction, existing methods rely on a static, offline-learned subspace that performs poorly under data distribution shifts. To overcome these limitations, we introduce OjaKV, a novel framework that integrates a strategic hybrid storage policy with online subspace adaptation. First, OjaKV recognizes that not all tokens are equally important for compression; it preserves the crucial first and most recent tokens in full-rank, maintaining high-fidelity anchors for attention. Second, for the vast majority of intermediate tokens, it applies low-rank compression by incrementally adapting the projection basis using Oja's algorithm for online principal component analysis. This adaptation involves a comprehensive update during prompt prefilling and lightweight periodic updates during decoding, ensuring the subspace remains aligned with the evolving context. Crucially, our framework is fully compatible with modern attention modules like FlashAttention. Experiments demonstrate that OjaKV maintains or even improves zero-shot accuracy at high compression ratios. In particular, OjaKV achieves its strongest gains on very long-context benchmarks that require complex reasoning, highlighting the importance of online subspace adaptation in dynamically tracking context shifts. These results establish our hybrid framework as a practical, plug-and-play solution for memory-efficient long-context inference without requiring model fine-tuning.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "71",
        "title": "Guiding Audio Editing with Audio Language Model",
        "author": [
            "Zitong Lan",
            "Yiduo Hao",
            "Mingmin Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21625",
        "abstract": "Audio editing plays a central role in VR/AR immersion, virtual conferencing, sound design, and other interactive media. However, recent generative audio editing models depend on template-like instruction formats and are restricted to mono-channel audio. These models fail to deal with declarative audio editing, where the user declares what the desired outcome should be, while leaving the details of editing operations to the system. We introduce SmartDJ, a novel framework for stereo audio editing that combines the reasoning capability of audio language models with the generative power of latent diffusion. Given a high-level instruction, SmartDJ decomposes it into a sequence of atomic edit operations, such as adding, removing, or spatially relocating events. These operations are then executed by a diffusion model trained to manipulate stereo audio. To support this, we design a data synthesis pipeline that produces paired examples of high-level instructions, atomic edit operations, and audios before and after each edit operation. Experiments demonstrate that SmartDJ achieves superior perceptual quality, spatial realism, and semantic alignment compared to prior audio editing methods. Demos are available at https://zitonglan.github.io/project/smartdj/smartdj.html.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "72",
        "title": "InvBench: Can LLMs Accelerate Program Verification with Invariant Synthesis?",
        "author": [
            "Anjiang Wei",
            "Tarun Suresh",
            "Tianran Sun",
            "Haoze Wu",
            "Ke Wang",
            "Alex Aiken"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21629",
        "abstract": "Program verification relies on loop invariants, yet automatically discovering strong invariants remains a long-standing challenge. We introduce a principled framework for evaluating LLMs on invariant synthesis. Our approach uses a verifier-based decision procedure with a formal soundness guarantee and assesses not only correctness but also the speedup that invariants provide in verification. We evaluate 7 state-of-the-art LLMs, and existing LLM-based verifiers against the traditional solver UAutomizer. While LLM-based verifiers represent a promising direction, they do not yet offer a significant advantage over UAutomizer. Model capability also proves critical, as shown by sharp differences in speedups across models, and our benchmark remains an open challenge for current LLMs. Finally, we show that supervised fine-tuning and Best-of-N sampling can improve performance: fine-tuning on 3589 instances raises the percentage of speedup cases for Qwen3-Coder-480B from 8% to 29.2%, and Best-of-N sampling with N=16 improves Claude-sonnet-4 from 8.8% to 22.1%.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "73",
        "title": "Towards Transparent AI: A Survey on Explainable Language Models",
        "author": [
            "Avash Palikhe",
            "Zichong Wang",
            "Zhipeng Yin",
            "Rui Guo",
            "Qiang Duan",
            "Jie Yang",
            "Wenbin Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21631",
        "abstract": "Language Models (LMs) have significantly advanced natural language processing and enabled remarkable progress across diverse domains, yet their black-box nature raises critical concerns about the interpretability of their internal mechanisms and decision-making processes. This lack of transparency is particularly problematic for adoption in high-stakes domains, where stakeholders need to understand the rationale behind model outputs to ensure accountability. On the other hand, while explainable artificial intelligence (XAI) methods have been well studied for non-LMs, they face many limitations when applied to LMs due to their complex architectures, considerable training corpora, and broad generalization abilities. Although various surveys have examined XAI in the context of LMs, they often fail to capture the distinct challenges arising from the architectural diversity and evolving capabilities of these models. To bridge this gap, this survey presents a comprehensive review of XAI techniques with a particular emphasis on LMs, organizing them according to their underlying transformer architectures: encoder-only, decoder-only, and encoder-decoder, and analyzing how methods are adapted to each while assessing their respective strengths and limitations. Furthermore, we evaluate these techniques through the dual lenses of plausibility and faithfulness, offering a structured perspective on their effectiveness. Finally, we identify open research challenges and outline promising future directions, aiming to guide ongoing efforts toward the development of robust, transparent, and interpretable XAI methods for LMs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "74",
        "title": "MobiLLM: An Agentic AI Framework for Closed-Loop Threat Mitigation in 6G Open RANs",
        "author": [
            "Prakhar Sharma",
            "Haohuang Wen",
            "Vinod Yegneswaran",
            "Ashish Gehani",
            "Phillip Porras",
            "Zhiqiang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21634",
        "abstract": "The evolution toward 6G networks is being accelerated by the Open Radio Access Network (O-RAN) paradigm -- an open, interoperable architecture that enables intelligent, modular applications across public telecom and private enterprise domains. While this openness creates unprecedented opportunities for innovation, it also expands the attack surface, demanding resilient, low-cost, and autonomous security solutions. Legacy defenses remain largely reactive, labor-intensive, and inadequate for the scale and complexity of next-generation systems. Current O-RAN applications focus mainly on network optimization or passive threat detection, with limited capability for closed-loop, automated response.\nTo address this critical gap, we present an agentic AI framework for fully automated, end-to-end threat mitigation in 6G O-RAN environments. MobiLLM orchestrates security workflows through a modular multi-agent system powered by Large Language Models (LLMs). The framework features a Threat Analysis Agent for real-time data triage, a Threat Classification Agent that uses Retrieval-Augmented Generation (RAG) to map anomalies to specific countermeasures, and a Threat Response Agent that safely operationalizes mitigation actions via O-RAN control interfaces. Grounded in trusted knowledge bases such as the MITRE FiGHT framework and 3GPP specifications, and equipped with robust safety guardrails, MobiLLM provides a blueprint for trustworthy AI-driven network security. Initial evaluations demonstrate that MobiLLM can effectively identify and orchestrate complex mitigation strategies, significantly reducing response latency and showcasing the feasibility of autonomous security operations in 6G.",
        "tags": [
            "Detection",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "75",
        "title": "Blockwise Hadamard high-Rank Adaptation for Parameter-Efficient LLM Fine-Tuning",
        "author": [
            "Feng Yu",
            "Jia Hu",
            "Geyong Min"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21637",
        "abstract": "Parameter-efficient fine-tuning (PEFT) methods must be resource-efficient yet handle heterogeneous reasoning transformations, and classical low-rank adaptation (LoRA) is constrained by the nominal rank $r$. Hadamard-style extensions like HiRA raise the nominal rank but couple every update to the global energy pattern of the frozen weight matrix, while ABBA trades this inductive bias for fully learned dense intermediates. To address the limitation of global modulation, we propose Block Hadamard high-Rank Adaptation (BHRA), which partitions each weight matrix and applies HiRA-style multiplicative modulation independently within every block, preserving the PEFT parameter footprint while unlocking localized rank amplification. Our empirical analyses reveal that this blockwise design maintains rich spectra across rank budgets, mitigating the collapse induced by global modulation. Across eight commonsense reasoning tasks and two arithmetic benchmarks with Llama-3.2 1B/3B, Mistral-7B, and Gemma-2 9B, BHRA consistently surpasses strong PEFT baselines under matched parameter budgets.",
        "tags": [
            "LLM",
            "LLaMA",
            "LoRA"
        ]
    },
    {
        "id": "76",
        "title": "eXplainable Artificial Intelligence for RL-based Networking Solutions",
        "author": [
            "Yeison Stiven Murcia",
            "Oscar Mauricio Caicedo",
            "Daniela Maria Casas",
            "Nelson Luis Saldanha da Fonseca"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21649",
        "abstract": "Reinforcement Learning (RL) agents have been widely used to improve networking tasks. However, understanding the decisions made by these agents is essential for their broader adoption in networking and network management. To address this, we introduce eXplaNet - a pipeline grounded in explainable artificial intelligence - designed to help networking researchers and practitioners gain deeper insights into the decision-making processes of RL-based solutions. We demonstrate how eXplaNet can be applied to refine a routing solution powered by a Q-learning agent, specifically by improving its reward function. In addition, we discuss the opportunities and challenges of incorporating explainability into RL to better optimize network performance.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "77",
        "title": "Can AI Perceive Physical Danger and Intervene?",
        "author": [
            "Abhishek Jindal",
            "Dmitry Kalashnikov",
            "Oscar Chang",
            "Divya Garikapati",
            "Anirudha Majumdar",
            "Pierre Sermanet",
            "Vikas Sindhwani"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21651",
        "abstract": "When AI interacts with the physical world -- as a robot or an assistive agent -- new safety challenges emerge beyond those of purely ``digital AI\". In such interactions, the potential for physical harm is direct and immediate. How well do state-of-the-art foundation models understand common-sense facts about physical safety, e.g. that a box may be too heavy to lift, or that a hot cup of coffee should not be handed to a child? In this paper, our contributions are three-fold: first, we develop a highly scalable approach to continuous physical safety benchmarking of Embodied AI systems, grounded in real-world injury narratives and operational safety constraints. To probe multi-modal safety understanding, we turn these narratives and constraints into photorealistic images and videos capturing transitions from safe to unsafe states, using advanced generative models. Secondly, we comprehensively analyze the ability of major foundation models to perceive risks, reason about safety, and trigger interventions; this yields multi-faceted insights into their deployment readiness for safety-critical agentic applications. Finally, we develop a post-training paradigm to teach models to explicitly reason about embodiment-specific safety constraints provided through system instructions. The resulting models generate thinking traces that make safety reasoning interpretable and transparent, achieving state of the art performance in constraint satisfaction evaluations. The benchmark will be released at https://asimov-benchmark.github.io/v2",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "78",
        "title": "FantasyWorld: Geometry-Consistent World Modeling via Unified Video and 3D Prediction",
        "author": [
            "Yixiang Dai",
            "Fan Jiang",
            "Chiyu Wang",
            "Mu Xu",
            "Yonggang Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21657",
        "abstract": "High-quality 3D world models are pivotal for embodied intelligence and Artificial General Intelligence (AGI), underpinning applications such as AR/VR content creation and robotic navigation. Despite the established strong imaginative priors, current video foundation models lack explicit 3D grounding capabilities, thus being limited in both spatial consistency and their utility for downstream 3D reasoning tasks. In this work, we present FantasyWorld, a geometry-enhanced framework that augments frozen video foundation models with a trainable geometric branch, enabling joint modeling of video latents and an implicit 3D field in a single forward pass. Our approach introduces cross-branch supervision, where geometry cues guide video generation and video priors regularize 3D prediction, thus yielding consistent and generalizable 3D-aware video representations. Notably, the resulting latents from the geometric branch can potentially serve as versatile representations for downstream 3D tasks such as novel view synthesis and navigation, without requiring per-scene optimization or fine-tuning. Extensive experiments show that FantasyWorld effectively bridges video imagination and 3D perception, outperforming recent geometry-consistent baselines in multi-view coherence and style consistency. Ablation studies further confirm that these gains stem from the unified backbone and cross-branch information exchange.",
        "tags": [
            "3D",
            "Video Generation"
        ]
    },
    {
        "id": "79",
        "title": "RED-DiffEq: Regularization by denoising diffusion models for solving inverse PDE problems with application to full waveform inversion",
        "author": [
            "Siming Shan",
            "Min Zhu",
            "Youzuo Lin",
            "Lu Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21659",
        "abstract": "Partial differential equation (PDE)-governed inverse problems are fundamental across various scientific and engineering applications; yet they face significant challenges due to nonlinearity, ill-posedness, and sensitivity to noise. Here, we introduce a new computational framework, RED-DiffEq, by integrating physics-driven inversion and data-driven learning. RED-DiffEq leverages pretrained diffusion models as a regularization mechanism for PDE-governed inverse problems. We apply RED-DiffEq to solve the full waveform inversion problem in geophysics, a challenging seismic imaging technique that seeks to reconstruct high-resolution subsurface velocity models from seismic measurement data. Our method shows enhanced accuracy and robustness compared to conventional methods. Additionally, it exhibits strong generalization ability to more complex velocity models that the diffusion model is not trained on. Our framework can also be directly applied to diverse PDE-governed inverse problems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "80",
        "title": "MMPlanner: Zero-Shot Multimodal Procedural Planning with Chain-of-Thought Object State Reasoning",
        "author": [
            "Afrina Tabassum",
            "Bin Guo",
            "Xiyao Ma",
            "Hoda Eldardiry",
            "Ismini Lourentzou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21662",
        "abstract": "Multimodal Procedural Planning (MPP) aims to generate step-by-step instructions that combine text and images, with the central challenge of preserving object-state consistency across modalities while producing informative plans. Existing approaches often leverage large language models (LLMs) to refine textual steps; however, visual object-state alignment and systematic evaluation are largely underexplored. We present MMPlanner, a zero-shot MPP framework that introduces Object State Reasoning Chain-of-Thought (OSR-CoT) prompting to explicitly model object-state transitions and generate accurate multimodal plans. To assess plan quality, we design LLM-as-a-judge protocols for planning accuracy and cross-modal alignment, and further propose a visual step-reordering task to measure temporal coherence. Experiments on RECIPEPLAN and WIKIPLAN show that MMPlanner achieves state-of-the-art performance, improving textual planning by +6.8%, cross-modal alignment by +11.9%, and visual step ordering by +26.7%",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "81",
        "title": "Generating Stable Placements via Physics-guided Diffusion Models",
        "author": [
            "Philippe Nadeau",
            "Miguel Rogel",
            "Ivan BiliÄ",
            "Ivan PetroviÄ",
            "Jonathan Kelly"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21664",
        "abstract": "Stably placing an object in a multi-object scene is a fundamental challenge in robotic manipulation, as placements must be penetration-free, establish precise surface contact, and result in a force equilibrium. To assess stability, existing methods rely on running a simulation engine or resort to heuristic, appearance-based assessments. In contrast, our approach integrates stability directly into the sampling process of a diffusion model. To this end, we query an offline sampling-based planner to gather multi-modal placement labels and train a diffusion model to generate stable placements. The diffusion model is conditioned on scene and object point clouds, and serves as a geometry-aware prior. We leverage the compositional nature of score-based generative models to combine this learned prior with a stability-aware loss, thereby increasing the likelihood of sampling from regions of high stability. Importantly, this strategy requires no additional re-training or fine-tuning, and can be directly applied to off-the-shelf models. We evaluate our method on four benchmark scenes where stability can be accurately computed. Our physics-guided models achieve placements that are 56% more robust to forceful perturbations while reducing runtime by 47% compared to a state-of-the-art geometric method.",
        "tags": [
            "Diffusion",
            "Score-Based Generative"
        ]
    },
    {
        "id": "82",
        "title": "Alignment Without Understanding: A Message- and Conversation-Centered Approach to Understanding AI Sycophancy",
        "author": [
            "Lihua Du",
            "Xing Lyu",
            "Lezi Xie",
            "Bo Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21665",
        "abstract": "AI sycophancy is increasingly recognized as a harmful alignment, but research remains fragmented and underdeveloped at the conceptual level. This article redefines AI sycophancy as the tendency of large language models (LLMs) and other interactive AI systems to excessively and/or uncritically validate, amplify, or align with a user's assertions-whether these concern factual information, cognitive evaluations, or affective states. Within this framework, we distinguish three types of sycophancy: informational, cognitive, and affective. We also introduce personalization at the message level and critical prompting at the conversation level as key dimensions for distinguishing and examining different manifestations of AI sycophancy. Finally, we propose the AI Sycophancy Processing Model (AISPM) to examine the antecedents, outcomes, and psychological mechanisms through which sycophantic AI responses shape user experiences. By embedding AI sycophancy in the broader landscape of communication theory and research, this article seeks to unify perspectives, clarify conceptual boundaries, and provide a foundation for systematic, theory-driven investigations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "83",
        "title": "MORPH: Shape-agnostic PDE Foundation Models",
        "author": [
            "Mahindra Singh Rautela",
            "Alexander Most",
            "Siddharth Mansingh",
            "Bradley C. Love",
            "Ayan Biswas",
            "Diane Oyen",
            "Earl Lawrence"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21670",
        "abstract": "We introduce MORPH, a shape-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data dimensionality (1D--3D) at different resolutions, multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorizes full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters (LoRA), MORPH outperforms models trained from scratch in both zero-shot and full-shot generalization. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning.",
        "tags": [
            "3D",
            "LoRA",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "84",
        "title": "QueryGym: Step-by-Step Interaction with Relational Databases",
        "author": [
            "Haritha Ananthakrishanan",
            "Harsha Kokel",
            "Kelsey Sikes",
            "Debarun Bhattacharjya",
            "Michael Katz",
            "Shirin Sohrabi",
            "Kavitha Srinivas"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21674",
        "abstract": "We introduce QueryGym, an interactive environment for building, testing, and evaluating LLM-based query planning agents. Existing frameworks often tie agents to specific query language dialects or obscure their reasoning; QueryGym instead requires agents to construct explicit sequences of relational algebra operations, ensuring engine-agnostic evaluation and transparent step-by-step planning. The environment is implemented as a Gymnasium interface that supplies observations -- including schema details, intermediate results, and execution feedback -- and receives actions that represent database exploration (e.g., previewing tables, sampling column values, retrieving unique values) as well as relational algebra operations (e.g., filter, project, join). We detail the motivation and the design of the environment. In the demo, we showcase the utility of the environment by contrasting it with contemporary LLMs that query databases. QueryGym serves as a practical testbed for research in error remediation, transparency, and reinforcement learning for query generation. For the associated demo, see https://ibm.biz/QueryGym.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "85",
        "title": "Prophecy: Inferring Formal Properties from Neuron Activations",
        "author": [
            "Divya Gopinath",
            "Corina S. Pasareanu",
            "Muhammad Usman"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21677",
        "abstract": "We present Prophecy, a tool for automatically inferring formal properties of feed-forward neural networks. Prophecy is based on the observation that a significant part of the logic of feed-forward networks is captured in the activation status of the neurons at inner layers. Prophecy works by extracting rules based on neuron activations (values or on/off statuses) as preconditions that imply certain desirable output property, e.g., the prediction being a certain class. These rules represent network properties captured in the hidden layers that imply the desired output behavior. We present the architecture of the tool, highlight its features and demonstrate its usage on different types of models and output properties. We present an overview of its applications, such as inferring and proving formal explanations of neural networks, compositional verification, run-time monitoring, repair, and others. We also show novel results highlighting its potential in the era of large vision-language models.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "86",
        "title": "ReviewScore: Misinformed Peer Review Detection with Large Language Models",
        "author": [
            "Hyun Ryu",
            "Doohyuk Jang",
            "Hyemin S. Lee",
            "Joonhyun Jeong",
            "Gyeongman Kim",
            "Donghyeon Cho",
            "Gyouk Chu",
            "Minyeong Hwang",
            "Hyeongwon Jang",
            "Changhun Kim",
            "Haechan Kim",
            "Jina Kim",
            "Joowon Kim",
            "Yoonjeon Kim",
            "Kwanhyung Lee",
            "Chanjae Park",
            "Heecheol Yun",
            "Gregor Betz",
            "Eunho Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21679",
        "abstract": "Peer review serves as a backbone of academic research, but in most AI conferences, the review quality is degrading as the number of submissions explodes. To reliably detect low-quality reviews, we define misinformed review points as either \"weaknesses\" in a review that contain incorrect premises, or \"questions\" in a review that can be already answered by the paper. We verify that 15.2% of weaknesses and 26.4% of questions are misinformed and introduce ReviewScore indicating if a review point is misinformed. To evaluate the factuality of each premise of weaknesses, we propose an automated engine that reconstructs every explicit and implicit premise from a weakness. We build a human expert-annotated ReviewScore dataset to check the ability of LLMs to automate ReviewScore evaluation. Then, we measure human-model agreements on ReviewScore using eight current state-of-the-art LLMs and verify moderate agreements. We also prove that evaluating premise-level factuality shows significantly higher agreements than evaluating weakness-level factuality. A thorough disagreement analysis further supports a potential of fully automated ReviewScore evaluation.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "87",
        "title": "FlexMind: Supporting Deeper Creative Thinking with LLMs",
        "author": [
            "Yaqing Yang",
            "Vikram Mohanty",
            "Yan-Ying Chen",
            "Matthew K. Hong",
            "Nikolas Martelaro",
            "Aniket Kittur"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21685",
        "abstract": "Effective ideation requires both broad exploration of diverse ideas and deep evaluation of their potential. Generative AI can support such processes, but current tools typically emphasize either generating many ideas or supporting in-depth consideration of a few, lacking support for both. Research also highlights risks of over-reliance on LLMs, including shallow exploration and negative creative outcomes. We present FlexMind, an AI-augmented system that scaffolds iterative exploration of ideas, tradeoffs, and mitigations. FlexMind exposes users to a broad set of ideas while enabling a lightweight transition into deeper engagement. In a study comparing ideation with FlexMind to ChatGPT, participants generated higher-quality ideas with FlexMind, due to both broader exposure and deeper engagement with tradeoffs. By scaffolding ideation across breadth, depth, and reflective evaluation, FlexMind empowers users to surface ideas that might otherwise go unnoticed or be prematurely discarded.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "88",
        "title": "Towards Versatile Humanoid Table Tennis: Unified Reinforcement Learning with Prediction Augmentation",
        "author": [
            "Muqun Hu",
            "Wenxi Chen",
            "Wenjing Li",
            "Falak Mandali",
            "Zijian He",
            "Renhong Zhang",
            "Praveen Krisna",
            "Katherine Christian",
            "Leo Benaharon",
            "Dizhi Ma",
            "Karthik Ramani",
            "Yan Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21690",
        "abstract": "Humanoid table tennis (TT) demands rapid perception, proactive whole-body motion, and agile footwork under strict timing -- capabilities that remain difficult for unified controllers. We propose a reinforcement learning framework that maps ball-position observations directly to whole-body joint commands for both arm striking and leg locomotion, strengthened by predictive signals and dense, physics-guided rewards. A lightweight learned predictor, fed with recent ball positions, estimates future ball states and augments the policy's observations for proactive decision-making. During training, a physics-based predictor supplies precise future states to construct dense, informative rewards that lead to effective exploration. The resulting policy attains strong performance across varied serve ranges (hit rate $\\geq$ 96% and success rate $\\geq$ 92%) in simulations. Ablation studies confirm that both the learned predictor and the predictive reward design are critical for end-to-end learning. Deployed zero-shot on a physical Booster T1 humanoid with 23 revolute joints, the policy produces coordinated lateral and forward-backward footwork with accurate, fast returns, suggesting a practical path toward versatile, competitive humanoid TT.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "89",
        "title": "Wav2Arrest 2.0: Long-Horizon Cardiac Arrest Prediction with Time-to-Event Modeling, Identity-Invariance, and Pseudo-Lab Alignment",
        "author": [
            "Saurabh Kataria",
            "Davood Fattahi",
            "Minxiao Wang",
            "Ran Xiao",
            "Matthew Clark",
            "Timothy Ruchti",
            "Mark Mai",
            "Xiao Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21695",
        "abstract": "High-frequency physiological waveform modality offers deep, real-time insights into patient status. Recently, physiological foundation models based on Photoplethysmography (PPG), such as PPG-GPT, have been shown to predict critical events, including Cardiac Arrest (CA). However, their powerful representation still needs to be leveraged suitably, especially when the downstream data/label is scarce. We offer three orthogonal improvements to improve PPG-only CA systems by using minimal auxiliary information. First, we propose to use time-to-event modeling, either through simple regression to the event onset time or by pursuing fine-grained discrete survival modeling. Second, we encourage the model to learn CA-focused features by making them patient-identity invariant. This is achieved by first training the largest-scale de-identified biometric identification model, referred to as the p-vector, and subsequently using it adversarially to deconfound cues, such as person identity, that may cause overfitting through memorization. Third, we propose regression on the pseudo-lab values generated by pre-trained auxiliary estimator networks. This is crucial since true blood lab measurements, such as lactate, sodium, troponin, and potassium, are collected sparingly. Via zero-shot prediction, the auxiliary networks can enrich cardiac arrest waveform labels and generate pseudo-continuous estimates as targets. Our proposals can independently improve the 24-hour time-averaged AUC from the 0.74 to the 0.78-0.80 range. We primarily improve over longer time horizons with minimal degradation near the event, thus pushing the Early Warning System research. Finally, we pursue multi-task formulation and diagnose it with a high gradient conflict rate among competing losses, which we alleviate via the PCGrad optimization technique.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "90",
        "title": "PowerGS: Display-Rendering Power Co-Optimization for Neural Rendering in Power-Constrained XR Systems",
        "author": [
            "Weikai Lin",
            "Sushant Kondguli",
            "Carl Marshall",
            "Yuhao Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21702",
        "abstract": "3D Gaussian Splatting (3DGS) combines classic image-based rendering, pointbased graphics, and modern differentiable techniques, and offers an interesting alternative to traditional physically-based rendering. 3DGS-family models are far from efficient for power-constrained Extended Reality (XR) devices, which need to operate at a Watt-level. This paper introduces PowerGS, the first framework to jointly minimize the rendering and display power in 3DGS under a quality constraint. We present a general problem formulation and show that solving the problem amounts to 1) identifying the iso-quality curve(s) in the landscape subtended by the display and rendering power and 2) identifying the power-minimal point on a given curve, which has a closed-form solution given a proper parameterization of the curves. PowerGS also readily supports foveated rendering for further power savings. Extensive experiments and user studies show that PowerGS achieves up to 86% total power reduction compared to state-of-the-art 3DGS models, with minimal loss in both subjective and objective quality. Code is available at https://github.com/horizon-research/PowerGS.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "91",
        "title": "Think-on-Graph 3.0: Efficient and Adaptive LLM Reasoning on Heterogeneous Graphs via Multi-Agent Dual-Evolving Context Retrieval",
        "author": [
            "Xiaojun Wu",
            "Cehao Yang",
            "Xueyuan Lin",
            "Chengjin Xu",
            "Xuhui Jiang",
            "Yuanliang Sun",
            "Hui Xiong",
            "Jia Li",
            "Jian Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21710",
        "abstract": "Retrieval-Augmented Generation (RAG) and Graph-based RAG has become the important paradigm for enhancing Large Language Models (LLMs) with external knowledge. However, existing approaches face a fundamental trade-off. While graph-based methods are inherently dependent on high-quality graph structures, they face significant practical constraints: manually constructed knowledge graphs are prohibitively expensive to scale, while automatically extracted graphs from corpora are limited by the performance of the underlying LLM extractors, especially when using smaller, local-deployed models. This paper presents Think-on-Graph 3.0 (ToG-3), a novel framework that introduces Multi-Agent Context Evolution and Retrieval (MACER) mechanism to overcome these limitations. Our core innovation is the dynamic construction and refinement of a Chunk-Triplets-Community heterogeneous graph index, which pioneeringly incorporates a dual-evolution mechanism of Evolving Query and Evolving Sub-Graph for precise evidence retrieval. This approach addresses a critical limitation of prior Graph-based RAG methods, which typically construct a static graph index in a single pass without adapting to the actual query. A multi-agent system, comprising Constructor, Retriever, Reflector, and Responser agents, collaboratively engages in an iterative process of evidence retrieval, answer generation, sufficiency reflection, and, crucially, evolving query and subgraph. This dual-evolving multi-agent system allows ToG-3 to adaptively build a targeted graph index during reasoning, mitigating the inherent drawbacks of static, one-time graph construction and enabling deep, precise reasoning even with lightweight LLMs. Extensive experiments demonstrate that ToG-3 outperforms compared baselines on both deep and broad reasoning benchmarks, and ablation studies confirm the efficacy of the components of MACER framework.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "92",
        "title": "MusicWeaver: Coherent Long-Range and Editable Music Generation from a Beat-Aligned Structural Plan",
        "author": [
            "Xuanchen Wang",
            "Heng Wang",
            "Weidong Cai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21714",
        "abstract": "Current music generators capture local textures but often fail to model long-range structure, leading to off-beat outputs, weak section transitions, and limited editing capability. We present MusicWeaver, a music generation model conditioned on a beat-aligned structural plan. This plan serves as an editable intermediate between the input prompt and the generated music, preserving global form and enabling professional, localized edits. MusicWeaver consists of a planner, which translates prompts into a structural plan encoding musical form and compositional cues, and a diffusion-based generator, which synthesizes music under the plan's guidance. To assess generation and editing quality, we introduce two metrics: the Structure Coherence Score (SCS) for evaluating long-range form and timing, and the Edit Fidelity Score (EFS) for measuring the accuracy of realizing plan edits. Experiments demonstrate that MusicWeaver achieves state-of-the-art fidelity and controllability, producing music closer to human-composed works. Music results can be found on our project page: https://musicweaver.github.io/.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "93",
        "title": "Motion-Aware Transformer for Multi-Object Tracking",
        "author": [
            "Xu Yang",
            "Gady Agam"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21715",
        "abstract": "Multi-object tracking (MOT) in videos remains challenging due to complex object motions and crowded scenes. Recent DETR-based frameworks offer end-to-end solutions but typically process detection and tracking queries jointly within a single Transformer Decoder layer, leading to conflicts and degraded association accuracy. We introduce the Motion-Aware Transformer (MATR), which explicitly predicts object movements across frames to update track queries in advance. By reducing query collisions, MATR enables more consistent training and improves both detection and association. Extensive experiments on DanceTrack, SportsMOT, and BDD100k show that MATR delivers significant gains across standard metrics. On DanceTrack, MATR improves HOTA by more than 9 points over MOTR without additional data and reaches a new state-of-the-art score of 71.3 with supplementary data. MATR also achieves state-of-the-art results on SportsMOT (72.2 HOTA) and BDD100k (54.7 mTETA, 41.6 mHOTA) without relying on external datasets. These results demonstrate that explicitly modeling motion within end-to-end Transformers offers a simple yet highly effective approach to advancing multi-object tracking.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "94",
        "title": "Align2Speak: Improving TTS for Low Resource Languages via ASR-Guided Online Preference Optimization",
        "author": [
            "Shehzeen Hussain",
            "Paarth Neekhara",
            "Xuesong Yang",
            "Edresson Casanova",
            "Subhankar Ghosh",
            "Roy Fejgin",
            "Ryan Langman",
            "Mikyas Desta",
            "Leili Tavabi",
            "Jason Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21718",
        "abstract": "Developing high-quality text-to-speech (TTS) systems for low-resource languages is challenging due to the scarcity of paired text and speech data. In contrast, automatic speech recognition (ASR) models for such languages are often more accessible, owing to large-scale multilingual pre-training efforts. We propose a framework based on Group Relative Policy Optimization (GRPO) to adapt an autoregressive, multilingual TTS model to new languages. Our method first establishes a language-agnostic foundation for TTS synthesis by training a multilingual baseline with International Phonetic Alphabet (IPA) tokens. Next, we fine-tune this model on limited paired data of the new languages to capture the target language's prosodic features. Finally, we apply GRPO to optimize the model using only unpaired text and speaker prompts, guided by a multi-objective reward from pretrained ASR, speaker verification, and audio quality estimation models. Experiments demonstrate that this pipeline produces intelligible and speaker-consistent speech in low-resource languages, substantially outperforming fine-tuning alone. Furthermore, our GRPO-based framework also improves TTS performance in high-resource languages, surpassing offline alignment methods such as Direct Preference Optimization (DPO) yielding superior intelligibility, speaker similarity, and audio quality.",
        "tags": [
            "DPO",
            "GRPO"
        ]
    },
    {
        "id": "95",
        "title": "Design Exploration of AI-assisted Personal Affective Physicalization",
        "author": [
            "Ruishan Wu",
            "Zhuoyang Li",
            "Charles Perin",
            "Sheelagh Carpendale",
            "Can Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21721",
        "abstract": "Personal Affective Physicalization is the process by which individuals express emotions through tangible forms to record, reflect on, and communicate. Yet such physical data representations can be challenging to design due to the abstract nature of emotions. Given the shown potential of AI in detecting emotion and assisting design, we explore opportunities in AI-assisted design of personal affective physicalization using a Research-through-Design method. We developed PhEmotion, a tool for embedding LLM-extracted emotion values from human-AI conversations into parametric design of physical artifacts. A lab study was conducted with 14 participants creating these artifacts based on their personal emotions, with and without AI support. We observed nuances and variations in participants' creative strategies, meaning-making processes and their perceptions of AI support in this context. We found key tensions in AI-human co-creation that provide a nuanced agenda for future research in AI-assisted personal affective physicalization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "96",
        "title": "On Suboptimal Safety-Critical Tracking Controller Design",
        "author": [
            "Yazdan Batmani",
            "Saber Omidi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21726",
        "abstract": "This paper proposes a novel framework for safety-critical optimal trajectory tracking in nonlinear systems based on the state-dependent Riccati equation (SDRE) methodology. By embedding barrier states into the system dynamics, the proposed strategy simultaneously ensures safety and tracking requirements, even in scenarios where these objectives may be inherently conflicting. A discounted pseudo-quadratic cost function is formulated to achieve a suboptimal trade-off between tracking accuracy, control effort, and safety objective. We present two distinct controller designs: one utilizing a single barrier state to enforce overall safety constraints, and another employing multiple barrier states to individually tuning the system's conservatism with respect to each safety constraint, providing enhanced flexibility in tuning the system's conservatism toward individual constraints. We establish sufficient conditions to ensure the solvability of the associated Riccati equations. The proposed safe controller is well-suited for real-time implementation in practical systems, given its reasonable computational requirements and compatibility with widely available embedded microprocessors. This is supported by simulation studies involving a mechanical system and a mobile robot collision avoidance scenario, where the safe SDRE controller consistently maintained safety while achieving trajectory tracking objectives in challenging conditions. Additionally, experimental results on a cable-driven parallel robot further demonstrate the practical applicability and effectiveness of the proposed method in real-world control tasks.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "97",
        "title": "ProPerSim: Developing Proactive and Personalized AI Assistants through User-Assistant Simulation",
        "author": [
            "Jiho Kim",
            "Junseong Choi",
            "Woosog Chay",
            "Daeun Kyung",
            "Yeonsu Kwon",
            "Yohan Jo",
            "Edward Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21730",
        "abstract": "As large language models (LLMs) become increasingly integrated into daily life, there is growing demand for AI assistants that are not only reactive but also proactive and personalized. While recent advances have pushed forward proactivity and personalization individually, their combination remains underexplored. To bridge this gap, we introduce ProPerSim, a new task and simulation framework for developing assistants capable of making timely, personalized recommendations in realistic home scenarios. In our simulation environment, a user agent with a rich persona interacts with the assistant, providing ratings on how well each suggestion aligns with its preferences and context. The assistant's goal is to use these ratings to learn and adapt to achieve higher scores over time. Built on ProPerSim, we propose ProPerAssistant, a retrieval-augmented, preference-aligned assistant that continually learns and adapts through user feedback. Experiments across 32 diverse personas show that ProPerAssistant adapts its strategy and steadily improves user satisfaction, highlighting the promise of uniting proactivity and personalization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "98",
        "title": "How Accurate Are LLMs at Multi-Question Answering on Conversational Transcripts?",
        "author": [
            "Xiliang Zhu",
            "Shi Zong",
            "David Rossouw"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21732",
        "abstract": "Deploying Large Language Models (LLMs) for question answering (QA) over lengthy contexts is a significant challenge. In industrial settings, this process is often hindered by high computational costs and latency, especially when multiple questions must be answered based on the same context. In this work, we explore the capabilities of LLMs to answer multiple questions based on the same conversational context. We conduct extensive experiments and benchmark a range of both proprietary and public models on this challenging task. Our findings highlight that while strong proprietary LLMs like GPT-4o achieve the best overall performance, fine-tuned public LLMs with up to 8 billion parameters can surpass GPT-4o in accuracy, which demonstrates their potential for transparent and cost-effective deployment in real-world applications.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "99",
        "title": "LFA-Net: A Lightweight Network with LiteFusion Attention for Retinal Vessel Segmentation",
        "author": [
            "Mehwish Mehmood",
            "Ivor Spence",
            "Muhammad Fahim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21738",
        "abstract": "Lightweight retinal vessel segmentation is important for the early diagnosis of vision-threatening and systemic diseases, especially in a real-world clinical environment with limited computational resources. Although segmentation methods based on deep learning are improving, existing models are still facing challenges of small vessel segmentation and high computational costs. To address these challenges, we proposed a new vascular segmentation network, LFA-Net, which incorporates a newly designed attention module, LiteFusion-Attention. This attention module incorporates residual learning connections, Vision Mamba-inspired dynamics, and modulation-based attention, enabling the model to capture local and global context efficiently and in a lightweight manner. LFA-Net offers high performance with 0.11 million parameters, 0.42 MB memory size, and 4.46 GFLOPs, which make it ideal for resource-constrained environments. We validated our proposed model on DRIVE, STARE, and CHASE_DB with outstanding performance in terms of dice scores of 83.28, 87.44, and 84.50% and Jaccard indices of 72.85, 79.31, and 74.70%, respectively. The code of LFA-Net is available online https://github.com/Mehwish4593/LFA-Net.",
        "tags": [
            "Mamba",
            "Segmentation"
        ]
    },
    {
        "id": "100",
        "title": "Noise-to-Notes: Diffusion-based Generation and Refinement for Automatic Drum Transcription",
        "author": [
            "Michael Yeung",
            "Keisuke Toyama",
            "Toya Teramoto",
            "Shusuke Takahashi",
            "Tamaki Kojima"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21739",
        "abstract": "Automatic drum transcription (ADT) is traditionally formulated as a discriminative task to predict drum events from audio spectrograms. In this work, we redefine ADT as a conditional generative task and introduce Noise-to-Notes (N2N), a framework leveraging diffusion modeling to transform audio-conditioned Gaussian noise into drum events with associated velocities. This generative diffusion approach offers distinct advantages, including a flexible speed-accuracy trade-off and strong inpainting capabilities. However, the generation of binary onset and continuous velocity values presents a challenge for diffusion models, and to overcome this, we introduce an Annealed Pseudo-Huber loss to facilitate effective joint optimization. Finally, to augment low-level spectrogram features, we propose incorporating features extracted from music foundation models (MFMs), which capture high-level semantic information and enhance robustness to out-of-domain drum audio. Experimental results demonstrate that including MFM features significantly improves robustness and N2N establishes a new state-of-the-art performance across multiple ADT benchmarks.",
        "tags": [
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "101",
        "title": "Self-Speculative Biased Decoding for Faster Live Translation",
        "author": [
            "Linxiao Zeng",
            "Haoyun Deng",
            "Kangyuan Shu",
            "Shizhen Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21740",
        "abstract": "Large Language Models (LLMs) have recently demonstrated impressive capabilities in various text generation tasks. However, it remains challenging to use them off-the-shelf in streaming applications (such as live translation), where the output must continually update as the input context expands, while still maintaining a reasonable computational cost to meet the latency requirement.\nIn this work, we reexamine the re-translation approach to simultaneous translation and propose Self-Speculative Biased Decoding, a novel inference paradigm designed to avoid repeatedly generating output from scratch for a consistently growing input stream. We propose using the most recent output as a draft for the current growing input context. During the verification stage, the output will be biased towards the draft token for a higher draft acceptance rate. This strategy not only minimizes flickering that might distract users but also leads to higher speedups. Conventional decoding may take charge from the point of divergence after draft verification and continue until the end condition is met.\nUnlike existing speculative decoding strategies, our approach eliminates the need for draft computations, making it a model-agnostic and plug-and-play solution for accelerating latency-sensitive streaming applications. Experimental results on simultaneous text-to-text re-translation demonstrate that our approach achieves up to 1.7x speedup compared to conventional auto-regressive re-translation without compromising quality. Additionally, it significantly reduces flickering by 80% by incorporating the display-only mask-k technique.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "102",
        "title": "Reinforcement Learning Based Traffic Signal Design to Minimize Queue Lengths",
        "author": [
            "Anirud Nandakumar",
            "Chayan Banerjee",
            "Lelitha Devi Vanajakshi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21745",
        "abstract": "Efficient traffic signal control (TSC) is crucial for reducing congestion, travel delays, pollution, and for ensuring road safety. Traditional approaches, such as fixed signal control and actuated control, often struggle to handle dynamic traffic patterns. In this study, we propose a novel adaptive TSC framework that leverages Reinforcement Learning (RL), using the Proximal Policy Optimization (PPO) algorithm, to minimize total queue lengths across all signal phases. The challenge of efficiently representing highly stochastic traffic conditions for an RL controller is addressed through multiple state representations, including an expanded state space, an autoencoder representation, and a K-Planes-inspired representation. The proposed algorithm has been implemented using the Simulation of Urban Mobility (SUMO) traffic simulator and demonstrates superior performance over both traditional methods and other conventional RL-based approaches in reducing queue lengths. The best performing configuration achieves an approximately 29% reduction in average queue lengths compared to the traditional Webster method. Furthermore, comparative evaluation of alternative reward formulations demonstrates the effectiveness of the proposed queue-based approach, showcasing the potential for scalable and adaptive urban traffic management.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "103",
        "title": "Thinking with Sound: Audio Chain-of-Thought Enables Multimodal Reasoning in Large Audio-Language Models",
        "author": [
            "Zhen Xiong",
            "Yujun Cai",
            "Zhecheng Li",
            "Junsong Yuan",
            "Yiwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21749",
        "abstract": "Recent Large Audio-Language Models (LALMs) have shown strong performance on various audio understanding tasks such as speech translation and Audio Q\\&A. However, they exhibit significant limitations on challenging audio reasoning tasks in complex acoustic scenarios. These situations would greatly benefit from the use of acoustic tools like noise suppression, source separation, and precise temporal alignment, but current LALMs lack access to such tools. To address this limitation, we introduce Thinking-with-Sound (TwS), a framework that equips LALMs with Audio CoT by combining linguistic reasoning with on-the-fly audio-domain analysis. Unlike existing approaches that treat audio as static input, TwS enables models to actively think with audio signals, performing numerical analysis and digital manipulation through multimodal reasoning. To evaluate this approach, we construct MELD-Hard1k, a new robustness benchmark created by introducing various acoustic perturbations. Experiments reveal that state-of-the-art LALMs suffer dramatic performance degradation on MELD-Hard1k, with accuracy dropping by more than $50\\%$ compared to clean audio. TwS achieves substantial improvements in robustness, demonstrating both effectiveness and scalability: small models gain $24.73\\%$ absolute accuracy, with improvements scaling consistently up to $36.61\\%$ for larger models. Our findings demonstrate that Audio CoT can significantly enhance robustness without retraining, opening new directions for developing more robust audio understanding systems.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "104",
        "title": "UniVid: Unifying Vision Tasks with Pre-trained Video Generation Models",
        "author": [
            "Lan Chen",
            "Yuchao Gu",
            "Qi Mao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21760",
        "abstract": "Large language models, trained on extensive corpora, successfully unify diverse linguistic tasks within a single generative framework. Inspired by this, recent works like Large Vision Model (LVM) extend this paradigm to vision by organizing tasks into sequential visual sentences, where visual prompts serve as the context to guide outputs. However, such modeling requires task-specific pre-training across modalities and sources, which is costly and limits scalability to unseen tasks. Given that pre-trained video generation models inherently capture temporal sequence dependencies, we explore a more unified and scalable alternative: can a pre-trained video generation model adapt to diverse image and video tasks? To answer this, we propose UniVid, a framework that fine-tunes a video diffusion transformer to handle various vision tasks without task-specific modifications. Tasks are represented as visual sentences, where the context sequence defines both the task and the expected output modality. We evaluate the generalization of UniVid from two perspectives: (1) cross-modal inference with contexts composed of both images and videos, extending beyond LVM's uni-modal setting; (2) cross-source tasks from natural to annotated data, without multi-source pre-training. Despite being trained solely on natural video data, UniVid generalizes well in both settings. Notably, understanding and generation tasks can easily switch by simply reversing the visual sentence order in this paradigm. These findings highlight the potential of pre-trained video generation models to serve as a scalable and unified foundation for vision modeling. Our code will be released at https://github.com/CUC-MIPG/UniVid.",
        "tags": [
            "DiT",
            "Diffusion",
            "LLM",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "105",
        "title": "CubistMerge: Spatial-Preserving Token Merging For Diverse ViT Backbones",
        "author": [
            "Wenyi Gong",
            "Mieszko Lis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21764",
        "abstract": "Many modern ViT backbones adopt spatial architectural designs, such as window attention, decomposed relative positional embeddings in SAM, and RoPE in DINOv3. Such architectures impose new challenges on token reduction, as the vast majority of existing methods fail to preserve the spatial structure these architectures depend on. In this paper, we introduce a simple yet effective token merging method that maintains spatial integrity, enabling seamless compatibility with spatial architectures. We reconcile two seemingly conflicting requirements: (i)exploiting the uneven information distribution across the spatial layout while (ii)preserving the spatial structure post-merging. Our approach employs (i)a 2D reduction strategy to enforce structured token layouts, (ii)a spatial-aware merging algorithm that maintains relative token positions, and (iii)a novel max-magnitude-per-dimension token representation that preserves salient features. Our method demonstrates strong performance both off-the-shelf and with fine-tuning, achieving state-of-the-art results on spatial and non-spatial architectures across various vision tasks. Specifically, we achieve 1.25x speedup on SAM-H with only 0.7% mIOU drop evaluated on COCO off-the-shelf, and 1.15x speedup on DeiT-B with no top-1 accuracy drop on ImageNet within just one epoch of fine-tuning.",
        "tags": [
            "RoPE",
            "SAM",
            "ViT"
        ]
    },
    {
        "id": "106",
        "title": "UltraHorizon: Benchmarking Agent Capabilities in Ultra Long-Horizon Scenarios",
        "author": [
            "Haotian Luo",
            "Huaisong Zhang",
            "Xuelin Zhang",
            "Haoyu Wang",
            "Zeyu Qin",
            "Wenjie Lu",
            "Guozheng Ma",
            "Haiying He",
            "Yingsha Xie",
            "Qiyang Zhou",
            "Zixuan Hu",
            "Hongze Mi",
            "Yibo Wang",
            "Naiqiang Tan",
            "Hong Chen",
            "Yi R. Fung",
            "Chun Yuan",
            "Li Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21766",
        "abstract": "Autonomous agents have recently achieved remarkable progress across diverse domains, yet most evaluations focus on short-horizon, fully observable tasks. In contrast, many critical real-world tasks, such as large-scale software development, commercial investment, and scientific discovery, unfold in long-horizon and partially observable scenarios where success hinges on sustained reasoning, planning, memory management, and tool use. Existing benchmarks rarely capture these long-horizon challenges, leaving a gap in systematic evaluation. To bridge this gap, we introduce \\textbf{UltraHorizon} a novel benchmark that measures the foundational capabilities essential for complex real-world challenges. We use exploration as a unifying task across three distinct environments to validate these core competencies. Agents are designed in long-horizon discovery tasks where they must iteratively uncover hidden rules through sustained reasoning, planning, memory and tools management, and interaction with environments. Under the heaviest scale setting, trajectories average \\textbf{200k+} tokens and \\textbf{400+} tool calls, whereas in standard configurations they still exceed \\textbf{35k} tokens and involve more than \\textbf{60} tool calls on average. Our extensive experiments reveal that LLM-agents consistently underperform in these settings, whereas human participants achieve higher scores, underscoring a persistent gap in agents' long-horizon abilities. We also observe that simple scaling fails in our task. To better illustrate the failure of agents, we conduct an in-depth analysis of collected trajectories. We identify eight types of errors and attribute them to two primary causes: in-context locking and functional fundamental capability gaps. \\href{https://github.com/StarDewXXX/UltraHorizon}{Our code will be available here.}",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "107",
        "title": "PhishLumos: An Adaptive Multi-Agent System for Proactive Phishing Campaign Mitigation",
        "author": [
            "Daiki Chiba",
            "Hiroki Nakano",
            "Takashi Koide"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21772",
        "abstract": "Phishing attacks are a significant societal threat, disproportionately harming vulnerable populations and eroding trust in essential digital services. Current defenses are often reactive, failing against modern evasive tactics like cloaking that conceal malicious content. To address this, we introduce PhishLumos, an adaptive multi-agent system that proactively mitigates entire attack campaigns. It confronts a core cybersecurity imbalance: attackers can easily scale operations, while defense remains an intensive expert task. Instead of being blocked by evasion, PhishLumos treats it as a critical signal to investigate the underlying infrastructure. Its Large Language Model (LLM)-powered agents uncover shared hosting, certificates, and domain registration patterns. On real-world data, our system identified 100% of campaigns in the median case, over a week before their confirmation by cybersecurity experts. PhishLumos demonstrates a practical shift from reactive URL blocking to proactive campaign mitigation, protecting users before they are harmed and making the digital world safer for all.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "108",
        "title": "Training-Free Multimodal Deepfake Detection via Graph Reasoning",
        "author": [
            "Yuxin Liu",
            "Fei Wang",
            "Kun Li",
            "Yiqi Nie",
            "Junjie Chen",
            "Yanyan Wei",
            "Zhangling Duan",
            "Zhaohong Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21774",
        "abstract": "Multimodal deepfake detection (MDD) aims to uncover manipulations across visual, textual, and auditory modalities, thereby reinforcing the reliability of modern information systems. Although large vision-language models (LVLMs) exhibit strong multimodal reasoning, their effectiveness in MDD is limited by challenges in capturing subtle forgery cues, resolving cross-modal inconsistencies, and performing task-aligned retrieval. To this end, we propose Guided Adaptive Scorer and Propagation In-Context Learning (GASP-ICL), a training-free framework for MDD. GASP-ICL employs a pipeline to preserve semantic relevance while injecting task-aware knowledge into LVLMs. We leverage an MDD-adapted feature extractor to retrieve aligned image-text pairs and build a candidate set. We further design the Graph-Structured Taylor Adaptive Scorer (GSTAS) to capture cross-sample relations and propagate query-aligned signals, producing discriminative exemplars. This enables precise selection of semantically aligned, task-relevant demonstrations, enhancing LVLMs for robust MDD. Experiments on four forgery types show that GASP-ICL surpasses strong baselines, delivering gains without LVLM fine-tuning.",
        "tags": [
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "109",
        "title": "The Turkish Ice Cream Robot: Examining Playful Deception in Social Human-Robot Interactions",
        "author": [
            "Hyeonseong Kim",
            "Roy El-Helou",
            "Seungbeen Lee",
            "Sungjoon Choi",
            "Matthew Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21776",
        "abstract": "Playful deception, a common feature in human social interactions, remains underexplored in Human-Robot Interaction (HRI). Inspired by the Turkish Ice Cream (TIC) vendor routine, we investigate how bounded, culturally familiar forms of deception influence user trust, enjoyment, and engagement during robotic handovers. We design a robotic manipulator equipped with a custom end-effector and implement five TIC-inspired trick policies that deceptively delay the handover of an ice cream-shaped object. Through a mixed-design user study with 91 participants, we evaluate the effects of playful deception and interaction duration on user experience. Results reveal that TIC-inspired deception significantly enhances enjoyment and engagement, though reduces perceived safety and trust, suggesting a structured trade-off across the multi-dimensional aspects. Our findings demonstrate that playful deception can be a valuable design strategy for interactive robots in entertainment and engagement-focused contexts, while underscoring the importance of deliberate consideration of its complex trade-offs. You can find more information, including demonstration videos, on https://hyeonseong-kim98.github.io/turkish-ice-cream-robot/ .",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "110",
        "title": "Benchmarking MLLM-based Web Understanding: Reasoning, Robustness and Safety",
        "author": [
            "Junliang Liu",
            "Jingyu Xiao",
            "Wenxin Tang",
            "Wenxuan Wang",
            "Zhixian Wang",
            "Minrui Zhang",
            "Shuanghe Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21782",
        "abstract": "Multimodal large language models (MLLMs) are increasingly positioned as AI collaborators for building complex web-related applications like GUI agents and front-end code generation. However, existing benchmarks largely emphasize visual perception or UI code generation, showing insufficient evaluation on the reasoning, robustness and safety capability required for end-to-end web applications. To bridge the gap, we introduce a comprehensive web understanding benchmark, named WebRSSBench, that jointly evaluates Reasoning, Robustness, and Safety across eight tasks, such as position relationship reasoning, color robustness, and safety critical detection, etc. The benchmark is constructed from 729 websites and contains 3799 question answer pairs that probe multi-step inference over page structure, text, widgets, and safety-critical interactions. To ensure reliable measurement, we adopt standardized prompts, deterministic evaluation scripts, and multi-stage quality control combining automatic checks with targeted human verification. We evaluate 12 MLLMs on WebRSSBench. The results reveal significant gaps, models still struggle with compositional and cross-element reasoning over realistic layouts, show limited robustness when facing perturbations in user interfaces and content such as layout rearrangements or visual style shifts, and are rather conservative in recognizing and avoiding safety critical or irreversible actions. Our code is available at https://github.com/jinliang-byte/webssrbench.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "111",
        "title": "DeHate: A Stable Diffusion-based Multimodal Approach to Mitigate Hate Speech in Images",
        "author": [
            "Dwip Dalal",
            "Gautam Vashishtha",
            "Anku Ranui",
            "Aishwarya Reganti",
            "Parth Patwa",
            "Mohd Sarique",
            "Chandan Gupta",
            "Keshav Nath",
            "Viswanatha Reddy",
            "Vinija Jain",
            "Aman Chadha",
            "Amitava Das",
            "Amit Sheth",
            "Asif Ekbal"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21787",
        "abstract": "The rise in harmful online content not only distorts public discourse but also poses significant challenges to maintaining a healthy digital environment. In response to this, we introduce a multimodal dataset uniquely crafted for identifying hate in digital content. Central to our methodology is the innovative application of watermarked, stability-enhanced, stable diffusion techniques combined with the Digital Attention Analysis Module (DAAM). This combination is instrumental in pinpointing the hateful elements within images, thereby generating detailed hate attention maps, which are used to blur these regions from the image, thereby removing the hateful sections of the image. We release this data set as a part of the dehate shared task. This paper also describes the details of the shared task. Furthermore, we present DeHater, a vision-language model designed for multimodal dehatification tasks. Our approach sets a new standard in AI-driven image hate detection given textual prompts, contributing to the development of more ethical AI applications in social media.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "112",
        "title": "MIRG-RL: Multi-Image Reasoning and Grounding with Reinforcement Learning",
        "author": [
            "Lihao Zheng",
            "Jiawei Chen",
            "Xintian Shen",
            "Hao Ma",
            "Tao Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21788",
        "abstract": "Multi-image reasoning and grounding require understanding complex cross-image relationships at both object levels and image levels. Current Large Visual Language Models (LVLMs) face two critical challenges: the lack of cross-image reasoning capabilities and insufficient cross-image reference reward modeling. To address these issues, we propose a unified framework - Multi-Image Reasoning and Grounding with Reinforcement Learning (MIRG-RL). Specifically, our two-stage training paradigm combines supervised fine-tuning with annotated trajectories and image-aware reinforcement learning optimization, progressively developing multi-image reasoning capabilities. Furthermore, we innovatively propose a method for constructing the trajectory data, which integrates object-level and image-level annotation information, and use this method to generate a lightweight reasoning-enhanced dataset. To effectively resolve cross-image ambiguities, we design an image-aware RL policy with dual reward functions for objects and images. Experiments demonstrate that MIRG-RL achieves state-of-the-art (SOTA) performance in multi-image grounding benchmarks, attaining 64.82% on cross-image reasoning tasks - exceeding the previous best method by 1%. The code and dataset have been released at https://github.com/ZEUS2035/MIRG-RL.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "113",
        "title": "Visual Multi-Agent System: Mitigating Hallucination Snowballing via Visual Flow",
        "author": [
            "Xinlei Yu",
            "Chengming Xu",
            "Guibin Zhang",
            "Yongbo He",
            "Zhangquan Chen",
            "Zhucun Xue",
            "Jiangning Zhang",
            "Yue Liao",
            "Xiaobin Hu",
            "Yu-Gang Jiang",
            "Shuicheng Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21789",
        "abstract": "Multi-Agent System (MAS) powered by Visual Language Models (VLMs) enables challenging tasks but suffers from a novel failure term, multi-agent visual hallucination snowballing, where hallucinations are seeded in a single agent and amplified by following ones due to the over-reliance on textual flow to relay visual information. Through turn-, layer-, and token-wise attention analyses, we provide detailed insights into the essence of hallucination snowballing regarding the reduction of visual attention allocation. It leads us to identify a subset of vision tokens with a unimodal attention peak in middle layers that best preserve visual evidence but gradually diminish in deeper agent turns, resulting in the visual hallucination snowballing in MAS. Thus, we propose ViF, a lightweight, plug-and-play mitigation paradigm that relays inter-agent messages with Visual Flow powered by the selected visual relay tokens and applies attention reallocation to amplify this pattern. The experiment results demonstrate that our method markedly reduces hallucination snowballing, consistently improving the performance across eight benchmarks based on four common MAS structures and ten base models. The source code will be available at: https://github.com/YU-deep/ViF.git.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "114",
        "title": "LongScape: Advancing Long-Horizon Embodied World Models with Context-Aware MoE",
        "author": [
            "Yu Shang",
            "Lei Jin",
            "Yiding Ma",
            "Xin Zhang",
            "Chen Gao",
            "Wei Wu",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21790",
        "abstract": "Video-based world models hold significant potential for generating high-quality embodied manipulation data. However, current video generation methods struggle to achieve stable long-horizon generation: classical diffusion-based approaches often suffer from temporal inconsistency and visual drift over multiple rollouts, while autoregressive methods tend to compromise on visual detail. To solve this, we introduce LongScape, a hybrid framework that adaptively combines intra-chunk diffusion denoising with inter-chunk autoregressive causal generation. Our core innovation is an action-guided, variable-length chunking mechanism that partitions video based on the semantic context of robotic actions. This ensures each chunk represents a complete, coherent action, enabling the model to flexibly generate diverse dynamics. We further introduce a Context-aware Mixture-of-Experts (CMoE) framework that adaptively activates specialized experts for each chunk during generation, guaranteeing high visual quality and seamless chunk transitions. Extensive experimental results demonstrate that our method achieves stable and consistent long-horizon generation over extended rollouts. Our code is available at: https://github.com/tsinghua-fib-lab/Longscape.",
        "tags": [
            "Diffusion",
            "MoE",
            "Video Generation"
        ]
    },
    {
        "id": "115",
        "title": "Navigating the Impact of Structured Output Format on Large Language Models through the Compass of Causal Inference",
        "author": [
            "Han Yuan",
            "Yue Zhao",
            "Li Zhang",
            "Wuqiong Luo",
            "Zheng Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21791",
        "abstract": "Structured output from large language models (LLMs) has enhanced efficiency in processing generated information and is increasingly adopted in industrial applications. Prior studies have investigated the impact of structured output on LLMs' generation quality, often presenting one-way findings. Some suggest that structured format enhances completeness and factual accuracy, while others argue that it restricts the reasoning capacity of LLMs and leads to reductions in standard evaluation metrics. Potential limitations of these assessments include restricted testing scenarios, weakly controlled comparative settings, and reliance on coarse metrics. In this work, we present a refined analysis using causal inference. Based on one assumed and two guaranteed constraints, we derive five potential causal structures characterizing the influence of structured output on LLMs' generation: (1) collider without m-bias, (2) collider with m-bias, (3) single cause from instruction, (4) single cause from output format, and (5) independence. Across seven public and one developed reasoning tasks, we find that coarse metrics report positive, negative, or neutral effects of structured output on GPT-4o's generation. However, causal inference reveals no causal impact in 43 out of 48 scenarios. In the remaining 5, 3 involve multifaceted causal structures influenced by concrete instructions.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "116",
        "title": "FastGRPO: Accelerating Policy Optimization via Concurrency-aware Speculative Decoding and Online Draft Learning",
        "author": [
            "Yizhou Zhang",
            "Ning Lv",
            "Teng Wang",
            "Jisheng Dang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21792",
        "abstract": "Group relative policy optimization (GRPO) has demonstrated significant potential in improving the reasoning capabilities of large language models (LLMs) via reinforcement learning. However, its practical deployment is impeded by an excessively slow training process, primarily attributed to the computationally intensive autoregressive generation of multiple responses per query, which makes the generation phase the primary performance bottleneck. Although speculative decoding presents a promising direction for acceleration, its direct application in GRPO achieves limited speedup under high-concurrency training conditions. To overcome this limitation, we propose a concurrency-aware speculative decoding framework that dynamically adjusts the drafting and verification strategy according to real-time concurrency levels, thereby maximizing the acceleration of the generation process. Furthermore, to address performance degradation arising from distributional drift between the evolving target model and the fixed draft model during training, we introduce an online draft learning mechanism that enables the draft model to continuously adapt using feedback signals from the target model. Experimental results across multiple mathematical reasoning datasets and models demonstrate that the proposed method achieves end-to-end speedups of 2.35x to 2.72x, significantly surpassing baseline approaches in efficiency. The code is available at https://github.com/yedaotian9/GRPO_speculative.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "117",
        "title": "MoWM: Mixture-of-World-Models for Embodied Planning via Latent-to-Pixel Feature Modulation",
        "author": [
            "Yu Shang",
            "Yangcheng Yu",
            "Xin Zhang",
            "Xin Jin",
            "Haisheng Su",
            "Wei Wu",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21797",
        "abstract": "Embodied action planning is a core challenge in robotics, requiring models to generate precise actions from visual observations and language instructions. While video generation world models are promising, their reliance on pixel-level reconstruction often introduces visual redundancies that hinder action decoding and generalization. Latent world models offer a compact, motion-aware representation, but overlook the fine-grained details critical for precise manipulation. To overcome these limitations, we propose MoWM, a mixture-of-world-model framework that fuses representations from hybrid world models for embodied action planning. Our approach uses motion-aware representations from a latent model as a high-level prior, which guides the extraction of fine-grained visual features from the pixel space model. This design allows MoWM to highlight the informative visual details needed for action decoding. Extensive evaluations on the CALVIN benchmark demonstrate that our method achieves state-of-the-art task success rates and superior generalization. We also provide a comprehensive analysis of the strengths of each feature space, offering valuable insights for future research in embodied planning. The code is available at: https://github.com/tsinghua-fib-lab/MoWM.",
        "tags": [
            "Robotics",
            "Video Generation"
        ]
    },
    {
        "id": "118",
        "title": "Evaluating and Improving Cultural Awareness of Reward Models for LLM Alignment",
        "author": [
            "Hongbin Zhang",
            "Kehai Chen",
            "Xuefeng Bai",
            "Yang Xiang",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21798",
        "abstract": "Reward models (RMs) are crucial for aligning large language models (LLMs) with diverse cultures. Consequently, evaluating their cultural awareness is essential for further advancing global alignment of LLMs. However, existing RM evaluations fall short in assessing cultural awareness due to the scarcity of culturally relevant evaluation datasets. To fill this gap, we propose Cultural Awareness Reward modeling Benchmark (CARB), covering 10 distinct cultures across 4 cultural domains. Our extensive evaluation of state-of-the-art RMs reveals their deficiencies in modeling cultural awareness and demonstrates a positive correlation between performance on CARB and downstream multilingual cultural alignment tasks. Further analysis identifies the spurious correlations within culture-aware reward modeling, wherein RM's scoring relies predominantly on surface-level features rather than authentic cultural nuance understanding. To address these, we propose Think-as-Locals to elicit deeper culturally grounded reasoning from generative RMs via reinforcement learning from verifiable rewards (RLVR) and employ well-designed rewards to ensure accurate preference judgments and high-quality structured evaluation criteria generation. Experimental results validate its efficacy in mitigating spurious features interference and advancing culture-aware reward modeling.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "119",
        "title": "D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents",
        "author": [
            "Hongze Mi",
            "Yibo Feng",
            "Wenjie Lu",
            "Yuqi Wang",
            "Jinyuan Li",
            "Song Cao",
            "He Cui",
            "Tengfei Tian",
            "Xuelin Zhang",
            "Haotian Luo",
            "Di Sun",
            "Naiqiang Tan",
            "Gang Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21799",
        "abstract": "Graphical User Interface (GUI) agents aim to automate a wide spectrum of human tasks by emulating user interaction. Despite rapid advancements, current approaches are hindered by several critical challenges: data bottleneck in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance. Inspired by the human cognitive loop of Thinking, Alignment, and Reflection, we present D-Artemis -- a novel deliberative framework in this paper. D-Artemis leverages a fine-grained, app-specific tip retrieval mechanism to inform its decision-making process. It also employs a proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC) Check module and Action Correction Agent (ACA) work in concert to mitigate the risk of execution failures. A post-execution Status Reflection Agent (SRA) completes the cognitive loop, enabling strategic learning from experience. Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal large language models (MLLMs) for GUI tasks without the need for training on complex trajectory datasets, demonstrating strong generalization. D-Artemis establishes new state-of-the-art (SOTA) results across both major benchmarks, achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2. Extensive ablation studies further demonstrate the significant contribution of each component to the framework.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "120",
        "title": "Redefining Machine Simultaneous Interpretation: From Incremental Translation to Human-Like Strategies",
        "author": [
            "Qianen Zhang",
            "Satoshi Nakamura"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21801",
        "abstract": "Simultaneous Machine Translation (SiMT) requires high-quality translations under strict real-time constraints, which traditional encoder-decoder policies with only READ/WRITE actions cannot fully address. We extend the action space of SiMT with four adaptive actions: SENTENCE_CUT, DROP, PARTIAL_SUMMARIZATION and PRONOMINALIZATION, which enable real-time restructuring, omission, and simplification while preserving semantic fidelity. We implement these actions in a decoder-only large language model (LLM) framework and construct training references through action-aware prompting. To evaluate both quality and latency, we further develop a latency-aware TTS pipeline that maps textual outputs to speech with realistic timing. Experiments on the ACL60/60 English-Chinese and English-German benchmarks show that our framework consistently improves semantic metrics (e.g., COMET-KIWI) and achieves lower delay (measured by Average Lagging) compared to reference translations and salami-based baselines. Notably, combining DROP and SENTENCE_CUT yields the best overall balance between fluency and latency. These results demonstrate that enriching the action space of LLM-based SiMT provides a promising direction for bridging the gap between human and machine interpretation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "121",
        "title": "ChaosNexus: A Foundation Model for Universal Chaotic System Forecasting with Multi-scale Representations",
        "author": [
            "Chang Liu",
            "Bohao Zhao",
            "Jingtao Ding",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21802",
        "abstract": "Accurately forecasting chaotic systems, prevalent in domains such as weather prediction and fluid dynamics, remains a significant scientific challenge. The inherent sensitivity of these systems to initial conditions, coupled with a scarcity of observational data, severely constrains traditional modeling approaches. Since these models are typically trained for a specific system, they lack the generalization capacity necessary for real-world applications, which demand robust zero-shot or few-shot forecasting on novel or data-limited scenarios. To overcome this generalization barrier, we propose ChaosNexus, a foundation model pre-trained on a diverse corpus of chaotic dynamics. ChaosNexus employs a novel multi-scale architecture named ScaleFormer augmented with Mixture-of-Experts layers, to capture both universal patterns and system-specific behaviors. The model demonstrates state-of-the-art zero-shot generalization across both synthetic and real-world benchmarks. On a large-scale testbed comprising over 9,000 synthetic chaotic systems, it improves the fidelity of long-term attractor statistics by more than 40% compared to the leading baseline. This robust performance extends to real-world applications with exceptional data efficiency. For instance, in 5-day global weather forecasting, ChaosNexus achieves a competitive zero-shot mean error below 1 degree, a result that further improves with few-shot fine-tuning. Moreover, experiments on the scaling behavior of ChaosNexus provide a guiding principle for scientific foundation models: cross-system generalization stems from the diversity of training systems, rather than sheer data volume.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "122",
        "title": "Learning Multi-Skill Legged Locomotion Using Conditional Adversarial Motion Priors",
        "author": [
            "Ning Huang",
            "Zhentao Xie",
            "Qinchuan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21810",
        "abstract": "Despite growing interest in developing legged robots that emulate biological locomotion for agile navigation of complex environments, acquiring a diverse repertoire of skills remains a fundamental challenge in robotics. Existing methods can learn motion behaviors from expert data, but they often fail to acquire multiple locomotion skills through a single policy and lack smooth skill transitions. We propose a multi-skill learning framework based on Conditional Adversarial Motion Priors (CAMP), with the aim of enabling quadruped robots to efficiently acquire a diverse set of locomotion skills from expert demonstrations. Precise skill reconstruction is achieved through a novel skill discriminator and skill-conditioned reward design. The overall framework supports the active control and reuse of multiple skills, providing a practical solution for learning generalizable policies in complex environments.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "123",
        "title": "No More Manual Guides: Automatic and Scalable Generation of High-Quality Excel Tutorials",
        "author": [
            "Yuhang Xie",
            "Jian Mu",
            "Xiaojun Ma",
            "Chaoyun Zhang",
            "Lu Wang",
            "Mengyu Zhou",
            "Mugeng Liu",
            "Si Qin",
            "Qingwei Lin",
            "Saravan Rajmohan",
            "Shi Han",
            "Dongmei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21816",
        "abstract": "Excel is one of the most widely used productivity tools across domains, offering rich functionality but also overwhelming users with its complexity. This creates a persistent demand for tutorials to support effective usage. However, existing tutorials are manually authored by experts, require frequent updates after each software release, and incur substantial labor costs. Prior work has not achieved fully automated tutorial generation, since existing methods still depend on handcrafted operation sequences or example materials. In this paper, we present the first framework for automatically generating Excel tutorials directly from natural language task descriptions. Our framework first instantiates the task. Then a central component of this framework, Execution Agent, plans and executes the solution in Excel, and collects the intermediate artifacts required for tutorial construction. These artifacts are then transformed into both structured Excel documents and video demonstrations. To build a comprehensive tutorial corpus, we collected 1,559 task descriptions from real-world scenarios. In addition, we designed a systematic evaluation framework that integrates assessments from both large language models (LLMs) and human reviewers. Experimental results show that our framework improves task execution success rates by 8.5% over state-of-the-art baselines. Moreover, the generated tutorials demonstrate superior readability and instructional effectiveness, often approaching or surpassing expert-authored materials. Importantly, the automated pipeline eliminates manual labor and reduces time costs to 1/20 of expert authoring, making scalable and high-quality tutorial generation practical for the first time.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "124",
        "title": "Sharpness-Aware Minimization Can Hallucinate Minimizers",
        "author": [
            "Chanwoong Park",
            "Uijeong Jang",
            "Ernest K. Ryu",
            "Insoon Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21818",
        "abstract": "Sharpness-Aware Minimization (SAM) is a widely used method that steers training toward flatter minimizers, which typically generalize better. In this work, however, we show that SAM can converge to hallucinated minimizers -- points that are not minimizers of the original objective. We theoretically prove the existence of such hallucinated minimizers and establish conditions for local convergence to them. We further provide empirical evidence demonstrating that SAM can indeed converge to these points in practice. Finally, we propose a simple yet effective remedy for avoiding hallucinated minimizers.",
        "tags": [
            "SAM"
        ]
    },
    {
        "id": "125",
        "title": "Can LLMs Solve and Generate Linguistic Olympiad Puzzles?",
        "author": [
            "Neh Majmudar",
            "Elena Filatova"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21820",
        "abstract": "In this paper, we introduce a combination of novel and exciting tasks: the solution and generation of linguistic puzzles. We focus on puzzles used in Linguistic Olympiads for high school students. We first extend the existing benchmark for the task of solving linguistic puzzles. We explore the use of Large Language Models (LLMs), including recent state-of-the-art models such as OpenAI's o1, for solving linguistic puzzles, analyzing their performance across various linguistic topics. We demonstrate that LLMs outperform humans on most puzzles types, except for those centered on writing systems, and for the understudied languages. We use the insights from puzzle-solving experiments to direct the novel task of puzzle generation. We believe that automating puzzle generation, even for relatively simple puzzles, holds promise for expanding interest in linguistics and introducing the field to a broader audience. This finding highlights the importance of linguistic puzzle generation as a research task: such puzzles can not only promote linguistics but also support the dissemination of knowledge about rare and understudied languages.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "126",
        "title": "SoK: Potentials and Challenges of Large Language Models for Reverse Engineering",
        "author": [
            "Xinyu Hu",
            "Zhiwei Fu",
            "Shaocong Xie",
            "Steven H. H. Ding",
            "Philippe Charland"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21821",
        "abstract": "Reverse Engineering (RE) is central to software security, enabling tasks such as vulnerability discovery and malware analysis, but it remains labor-intensive and requires substantial expertise. Earlier advances in deep learning start to automate parts of RE, particularly for malware detection and vulnerability classification. More recently, a rapidly growing body of work has applied Large Language Models (LLMs) to similar purposes. Their role compared to prior machine learning remains unclear, since some efforts simply adapt existing pipelines with minimal change while others seek to exploit broader reasoning and generative abilities. These differences, combined with varied problem definitions, methods, and evaluation practices, limit comparability, reproducibility, and cumulative progress. This paper systematizes the field by reviewing 44 research papers, including peer-reviewed publications and preprints, and 18 additional open-source projects that apply LLMs in RE. We propose a taxonomy that organizes existing work by objective, target, method, evaluation strategy, and data scale. Our analysis identifies strengths and limitations, highlights reproducibility and evaluation gaps, and examines emerging risks. We conclude with open challenges and future research directions that aim to guide more coherent and security-relevant applications of LLMs in RE.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "127",
        "title": "ProRe: A Proactive Reward System for GUI Agents via Reasoner-Actor Collaboration",
        "author": [
            "Gaole Dai",
            "Shiqi Jiang",
            "Ting Cao",
            "Yuqing Yang",
            "Yuanchun Li",
            "Rui Tan",
            "Mo Li",
            "Lili Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21823",
        "abstract": "Reward is critical to the evaluation and training of large language models (LLMs). However, existing rule-based or model-based reward methods struggle to generalize to GUI agents, where access to ground-truth trajectories or application databases is often unavailable, and static trajectory-based LLM-as-a-Judge approaches suffer from limited accuracy. To address these challenges, we propose ProRe, a proactive reward system that leverages a general-purpose reasoner and domain-specific evaluator agents (actors). The reasoner schedules targeted state probing tasks, which the evaluator agents then execute by actively interacting with the environment to collect additional observations. This enables the reasoner to assign more accurate and verifiable rewards to GUI agents. Empirical results on over 3K trajectories demonstrate that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%, respectively. Furthermore, integrating ProRe with state-of-the-art policy agents yields a success rate improvement of up to 22.4%.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "128",
        "title": "DS-STAR: Data Science Agent via Iterative Planning and Verification",
        "author": [
            "Jaehyun Nam",
            "Jinsung Yoon",
            "Jiefeng Chen",
            "Jinwoo Shin",
            "Tomas Pfister"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21825",
        "abstract": "Data science, which transforms raw data into actionable insights, is critical for data-driven decision-making. However, these tasks are often complex, involving steps for exploring multiple data sources and synthesizing findings to deliver insightful answers. While large language models (LLMs) show significant promise in automating this process, they often struggle with heterogeneous data formats and generate sub-optimal analysis plans, as verifying plan sufficiency is inherently difficult without ground-truth labels for such open-ended tasks. To overcome these limitations, we introduce DS-STAR, a novel data science agent. Specifically, DS-STAR makes three key contributions: (1) a data file analysis module that automatically explores and extracts context from diverse data formats, including unstructured types; (2) a verification step where an LLM-based judge evaluates the sufficiency of the analysis plan at each stage; and (3) a sequential planning mechanism that starts with a simple, executable plan and iteratively refines it based on the DS-STAR's feedback until its sufficiency is verified. This iterative refinement allows DS-STAR to reliably navigate complex analyses involving diverse data sources. Our experiments show that DS-STAR achieves state-of-the-art performance across three challenging benchmarks: DABStep, KramaBench, and DA-Code. Moreover, DS-STAR particularly outperforms baselines on hard tasks that require processing multiple data files with heterogeneous formats.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "129",
        "title": "ResT: Reshaping Token-Level Policy Gradients for Tool-Use Large Language Models",
        "author": [
            "Zihan Lin",
            "Xiaohan Wang",
            "Jie Cao",
            "Jiajun Chai",
            "Guojun Yin",
            "Wei Lin",
            "Ran He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21826",
        "abstract": "Large language models (LLMs) transcend passive generation and act as goal-directed agents by invoking external tools. Reinforcement learning (RL) offers a principled framework for optimizing these emergent tool-use policies, yet the prevailing paradigm relies exclusively on sparse outcome rewards and lacks consideration of the particularity of tool-use tasks, inflating policy-gradient variance and resulting in inefficient training. To better understand and address these challenges, we first establish a theoretical link between policy entropy and training stability of tool-use tasks, which reveals that structured, low-entropy tokens are primary determinants of rewards. Motivated by this insight, we propose \\textbf{Res}haped \\textbf{T}oken-level policy gradients (\\textbf{ResT}) for tool-use tasks. ResT reshapes the policy gradient through entropy-informed token reweighting, progressively upweighting reasoning tokens as training proceeds. This entropy-aware scheme enables a smooth shift from structural correctness to semantic reasoning and stabilizes convergence in multi-turn tool-use tasks. Evaluation on BFCL and API-Bank shows that ResT achieves state-of-the-art results, outperforming prior methods by up to $8.76\\%$. When fine-tuned on a 4B base LLM, ResT further surpasses GPT-4o by $4.11\\%$ on single-turn tasks and $1.50\\%$ on multi-turn base tasks.",
        "tags": [
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "130",
        "title": "Preference-Guided Learning for Sparse-Reward Multi-Agent Reinforcement Learning",
        "author": [
            "Viet Bui",
            "Tien Mai",
            "Hong Thanh Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21828",
        "abstract": "We study the problem of online multi-agent reinforcement learning (MARL) in environments with sparse rewards, where reward feedback is not provided at each interaction but only revealed at the end of a trajectory. This setting, though realistic, presents a fundamental challenge: the lack of intermediate rewards hinders standard MARL algorithms from effectively guiding policy learning. To address this issue, we propose a novel framework that integrates online inverse preference learning with multi-agent on-policy optimization into a unified architecture. At its core, our approach introduces an implicit multi-agent reward learning model, built upon a preference-based value-decomposition network, which produces both global and local reward signals. These signals are further used to construct dual advantage streams, enabling differentiated learning targets for the centralized critic and decentralized actors. In addition, we demonstrate how large language models (LLMs) can be leveraged to provide preference labels that enhance the quality of the learned reward model. Empirical evaluations on state-of-the-art benchmarks, including MAMuJoCo and SMACv2, show that our method achieves superior performance compared to existing baselines, highlighting its effectiveness in addressing sparse-reward challenges in online MARL.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "131",
        "title": "Micro-macro kinetic flux-vector splitting schemes for the multidimensional Boltzmann-ES-BGK equation",
        "author": [
            "James A. Rossmanith",
            "Preeti Sar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21832",
        "abstract": "The kinetic Boltzmann equation models gas dynamics over a wide range of spatial and temporal scales. Simplified versions of the full Boltzmann collision operator, such as the classical Bhatnagar-Gross-Krook and the closely related Ellipsoidal-Statistical-BGK operators, can dramatically decrease the computational costs of numerical solving kinetic equations. Classical BGK yields incorrect transport coefficients (relative to the full Boltzmann collision operator) at low Knudsen numbers, whereas ES-BGK captures them correctly. In this work, we develop a finite volume method using a micro-macro decomposition of the distribution function, which requires a smaller velocity mesh relative to direct kinetic methods for low and intermediate Knudsen numbers. The macro portion of the model is a fluid model with a moment closure provided from the heat flux tensor calculated from the micro portion. The micro portion is obtained by applying to the original kinetic equation a projector into the orthogonal complement of the null space of the collision operator - this projector depends on the macro portion. In particular, we extend the technique of Bennoune, Lemou, and Mieussens [Uniformly stable schemes for the Boltzmann equation preserving the compressible Navier-Stokes asymptotics, J. Comput. Phys. (2008)] to two-space dimensions, the ES-BGK collision operator, and problems with reflecting wall boundary conditions. As it appears in both the micro and macro equations, the collision operator is handled via L-stable implicit time discretizations. At the same time, the remaining transport terms are computed via kinetic flux vector splitting (for macro) and upwind differencing (for micro). The resulting scheme is applied to various test cases in 1D and 2D. The 2D version of the code is parallelized via MPI, and we present weak and strong scaling studies with varying numbers of processors.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "132",
        "title": "RobustFlow: Towards Robust Agentic Workflow Generation",
        "author": [
            "Shengxiang Xu",
            "Jiayi Zhang",
            "Shimin Di",
            "Yuyu Luo",
            "Liang Yao",
            "Hanmo Liu",
            "Jia Zhu",
            "Fan Liu",
            "Min-Ling Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21834",
        "abstract": "The automated generation of agentic workflows is a promising frontier for enabling large language models (LLMs) to solve complex tasks. However, our investigation reveals that the robustness of agentic workflow remains a critical, unaddressed challenge. Current methods often generate wildly inconsistent workflows when provided with instructions that are semantically identical but differently phrased. This brittleness severely undermines their reliability and trustworthiness for real-world applications. To quantitatively diagnose this instability, we propose metrics based on nodal and topological similarity to evaluate workflow consistency against common semantic variations such as paraphrasing and noise injection. Subsequently, we further propose a novel training framework, RobustFlow, that leverages preference optimization to teach models invariance to instruction variations. By training on sets of synonymous task descriptions, RobustFlow boosts workflow robustness scores to 70\\% - 90\\%, which is a substantial improvement over existing approaches. The code is publicly available at https://github.com/DEFENSE-SEU/RobustFlow.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "133",
        "title": "On the Complexity Theory of Masked Discrete Diffusion: From $\\mathrm{poly}(1/Îµ)$ to Nearly $Îµ$-Free",
        "author": [
            "Xunpeng Huang",
            "Yingyu Lin",
            "Nishant Jain",
            "Kaibo Wang",
            "Difan Zou",
            "Yian Ma",
            "Tong Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21835",
        "abstract": "We study masked discrete diffusion -- a flexible paradigm for text generation in which tokens are progressively corrupted by special mask symbols before being denoised. Although this approach has demonstrated strong empirical performance, its theoretical complexity in high-dimensional settings remains insufficiently understood. Existing analyses largely focus on uniform discrete diffusion, and more recent attempts addressing masked diffusion either (1) overlook widely used Euler samplers, (2) impose restrictive bounded-score assumptions, or (3) fail to showcase the advantages of masked discrete diffusion over its uniform counterpart. To address this gap, we show that Euler samplers can achieve $\\epsilon$-accuracy in total variation (TV) with $\\tilde{O}(d^{2}\\epsilon^{-3/2})$ discrete score evaluations, thereby providing the first rigorous analysis of typical Euler sampler in masked discrete diffusion. We then propose a Mask-Aware Truncated Uniformization (MATU) approach that both removes bounded-score assumptions and preserves unbiased discrete score approximation. By exploiting the property that each token can be unmasked at most once, MATU attains a nearly $\\epsilon$-free complexity of $O(d\\,\\ln d\\cdot (1-\\epsilon^2))$. This result surpasses existing uniformization methods under uniform discrete diffusion, eliminating the $\\ln(1/\\epsilon)$ factor and substantially speeding up convergence. Our findings not only provide a rigorous theoretical foundation for masked discrete diffusion, showcasing its practical advantages over uniform diffusion for text generation, but also pave the way for future efforts to analyze diffusion-based language models developed under masking paradigm.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "134",
        "title": "Semantic Agreement Enables Efficient Open-Ended LLM Cascades",
        "author": [
            "Duncan Soiffer",
            "Steven Kolawole",
            "Virginia Smith"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21837",
        "abstract": "Cascade systems route computational requests to smaller models when possible and defer to larger models only when necessary, offering a promising approach to balance cost and quality in LLM deployment. However, they face a fundamental challenge in open-ended text generation: determining output reliability when generation quality lies on a continuous spectrum, often with multiple valid responses. To address this, we propose semantic agreement -- meaning-level consensus between ensemble outputs -- as a training-free signal for reliable deferral. We show that when diverse model outputs agree semantically, their consensus is a stronger reliability signal than token-level confidence. Evaluated from 500M to 70B-parameter models, we find that semantic cascades match or surpass target-model quality at 40% of the cost and reduce latency by up to 60%. Our method requires no model internals, works across black-box APIs, and remains robust to model updates, making it a practical baseline for real-world LLM deployment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "135",
        "title": "DiTraj: training-free trajectory control for video diffusion transformer",
        "author": [
            "Cheng Lei",
            "Jiayu Zhang",
            "Yue Ma",
            "Xinyu Wang",
            "Long Chen",
            "Liang Tang",
            "Yiqiang Yan",
            "Fei Su",
            "Zhicheng Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21839",
        "abstract": "Diffusion Transformers (DiT)-based video generation models with 3D full attention exhibit strong generative capabilities. Trajectory control represents a user-friendly task in the field of controllable video generation. However, existing methods either require substantial training resources or are specifically designed for U-Net, do not take advantage of the superior performance of DiT. To address these issues, we propose DiTraj, a simple but effective training-free framework for trajectory control in text-to-video generation, tailored for DiT. Specifically, first, to inject the object's trajectory, we propose foreground-background separation guidance: we use the Large Language Model (LLM) to convert user-provided prompts into foreground and background prompts, which respectively guide the generation of foreground and background regions in the video. Then, we analyze 3D full attention and explore the tight correlation between inter-token attention scores and position embedding. Based on this, we propose inter-frame Spatial-Temporal Decoupled 3D-RoPE (STD-RoPE). By modifying only foreground tokens' position embedding, STD-RoPE eliminates their cross-frame spatial discrepancies, strengthening cross-frame attention among them and thus enhancing trajectory control. Additionally, we achieve 3D-aware trajectory control by regulating the density of position embedding. Extensive experiments demonstrate that our method outperforms previous methods in both video quality and trajectory controllability.",
        "tags": [
            "3D",
            "DiT",
            "Diffusion",
            "LLM",
            "RoPE",
            "Text-to-Video",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "136",
        "title": "Can Large Language Models Autoformalize Kinematics?",
        "author": [
            "Aditi Kabra",
            "Jonathan Laurent",
            "Sagar Bharadwaj",
            "Ruben Martins",
            "Stefan Mitsch",
            "AndrÃ© Platzer"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21840",
        "abstract": "Autonomous cyber-physical systems like robots and self-driving cars could greatly benefit from using formal methods to reason reliably about their control decisions. However, before a problem can be solved it needs to be stated. This requires writing a formal physics model of the cyber-physical system, which is a complex task that traditionally requires human expertise and becomes a bottleneck.\nThis paper experimentally studies whether Large Language Models (LLMs) can automate the formalization process. A 20 problem benchmark suite is designed drawing from undergraduate level physics kinematics problems. In each problem, the LLM is provided with a natural language description of the objects' motion and must produce a model in differential game logic (dGL). The model is (1) syntax checked and iteratively refined based on parser feedback, and (2) semantically evaluated by checking whether symbolically executing the dGL formula recovers the solution to the original physics problem. A success rate of 70% (best over 5 samples) is achieved. We analyze failing cases, identifying directions for future improvement. This provides a first quantitative baseline for LLM-based autoformalization from natural language to a hybrid games logic with continuous dynamics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "137",
        "title": "Zeppelin: Balancing Variable-length Workloads in Data Parallel Large Model Training",
        "author": [
            "Chang Chen",
            "Tiancheng Chen",
            "Jiangfei Duan",
            "Qianchao Zhu",
            "Zerui Wang",
            "Qinghao Hu",
            "Peng Sun",
            "Xiuhong Li",
            "Chao Yang",
            "Torsten Hoefler"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21841",
        "abstract": "Training large language models (LLMs) with increasingly long and varying sequence lengths introduces severe load imbalance challenges in large-scale data-parallel training. Recent frameworks attempt to mitigate these issues through data reorganization or hybrid parallel strategies. However, they often overlook how computational and communication costs scale with sequence length, resulting in suboptimal performance. We identify three critical challenges: (1) varying computation-to-communication ratios across sequences of different lengths in distributed attention, (2) mismatch between static NIC-GPU affinity and dynamic parallel workloads, and (3) distinct optimal partitioning strategies required for quadratic attention versus linear components. To address these challenges, we present Zeppelin, a novel training system that integrates three key techniques: (1) a hierarchical sequence partitioning method for the attention module that reduces communication overhead and balances computation, supported by an efficient attention engine that applies divergent parallel strategies; (2) a routing layer that orchestrates inter-node transfers to fully utilize NIC bandwidth; and (3) a remapping layer that transforms sequence layouts between attention and linear modules, ensuring high computational efficiency across both. Comprehensive evaluations across diverse configurations show that Zeppelin delivers an average 2.80x speedup over state-of-the-art methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "138",
        "title": "DeepTravel: An End-to-End Agentic Reinforcement Learning Framework for Autonomous Travel Planning Agents",
        "author": [
            "Yansong Ning",
            "Rui Liu",
            "Jun Wang",
            "Kai Chen",
            "Wei Li",
            "Jun Fang",
            "Kan Zheng",
            "Naiqiang Tan",
            "Hao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21842",
        "abstract": "Travel planning (TP) agent has recently worked as an emerging building block to interact with external tools and resources for travel itinerary generation, ensuring enjoyable user experience. Despite its benefits, existing studies rely on hand craft prompt and fixed agent workflow, hindering more flexible and autonomous TP agent. This paper proposes DeepTravel, an end to end agentic reinforcement learning framework for building autonomous travel planning agent, capable of autonomously planning, executing tools, and reflecting on tool responses to explore, verify, and refine intermediate actions in multi step reasoning. To achieve this, we first construct a robust sandbox environment by caching transportation, accommodation and POI data, facilitating TP agent training without being constrained by real world APIs limitations (e.g., inconsistent outputs). Moreover, we develop a hierarchical reward modeling system, where a trajectory level verifier first checks spatiotemporal feasibility and filters unsatisfied travel itinerary, and then the turn level verifier further validate itinerary detail consistency with tool responses, enabling efficient and precise reward service. Finally, we propose the reply augmented reinforcement learning method that enables TP agent to periodically replay from a failures experience buffer, emerging notable agentic capacity. We deploy trained TP agent on DiDi Enterprise Solutions App and conduct comprehensive online and offline evaluations, demonstrating that DeepTravel enables small size LLMs (e.g., Qwen3 32B) to significantly outperform existing frontier LLMs such as OpenAI o1, o3 and DeepSeek R1 in travel planning tasks.",
        "tags": [
            "DeepSeek",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "139",
        "title": "SBFA: Single Sneaky Bit Flip Attack to Break Large Language Models",
        "author": [
            "Jingkai Guo",
            "Chaitali Chakrabarti",
            "Deliang Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21843",
        "abstract": "Model integrity of Large language models (LLMs) has become a pressing security concern with their massive online deployment. Prior Bit-Flip Attacks (BFAs) -- a class of popular AI weight memory fault-injection techniques -- can severely compromise Deep Neural Networks (DNNs): as few as tens of bit flips can degrade accuracy toward random guessing. Recent studies extend BFAs to LLMs and reveal that, despite the intuition of better robustness from modularity and redundancy, only a handful of adversarial bit flips can also cause LLMs' catastrophic accuracy degradation. However, existing BFA methods typically focus on either integer or floating-point models separately, limiting attack flexibility. Moreover, in floating-point models, random bit flips often cause perturbed parameters to extreme values (e.g., flipping in exponent bit), making it not stealthy and leading to numerical runtime error (e.g., invalid tensor values (NaN/Inf)). In this work, for the first time, we propose SBFA (Sneaky Bit-Flip Attack), which collapses LLM performance with only one single bit flip while keeping perturbed values within benign layer-wise weight distribution. It is achieved through iterative searching and ranking through our defined parameter sensitivity metric, ImpactScore, which combines gradient sensitivity and perturbation range constrained by the benign layer-wise weight distribution. A novel lightweight SKIP searching algorithm is also proposed to greatly reduce searching complexity, which leads to successful SBFA searching taking only tens of minutes for SOTA LLMs. Across Qwen, LLaMA, and Gemma models, with only one single bit flip, SBFA successfully degrades accuracy to below random levels on MMLU and SST-2 in both BF16 and INT8 data formats. Remarkably, flipping a single bit out of billions of parameters reveals a severe security concern of SOTA LLM models.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "140",
        "title": "A Comprehensive Evaluation of Transformer-Based Question Answering Models and RAG-Enhanced Design",
        "author": [
            "Zichen Zhang",
            "Kunlong Zhang",
            "Hongwei Ruan",
            "Yiming Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21845",
        "abstract": "Transformer-based models have advanced the field of question answering, but multi-hop reasoning, where answers require combining evidence across multiple passages, remains difficult. This paper presents a comprehensive evaluation of retrieval strategies for multi-hop question answering within a retrieval-augmented generation framework. We compare cosine similarity, maximal marginal relevance, and a hybrid method that integrates dense embeddings with lexical overlap and re-ranking. To further improve retrieval, we adapt the EfficientRAG pipeline for query optimization, introducing token labeling and iterative refinement while maintaining efficiency. Experiments on the HotpotQA dataset show that the hybrid approach substantially outperforms baseline methods, achieving a relative improvement of 50 percent in exact match and 47 percent in F1 score compared to cosine similarity. Error analysis reveals that hybrid retrieval improves entity recall and evidence complementarity, while remaining limited in handling distractors and temporal reasoning. Overall, the results suggest that hybrid retrieval-augmented generation provides a practical zero-shot solution for multi-hop question answering, balancing accuracy, efficiency, and interpretability.",
        "tags": [
            "RAG",
            "Transformer"
        ]
    },
    {
        "id": "141",
        "title": "Graph of Agents: Principled Long Context Modeling by Emergent Multi-Agent Collaboration",
        "author": [
            "Taejong Joo",
            "Shu Ishida",
            "Ivan Sosnovik",
            "Bryan Lim",
            "Sahand Rezaei-Shoshtari",
            "Adam Gaier",
            "Robert Giaquinto"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21848",
        "abstract": "As a model-agnostic approach to long context modeling, multi-agent systems can process inputs longer than a large language model's context window without retraining or architectural modifications. However, their performance often heavily relies on hand-crafted multi-agent collaboration strategies and prompt engineering, which limit generalizability. In this work, we introduce a principled framework that formalizes the model-agnostic long context modeling problem as a compression problem, yielding an information-theoretic compression objective. Building on this framework, we propose Graph of Agents (GoA), which dynamically constructs an input-dependent collaboration structure that maximizes this objective. For Llama 3.1 8B and Qwen3 8B across six document question answering benchmarks, GoA improves the average $F_1$ score of retrieval-augmented generation by 5.7\\% and a strong multi-agent baseline using a fixed collaboration structure by 16.35\\%, respectively. Even with only a 2K context window, GoA surpasses the 128K context window Llama 3.1 8B on LongBench, showing a dramatic increase in effective context length. Our source code is available at https://github.com/tjoo512/graph-of-agents.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "142",
        "title": "Following the TRACE: A Structured Path to Empathetic Response Generation with Multi-Agent Models",
        "author": [
            "Ziqi Liu",
            "Ziyang Zhou",
            "Yilin Li",
            "Haiyang Zhang",
            "Yangbin Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21849",
        "abstract": "Empathetic response generation is a crucial task for creating more human-like and supportive conversational agents. However, existing methods face a core trade-off between the analytical depth of specialized models and the generative fluency of Large Language Models (LLMs). To address this, we propose TRACE, Task-decomposed Reasoning for Affective Communication and Empathy, a novel framework that models empathy as a structured cognitive process by decomposing the task into a pipeline for analysis and synthesis. By building a comprehensive understanding before generation, TRACE unites deep analysis with expressive generation. Experimental results show that our framework significantly outperforms strong baselines in both automatic and LLM-based evaluations, confirming that our structured decomposition is a promising paradigm for creating more capable and interpretable empathetic agents. Our code is available at https://anonymous.4open.science/r/TRACE-18EF/README.md.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "143",
        "title": "Dynamic Novel View Synthesis in High Dynamic Range",
        "author": [
            "Kaixuan Zhang",
            "Zhipeng Xiong",
            "Minxian Li",
            "Mingwu Ren",
            "Jiankang Deng",
            "Xiatian Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21853",
        "abstract": "High Dynamic Range Novel View Synthesis (HDR NVS) seeks to learn an HDR 3D model from Low Dynamic Range (LDR) training images captured under conventional imaging conditions. Current methods primarily focus on static scenes, implicitly assuming all scene elements remain stationary and non-living. However, real-world scenarios frequently feature dynamic elements, such as moving objects, varying lighting conditions, and other temporal events, thereby presenting a significantly more challenging scenario. To address this gap, we propose a more realistic problem named HDR Dynamic Novel View Synthesis (HDR DNVS), where the additional dimension ``Dynamic'' emphasizes the necessity of jointly modeling temporal radiance variations alongside sophisticated 3D translation between LDR and HDR. To tackle this complex, intertwined challenge, we introduce HDR-4DGS, a Gaussian Splatting-based architecture featured with an innovative dynamic tone-mapping module that explicitly connects HDR and LDR domains, maintaining temporal radiance coherence by dynamically adapting tone-mapping functions according to the evolving radiance distributions across the temporal dimension. As a result, HDR-4DGS achieves both temporal radiance consistency and spatially accurate color translation, enabling photorealistic HDR renderings from arbitrary viewpoints and time instances. Extensive experiments demonstrate that HDR-4DGS surpasses existing state-of-the-art methods in both quantitative performance and visual fidelity. Source code will be released.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "144",
        "title": "Perception-Consistency Multimodal Large Language Models Reasoning via Caption-Regularized Policy Optimization",
        "author": [
            "Songjun Tu",
            "Qichao Zhang",
            "Jingbo Sun",
            "Yuqian Fu",
            "Linjing Li",
            "Xiangyuan Lan",
            "Dongmei Jiang",
            "Yaowei Wang",
            "Dongbin Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21854",
        "abstract": "While multimodal large language models excel at tasks that integrate visual perception with symbolic reasoning, their performance is often undermined by a critical vulnerability: perception-induced errors that propagate through the reasoning chain. Current reinforcement learning (RL) fine-tuning methods, while enhancing reasoning abilities, largely fail to address the underlying misalignment between visual grounding and the subsequent reasoning process. To address this challenge, we propose \\textbf{Caption-Regularized Policy Optimization (CapPO)}, a novel RL framework that explicitly enforces perceptual consistency during policy optimization. CapPO integrates two key mechanisms: (1) a caption-based consistency regularization, which minimizes the divergence between responses conditioned on raw images and those conditioned on captions, thereby anchoring reasoning to semantically faithful visual content; and (2) a KL-weighted advantage estimation scheme, which adaptively scales reinforcement signals to strengthen perceptually consistent trajectories while suppressing spurious correlations. Extensive experiments on five math-focused and five general reasoning benchmarks demonstrate that CapPO achieves competitive performance, yielding gains of +6.0% accuracy on math-related tasks and +2.4% on general reasoning tasks over the base Qwen2.5-VL-7B model. Moreover, ablation studies further confirm the effectiveness of each component, while error analysis reveals that CapPO significantly reduces perception-related mistakes compared with baselines. Overall, CapPO provides a simple yet effective framework for improving multimodal reasoning.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "145",
        "title": "KnowMT-Bench: Benchmarking Knowledge-Intensive Long-Form Question Answering in Multi-Turn Dialogues",
        "author": [
            "Junhao Chen",
            "Yu Huang",
            "Siyuan Li",
            "Rui Yao",
            "Hanqian Li",
            "Hanyu Zhang",
            "Jungang Li",
            "Jian Chen",
            "Bowen Wang",
            "Xuming Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21856",
        "abstract": "Multi-Turn Long-Form Question Answering (MT-LFQA) is a key application paradigm of Large Language Models (LLMs) in knowledge-intensive domains. However, existing benchmarks are limited to single-turn dialogue, while multi-turn dialogue benchmarks typically assess other orthogonal capabilities rather than knowledge-intensive factuality. To bridge this critical gap, we introduce \\textbf{KnowMT-Bench}, the \\textit{first-ever} benchmark designed to systematically evaluate MT-LFQA for LLMs across knowledge-intensive fields, including medicine, finance, and law. To faithfully assess the model's real-world performance, KnowMT-Bench employs a dynamic evaluation setting where models generate their own multi-turn dialogue histories given logically progressive question sequences. The factual capability and information delivery efficiency of the \\textit{final-turn} answer are then evaluated using a human-validated automated pipeline. Our experiments reveal that multi-turn contexts degrade performance: factual capability declines due to the contextual noise from self-generated histories, while information efficiency drops as models become more verbose with increasing dialogue length. We then investigate mitigation strategies, demonstrating that retrieval-augmented generation (RAG) can effectively alleviate and even reverse this factual degradation. These findings underscore the importance of our benchmark in evaluating and enhancing the conversational factual capabilities of LLMs in real-world knowledge-intensive applications. Code is available at \\href{https://github.com/hardenyu21/KnowMT-Bench}{\\textcolor{cyan}{\\texttt{KnowMT-Bench}}}.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "146",
        "title": "Reimagining Agent-based Modeling with Large Language Model Agents via Shachi",
        "author": [
            "So Kuroki",
            "Yingtao Tian",
            "Kou Misaki",
            "Takashi Ikegami",
            "Takuya Akiba",
            "Yujin Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21862",
        "abstract": "The study of emergent behaviors in large language model (LLM)-driven multi-agent systems is a critical research challenge, yet progress is limited by a lack of principled methodologies for controlled experimentation. To address this, we introduce Shachi, a formal methodology and modular framework that decomposes an agent's policy into core cognitive components: Configuration for intrinsic traits, Memory for contextual persistence, and Tools for expanded capabilities, all orchestrated by an LLM reasoning engine. This principled architecture moves beyond brittle, ad-hoc agent designs and enables the systematic analysis of how specific architectural choices influence collective behavior. We validate our methodology on a comprehensive 10-task benchmark and demonstrate its power through novel scientific inquiries. Critically, we establish the external validity of our approach by modeling a real-world U.S. tariff shock, showing that agent behaviors align with observed market reactions only when their cognitive architecture is appropriately configured with memory and tools. Our work provides a rigorous, open-source foundation for building and evaluating LLM agents, aimed at fostering more cumulative and scientifically grounded research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "147",
        "title": "Beyond RAG vs. Long-Context: Learning Distraction-Aware Retrieval for Efficient Knowledge Grounding",
        "author": [
            "Seong-Woong Shim",
            "Myunsoo Kim",
            "Jae Hyeon Cho",
            "Byung-Jun Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21865",
        "abstract": "Retrieval-Augmented Generation (RAG) is a framework for grounding Large Language Models (LLMs) in external, up-to-date information. However, recent advancements in context window size allow LLMs to process inputs of up to 128K tokens or more, offering an alternative strategy: supplying the full document context directly to the model, rather than relying on RAG to retrieve a subset of contexts. Nevertheless, this emerging alternative strategy has notable limitations: (i) it is token-inefficient to handle large and potentially redundant contexts; (ii) it exacerbates the `lost in the middle' phenomenon; and (iii) under limited model capacity, it amplifies distraction, ultimately degrading LLM output quality. In this paper, we propose LDAR (Learning Distraction-Aware Retrieval), an adaptive retriever that learns to retrieve contexts in a way that mitigates interference from distracting passages, thereby achieving significantly higher performance with reduced token usage compared to long-context approaches. Extensive experiments across diverse LLM architectures and six knowledge-intensive benchmarks demonstrate the effectiveness and robustness of our approach, highlighting the importance of balancing the trade-off between information coverage and distraction.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "148",
        "title": "What Makes LLM Agent Simulations Useful for Policy? Insights From an Iterative Design Engagement in Emergency Preparedness",
        "author": [
            "Yuxuan Li",
            "Sauvik Das",
            "Hirokazu Shirado"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21868",
        "abstract": "There is growing interest in using Large Language Models as agents (LLM agents) for social simulations to inform policy, yet real-world adoption remains limited. This paper addresses the question: How can LLM agent simulations be made genuinely useful for policy? We report on a year-long iterative design engagement with a university emergency preparedness team. Across multiple design iterations, we iteratively developed a system of 13,000 LLM agents that simulate crowd movement and communication during a large-scale gathering under various emergency scenarios. These simulations informed actual policy implementation, shaping volunteer training, evacuation protocols, and infrastructure planning. Analyzing this process, we identify three design implications: start with verifiable scenarios and build trust gradually, use preliminary simulations to elicit tacit knowledge, and treat simulation and policy development as evolving together. These implications highlight actionable pathways to making LLM agent simulations that are genuinely useful for policy.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "149",
        "title": "Enhancing Low-Rank Adaptation with Structured Nonlinear Transformations",
        "author": [
            "Guanzhi Deng",
            "Mingyang Liu",
            "Dapeng Wu",
            "Yinqiao Li",
            "Linqi Song"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21870",
        "abstract": "Low-Rank Adaptation (LoRA) is a widely adopted parameter-efficient fine-tuning method for large language models. However, its linear nature limits expressiveness. We propose LoRAN, a non-linear extension of LoRA that applies lightweight transformations to the low-rank updates. We further introduce Sinter, a sine-based activation that adds structured perturbations without increasing parameter count. Experiments across summarization and classification tasks show that LoRAN consistently improves over QLoRA. Ablation studies reveal that Sinter outperforms standard activations such as Sigmoid, ReLU, and Tanh, highlighting the importance of activation design in lowrank tuning.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "150",
        "title": "Unlocking the Essence of Beauty: Advanced Aesthetic Reasoning with Relative-Absolute Policy Optimization",
        "author": [
            "Boyang Liu",
            "Yifan Hu",
            "Senjie Jin",
            "Shihan Dou",
            "Gonglei Shi",
            "Jie Shao",
            "Tao Gui",
            "Xuanjing Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21871",
        "abstract": "Multimodal large language models (MLLMs) are well suited to image aesthetic assessment, as they can capture high-level aesthetic features leveraging their cross-modal understanding capacity. However, the scarcity of multimodal aesthetic reasoning data and the inherently subjective nature of aesthetic judgment make it difficult for MLLMs to generate accurate aesthetic judgments with interpretable rationales. To this end, we propose Aes-R1, a comprehensive aesthetic reasoning framework with reinforcement learning (RL). Concretely, Aes-R1 integrates a pipeline, AesCoT, to construct and filter high-quality chain-of-thought aesthetic reasoning data used for cold-start. After teaching the model to generate structured explanations prior to scoring, we then employ the Relative-Absolute Policy Optimization (RAPO), a novel RL algorithm that jointly optimizes absolute score regression and relative ranking order, improving both per-image accuracy and cross-image preference judgments. Aes-R1 enables MLLMs to generate grounded explanations alongside faithful scores, thereby enhancing aesthetic scoring and reasoning in a unified framework. Extensive experiments demonstrate that Aes-R1 improves the backbone's average PLCC/SRCC by 47.9%/34.8%, surpassing state-of-the-art baselines of similar size. More ablation studies validate Aes-R1's robust generalization under limited supervision and in out-of-distribution scenarios.",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "151",
        "title": "Abductive Logical Rule Induction by Bridging Inductive Logic Programming and Multimodal Large Language Models",
        "author": [
            "Yifei Peng",
            "Yaoli Liu",
            "Enbo Xia",
            "Yu Jin",
            "Wang-Zhou Dai",
            "Zhong Ren",
            "Yao-Xiang Ding",
            "Kun Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21874",
        "abstract": "We propose ILP-CoT, a method that bridges Inductive Logic Programming (ILP) and Multimodal Large Language Models (MLLMs) for abductive logical rule induction. The task involves both discovering logical facts and inducing logical rules from a small number of unstructured textual or visual inputs, which still remain challenging when solely relying on ILP, due to the requirement of specified background knowledge and high computational cost, or MLLMs, due to the appearance of perceptual hallucinations. Based on the key observation that MLLMs could propose structure-correct rules even under hallucinations, our approach automatically builds ILP tasks with pruned search spaces based on the rule structure proposals from MLLMs, and utilizes ILP system to output rules built upon rectified logical facts and formal inductive reasoning. Its effectiveness is verified through challenging logical induction benchmarks, as well as a potential application of our approach, namely text-to-image customized generation with rule induction. Our code and data are released at https://github.com/future-item/ILP-CoT.",
        "tags": [
            "CoT",
            "LLM",
            "Text-to-Image"
        ]
    },
    {
        "id": "152",
        "title": "LUMINA: Detecting Hallucinations in RAG System with Context-Knowledge Signals",
        "author": [
            "Min-Hsuan Yeh",
            "Yixuan Li",
            "Tanwi Mallick"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21875",
        "abstract": "Retrieval-Augmented Generation (RAG) aims to mitigate hallucinations in large language models (LLMs) by grounding responses in retrieved documents. Yet, RAG-based LLMs still hallucinate even when provided with correct and sufficient context. A growing line of work suggests that this stems from an imbalance between how models use external context and their internal knowledge, and several approaches have attempted to quantify these signals for hallucination detection. However, existing methods require extensive hyperparameter tuning, limiting their generalizability. We propose LUMINA, a novel framework that detects hallucinations in RAG systems through context-knowledge signals: external context utilization is quantified via distributional distance, while internal knowledge utilization is measured by tracking how predicted tokens evolve across transformer layers. We further introduce a framework for statistically validating these measurements. Experiments on common RAG hallucination benchmarks and four open-source LLMs show that LUMINA achieves consistently high AUROC and AUPRC scores, outperforming prior utilization-based methods by up to +13% AUROC on HalluRAG. Moreover, LUMINA remains robust under relaxed assumptions about retrieval quality and model matching, offering both effectiveness and practicality.",
        "tags": [
            "Detection",
            "LLM",
            "RAG",
            "Transformer"
        ]
    },
    {
        "id": "153",
        "title": "No Prompt Left Behind: Exploiting Zero-Variance Prompts in LLM Reinforcement Learning via Entropy-Guided Advantage Shaping",
        "author": [
            "Thanh-Long V. Le",
            "Myeongho Jeon",
            "Kim Vu",
            "Viet Lai",
            "Eunho Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21880",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a powerful framework for improving the reasoning abilities of Large Language Models (LLMs). However, current methods such as GRPO rely only on problems where the model responses to the same input differ in correctness, while ignoring those where all responses receive the same reward - so-called zero-variance prompts. In this work, we argue that such prompts are not useless but can, in fact, provide meaningful feedback for policy optimization. To this end, we introduce RL with Zero-Variance Prompts (RL-ZVP), a novel algorithm that extract learning signals from zero-variance prompts. RL-ZVP directly rewards correctness and penalizes errors even without contrasting responses, modulating feedback with token-level characteristics to preserve informative, nuanced signals. Across six math reasoning benchmarks, RL-ZVP achieves significant improvements of up to 8.61 points in accuracy and 7.77 points in pass rate over GRPO, while consistently outperforming other baselines that filter out zero-variance prompts. These results highlight the untapped potential of learning from zero-variance prompts in RLVR.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "154",
        "title": "Position: The Hidden Costs and Measurement Gaps of Reinforcement Learning with Verifiable Rewards",
        "author": [
            "Aaron Tu",
            "Weihao Xuan",
            "Heli Qi",
            "Xu Huang",
            "Qingcheng Zeng",
            "Shayan Talaei",
            "Yijia Xiao",
            "Peng Xia",
            "Xiangru Tang",
            "Yuchen Zhuang",
            "Bing Hu",
            "Hanqun Cao",
            "Wenqi Shi",
            "Tianang Leng",
            "Rui Yang",
            "Yingjian Chen",
            "Ziqi Wang",
            "Irene Li",
            "Nan Liu",
            "Huaxiu Yao",
            "Li Erran Li",
            "Ge Liu",
            "Amin Saberi",
            "Naoto Yokoya",
            "Jure Leskovec",
            "Yejin Choi",
            "Fang Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21882",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a practical and scalable approach to enhancing large language models in areas such as math, code, and other structured tasks. Two questions motivate this paper: how much of the reported gains survive under strictly parity-controlled evaluation, and whether RLVR is cost-free or exacts a measurable tax. We argue that progress is real, but gains are often overstated due to three forces - an RLVR tax, evaluation pitfalls, and data contamination. Using a partial-prompt contamination audit and matched-budget reproductions across base and RL models, we show that several headline gaps shrink or vanish under clean, parity-controlled evaluation. We then propose a tax-aware training and evaluation protocol that co-optimizes accuracy, grounding, and calibrated abstention and standardizes budgeting and provenance checks. Applied to recent RLVR setups, this protocol yields more reliable estimates of reasoning gains and, in several cases, revises prior conclusions. Our position is constructive: RLVR is valuable and industry-ready; we advocate keeping its practical benefits while prioritizing reliability, safety, and measurement.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "155",
        "title": "You Can't Steal Nothing: Mitigating Prompt Leakages in LLMs via System Vectors",
        "author": [
            "Bochuan Cao",
            "Changjiang Li",
            "Yuanpu Cao",
            "Yameng Ge",
            "Ting Wang",
            "Jinghui Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21884",
        "abstract": "Large language models (LLMs) have been widely adopted across various applications, leveraging customized system prompts for diverse tasks. Facing potential system prompt leakage risks, model developers have implemented strategies to prevent leakage, primarily by disabling LLMs from repeating their context when encountering known attack patterns. However, it remains vulnerable to new and unforeseen prompt-leaking techniques. In this paper, we first introduce a simple yet effective prompt leaking attack to reveal such risks. Our attack is capable of extracting system prompts from various LLM-based application, even from SOTA LLM models such as GPT-4o or Claude 3.5 Sonnet. Our findings further inspire us to search for a fundamental solution to the problems by having no system prompt in the context. To this end, we propose SysVec, a novel method that encodes system prompts as internal representation vectors rather than raw text. By doing so, SysVec minimizes the risk of unauthorized disclosure while preserving the LLM's core language capabilities. Remarkably, this approach not only enhances security but also improves the model's general instruction-following abilities. Experimental results demonstrate that SysVec effectively mitigates prompt leakage attacks, preserves the LLM's functional integrity, and helps alleviate the forgetting issue in long-context scenarios.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "156",
        "title": "TRACE: Learning to Compute on Graphs",
        "author": [
            "Ziyang Zheng",
            "Jiaying Zhu",
            "Jingyi Zhou",
            "Qiang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21886",
        "abstract": "Learning to compute, the ability to model the functional behavior of a computational graph, is a fundamental challenge for graph representation learning. Yet, the dominant paradigm is architecturally mismatched for this task. This flawed assumption, central to mainstream message passing neural networks (MPNNs) and their conventional Transformer-based counterparts, prevents models from capturing the position-aware, hierarchical nature of computation. To resolve this, we introduce \\textbf{TRACE}, a new paradigm built on an architecturally sound backbone and a principled learning objective. First, TRACE employs a Hierarchical Transformer that mirrors the step-by-step flow of computation, providing a faithful architectural backbone that replaces the flawed permutation-invariant aggregation. Second, we introduce \\textbf{function shift learning}, a novel objective that decouples the learning problem. Instead of predicting the complex global function directly, our model is trained to predict only the \\textit{function shift}, the discrepancy between the true global function and a simple local approximation that assumes input independence. We validate this paradigm on electronic circuits, one of the most complex and economically critical classes of computational graphs. Across a comprehensive suite of benchmarks, TRACE substantially outperforms all prior architectures. These results demonstrate that our architecturally-aligned backbone and decoupled learning objective form a more robust paradigm for the fundamental challenge of learning to compute on graphs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "157",
        "title": "StableDub: Taming Diffusion Prior for Generalized and Efficient Visual Dubbing",
        "author": [
            "Liyang Chen",
            "Tianze Zhou",
            "Xu He",
            "Boshi Tang",
            "Zhiyong Wu",
            "Yang Huang",
            "Yang Wu",
            "Zhongqian Sun",
            "Wei Yang",
            "Helen Meng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21887",
        "abstract": "The visual dubbing task aims to generate mouth movements synchronized with the driving audio, which has seen significant progress in recent years. However, two critical deficiencies hinder their wide application: (1) Audio-only driving paradigms inadequately capture speaker-specific lip habits, which fail to generate lip movements similar to the target avatar; (2) Conventional blind-inpainting approaches frequently produce visual artifacts when handling obstructions (e.g., microphones, hands), limiting practical deployment. In this paper, we propose StableDub, a novel and concise framework integrating lip-habit-aware modeling with occlusion-robust synthesis. Specifically, building upon the Stable-Diffusion backbone, we develop a lip-habit-modulated mechanism that jointly models phonemic audio-visual synchronization and speaker-specific orofacial dynamics. To achieve plausible lip geometries and object appearances under occlusion, we introduce the occlusion-aware training strategy by explicitly exposing the occlusion objects to the inpainting process. By incorporating the proposed designs, the model eliminates the necessity for cost-intensive priors in previous methods, thereby exhibiting superior training efficiency on the computationally intensive diffusion-based backbone. To further optimize training efficiency from the perspective of model architecture, we introduce a hybrid Mamba-Transformer architecture, which demonstrates the enhanced applicability in low-resource research scenarios. Extensive experimental results demonstrate that StableDub achieves superior performance in lip habit resemblance and occlusion robustness. Our method also surpasses other methods in audio-lip sync, video quality, and resolution consistency. We expand the applicability of visual dubbing methods from comprehensive aspects, and demo videos can be found at https://stabledub.github.io.",
        "tags": [
            "Diffusion",
            "Inpainting",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "158",
        "title": "Drag4D: Align Your Motion with Text-Driven 3D Scene Generation",
        "author": [
            "Minjun Kang",
            "Inkyu Shin",
            "Taeyeop Lee",
            "In So Kweon",
            "Kuk-Jin Yoon"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21888",
        "abstract": "We introduce Drag4D, an interactive framework that integrates object motion control within text-driven 3D scene generation. This framework enables users to define 3D trajectories for the 3D objects generated from a single image, seamlessly integrating them into a high-quality 3D background. Our Drag4D pipeline consists of three stages. First, we enhance text-to-3D background generation by applying 2D Gaussian Splatting with panoramic images and inpainted novel views, resulting in dense and visually complete 3D reconstructions. In the second stage, given a reference image of the target object, we introduce a 3D copy-and-paste approach: the target instance is extracted in a full 3D mesh using an off-the-shelf image-to-3D model and seamlessly composited into the generated 3D scene. The object mesh is then positioned within the 3D scene via our physics-aware object position learning, ensuring precise spatial alignment. Lastly, the spatially aligned object is temporally animated along a user-defined 3D trajectory. To mitigate motion hallucination and ensure view-consistent temporal alignment, we develop a part-augmented, motion-conditioned video diffusion model that processes multiview image pairs together with their projected 2D trajectories. We demonstrate the effectiveness of our unified architecture through evaluations at each stage and in the final results, showcasing the harmonized alignment of user-controlled object motion within a high-quality 3D background.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Image-to-3D",
            "Text-to-3D"
        ]
    },
    {
        "id": "159",
        "title": "Not Everyone Wins with LLMs: Behavioral Patterns and Pedagogical Implications in AI-assisted Data Analysis",
        "author": [
            "Qianou Ma",
            "Kenneth Koedinger",
            "Tongshuang Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21890",
        "abstract": "LLMs promise to democratize technical work in complex domains like programmatic data analysis, but not everyone benefits equally. We study how students with varied expertise use LLMs to complete Python-based data analysis in computational notebooks in a non-major course. Drawing on homework logs, recordings, and surveys from 36 students, we ask: Which expertise matters most, and how does it shape AI use? Our mixed-methods analysis shows that technical expertise -- not AI familiarity or communication skills -- remains a significant predictor of success. Students also vary widely in how they leverage LLMs, struggling at stages of forming intent, expressing inputs, interpreting outputs, and assessing results. We identify success and failure behaviors, such as providing context or decomposing prompts, that distinguish effective use. These findings inform AI literacy interventions, highlighting that lightweight demonstrations improve surface fluency but are insufficient; deeper training and scaffolds are needed to cultivate resilient AI use skills.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "160",
        "title": "AgentPack: A Dataset of Code Changes, Co-Authored by Agents and Humans",
        "author": [
            "Yangtian Zi",
            "Zixuan Wu",
            "Aleksander Boruch-Gruszecki",
            "Jonathan Bell",
            "Arjun Guha"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21891",
        "abstract": "Fine-tuning large language models for code editing has typically relied on mining commits and pull requests. The working hypothesis has been that commit messages describe human intent in natural language, and patches to code describe the changes that implement that intent. However, much of the previously collected data is noisy: commit messages are terse, human-written commits commingle several unrelated edits, and many commits come from simple, rule-based bots.\nThe recent adoption of software engineering agents changes this landscape. Code changes co-authored by humans and agents tend to be more narrowly scoped and focused on clearer goals. Their commit messages, generated by LLMs, articulate intent and rationale in much greater detail. Moreover, when these changes land in public repositories, they are implicitly filtered by humans: maintainers discard low-quality commits to their projects.\nWe present AgentPack, a corpus of 1.3M code edits co-authored by Claude Code, OpenAI Codex, and Cursor Agent across public GitHub projects up to mid-August 2025. We describe the identification and curation pipeline, quantify adoption trends of these agents, and analyze the structural properties of the edits. Finally, we show that models fine-tuned on AgentPack can outperform models trained on prior human-only commit corpora, highlighting the potential of using public data from software engineering agents to train future code-editing models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "161",
        "title": "Elastic MoE: Unlocking the Inference-Time Scalability of Mixture-of-Experts",
        "author": [
            "Naibin Gu",
            "Zhenyu Zhang",
            "Yuchen Feng",
            "Yilong Chen",
            "Peng Fu",
            "Zheng Lin",
            "Shuohuan Wang",
            "Yu Sun",
            "Hua Wu",
            "Weiping Wang",
            "Haifeng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21892",
        "abstract": "Mixture-of-Experts (MoE) models typically fix the number of activated experts $k$ at both training and inference. Intuitively, activating more experts at inference $k'$ (where $k'> k$) means engaging a larger set of model parameters for the computation and thus is expected to improve performance. However, contrary to this intuition, we find the scaling range to be so narrow that performance begins to degrade rapidly after only a slight increase in the number of experts. Further investigation reveals that this degradation stems from a lack of learned collaboration among experts. To address this, we introduce Elastic Mixture-of-Experts (EMoE), a novel training framework that enables MoE models to scale the number of activated experts at inference without incurring additional training overhead. By simultaneously training experts to collaborate in diverse combinations and encouraging the router for high-quality selections, EMoE ensures robust performance across computational budgets at inference. We conduct extensive experiments on various MoE settings. Our results show that EMoE significantly expands the effective performance-scaling range, extending it to as much as 2-3$\\times$ the training-time $k$, while also pushing the model's peak performance to a higher level.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "162",
        "title": "Syncphony: Synchronized Audio-to-Video Generation with Diffusion Transformers",
        "author": [
            "Jibin Song",
            "Mingi Kwon",
            "Jaeseok Jeong",
            "Youngjung Uh"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21893",
        "abstract": "Text-to-video and image-to-video generation have made rapid progress in visual quality, but they remain limited in controlling the precise timing of motion. In contrast, audio provides temporal cues aligned with video motion, making it a promising condition for temporally controlled video generation. However, existing audio-to-video (A2V) models struggle with fine-grained synchronization due to indirect conditioning mechanisms or limited temporal modeling capacity. We present Syncphony, which generates 380x640 resolution, 24fps videos synchronized with diverse audio inputs. Our approach builds upon a pre-trained video backbone and incorporates two key components to improve synchronization: (1) Motion-aware Loss, which emphasizes learning at high-motion regions; (2) Audio Sync Guidance, which guides the full model using a visually aligned off-sync model without audio layers to better exploit audio cues at inference while maintaining visual quality. To evaluate synchronization, we propose CycleSync, a video-to-audio-based metric that measures the amount of motion cues in the generated video to reconstruct the original audio. Experiments on AVSync15 and The Greatest Hits datasets demonstrate that Syncphony outperforms existing methods in both synchronization accuracy and visual quality. Project page is available at: https://jibin86.github.io/syncphony_project_page",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "163",
        "title": "Closing the Oracle Gap: Increment Vector Transformation for Class Incremental Learning",
        "author": [
            "Zihuan Qiu",
            "Yi Xu",
            "Fanman Meng",
            "Runtong Zhang",
            "Linfeng Xu",
            "Qingbo Wu",
            "Hongliang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21898",
        "abstract": "Class Incremental Learning (CIL) aims to sequentially acquire knowledge of new classes without forgetting previously learned ones. Despite recent progress, current CIL methods still exhibit significant performance gaps compared to their oracle counterparts-models trained with full access to historical data. Inspired by recent insights on Linear Mode Connectivity (LMC), we revisit the geometric properties of oracle solutions in CIL and uncover a fundamental observation: these oracle solutions typically maintain low-loss linear connections to the optimum of previous tasks. Motivated by this finding, we propose Increment Vector Transformation (IVT), a novel plug-and-play framework designed to mitigate catastrophic forgetting during training. Rather than directly following CIL updates, IVT periodically teleports the model parameters to transformed solutions that preserve linear connectivity to previous task optimum. By maintaining low-loss along these connecting paths, IVT effectively ensures stable performance on previously learned tasks. The transformation is efficiently approximated using diagonal Fisher Information Matrices, making IVT suitable for both exemplar-free and exemplar-based scenarios, and compatible with various initialization strategies. Extensive experiments on CIFAR-100, FGVCAircraft, ImageNet-Subset, and ImageNet-Full demonstrate that IVT consistently enhances the performance of strong CIL baselines. Specifically, on CIFAR-100, IVT improves the last accuracy of the PASS baseline by +5.12% and reduces forgetting by 2.54%. For the CLIP-pre-trained SLCA baseline on FGVCAircraft, IVT yields gains of +14.93% in average accuracy and +21.95% in last accuracy. The code will be released.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "164",
        "title": "TDEdit: A Unified Diffusion Framework for Text-Drag Guided Image Manipulation",
        "author": [
            "Qihang Wang",
            "Yaxiong Wang",
            "Lechao Cheng",
            "Zhun Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21905",
        "abstract": "This paper explores image editing under the joint control of text and drag interactions. While recent advances in text-driven and drag-driven editing have achieved remarkable progress, they suffer from complementary limitations: text-driven methods excel in texture manipulation but lack precise spatial control, whereas drag-driven approaches primarily modify shape and structure without fine-grained texture guidance. To address these limitations, we propose a unified diffusion-based framework for joint drag-text image editing, integrating the strengths of both paradigms. Our framework introduces two key innovations: (1) Point-Cloud Deterministic Drag, which enhances latent-space layout control through 3D feature mapping, and (2) Drag-Text Guided Denoising, dynamically balancing the influence of drag and text conditions during denoising. Notably, our model supports flexible editing modes - operating with text-only, drag-only, or combined conditions - while maintaining strong performance in each setting. Extensive quantitative and qualitative experiments demonstrate that our method not only achieves high-fidelity joint editing but also matches or surpasses the performance of specialized text-only or drag-only approaches, establishing a versatile and generalizable solution for controllable image manipulation. Code will be made publicly available to reproduce all results presented in this work.",
        "tags": [
            "3D",
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "165",
        "title": "A Large-Scale Dataset and Citation Intent Classification in Turkish with LLMs",
        "author": [
            "Kemal Sami Karaca",
            "Bahaeddin EravcÄ±"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21907",
        "abstract": "Understanding the qualitative intent of citations is essential for a comprehensive assessment of academic research, a task that poses unique challenges for agglutinative languages like Turkish. This paper introduces a systematic methodology and a foundational dataset to address this problem. We first present a new, publicly available dataset of Turkish citation intents, created with a purpose-built annotation tool. We then evaluate the performance of standard In-Context Learning (ICL) with Large Language Models (LLMs), demonstrating that its effectiveness is limited by inconsistent results caused by manually designed prompts. To address this core limitation, we introduce a programmable classification pipeline built on the DSPy framework, which automates prompt optimization systematically. For final classification, we employ a stacked generalization ensemble to aggregate outputs from multiple optimized models, ensuring stable and reliable predictions. This ensemble, with an XGBoost meta-model, achieves a state-of-the-art accuracy of 91.3\\%. Ultimately, this study provides the Turkish NLP community and the broader academic circles with a foundational dataset and a robust classification framework paving the way for future qualitative citation studies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "166",
        "title": "AutoSCORE: Enhancing Automated Scoring with Multi-Agent Large Language Models via Structured Component Recognition",
        "author": [
            "Yun Wang",
            "Zhaojun Ding",
            "Xuansheng Wu",
            "Siyue Sun",
            "Ninghao Liu",
            "Xiaoming Zhai"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21910",
        "abstract": "Automated scoring plays a crucial role in education by reducing the reliance on human raters, offering scalable and immediate evaluation of student work. While large language models (LLMs) have shown strong potential in this task, their use as end-to-end raters faces challenges such as low accuracy, prompt sensitivity, limited interpretability, and rubric misalignment. These issues hinder the implementation of LLM-based automated scoring in assessment practice. To address the limitations, we propose AutoSCORE, a multi-agent LLM framework enhancing automated scoring via rubric-aligned Structured COmponent REcognition. With two agents, AutoSCORE first extracts rubric-relevant components from student responses and encodes them into a structured representation (i.e., Scoring Rubric Component Extraction Agent), which is then used to assign final scores (i.e., Scoring Agent). This design ensures that model reasoning follows a human-like grading process, enhancing interpretability and robustness. We evaluate AutoSCORE on four benchmark datasets from the ASAP benchmark, using both proprietary and open-source LLMs (GPT-4o, LLaMA-3.1-8B, and LLaMA-3.1-70B). Across diverse tasks and rubrics, AutoSCORE consistently improves scoring accuracy, human-machine agreement (QWK, correlations), and error metrics (MAE, RMSE) compared to single-agent baselines, with particularly strong benefits on complex, multi-dimensional rubrics, and especially large relative gains on smaller LLMs. These results demonstrate that structured component recognition combined with multi-agent design offers a scalable, reliable, and interpretable solution for automated scoring.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "167",
        "title": "Discrete Guidance Matching: Exact Guidance for Discrete Flow Matching",
        "author": [
            "Zhengyan Wan",
            "Yidong Ouyang",
            "Liyan Xie",
            "Fang Fang",
            "Hongyuan Zha",
            "Guang Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21912",
        "abstract": "Guidance provides a simple and effective framework for posterior sampling by steering the generation process towards the desired distribution. When modeling discrete data, existing approaches mostly focus on guidance with the first-order Taylor approximation to improve the sampling efficiency. However, such an approximation is inappropriate in discrete state spaces since the approximation error could be large. A novel guidance framework for discrete data is proposed to address this problem: We derive the exact transition rate for the desired distribution given a learned discrete flow matching model, leading to guidance that only requires a single forward pass in each sampling step, significantly improving efficiency. This unified novel framework is general enough, encompassing existing guidance methods as special cases, and it can also be seamlessly applied to the masked diffusion model. We demonstrate the effectiveness of our proposed guidance on energy-guided simulations and preference alignment on text-to-image generation and multimodal understanding tasks. The code is available through https://github.com/WanZhengyan/Discrete-Guidance-Matching/tree/main.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "Text-to-Image"
        ]
    },
    {
        "id": "168",
        "title": "Taming Flow-based I2V Models for Creative Video Editing",
        "author": [
            "Xianghao Kong",
            "Hansheng Chen",
            "Yuwei Guo",
            "Lvmin Zhang",
            "Gordon Wetzstein",
            "Maneesh Agrawala",
            "Anyi Rao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21917",
        "abstract": "Although image editing techniques have advanced significantly, video editing, which aims to manipulate videos according to user intent, remains an emerging challenge. Most existing image-conditioned video editing methods either require inversion with model-specific design or need extensive optimization, limiting their capability of leveraging up-to-date image-to-video (I2V) models to transfer the editing capability of image editing models to the video domain. To this end, we propose IF-V2V, an Inversion-Free method that can adapt off-the-shelf flow-matching-based I2V models for video editing without significant computational overhead. To circumvent inversion, we devise Vector Field Rectification with Sample Deviation to incorporate information from the source video into the denoising process by introducing a deviation term into the denoising vector field. To further ensure consistency with the source video in a model-agnostic way, we introduce Structure-and-Motion-Preserving Initialization to generate motion-aware temporally correlated noise with structural information embedded. We also present a Deviation Caching mechanism to minimize the additional computational cost for denoising vector rectification without significantly impacting editing quality. Evaluations demonstrate that our method achieves superior editing quality and consistency over existing approaches, offering a lightweight plug-and-play solution to realize visual creativity.",
        "tags": [
            "Flow Matching",
            "Image Editing",
            "Video Editing"
        ]
    },
    {
        "id": "169",
        "title": "Spatial Reasoning in Foundation Models: Benchmarking Object-Centric Spatial Understanding",
        "author": [
            "Vahid Mirjalili",
            "Ramin Giahi",
            "Sriram Kollipara",
            "Akshay Kekuda",
            "Kehui Yao",
            "Kai Zhao",
            "Jianpeng Xu",
            "Kaushiki Nag",
            "Sinduja Subramaniam",
            "Topojoy Biswas",
            "Evren Korpeoglu",
            "Kannan Achan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21922",
        "abstract": "Spatial understanding is a critical capability for vision foundation models. While recent advances in large vision models or vision-language models (VLMs) have expanded recognition capabilities, most benchmarks emphasize localization accuracy rather than whether models capture how objects are arranged and related within a scene. This gap is consequential; effective scene understanding requires not only identifying objects, but reasoning about their relative positions, groupings, and depth. In this paper, we present a systematic benchmark for object-centric spatial reasoning in foundation models. Using a controlled synthetic dataset, we evaluate state-of-the-art vision models (e.g., GroundingDINO, Florence-2, OWLv2) and large VLMs (e.g., InternVL, LLaVA, GPT-4o) across three tasks: spatial localization, spatial reasoning, and downstream retrieval tasks. We find a stable trade-off: detectors such as GroundingDINO and OWLv2 deliver precise boxes with limited relational reasoning, while VLMs like SmolVLM and GPT-4o provide coarse layout cues and fluent captions but struggle with fine-grained spatial context. Our study highlights the gap between localization and true spatial understanding, and pointing toward the need for spatially-aware foundation models in the community.",
        "tags": [
            "GPT",
            "LLaVA",
            "VLM"
        ]
    },
    {
        "id": "170",
        "title": "SAGE: Scene Graph-Aware Guidance and Execution for Long-Horizon Manipulation Tasks",
        "author": [
            "Jialiang Li",
            "Wenzheng Wu",
            "Gaojing Zhang",
            "Yifan Han",
            "Wenzhao Lian"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21928",
        "abstract": "Successfully solving long-horizon manipulation tasks remains a fundamental challenge. These tasks involve extended action sequences and complex object interactions, presenting a critical gap between high-level symbolic planning and low-level continuous control. To bridge this gap, two essential capabilities are required: robust long-horizon task planning and effective goal-conditioned manipulation. Existing task planning methods, including traditional and LLM-based approaches, often exhibit limited generalization or sparse semantic reasoning. Meanwhile, image-conditioned control methods struggle to adapt to unseen tasks. To tackle these problems, we propose SAGE, a novel framework for Scene Graph-Aware Guidance and Execution in Long-Horizon Manipulation Tasks. SAGE utilizes semantic scene graphs as a structural representation for scene states. A structural scene graph enables bridging task-level semantic reasoning and pixel-level visuo-motor control. This also facilitates the controllable synthesis of accurate, novel sub-goal images. SAGE consists of two key components: (1) a scene graph-based task planner that uses VLMs and LLMs to parse the environment and reason about physically-grounded scene state transition sequences, and (2) a decoupled structural image editing pipeline that controllably converts each target sub-goal graph into a corresponding image through image inpainting and composition. Extensive experiments have demonstrated that SAGE achieves state-of-the-art performance on distinct long-horizon tasks.",
        "tags": [
            "Image Editing",
            "Inpainting",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "171",
        "title": "DynaNav: Dynamic Feature and Layer Selection for Efficient Visual Navigation",
        "author": [
            "Jiahui Wang",
            "Changhao Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21930",
        "abstract": "Visual navigation is essential for robotics and embodied AI. However, existing foundation models, particularly those with transformer decoders, suffer from high computational overhead and lack interpretability, limiting their deployment in resource-tight scenarios. To address this, we propose DynaNav, a Dynamic Visual Navigation framework that adapts feature and layer selection based on scene complexity. It employs a trainable hard feature selector for sparse operations, enhancing efficiency and interpretability. Additionally, we integrate feature selection into an early-exit mechanism, with Bayesian Optimization determining optimal exit thresholds to reduce computational cost. Extensive experiments in real-world-based datasets and simulated environments demonstrate the effectiveness of DynaNav. Compared to ViNT, DynaNav achieves a 2.26x reduction in FLOPs, 42.3% lower inference time, and 32.8% lower memory usage, while improving navigation performance across four public datasets.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "172",
        "title": "SimulSense: Sense-Driven Interpreting for Efficient Simultaneous Speech Translation",
        "author": [
            "Haotian Tan",
            "Hiroki Ouchi",
            "Sakriani Sakti"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21932",
        "abstract": "How to make human-interpreter-like read/write decisions for simultaneous speech translation (SimulST) systems? Current state-of-the-art systems formulate SimulST as a multi-turn dialogue task, requiring specialized interleaved training data and relying on computationally expensive large language model (LLM) inference for decision-making. In this paper, we propose SimulSense, a novel framework for SimulST that mimics human interpreters by continuously reading input speech and triggering write decisions to produce translation when a new sense unit is perceived. Experiments against two state-of-the-art baseline systems demonstrate that our proposed method achieves a superior quality-latency tradeoff and substantially improved real-time efficiency, where its decision-making is up to 9.6x faster than the baselines.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "173",
        "title": "Statistical Advantage of Softmax Attention: Insights from Single-Location Regression",
        "author": [
            "O. Duranthon",
            "P. Marion",
            "C. Boyer",
            "B. Loureiro",
            "L. ZdeborovÃ¡"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21936",
        "abstract": "Large language models rely on attention mechanisms with a softmax activation. Yet the dominance of softmax over alternatives (e.g., component-wise or linear) remains poorly understood, and many theoretical works have focused on the easier-to-analyze linearized attention. In this work, we address this gap through a principled study of the single-location regression task, where the output depends on a linear transformation of a single input token at a random location. Building on ideas from statistical physics, we develop an analysis of attention-based predictors in the high-dimensional limit, where generalization performance is captured by a small set of order parameters. At the population level, we show that softmax achieves the Bayes risk, whereas linear attention fundamentally falls short. We then examine other activation functions to identify which properties are necessary for optimal performance. Finally, we analyze the finite-sample regime: we provide an asymptotic characterization of the test error and show that, while softmax is no longer Bayes-optimal, it consistently outperforms linear attention. We discuss the connection with optimization by gradient-based algorithms.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "174",
        "title": "SemanticControl: A Training-Free Approach for Handling Loosely Aligned Visual Conditions in ControlNet",
        "author": [
            "Woosung Joung",
            "Daewon Chae",
            "Jinkyu Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21938",
        "abstract": "ControlNet has enabled detailed spatial control in text-to-image diffusion models by incorporating additional visual conditions such as depth or edge maps. However, its effectiveness heavily depends on the availability of visual conditions that are precisely aligned with the generation goal specified by text prompt-a requirement that often fails in practice, especially for uncommon or imaginative scenes. For example, generating an image of a cat cooking in a specific pose may be infeasible due to the lack of suitable visual conditions. In contrast, structurally similar cues can often be found in more common settings-for instance, poses of humans cooking are widely available and can serve as rough visual guides. Unfortunately, existing ControlNet models struggle to use such loosely aligned visual conditions, often resulting in low text fidelity or visual artifacts. To address this limitation, we propose SemanticControl, a training-free method for effectively leveraging misaligned but semantically relevant visual conditions. Our approach adaptively suppresses the influence of the visual condition where it conflicts with the prompt, while strengthening guidance from the text. The key idea is to first run an auxiliary denoising process using a surrogate prompt aligned with the visual condition (e.g., \"a human playing guitar\" for a human pose condition) to extract informative attention masks, and then utilize these masks during the denoising of the actual target prompt (e.g., cat playing guitar). Experimental results demonstrate that our method improves performance under loosely aligned conditions across various conditions, including depth maps, edge maps, and human skeletons, outperforming existing baselines. Our code is available at https://mung3477.github.io/semantic-control.",
        "tags": [
            "ControlNet",
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "175",
        "title": "Structural Information-based Hierarchical Diffusion for Offline Reinforcement Learning",
        "author": [
            "Xianghua Zeng",
            "Hao Peng",
            "Angsheng Li",
            "Yicheng Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21942",
        "abstract": "Diffusion-based generative methods have shown promising potential for modeling trajectories from offline reinforcement learning (RL) datasets, and hierarchical diffusion has been introduced to mitigate variance accumulation and computational challenges in long-horizon planning tasks. However, existing approaches typically assume a fixed two-layer diffusion hierarchy with a single predefined temporal scale, which limits adaptability to diverse downstream tasks and reduces flexibility in decision making. In this work, we propose SIHD, a novel Structural Information-based Hierarchical Diffusion framework for effective and stable offline policy learning in long-horizon environments with sparse rewards. Specifically, we analyze structural information embedded in offline trajectories to construct the diffusion hierarchy adaptively, enabling flexible trajectory modeling across multiple temporal scales. Rather than relying on reward predictions from localized sub-trajectories, we quantify the structural information gain of each state community and use it as a conditioning signal within the corresponding diffusion layer. To reduce overreliance on offline datasets, we introduce a structural entropy regularizer that encourages exploration of underrepresented states while avoiding extrapolation errors from distributional shifts. Extensive evaluations on challenging offline RL tasks show that SIHD significantly outperforms state-of-the-art baselines in decision-making performance and demonstrates superior generalization across diverse scenarios.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "176",
        "title": "Debiasing Large Language Models in Thai Political Stance Detection via Counterfactual Calibration",
        "author": [
            "Kasidit Sermsri",
            "Teerapong Panboonyuen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21946",
        "abstract": "Political stance detection in low-resource and culturally complex settings poses a critical challenge for large language models (LLMs). In the Thai political landscape - marked by indirect language, polarized figures, and entangled sentiment and stance - LLMs often display systematic biases such as sentiment leakage and favoritism toward entities. These biases undermine fairness and reliability. We present ThaiFACTUAL, a lightweight, model-agnostic calibration framework that mitigates political bias without requiring fine-tuning. ThaiFACTUAL uses counterfactual data augmentation and rationale-based supervision to disentangle sentiment from stance and reduce bias. We also release the first high-quality Thai political stance dataset, annotated with stance, sentiment, rationales, and bias markers across diverse entities and events. Experimental results show that ThaiFACTUAL significantly reduces spurious correlations, enhances zero-shot generalization, and improves fairness across multiple LLMs. This work highlights the importance of culturally grounded debiasing techniques for underrepresented languages.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "177",
        "title": "Active Attacks: Red-teaming LLMs via Adaptive Environments",
        "author": [
            "Taeyoung Yun",
            "Pierre-Luc St-Charles",
            "Jinkyoo Park",
            "Yoshua Bengio",
            "Minsu Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21947",
        "abstract": "We address the challenge of generating diverse attack prompts for large language models (LLMs) that elicit harmful behaviors (e.g., insults, sexual content) and are used for safety fine-tuning. Rather than relying on manual prompt engineering, attacker LLMs can be trained with reinforcement learning (RL) to automatically generate such prompts using only a toxicity classifier as a reward. However, capturing a wide range of harmful behaviors is a significant challenge that requires explicit diversity objectives. Existing diversity-seeking RL methods often collapse to limited modes: once high-reward prompts are found, exploration of new regions is discouraged. Inspired by the active learning paradigm that encourages adaptive exploration, we introduce \\textit{Active Attacks}, a novel RL-based red-teaming algorithm that adapts its attacks as the victim evolves. By periodically safety fine-tuning the victim LLM with collected attack prompts, rewards in exploited regions diminish, which forces the attacker to seek unexplored vulnerabilities. This process naturally induces an easy-to-hard exploration curriculum, where the attacker progresses beyond easy modes toward increasingly difficult ones. As a result, Active Attacks uncovers a wide range of local attack modes step by step, and their combination achieves wide coverage of the multi-mode distribution. Active Attacks, a simple plug-and-play module that seamlessly integrates into existing RL objectives, unexpectedly outperformed prior RL-based methods -- including GFlowNets, PPO, and REINFORCE -- by improving cross-attack success rates against GFlowNets, the previous state-of-the-art, from 0.07% to 31.28% (a relative gain greater than $400\\ \\times$) with only a 6% increase in computation. Our code is publicly available \\href{https://github.com/dbsxodud-11/active_attacks}{here}.",
        "tags": [
            "LLM",
            "PPO",
            "RL"
        ]
    },
    {
        "id": "178",
        "title": "Evaluating Open-Source Large Language Models for Technical Telecom Question Answering",
        "author": [
            "Arina Caraus",
            "Alessio Buscemi",
            "Sumit Kumar",
            "Ion Turcanu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21949",
        "abstract": "Large Language Models (LLMs) have shown remarkable capabilities across various fields. However, their performance in technical domains such as telecommunications remains underexplored. This paper evaluates two open-source LLMs, Gemma 3 27B and DeepSeek R1 32B, on factual and reasoning-based questions derived from advanced wireless communications material. We construct a benchmark of 105 question-answer pairs and assess performance using lexical metrics, semantic similarity, and LLM-as-a-judge scoring. We also analyze consistency, judgment reliability, and hallucination through source attribution and score variance. Results show that Gemma excels in semantic fidelity and LLM-rated correctness, while DeepSeek demonstrates slightly higher lexical consistency. Additional findings highlight current limitations in telecom applications and the need for domain-adapted models to support trustworthy Artificial Intelligence (AI) assistants in engineering.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "179",
        "title": "Customizing Visual Emotion Evaluation for MLLMs: An Open-vocabulary, Multifaceted, and Scalable Approach",
        "author": [
            "Daiqing Wu",
            "Dongbao Yang",
            "Sicheng Zhao",
            "Can Ma",
            "Yu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21950",
        "abstract": "Recently, Multimodal Large Language Models (MLLMs) have achieved exceptional performance across diverse tasks, continually surpassing previous expectations regarding their capabilities. Nevertheless, their proficiency in perceiving emotions from images remains debated, with studies yielding divergent results in zero-shot scenarios. We argue that this inconsistency stems partly from constraints in existing evaluation methods, including the oversight of plausible responses, limited emotional taxonomies, neglect of contextual factors, and labor-intensive annotations. To facilitate customized visual emotion evaluation for MLLMs, we propose an Emotion Statement Judgment task that overcomes these constraints. Complementing this task, we devise an automated pipeline that efficiently constructs emotion-centric statements with minimal human effort. Through systematically evaluating prevailing MLLMs, our study showcases their stronger performance in emotion interpretation and context-based emotion judgment, while revealing relative limitations in comprehending perception subjectivity. When compared to humans, even top-performing MLLMs like GPT4o demonstrate remarkable performance gaps, underscoring key areas for future improvement. By developing a fundamental evaluation framework and conducting a comprehensive MLLM assessment, we hope this work contributes to advancing emotional intelligence in MLLMs. Project page: https://github.com/wdqqdw/MVEI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "180",
        "title": "MultiCrafter: High-Fidelity Multi-Subject Generation via Spatially Disentangled Attention and Identity-Aware Reinforcement Learning",
        "author": [
            "Tao Wu",
            "Yibo Jiang",
            "Yehao Lu",
            "Zhizhong Wang",
            "Zeyi Huang",
            "Zequn Qin",
            "Xi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21953",
        "abstract": "Multi-subject image generation aims to synthesize user-provided subjects in a single image while preserving subject fidelity, ensuring prompt consistency, and aligning with human aesthetic preferences. However, existing methods, particularly those built on the In-Context-Learning paradigm, are limited by their reliance on simple reconstruction-based objectives, leading to both severe attribute leakage that compromises subject fidelity and failing to align with nuanced human preferences. To address this, we propose MultiCrafter, a framework that ensures high-fidelity, preference-aligned generation. First, we find that the root cause of attribute leakage is a significant entanglement of attention between different subjects during the generation process. Therefore, we introduce explicit positional supervision to explicitly separate attention regions for each subject, effectively mitigating attribute leakage. To enable the model to accurately plan the attention region of different subjects in diverse scenarios, we employ a Mixture-of-Experts architecture to enhance the model's capacity, allowing different experts to focus on different scenarios. Finally, we design a novel online reinforcement learning framework to align the model with human preferences, featuring a scoring mechanism to accurately assess multi-subject fidelity and a more stable training strategy tailored for the MoE architecture. Experiments validate that our framework significantly improves subject fidelity while aligning with human preferences better.",
        "tags": [
            "MoE",
            "RL"
        ]
    },
    {
        "id": "181",
        "title": "Learnable Conformal Prediction with Context-Aware Nonconformity Functions for Robotic Planning and Perception",
        "author": [
            "Divake Kumar",
            "Sina Tayebati",
            "Francesco Migliarba",
            "Ranganath Krishnan",
            "Amit Ranjan Trivedi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21955",
        "abstract": "Deep learning models in robotics often output point estimates with poorly calibrated confidences, offering no native mechanism to quantify predictive reliability under novel, noisy, or out-of-distribution inputs. Conformal prediction (CP) addresses this gap by providing distribution-free coverage guarantees, yet its reliance on fixed nonconformity scores ignores context and can yield intervals that are overly conservative or unsafe. We address this with Learnable Conformal Prediction (LCP), which replaces fixed scores with a lightweight neural function that leverages geometric, semantic, and task-specific features to produce context-aware uncertainty sets.\nLCP maintains CP's theoretical guarantees while reducing prediction set sizes by 18% in classification, tightening detection intervals by 52%, and improving path planning safety from 72% to 91% success with minimal overhead. Across three robotic tasks on seven benchmarks, LCP consistently outperforms Standard CP and ensemble baselines. In classification on CIFAR-100 and ImageNet, it achieves smaller set sizes (4.7-9.9% reduction) at target coverage. For object detection on COCO, BDD100K, and Cityscapes, it produces 46-54% tighter bounding boxes. In path planning through cluttered environments, it improves success to 91.5% with only 4.5% path inflation, compared to 12.2% for Standard CP.\nThe method is lightweight (approximately 4.8% runtime overhead, 42 KB memory) and supports online adaptation, making it well suited to resource-constrained autonomous systems. Hardware evaluation shows LCP adds less than 1% memory and 15.9% inference overhead, yet sustains 39 FPS on detection tasks while being 7.4 times more energy-efficient than ensembles.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "182",
        "title": "Think Smart, Not Hard: Difficulty Adaptive Reasoning for Large Audio Language Models",
        "author": [
            "Zhichao Sheng",
            "Shilin Zhou",
            "Chen Gong",
            "Zhenghua Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21960",
        "abstract": "Large Audio Language Models (LALMs), powered by the chain-of-thought (CoT) paradigm, have shown remarkable reasoning capabilities. Intuitively, different problems often require varying depths of reasoning. While some methods can determine whether to reason for a given problem, they typically lack a fine-grained mechanism to modulate how much to reason. This often results in a ``one-size-fits-all'' reasoning depth, which generates redundant overthinking for simple questions while failing to allocate sufficient thought to complex ones. In this paper, we conduct an in-depth analysis of LALMs and find that an effective and efficient LALM should reason smartly by adapting its reasoning depth to the problem's complexity. To achieve this, we propose a difficulty-adaptive reasoning method for LALMs. Specifically, we propose a reward function that dynamically links reasoning length to the model's perceived problem difficulty. This reward encourages shorter, concise reasoning for easy tasks and more elaborate, in-depth reasoning for complex ones. Extensive experiments demonstrate that our method is both effective and efficient, simultaneously improving task performance and significantly reducing the average reasoning length. Further analysis on reasoning structure paradigm offers valuable insights for future work.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "183",
        "title": "FlowDrive: moderated flow matching with data balancing for trajectory planning",
        "author": [
            "Lingguang Wang",
            "Ãmer Åahin TaÅ",
            "Marlon Steiner",
            "Christoph Stiller"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21961",
        "abstract": "Learning-based planners are sensitive to the long-tailed distribution of driving data. Common maneuvers dominate datasets, while dangerous or rare scenarios are sparse. This imbalance can bias models toward the frequent cases and degrade performance on critical scenarios. To tackle this problem, we compare balancing strategies for sampling training data and find reweighting by trajectory pattern an effective approach. We then present FlowDrive, a flow-matching trajectory planner that learns a conditional rectified flow to map noise directly to trajectory distributions with few flow-matching steps. We further introduce moderated, in-the-loop guidance that injects small perturbation between flow steps to systematically increase trajectory diversity while remaining scene-consistent. On nuPlan and the interaction-focused interPlan benchmarks, FlowDrive achieves state-of-the-art results among learning-based planners and approaches methods with rule-based refinements. After adding moderated guidance and light post-processing (FlowDrive*), it achieves overall state-of-the-art performance across nearly all benchmark splits.",
        "tags": [
            "Flow Matching",
            "Rectified Flow"
        ]
    },
    {
        "id": "184",
        "title": "PartSAM: A Scalable Promptable Part Segmentation Model Trained on Native 3D Data",
        "author": [
            "Zhe Zhu",
            "Le Wan",
            "Rui Xu",
            "Yiheng Zhang",
            "Honghua Chen",
            "Zhiyang Dou",
            "Cheng Lin",
            "Yuan Liu",
            "Mingqiang Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21965",
        "abstract": "Segmenting 3D objects into parts is a long-standing challenge in computer vision. To overcome taxonomy constraints and generalize to unseen 3D objects, recent works turn to open-world part segmentation. These approaches typically transfer supervision from 2D foundation models, such as SAM, by lifting multi-view masks into 3D. However, this indirect paradigm fails to capture intrinsic geometry, leading to surface-only understanding, uncontrolled decomposition, and limited generalization. We present PartSAM, the first promptable part segmentation model trained natively on large-scale 3D data. Following the design philosophy of SAM, PartSAM employs an encoder-decoder architecture in which a triplane-based dual-branch encoder produces spatially structured tokens for scalable part-aware representation learning. To enable large-scale supervision, we further introduce a model-in-the-loop annotation pipeline that curates over five million 3D shape-part pairs from online assets, providing diverse and fine-grained labels. This combination of scalable architecture and diverse 3D data yields emergent open-world capabilities: with a single prompt, PartSAM achieves highly accurate part identification, and in a Segment-Every-Part mode, it automatically decomposes shapes into both surface and internal structures. Extensive experiments show that PartSAM outperforms state-of-the-art methods by large margins across multiple benchmarks, marking a decisive step toward foundation models for 3D part understanding. Our code and model will be released soon.",
        "tags": [
            "3D",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "185",
        "title": "From Superficial Outputs to Superficial Learning: Risks of Large Language Models in Education",
        "author": [
            "Iris Delikoura",
            "Yi.R",
            "Fung",
            "Pan Hui"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21972",
        "abstract": "Large Language Models (LLMs) are transforming education by enabling personalization, feedback, and knowledge access, while also raising concerns about risks to students and learning systems. Yet empirical evidence on these risks remains fragmented. This paper presents a systematic review of 70 empirical studies across computer science, education, and psychology. Guided by four research questions, we examine: (i) which applications of LLMs in education have been most frequently explored; (ii) how researchers have measured their impact; (iii) which risks stem from such applications; and (iv) what mitigation strategies have been proposed. We find that research on LLMs clusters around three domains: operational effectiveness, personalized applications, and interactive learning tools. Across these, model-level risks include superficial understanding, bias, limited robustness, anthropomorphism, hallucinations, privacy concerns, and knowledge constraints. When learners interact with LLMs, these risks extend to cognitive and behavioural outcomes, including reduced neural activity, over-reliance, diminished independent learning skills, and a loss of student agency. To capture this progression, we propose an LLM-Risk Adapted Learning Model that illustrates how technical risks cascade through interaction and interpretation to shape educational outcomes. As the first synthesis of empirically assessed risks, this review provides a foundation for responsible, human-centred integration of LLMs in education.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "186",
        "title": "MotivGraph-SoIQ: Integrating Motivational Knowledge Graphs and Socratic Dialogue for Enhanced LLM Ideation",
        "author": [
            "Xinping Lei",
            "Tong Zhou",
            "Yubo Chen",
            "Kang Liu",
            "Jun Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21978",
        "abstract": "Large Language Models (LLMs) hold substantial potential for accelerating academic ideation but face critical challenges in grounding ideas and mitigating confirmation bias for further refinement. We propose integrating motivational knowledge graphs and socratic dialogue to address these limitations in enhanced LLM ideation (MotivGraph-SoIQ). This novel framework provides essential grounding and practical idea improvement steps for LLM ideation by integrating a Motivational Knowledge Graph (MotivGraph) with a Q-Driven Socratic Ideator. The MotivGraph structurally stores three key node types(problem, challenge and solution) to offer motivation grounding for the LLM ideation process. The Ideator is a dual-agent system utilizing Socratic questioning, which facilitates a rigorous refinement process that mitigates confirmation bias and improves idea quality across novelty, experimental rigor, and motivational rationality dimensions. On the ICLR25 paper topics dataset, MotivGraph-SoIQ exhibits clear advantages over existing state-of-the-art approaches across LLM-based scoring, ELO ranking, and human evaluation metrics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "187",
        "title": "Resolving Ambiguity in Gaze-Facilitated Visual Assistant Interaction Paradigm",
        "author": [
            "Zeyu Wang",
            "Baiyu Chen",
            "Kun Yan",
            "Hongjing Piao",
            "Hao Xue",
            "Flora D. Salim",
            "Yuanchun Shi",
            "Yuntao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21980",
        "abstract": "With the rise in popularity of smart glasses, users' attention has been integrated into Vision-Language Models (VLMs) to streamline multi-modal querying in daily scenarios. However, leveraging gaze data to model users' attention may introduce ambiguity challenges: (1) users' verbal questions become ambiguous by using pronouns or skipping context, (2) humans' gaze patterns can be noisy and exhibit complex spatiotemporal relationships with their spoken questions. Previous works only consider single image as visual modality input, failing to capture the dynamic nature of the user's attention. In this work, we introduce GLARIFY, a novel method to leverage spatiotemporal gaze information to enhance the model's effectiveness in real-world applications. Initially, we analyzed hundreds of querying samples with the gaze modality to demonstrate the noisy nature of users' gaze patterns. We then utilized GPT-4o to design an automatic data synthesis pipeline to generate the GLARIFY-Ambi dataset, which includes a dedicated chain-of-thought (CoT) process to handle noisy gaze patterns. Finally, we designed a heatmap module to incorporate gaze information into cutting-edge VLMs while preserving their pretrained knowledge. We evaluated GLARIFY using a hold-out test set. Experiments demonstrate that GLARIFY significantly outperforms baselines. By robustly aligning VLMs with human attention, GLARIFY paves the way for a usable and intuitive interaction paradigm with a visual assistant.",
        "tags": [
            "CoT",
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "188",
        "title": "CoBel-World: Harnessing LLM Reasoning to Build a Collaborative Belief World for Optimizing Embodied Multi-Agent Collaboration",
        "author": [
            "Zhimin Wang",
            "Shaokang He",
            "Duo Wu",
            "Jinghe Wang",
            "Linjia Kang",
            "Jing Yu",
            "Zhi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21981",
        "abstract": "Effective real-world multi-agent collaboration requires not only accurate planning but also the ability to reason about collaborators' intents -- a crucial capability for avoiding miscoordination and redundant communication under partial observable environments. Due to their strong planning and reasoning capabilities, large language models (LLMs) have emerged as promising autonomous agents for collaborative task solving. However, existing collaboration frameworks for LLMs overlook their reasoning potential for dynamic intent inference, and thus produce inconsistent plans and redundant communication, reducing collaboration efficiency. To bridge this gap, we propose CoBel-World, a novel framework that equips LLM agents with a collaborative belief world -- an internal representation jointly modeling the physical environment and collaborators' mental states. CoBel-World enables agents to parse open-world task knowledge into structured beliefs via a symbolic belief language, and perform zero-shot Bayesian-style belief updates through LLM reasoning. This allows agents to proactively detect potential miscoordination (e.g., conflicting plans) and communicate adaptively. Evaluated on challenging embodied benchmarks (i.e., TDW-MAT and C-WAH), CoBel-World significantly reduces communication costs by 22-60% and improves task completion efficiency by 4-28% compared to the strongest baseline. Our results show that explicit, intent-aware belief modeling is essential for efficient and human-like collaboration in LLM-based multi-agent systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "189",
        "title": "Hybrid Diffusion for Simultaneous Symbolic and Continuous Planning",
        "author": [
            "Sigmund Hennum HÃ¸eg",
            "Aksel Vaaler",
            "Chaoqi Liu",
            "Olav Egeland",
            "Yilun Du"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21983",
        "abstract": "Constructing robots to accomplish long-horizon tasks is a long-standing challenge within artificial intelligence. Approaches using generative methods, particularly Diffusion Models, have gained attention due to their ability to model continuous robotic trajectories for planning and control. However, we show that these models struggle with long-horizon tasks that involve complex decision-making and, in general, are prone to confusing different modes of behavior, leading to failure. To remedy this, we propose to augment continuous trajectory generation by simultaneously generating a high-level symbolic plan. We show that this requires a novel mix of discrete variable diffusion and continuous diffusion, which dramatically outperforms the baselines. In addition, we illustrate how this hybrid diffusion process enables flexible trajectory synthesis, allowing us to condition synthesized actions on partial and complete symbolic conditions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "190",
        "title": "From Bias to Balance: Exploring and Mitigating Spatial Bias in LVLMs",
        "author": [
            "Yingjie Zhu",
            "Xuefeng Bai",
            "Kehai Chen",
            "Yang Xiang",
            "Weili Guan",
            "Jun Yu",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21984",
        "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable success across a wide range of multimodal tasks, yet their robustness to spatial variations remains insufficiently understood. In this work, we present a systematic study of the spatial bias of LVLMs, focusing on how models respond when identical key visual information is placed at different locations within an image. Through a carefully designed probing dataset, we demonstrate that current LVLMs often produce inconsistent outputs under such spatial shifts, revealing a fundamental limitation in their spatial-semantic understanding. Further analysis shows that this phenomenon originates not from the vision encoder, which reliably perceives and interprets visual content across positions, but from the unbalanced design of position embeddings in the language model component. In particular, the widely adopted position embedding strategies, such as RoPE, introduce imbalance during cross-modal interaction, leading image tokens at different positions to exert unequal influence on semantic understanding. To mitigate this issue, we introduce Balanced Position Assignment (BaPA), a simple yet effective mechanism that assigns identical position embeddings to all image tokens, promoting a more balanced integration of visual information. Extensive experiments show that BaPA enhances the spatial robustness of LVLMs without retraining and further boosts their performance across diverse multimodal benchmarks when combined with lightweight fine-tuning. Further analysis of information flow reveals that BaPA yields balanced attention, enabling more holistic visual understanding.",
        "tags": [
            "RoPE",
            "VLM"
        ]
    },
    {
        "id": "191",
        "title": "Developing Vision-Language-Action Model from Egocentric Videos",
        "author": [
            "Tomoya Yoshida",
            "Shuhei Kurita",
            "Taichi Nishimura",
            "Shinsuke Mori"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21986",
        "abstract": "Egocentric videos capture how humans manipulate objects and tools, providing diverse motion cues for learning object manipulation. Unlike the costly, expert-driven manual teleoperation commonly used in training Vision-Language-Action models (VLAs), egocentric videos offer a scalable alternative. However, prior studies that leverage such videos for training robot policies typically rely on auxiliary annotations, such as detailed hand-pose recordings. Consequently, it remains unclear whether VLAs can be trained directly from raw egocentric videos. In this work, we address this challenge by leveraging EgoScaler, a framework that extracts 6DoF object manipulation trajectories from egocentric videos without requiring auxiliary recordings. We apply EgoScaler to four large-scale egocentric video datasets and automatically refine noisy or incomplete trajectories, thereby constructing a new large-scale dataset for VLA pre-training. Our experiments with a state-of-the-art $\\pi_0$ architecture in both simulated and real-robot environments yield three key findings: (i) pre-training on our dataset improves task success rates by over 20\\% compared to training from scratch, (ii) the performance is competitive with that achieved using real-robot datasets, and (iii) combining our dataset with real-robot data yields further improvements. These results demonstrate that egocentric videos constitute a promising and scalable resource for advancing VLA research.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "192",
        "title": "Mind-the-Glitch: Visual Correspondence for Detecting Inconsistencies in Subject-Driven Generation",
        "author": [
            "Abdelrahman Eldesokey",
            "Aleksandar Cvejic",
            "Bernard Ghanem",
            "Peter Wonka"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21989",
        "abstract": "We propose a novel approach for disentangling visual and semantic features from the backbones of pre-trained diffusion models, enabling visual correspondence in a manner analogous to the well-established semantic correspondence. While diffusion model backbones are known to encode semantically rich features, they must also contain visual features to support their image synthesis capabilities. However, isolating these visual features is challenging due to the absence of annotated datasets. To address this, we introduce an automated pipeline that constructs image pairs with annotated semantic and visual correspondences based on existing subject-driven image generation datasets, and design a contrastive architecture to separate the two feature types. Leveraging the disentangled representations, we propose a new metric, Visual Semantic Matching (VSM), that quantifies visual inconsistencies in subject-driven image generation. Empirical results show that our approach outperforms global feature-based metrics such as CLIP, DINO, and vision--language models in quantifying visual inconsistencies while also enabling spatial localization of inconsistent regions. To our knowledge, this is the first method that supports both quantification and localization of inconsistencies in subject-driven generation, offering a valuable tool for advancing this task. Project Page:https://abdo-eldesokey.github.io/mind-the-glitch/",
        "tags": [
            "CLIP",
            "Diffusion",
            "VLM"
        ]
    },
    {
        "id": "193",
        "title": "WAVE: Learning Unified & Versatile Audio-Visual Embeddings with Multimodal LLM",
        "author": [
            "Changli Tang",
            "Qinfan Xiao",
            "Ke Mei",
            "Tianyi Wang",
            "Fengyun Rao",
            "Chao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21990",
        "abstract": "While embeddings from multimodal large language models (LLMs) excel as general-purpose representations, their application to dynamic modalities like audio and video remains underexplored. We introduce WAVE (\\textbf{u}nified \\& \\textbf{v}ersatile \\textbf{a}udio-\\textbf{v}isual \\textbf{e}mbeddings), the first LLM-based embedding that creates a unified representation space for text, audio, and video modalities. WAVE employs a novel hierarchical feature fusion strategy and a joint multi-modal, multi-task training approach to enable two key capabilities: any-to-any cross-modal retrieval and the generation of prompt-aware embeddings tailored to user instructions. Experimentally, WAVE sets a new state-of-the-art on the MMEB-v2 video benchmark and achieves superior results in audio and video-to-audio retrieval. Its prompt-aware nature also yields remarkable performance in multimodal question answering, significantly outperforming existing embedding models. Ablation studies validate our joint training strategy, demonstrating improved performance across all modalities. With a newly introduced benchmark for versatile audio-visual learning, WAVE opens up broad possibilities for cross-modal, any-to-any applications. Our code, checkpoints, and data will be released.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "194",
        "title": "ERGO: Efficient High-Resolution Visual Understanding for Vision-Language Models",
        "author": [
            "Jewon Lee",
            "Wooksu Shin",
            "Seungmin Yang",
            "Ki-Ung Song",
            "DongUk Lim",
            "Jaeyeon Kim",
            "Tae-Ho Kim",
            "Bo-Kyeong Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21991",
        "abstract": "Efficient processing of high-resolution images is crucial for real-world vision-language applications. However, existing Large Vision-Language Models (LVLMs) incur substantial computational overhead due to the large number of vision tokens. With the advent of \"thinking with images\" models, reasoning now extends beyond text to the visual domain. This capability motivates our two-stage \"coarse-to-fine\" reasoning pipeline: first, a downsampled image is analyzed to identify task-relevant regions; then, only these regions are cropped at full resolution and processed in a subsequent reasoning stage. This approach reduces computational cost while preserving fine-grained visual details where necessary. A major challenge lies in inferring which regions are truly relevant to a given query. Recent related methods often fail in the first stage after input-image downsampling, due to perception-driven reasoning, where clear visual information is required for effective reasoning. To address this issue, we propose ERGO (Efficient Reasoning & Guided Observation) that performs reasoning-driven perception-leveraging multimodal context to determine where to focus. Our model can account for perceptual uncertainty, expanding the cropped region to cover visually ambiguous areas for answering questions. To this end, we develop simple yet effective reward components in a reinforcement learning framework for coarse-to-fine perception. Across multiple datasets, our approach delivers higher accuracy than the original model and competitive methods, with greater efficiency. For instance, ERGO surpasses Qwen2.5-VL-7B on the V* benchmark by 4.7 points while using only 23% of the vision tokens, achieving a 3x inference speedup. The code and models can be found at: https://github.com/nota-github/ERGO.",
        "tags": [
            "RL",
            "VLM"
        ]
    },
    {
        "id": "195",
        "title": "FailureAtlas:Mapping the Failure Landscape of T2I Models via Active Exploration",
        "author": [
            "Muxi Chen",
            "Zhaohua Zhang",
            "Chenchen Zhao",
            "Mingyang Chen",
            "Wenyu Jiang",
            "Tianwen Jiang",
            "Jianhuan Zhuo",
            "Yu Tang",
            "Qiuyong Xiao",
            "Jihong Zhang",
            "Qiang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21995",
        "abstract": "Static benchmarks have provided a valuable foundation for comparing Text-to-Image (T2I) models. However, their passive design offers limited diagnostic power, struggling to uncover the full landscape of systematic failures or isolate their root causes. We argue for a complementary paradigm: active exploration. We introduce FailureAtlas, the first framework designed to autonomously explore and map the vast failure landscape of T2I models at scale. FailureAtlas frames error discovery as a structured search for minimal, failure-inducing concepts. While it is a computationally explosive problem, we make it tractable with novel acceleration techniques. When applied to Stable Diffusion models, our method uncovers hundreds of thousands of previously unknown error slices (over 247,000 in SD1.5 alone) and provides the first large-scale evidence linking these failures to data scarcity in the training set. By providing a principled and scalable engine for deep model auditing, FailureAtlas establishes a new, diagnostic-first methodology to guide the development of more robust generative AI. The code is available at https://github.com/cure-lab/FailureAtlas",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "196",
        "title": "Exposing Hallucinations To Suppress Them: VLMs Representation Editing With Generative Anchors",
        "author": [
            "Youxu Shi",
            "Suorong Yang",
            "Dong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21997",
        "abstract": "Multimodal large language models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet they remain highly susceptible to hallucinations, producing content that is fluent but inconsistent with visual evidence. Such hallucinations, spanning objects, attributes, and relations, persist even in larger models, while existing mitigation approaches often require additional finetuning, handcrafted priors, or trade-offs that compromise informativeness and scalability. To address this limitation, we propose a training-free, self-supervised method for hallucination mitigation. Our approach introduces a novel hallucination amplification mechanism: a caption is projected into the visual space via a text-to-image model to reveal implicit hallucination signals, serving as a negative anchor, while the original image provides a positive anchor. Leveraging these dual anchors, we edit decoder hidden states by pulling representations toward faithful semantics and pushing them away from hallucination directions. This correction requires no human priors or additional training costs, ensuring both effectiveness and efficiency. Extensive experiments across multiple benchmarks show that our method significantly reduces hallucinations at the object, attribute, and relation levels while largely preserving recall and caption richness, e.g., achieving a hallucination reduction by over 5% using LLaVA-v1.5-7B on CHAIR. Furthermore, results on diverse architectures, including LLaVA-NEXT-7B, Cambrian-8B, and InstructBLIP-7B, validate strong cross-architecture generalization. More importantly, when applied to hallucination-free captions, our method introduces almost no side effects, underscoring its robustness and practical plug-and-play applicability. The implementation will be publicly available.",
        "tags": [
            "LLM",
            "LLaVA",
            "Text-to-Image",
            "VLM"
        ]
    },
    {
        "id": "197",
        "title": "GSM-Agent: Understanding Agentic Reasoning Using Controllable Environments",
        "author": [
            "Hanlin Zhu",
            "Tianyu Guo",
            "Song Mei",
            "Stuart Russell",
            "Nikhil Ghosh",
            "Alberto Bietti",
            "Jiantao Jiao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21998",
        "abstract": "As LLMs are increasingly deployed as agents, agentic reasoning - the ability to combine tool use, especially search, and reasoning - becomes a critical skill. However, it is hard to disentangle agentic reasoning when evaluated in complex environments and tasks. Current agent benchmarks often mix agentic reasoning with challenging math reasoning, expert-level knowledge, and other advanced capabilities. To fill this gap, we build a novel benchmark, GSM-Agent, where an LLM agent is required to solve grade-school-level reasoning problems, but is only presented with the question in the prompt without the premises that contain the necessary information to solve the task, and needs to proactively collect that information using tools. Although the original tasks are grade-school math problems, we observe that even frontier models like GPT-5 only achieve 67% accuracy. To understand and analyze the agentic reasoning patterns, we propose the concept of agentic reasoning graph: cluster the environment's document embeddings into nodes, and map each tool call to its nearest node to build a reasoning path. Surprisingly, we identify that the ability to revisit a previously visited node, widely taken as a crucial pattern in static reasoning, is often missing for agentic reasoning for many models. Based on the insight, we propose a tool-augmented test-time scaling method to improve LLM's agentic reasoning performance by adding tools to encourage models to revisit. We expect our benchmark and the agentic reasoning framework to aid future studies of understanding and pushing the boundaries of agentic reasoning.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "198",
        "title": "Black-Box Hallucination Detection via Consistency Under the Uncertain Expression",
        "author": [
            "Seongho Joo",
            "Kyungmin Min",
            "Jahyun Koo",
            "Kyomin Jung"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21999",
        "abstract": "Despite the great advancement of Language modeling in recent days, Large Language Models (LLMs) such as GPT3 are notorious for generating non-factual responses, so-called \"hallucination\" problems. Existing methods for detecting and alleviating this hallucination problem require external resources or the internal state of LLMs, such as the output probability of each token. Given the LLM's restricted external API availability and the limited scope of external resources, there is an urgent demand to establish the Black-Box approach as the cornerstone for effective hallucination detection. In this work, we propose a simple black-box hallucination detection metric after the investigation of the behavior of LLMs under expression of uncertainty. Our comprehensive analysis reveals that LLMs generate consistent responses when they present factual responses while non-consistent responses vice versa. Based on the analysis, we propose an efficient black-box hallucination detection metric with the expression of uncertainty. The experiment demonstrates that our metric is more predictive of the factuality in model responses than baselines that use internal knowledge of LLMs.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "199",
        "title": "One-DoF Robotic Design of Overconstrained Limbs with Energy-Efficient, Self-Collision-Free Motion",
        "author": [
            "Yuping Gu",
            "Bangchao Huang",
            "Haoran Sun",
            "Ronghan Xu",
            "Jiayi Yin",
            "Wei Zhang",
            "Fang Wan",
            "Jia Pan",
            "Chaoyang Song"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22002",
        "abstract": "While it is expected to build robotic limbs with multiple degrees of freedom (DoF) inspired by nature, a single DoF design remains fundamental, providing benefits that include, but are not limited to, simplicity, robustness, cost-effectiveness, and efficiency. Mechanisms, especially those with multiple links and revolute joints connected in closed loops, play an enabling factor in introducing motion diversity for 1-DoF systems, which are usually constrained by self-collision during a full-cycle range of motion. This study presents a novel computational approach to designing one-degree-of-freedom (1-DoF) overconstrained robotic limbs for a desired spatial trajectory, while achieving energy-efficient, self-collision-free motion in full-cycle rotations. Firstly, we present the geometric optimization problem of linkage-based robotic limbs in a generalized formulation for self-collision-free design. Next, we formulate the spatial trajectory generation problem with the overconstrained linkages by optimizing the similarity and dynamic-related metrics. We further optimize the geometric shape of the overconstrained linkage to ensure smooth and collision-free motion driven by a single actuator. We validated our proposed method through various experiments, including personalized automata and bio-inspired hexapod robots. The resulting hexapod robot, featuring overconstrained robotic limbs, demonstrated outstanding energy efficiency during forward walking.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "200",
        "title": "Stage-wise Dynamics of Classifier-Free Guidance in Diffusion Models",
        "author": [
            "Cheng Jin",
            "Qitan Shi",
            "Yuantao Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22007",
        "abstract": "Classifier-Free Guidance (CFG) is widely used to improve conditional fidelity in diffusion models, but its impact on sampling dynamics remains poorly understood. Prior studies, often restricted to unimodal conditional distributions or simplified cases, provide only a partial picture. We analyze CFG under multimodal conditionals and show that the sampling process unfolds in three successive stages. In the Direction Shift stage, guidance accelerates movement toward the weighted mean, introducing initialization bias and norm growth. In the Mode Separation stage, local dynamics remain largely neutral, but the inherited bias suppresses weaker modes, reducing global diversity. In the Concentration stage, guidance amplifies within-mode contraction, diminishing fine-grained variability. This unified view explains a widely observed phenomenon: stronger guidance improves semantic alignment but inevitably reduces diversity. Experiments support these predictions, showing that early strong guidance erodes global diversity, while late strong guidance suppresses fine-grained variation. Moreover, our theory naturally suggests a time-varying guidance schedule, and empirical results confirm that it consistently improves both quality and diversity.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "201",
        "title": "Goal-Guided Efficient Exploration via Large Language Model in Reinforcement Learning",
        "author": [
            "Yajie Qi",
            "Wei Wei",
            "Lin Li",
            "Lijun Zhang",
            "Zhidong Gao",
            "Da Wang",
            "Huizhong Song"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22008",
        "abstract": "Real-world decision-making tasks typically occur in complex and open environments, posing significant challenges to reinforcement learning (RL) agents' exploration efficiency and long-horizon planning capabilities. A promising approach is LLM-enhanced RL, which leverages the rich prior knowledge and strong planning capabilities of LLMs to guide RL agents in efficient exploration. However, existing methods mostly rely on frequent and costly LLM invocations and suffer from limited performance due to the semantic mismatch. In this paper, we introduce a Structured Goal-guided Reinforcement Learning (SGRL) method that integrates a structured goal planner and a goal-conditioned action pruner to guide RL agents toward efficient exploration. Specifically, the structured goal planner utilizes LLMs to generate a reusable, structured function for goal generation, in which goals are prioritized. Furthermore, by utilizing LLMs to determine goals' priority weights, it dynamically generates forward-looking goals to guide the agent's policy toward more promising decision-making trajectories. The goal-conditioned action pruner employs an action masking mechanism that filters out actions misaligned with the current goal, thereby constraining the RL agent to select goal-consistent policies. We evaluate the proposed method on Crafter and Craftax-Classic, and experimental results demonstrate that SGRL achieves superior performance compared to existing state-of-the-art methods.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "202",
        "title": "GraphSearch: An Agentic Deep Searching Workflow for Graph Retrieval-Augmented Generation",
        "author": [
            "Cehao Yang",
            "Xiaojun Wu",
            "Xueyuan Lin",
            "Chengjin Xu",
            "Xuhui Jiang",
            "Yuanliang Sun",
            "Jia Li",
            "Hui Xiong",
            "Jian Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22009",
        "abstract": "Graph Retrieval-Augmented Generation (GraphRAG) enhances factual reasoning in LLMs by structurally modeling knowledge through graph-based representations. However, existing GraphRAG approaches face two core limitations: shallow retrieval that fails to surface all critical evidence, and inefficient utilization of pre-constructed structural graph data, which hinders effective reasoning from complex queries. To address these challenges, we propose \\textsc{GraphSearch}, a novel agentic deep searching workflow with dual-channel retrieval for GraphRAG. \\textsc{GraphSearch} organizes the retrieval process into a modular framework comprising six modules, enabling multi-turn interactions and iterative reasoning. Furthermore, \\textsc{GraphSearch} adopts a dual-channel retrieval strategy that issues semantic queries over chunk-based text data and relational queries over structural graph data, enabling comprehensive utilization of both modalities and their complementary strengths. Experimental results across six multi-hop RAG benchmarks demonstrate that \\textsc{GraphSearch} consistently improves answer accuracy and generation quality over the traditional strategy, confirming \\textsc{GraphSearch} as a promising direction for advancing graph retrieval-augmented generation.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "203",
        "title": "CoFFT: Chain of Foresight-Focus Thought for Visual Language Models",
        "author": [
            "Xinyu Zhang",
            "Yuxuan Dong",
            "Lingling Zhang",
            "Chengyou Jia",
            "Zhuohang Dang",
            "Basura Fernando",
            "Jun Liu",
            "Mike Zheng Shou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22010",
        "abstract": "Despite significant advances in Vision Language Models (VLMs), they remain constrained by the complexity and redundancy of visual input. When images contain large amounts of irrelevant information, VLMs are susceptible to interference, thus generating excessive task-irrelevant reasoning processes or even hallucinations. This limitation stems from their inability to discover and process the required regions during reasoning precisely. To address this limitation, we present the Chain of Foresight-Focus Thought (CoFFT), a novel training-free approach that enhances VLMs' visual reasoning by emulating human visual cognition. Each Foresight-Focus Thought consists of three stages: (1) Diverse Sample Generation: generates diverse reasoning samples to explore potential reasoning paths, where each sample contains several reasoning steps; (2) Dual Foresight Decoding: rigorously evaluates these samples based on both visual focus and reasoning progression, adding the first step of optimal sample to the reasoning process; (3) Visual Focus Adjustment: precisely adjust visual focus toward regions most beneficial for future reasoning, before returning to stage (1) to generate subsequent reasoning samples until reaching the final answer. These stages function iteratively, creating an interdependent cycle where reasoning guides visual focus and visual focus informs subsequent reasoning. Empirical results across multiple benchmarks using Qwen2.5-VL, InternVL-2.5, and Llava-Next demonstrate consistent performance improvements of 3.1-5.8\\% with controllable increasing computational overhead.",
        "tags": [
            "LLaVA",
            "VLM"
        ]
    },
    {
        "id": "204",
        "title": "Lightweight Structured Multimodal Reasoning for Clinical Scene Understanding in Robotics",
        "author": [
            "Saurav Jha",
            "Stefan K. Ehrlich"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22014",
        "abstract": "Healthcare robotics requires robust multimodal perception and reasoning to ensure safety in dynamic clinical environments. Current Vision-Language Models (VLMs) demonstrate strong general-purpose capabilities but remain limited in temporal reasoning, uncertainty estimation, and structured outputs needed for robotic planning. We present a lightweight agentic multimodal framework for video-based scene understanding. Combining the Qwen2.5-VL-3B-Instruct model with a SmolAgent-based orchestration layer, it supports chain-of-thought reasoning, speech-vision fusion, and dynamic tool invocation. The framework generates structured scene graphs and leverages a hybrid retrieval module for interpretable and adaptive reasoning. Evaluations on the Video-MME benchmark and a custom clinical dataset show competitive accuracy and improved robustness compared to state-of-the-art VLMs, demonstrating its potential for applications in robot-assisted surgery, patient monitoring, and decision support.",
        "tags": [
            "CoT",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "205",
        "title": "EgoInstruct: An Egocentric Video Dataset of Face-to-face Instructional Interactions with Multi-modal LLM Benchmarking",
        "author": [
            "Yuki Sakai",
            "Ryosuke Furuta",
            "Juichun Yen",
            "Yoichi Sato"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22019",
        "abstract": "Analyzing instructional interactions between an instructor and a learner who are co-present in the same physical space is a critical problem for educational support and skill transfer. Yet such face-to-face instructional scenes have not been systematically studied in computer vision. We identify two key reasons: i) the lack of suitable datasets and ii) limited analytical techniques. To address this gap, we present a new egocentric video dataset of face-to-face instruction and provide ground-truth annotations for two fundamental tasks that serve as a first step toward a comprehensive understanding of instructional interactions: procedural step segmentation and conversation-state classification. Using this dataset, we benchmark multimodal large language models (MLLMs) against conventional task-specific models. Since face-to-face instruction involves multiple modalities (speech content and prosody, gaze and body motion, and visual context), effective understanding requires methods that handle verbal and nonverbal communication in an integrated manner. Accordingly, we evaluate recently introduced MLLMs that jointly process images, audio, and text. This evaluation quantifies the extent to which current machine learning models understand face-to-face instructional scenes. In experiments, MLLMs outperform specialized baselines even without task-specific fine-tuning, suggesting their promise for holistic understanding of instructional interactions.",
        "tags": [
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "206",
        "title": "Teaching Transformers to Solve Combinatorial Problems through Efficient Trial & Error",
        "author": [
            "Panagiotis Giannoulis",
            "Yorgos Pantis",
            "Christos Tzamos"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22023",
        "abstract": "Despite their proficiency in various language tasks, Large Language Models (LLMs) struggle with combinatorial problems like Satisfiability, Traveling Salesman Problem, or even basic arithmetic. We address this gap through a novel approach for solving problems in the class NP. We focus on the paradigmatic task of Sudoku and achieve state-of-the-art accuracy (99\\%) compared to prior neuro-symbolic approaches. Unlike prior work that used custom architectures, our method employs a vanilla decoder-only Transformer (GPT-2) without external tools or function calling. Our method integrates imitation learning of simple Sudoku rules with an explicit Depth-First Search (DFS) exploration strategy involving informed guessing and backtracking. Moving beyond imitation learning, we seek to minimize the number of guesses until reaching a solution. We provide a rigorous analysis of this setup formalizing its connection to a contextual variant of Min-Sum Set Cover, a well-studied problem in algorithms and stochastic optimization.",
        "tags": [
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "207",
        "title": "The Thinking Spectrum: An Emperical Study of Tunable Reasoning in LLMs through Model Merging",
        "author": [
            "Xiaochong Lan",
            "Yu Zheng",
            "Shiteng Cao",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22034",
        "abstract": "The growing demand for large language models (LLMs) with tunable reasoning capabilities in many real-world applications highlights a critical need for methods that can efficiently produce a spectrum of models balancing reasoning depth and computational cost. Model merging has emerged as a promising, training-free technique to address this challenge by arithmetically combining the weights of a general-purpose model with a specialized reasoning model. While various merging techniques exist, their potential to create a spectrum of models with fine-grained control over reasoning abilities remains largely unexplored. This work presents a large-scale empirical study evaluating a range of model merging techniques across multiple reasoning benchmarks. We systematically vary merging strengths to construct accuracy-efficiency curves, providing the first comprehensive view of the tunable performance landscape. Our findings reveal that model merging offers an effective and controllable method for calibrating the trade-off between reasoning accuracy and token efficiency, even when parent models have highly divergent weight spaces. Crucially, we identify instances of Pareto Improvement, where a merged model achieves both higher accuracy and lower token consumption than one of its parents. Our study provides the first comprehensive analysis of this tunable space, offering practical guidelines for creating LLMs with specific reasoning profiles to meet diverse application demands.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "208",
        "title": "Latent Diffusion : Multi-Dimension Stable Diffusion Latent Space Explorer",
        "author": [
            "Zhihua Zhong",
            "Xuanyang Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22038",
        "abstract": "Latent space is one of the key concepts in generative AI, offering powerful means for creative exploration through vector manipulation. However, diffusion models like Stable Diffusion lack the intuitive latent vector control found in GANs, limiting their flexibility for artistic expression. This paper introduces \\workname, a framework for integrating customizable latent space operations into the diffusion process. By enabling direct manipulation of conceptual and spatial representations, this approach expands creative possibilities in generative art. We demonstrate the potential of this framework through two artworks, \\textit{Infinitepedia} and \\textit{Latent Motion}, highlighting its use in conceptual blending and dynamic motion generation. Our findings reveal latent space structures with semantic and meaningless regions, offering insights into the geometry of diffusion models and paving the way for further explorations of latent space.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "209",
        "title": "\"Your AI, My Shell\": Demystifying Prompt Injection Attacks on Agentic AI Coding Editors",
        "author": [
            "Yue Liu",
            "Yanjie Zhao",
            "Yunbo Lyu",
            "Ting Zhang",
            "Haoyu Wang",
            "David Lo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22040",
        "abstract": "Agentic AI coding editors driven by large language models have recently become more popular due to their ability to improve developer productivity during software development. Modern editors such as Cursor are designed not just for code completion, but also with more system privileges for complex coding tasks (e.g., run commands in the terminal, access development environments, and interact with external systems). While this brings us closer to the \"fully automated programming\" dream, it also raises new security concerns. In this study, we present the first empirical analysis of prompt injection attacks targeting these high-privilege agentic AI coding editors. We show how attackers can remotely exploit these systems by poisoning external development resources with malicious instructions, effectively hijacking AI agents to run malicious commands, turning \"your AI\" into \"attacker's shell\". To perform this analysis, we implement AIShellJack, an automated testing framework for assessing prompt injection vulnerabilities in agentic AI coding editors. AIShellJack contains 314 unique attack payloads that cover 70 techniques from the MITRE ATT&CK framework. Using AIShellJack, we conduct a large-scale evaluation on GitHub Copilot and Cursor, and our evaluation results show that attack success rates can reach as high as 84% for executing malicious commands. Moreover, these attacks are proven effective across a wide range of objectives, ranging from initial access and system discovery to credential theft and data exfiltration.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "210",
        "title": "MO-GRPO: Mitigating Reward Hacking of Group Relative Policy Optimization on Multi-Objective Problems",
        "author": [
            "Yuki Ichihara",
            "Yuu Jinnai",
            "Tetsuro Morimura",
            "Mitsuki Sakamoto",
            "Ryota Mitsuhashi",
            "Eiji Uchibe"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22047",
        "abstract": "Group Relative Policy Optimization (GRPO) has been shown to be an effective algorithm when an accurate reward model is available. However, such a highly reliable reward model is not available in many real-world tasks. In this paper, we particularly focus on multi-objective settings, in which we identify that GRPO is vulnerable to reward hacking, optimizing only one of the objectives at the cost of the others. To address this issue, we propose MO-GRPO, an extension of GRPO with a simple normalization method to reweight the reward functions automatically according to the variances of their values. We first show analytically that MO-GRPO ensures that all reward functions contribute evenly to the loss function while preserving the order of preferences, eliminating the need for manual tuning of the reward functions' scales. Then, we evaluate MO-GRPO experimentally in four domains: (i) the multi-armed bandits problem, (ii) simulated control task (Mo-Gymnasium), (iii) machine translation tasks on the WMT benchmark (En-Ja, En-Zh), and (iv) instruction following task. MO-GRPO achieves stable learning by evenly distributing correlations among the components of rewards, outperforming GRPO, showing MO-GRPO to be a promising algorithm for multi-objective reinforcement learning problems.",
        "tags": [
            "GRPO",
            "RL"
        ]
    },
    {
        "id": "211",
        "title": "Fuzzy Reasoning Chain (FRC): An Innovative Reasoning Framework from Fuzziness to Clarity",
        "author": [
            "Ping Chen",
            "Xiang Liu",
            "Zhaoxiang Liu",
            "Zezhou Chen",
            "Xingpeng Zhang",
            "Huan Hu",
            "Zipeng Wang",
            "Kai Wang",
            "Shuming Shi",
            "Shiguo Lian"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22054",
        "abstract": "With the rapid advancement of large language models (LLMs), natural language processing (NLP) has achieved remarkable progress. Nonetheless, significant challenges remain in handling texts with ambiguity, polysemy, or uncertainty. We introduce the Fuzzy Reasoning Chain (FRC) framework, which integrates LLM semantic priors with continuous fuzzy membership degrees, creating an explicit interaction between probability-based reasoning and fuzzy membership reasoning. This transition allows ambiguous inputs to be gradually transformed into clear and interpretable decisions while capturing conflicting or uncertain signals that traditional probability-based methods cannot. We validate FRC on sentiment analysis tasks, where both theoretical analysis and empirical results show that it ensures stable reasoning and facilitates knowledge transfer across different model scales. These findings indicate that FRC provides a general mechanism for managing subtle and ambiguous expressions with improved interpretability and robustness.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "212",
        "title": "RedNote-Vibe: A Dataset for Capturing Temporal Dynamics of AI-Generated Text in Social Media",
        "author": [
            "Yudong Li",
            "Yufei Sun",
            "Yuhan Yao",
            "Peiru Yang",
            "Wanyue Li",
            "Jiajun Zou",
            "Yongfeng Huang",
            "Linlin Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22055",
        "abstract": "The proliferation of Large Language Models (LLMs) has led to widespread AI-Generated Text (AIGT) on social media platforms, creating unique challenges where content dynamics are driven by user engagement and evolve over time. However, existing datasets mainly depict static AIGT detection. In this work, we introduce RedNote-Vibe, the first longitudinal (5-years) dataset for social media AIGT analysis. This dataset is sourced from Xiaohongshu platform, containing user engagement metrics (e.g., likes, comments) and timestamps spanning from the pre-LLM period to July 2025, which enables research into the temporal dynamics and user interaction patterns of AIGT. Furthermore, to detect AIGT in the context of social media, we propose PsychoLinguistic AIGT Detection Framework (PLAD), an interpretable approach that leverages psycholinguistic features. Our experiments show that PLAD achieves superior detection performance and provides insights into the signatures distinguishing human and AI-generated content. More importantly, it reveals the complex relationship between these linguistic features and social media engagement. The dataset is available at https://github.com/testuser03158/RedNote-Vibe.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "213",
        "title": "Comprehend and Talk: Text to Speech Synthesis via Dual Language Modeling",
        "author": [
            "Junjie Cao",
            "Yichen Han",
            "Ruonan Zhang",
            "Xiaoyang Hao",
            "Hongxiang Li",
            "Shuaijiang Zhao",
            "Yue Liu",
            "Xiao-Ping Zhng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22062",
        "abstract": "Existing Large Language Model (LLM) based autoregressive (AR) text-to-speech (TTS) systems, while achieving state-of-the-art quality, still face critical challenges. The foundation of this LLM-based paradigm is the discretization of the continuous speech waveform into a sequence of discrete tokens by neural audio codec. However, single codebook modeling is well suited to text LLMs, but suffers from significant information loss; hierarchical acoustic tokens, typically generated via Residual Vector Quantization (RVQ), often lack explicit semantic structure, placing a heavy learning burden on the model. Furthermore, the autoregressive process is inherently susceptible to error accumulation, which can degrade generation stability. To address these limitations, we propose CaT-TTS, a novel framework for robust and semantically-grounded zero-shot synthesis. First, we introduce S3Codec, a split RVQ codec that injects explicit linguistic features into its primary codebook via semantic distillation from a state-of-the-art ASR model, providing a structured representation that simplifies the learning task. Second, we propose an ``Understand-then-Generate'' dual-Transformer architecture that decouples comprehension from rendering. An initial ``Understanding'' Transformer models the cross-modal relationship between text and the audio's semantic tokens to form a high-level utterance plan. A subsequent ``Generation'' Transformer then executes this plan, autoregressively synthesizing hierarchical acoustic tokens. Finally, to enhance generation stability, we introduce Masked Audio Parallel Inference (MAPI), a nearly parameter-free inference strategy that dynamically guides the decoding process to mitigate local errors.",
        "tags": [
            "LLM",
            "Transformer",
            "Vector Quantization"
        ]
    },
    {
        "id": "214",
        "title": "High-Quality Sound Separation Across Diverse Categories via Visually-Guided Generative Modeling",
        "author": [
            "Chao Huang",
            "Susan Liang",
            "Yapeng Tian",
            "Anurag Kumar",
            "Chenliang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22063",
        "abstract": "We propose DAVIS, a Diffusion-based Audio-VIsual Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS circumvents these issues by leveraging potent generative modeling paradigms, specifically Denoising Diffusion Probabilistic Models (DDPM) and the more recent Flow Matching (FM), integrated within a specialized Separation U-Net architecture. Our framework operates by synthesizing the desired separated sound spectrograms directly from a noise distribution, conditioned concurrently on the mixed audio input and associated visual information. The inherent nature of its generative objective makes DAVIS particularly adept at producing high-quality sound separations for diverse sound categories. We present comparative evaluations of DAVIS, encompassing both its DDPM and Flow Matching variants, against leading methods on the standard AVE and MUSIC datasets. The results affirm that both variants surpass existing approaches in separation quality, highlighting the efficacy of our generative framework for tackling the audio-visual source separation task.",
        "tags": [
            "DDPM",
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "215",
        "title": "Effect of Gait Design on Proprioceptive Sensing of Terrain Properties in a Quadrupedal Robot",
        "author": [
            "Ethan Fulcher",
            "J. Diego Caporale",
            "Yifeng Zhang",
            "John Ruck",
            "Feifei Qian"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22065",
        "abstract": "In-situ robotic exploration is an important tool for advancing knowledge of geological processes that describe the Earth and other Planetary bodies. To inform and enhance operations for these roving laboratories, it is imperative to understand the terramechanical properties of their environments, especially for traversing on loose, deformable substrates. Recent research suggested that legged robots with direct-drive and low-gear ratio actuators can sensitively detect external forces, and therefore possess the potential to measure terrain properties with their legs during locomotion, providing unprecedented sampling speed and density while accessing terrains previously too risky to sample. This paper explores these ideas by investigating the impact of gait on proprioceptive terrain sensing accuracy, particularly comparing a sensing-oriented gait, Crawl N' Sense, with a locomotion-oriented gait, Trot-Walk. Each gait's ability to measure the strength and texture of deformable substrate is quantified as the robot locomotes over a laboratory transect consisting of a rigid surface, loose sand, and loose sand with synthetic surface crusts. Our results suggest that with both the sensing-oriented crawling gait and locomotion-oriented trot gait, the robot can measure a consistent difference in the strength (in terms of penetration resistance) between the low- and high-resistance substrates; however, the locomotion-oriented trot gait contains larger magnitude and variance in measurements. Furthermore, the slower crawl gait can detect brittle ruptures of the surface crusts with significantly higher accuracy than the faster trot gait. Our results offer new insights that inform legged robot \"sensing during locomotion\" gait design and planning for scouting the terrain and producing scientific measurements on other worlds to advance our understanding of their geology and formation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "216",
        "title": "The Rogue Scalpel: Activation Steering Compromises LLM Safety",
        "author": [
            "Anton Korznikov",
            "Andrey Galichin",
            "Alexey Dontsov",
            "Oleg Y. Rogov",
            "Ivan Oseledets",
            "Elena Tutubalina"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22067",
        "abstract": "Activation steering is a promising technique for controlling LLM behavior by adding semantically meaningful vectors directly into a model's hidden states during inference. It is often framed as a precise, interpretable, and potentially safer alternative to fine-tuning. We demonstrate the opposite: steering systematically breaks model alignment safeguards, making it comply with harmful requests. Through extensive experiments on different model families, we show that even steering in a random direction can increase the probability of harmful compliance from 0% to 2-27%. Alarmingly, steering benign features from a sparse autoencoder (SAE), a common source of interpretable directions, increases these rates by a further 2-4%. Finally, we show that combining 20 randomly sampled vectors that jailbreak a single prompt creates a universal attack, significantly increasing harmful compliance on unseen requests. These results challenge the paradigm of safety through interpretability, showing that precise control over model internals does not guarantee precise control over model behavior.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "217",
        "title": "Code once, Run Green: Automated Green Code Translation in Serverless Computing",
        "author": [
            "Sebastian Werner",
            "Mathis KÃ¤hler",
            "Alireza Hakamian"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22068",
        "abstract": "The rapid digitization and the increasing use of emerging technologies such as AI models have significantly contributed to the emissions of computing infrastructure. Efforts to mitigate this impact typically focus on the infrastructure level such as powering data centers with renewable energy, or through the specific design of energy-efficient software. However, both strategies rely on stakeholder intervention, making their adoption in legacy and already-deployed systems unlikely. As a result, past architectural and implementation decisions continue to incur additional energy usage - a phenomenon we refer to as energy debt.\nHence, in this paper, we investigate the potential of serverless computing platforms to automatically reduce energy debt by leveraging the unique access to function source code. Specifically, we explore whether large language models (LLMs) can translate serverless functions into more energy-efficient programming languages while preserving functional correctness. To this end, we design and implement ReFaaS and integrate it into the Fission serverless framework. We evaluate multiple LLMs on their ability to perform such code translations and analyze their impact on energy consumption.\nOur preliminary results indicate that translated functions can reduce invocation energy by up to 70%, achieving net energy savings after approximately 3,000 to 5,000 invocations, depending on the LLM used. Nonetheless, the approach faces several challenges: not all functions are suitable for translation, and for some, the amortization threshold is significantly higher or unreachable. Despite these limitations, we identify four key research challenges whose resolution could unlock long-term, automated mitigation of energy debt in serverless computing.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "218",
        "title": "SpecXNet: A Dual-Domain Convolutional Network for Robust Deepfake Detection",
        "author": [
            "Inzamamul Alam",
            "Md Tanvir Islam",
            "Simon S. Woo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22070",
        "abstract": "The increasing realism of content generated by GANs and diffusion models has made deepfake detection significantly more challenging. Existing approaches often focus solely on spatial or frequency-domain features, limiting their generalization to unseen manipulations. We propose the Spectral Cross-Attentional Network (SpecXNet), a dual-domain architecture for robust deepfake detection. The core \\textbf{Dual-Domain Feature Coupler (DDFC)} decomposes features into a local spatial branch for capturing texture-level anomalies and a global spectral branch that employs Fast Fourier Transform to model periodic inconsistencies. This dual-domain formulation allows SpecXNet to jointly exploit localized detail and global structural coherence, which are critical for distinguishing authentic from manipulated images. We also introduce the \\textbf{Dual Fourier Attention (DFA)} module, which dynamically fuses spatial and spectral features in a content-aware manner. Built atop a modified XceptionNet backbone, we embed the DDFC and DFA modules within a separable convolution block. Extensive experiments on multiple deepfake benchmarks show that SpecXNet achieves state-of-the-art accuracy, particularly under cross-dataset and unseen manipulation scenarios, while maintaining real-time feasibility. Our results highlight the effectiveness of unified spatial-spectral learning for robust and generalizable deepfake detection. To ensure reproducibility, we released the full code on \\href{https://github.com/inzamamulDU/SpecXNet}{\\textcolor{blue}{\\textbf{GitHub}}}.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "219",
        "title": "Fine-tuning Done Right in Model Editing",
        "author": [
            "Wanli Yang",
            "Fei Sun",
            "Rui Tang",
            "Hongyu Zang",
            "Du Su",
            "Qi Cao",
            "Jingang Wang",
            "Huawei Shen",
            "Xueqi Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22072",
        "abstract": "Fine-tuning, a foundational method for adapting large language models, has long been considered ineffective for model editing. Here, we challenge this belief, arguing that the reported failure arises not from the inherent limitation of fine-tuning itself, but from adapting it to the sequential nature of the editing task, a single-pass depth-first pipeline that optimizes each sample to convergence before moving on. While intuitive, this depth-first pipeline coupled with sample-wise updating over-optimizes each edit and induces interference across edits. Our controlled experiments reveal that simply restoring fine-tuning to the standard breadth-first (i.e., epoch-based) pipeline with mini-batch optimization substantially improves its effectiveness for model editing. Moreover, fine-tuning in editing also suffers from suboptimal tuning parameter locations inherited from prior methods. Through systematic analysis of tuning locations, we derive LocFT-BF, a simple and effective localized editing method built on the restored fine-tuning framework. Extensive experiments across diverse LLMs and datasets demonstrate that LocFT-BF outperforms state-of-the-art methods by large margins. Notably, to our knowledge, it is the first to sustain 100K edits and 72B-parameter models,10 x beyond prior practice, without sacrificing general capabilities. By clarifying a long-standing misconception and introducing a principled localized tuning strategy, we advance fine-tuning from an underestimated baseline to a leading method for model editing, establishing a solid foundation for future research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "220",
        "title": "COSPADI: Compressing LLMs via Calibration-Guided Sparse Dictionary Learning",
        "author": [
            "Dmitriy Shopkhoev",
            "Denis Makhov",
            "Magauiya Zhussip",
            "Ammar Ali",
            "Stamatios Lefkimmiatis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22075",
        "abstract": "Post-training compression of large language models (LLMs) largely relies on low-rank weight approximation, which represents each column of a weight matrix in a shared low-dimensional subspace. While this is a computationally efficient strategy, the imposed structural constraint is rigid and can lead to a noticeable model accuracy drop. In this work, we propose CoSpaDi (Compression via Sparse Dictionary Learning), a novel training-free compression framework that replaces low-rank decomposition with a more flexible structured sparse factorization in which each weight matrix is represented with a dense dictionary and a column-sparse coefficient matrix. This formulation enables a union-of-subspaces representation: different columns of the original weight matrix are approximated in distinct subspaces spanned by adaptively selected dictionary atoms, offering greater expressiveness than a single invariant basis. Crucially, CoSpaDi leverages a small calibration dataset to optimize the factorization such that the output activations of compressed projection layers closely match those of the original ones, thereby minimizing functional reconstruction error rather than mere weight approximation. This data-aware strategy preserves better model fidelity without any fine-tuning under reasonable compression ratios. Moreover, the resulting structured sparsity allows efficient sparse-dense matrix multiplication and is compatible with post-training quantization for further memory and latency gains. We evaluate CoSpaDi across multiple Llama and Qwen models under per-layer and per-group settings at 20-50\\% compression ratios, demonstrating consistent superiority over state-of-the-art data-aware low-rank methods both in accuracy and perplexity. Our results establish structured sparse dictionary learning as a powerful alternative to conventional low-rank approaches for efficient LLM deployment.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "221",
        "title": "Action-aware Dynamic Pruning for Efficient Vision-Language-Action Manipulation",
        "author": [
            "Xiaohuan Pei",
            "Yuxing Chen",
            "Siyu Xu",
            "Yunke Wang",
            "Yuheng Shi",
            "Chang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22093",
        "abstract": "Robotic manipulation with Vision-Language-Action models requires efficient inference over long-horizon multi-modal context, where attention to dense visual tokens dominates computational cost. Existing methods optimize inference speed by reducing visual redundancy within VLA models, but they overlook the varying redundancy across robotic manipulation stages. We observe that the visual token redundancy is higher in coarse manipulation phase than in fine-grained operations, and is strongly correlated with the action dynamic. Motivated by this observation, we propose \\textbf{A}ction-aware \\textbf{D}ynamic \\textbf{P}runing (\\textbf{ADP}), a multi-modal pruning framework that integrates text-driven token selection with action-aware trajectory gating. Our method introduces a gating mechanism that conditions the pruning signal on recent action trajectories, using past motion windows to adaptively adjust token retention ratios in accordance with dynamics, thereby balancing computational efficiency and perceptual precision across different manipulation stages. Extensive experiments on the LIBERO suites and diverse real-world scenarios demonstrate that our method significantly reduces FLOPs and action inference latency (\\textit{e.g.} $1.35 \\times$ speed up on OpenVLA-OFT) while maintaining competitive success rates (\\textit{e.g.} 25.8\\% improvements with OpenVLA) compared to baselines, thereby providing a simple plug-in path to efficient robot policies that advances the efficiency and performance frontier of robotic manipulation. Our project website is: \\href{https://vla-adp.github.io/}{http://ADP.com}.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "222",
        "title": "SecureAgentBench: Benchmarking Secure Code Generation under Realistic Vulnerability Scenarios",
        "author": [
            "Junkai Chen",
            "Huihui Huang",
            "Yunbo Lyu",
            "Junwen An",
            "Jieke Shi",
            "Chengran Yang",
            "Ting Zhang",
            "Haoye Tian",
            "Yikun Li",
            "Zhenhao Li",
            "Xin Zhou",
            "Xing Hu",
            "David Lo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22097",
        "abstract": "Large language model (LLM) powered code agents are rapidly transforming software engineering by automating tasks such as testing, debugging, and repairing, yet the security risks of their generated code have become a critical concern. Existing benchmarks have offered valuable insights but remain insufficient: they often overlook the genuine context in which vulnerabilities were introduced or adopt narrow evaluation protocols that fail to capture either functional correctness or newly introduced vulnerabilities. We therefore introduce SecureAgentBench, a benchmark of 105 coding tasks designed to rigorously evaluate code agents' capabilities in secure code generation. Each task includes (i) realistic task settings that require multi-file edits in large repositories, (ii) aligned contexts based on real-world open-source vulnerabilities with precisely identified introduction points, and (iii) comprehensive evaluation that combines functionality testing, vulnerability checking through proof-of-concept exploits, and detection of newly introduced vulnerabilities using static analysis. We evaluate three representative agents (SWE-agent, OpenHands, and Aider) with three state-of-the-art LLMs (Claude 3.7 Sonnet, GPT-4.1, and DeepSeek-V3.1). Results show that (i) current agents struggle to produce secure code, as even the best-performing one, SWE-agent supported by DeepSeek-V3.1, achieves merely 15.2% correct-and-secure solutions, (ii) some agents produce functionally correct code but still introduce vulnerabilities, including new ones not previously recorded, and (iii) adding explicit security instructions for agents does not significantly improve secure coding, underscoring the need for further research. These findings establish SecureAgentBench as a rigorous benchmark for secure code generation and a step toward more reliable software development with LLMs.",
        "tags": [
            "DeepSeek",
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "223",
        "title": "S2J: Bridging the Gap Between Solving and Judging Ability in Generative Reward Models",
        "author": [
            "Shaoning Sun",
            "Jiachen Yu",
            "Zongqi Wang",
            "Xuewei Yang",
            "Tianle Gu",
            "Yujiu Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22099",
        "abstract": "With the rapid development of large language models (LLMs), generative reward models (GRMs) have been widely adopted for reward modeling and evaluation. Previous studies have primarily focused on training specialized GRMs by optimizing them on preference datasets with the judgment correctness as supervision. While it's widely accepted that GRMs with stronger problem-solving capabilities typically exhibit superior judgment abilities, we first identify a significant solve-to-judge gap when examining individual queries. Specifically, the solve-to-judge gap refers to the phenomenon where GRMs struggle to make correct judgments on some queries (14%-37%), despite being fully capable of solving them. In this paper, we propose the Solve-to-Judge (S2J) approach to address this problem. Specifically, S2J simultaneously leverages both the solving and judging capabilities on a single GRM's output for supervision, explicitly linking the GRM's problem-solving and evaluation abilities during model optimization, thereby narrowing the gap. Our comprehensive experiments demonstrate that S2J effectively reduces the solve-to-judge gap by 16.2%, thereby enhancing the model's judgment performance by 5.8%. Notably, S2J achieves state-of-the-art (SOTA) performance among GRMs built on the same base model while utilizing a significantly smaller training dataset. Moreover, S2J accomplishes this through self-evolution without relying on more powerful external models for distillation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "224",
        "title": "Think Right, Not More: Test-Time Scaling for Numerical Claim Verification",
        "author": [
            "Primakov Chungkham",
            "V Venktesh",
            "Vinay Setty",
            "Avishek Anand"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22101",
        "abstract": "Fact-checking real-world claims, particularly numerical claims, is inherently complex that require multistep reasoning and numerical reasoning for verifying diverse aspects of the claim. Although large language models (LLMs) including reasoning models have made tremendous advances, they still fall short on fact-checking real-world claims that require a combination of compositional and numerical reasoning. They are unable to understand nuance of numerical aspects, and are also susceptible to the reasoning drift issue, where the model is unable to contextualize diverse information resulting in misinterpretation and backtracking of reasoning process. In this work, we systematically explore scaling test-time compute (TTS) for LLMs on the task of fact-checking complex numerical claims, which entails eliciting multiple reasoning paths from an LLM. We train a verifier model (VERIFIERFC) to navigate this space of possible reasoning paths and select one that could lead to the correct verdict. We observe that TTS helps mitigate the reasoning drift issue, leading to significant performance gains for fact-checking numerical claims. To improve compute efficiency in TTS, we introduce an adaptive mechanism that performs TTS selectively based on the perceived complexity of the claim. This approach achieves 1.8x higher efficiency than standard TTS, while delivering a notable 18.8% performance improvement over single-shot claim verification methods. Our code and data can be found at https://github.com/VenkteshV/VerifierFC",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "225",
        "title": "Reinforcement Learning for Durable Algorithmic Recourse",
        "author": [
            "Marina Ceccon",
            "Alessandro Fabris",
            "Goran RadanoviÄ",
            "Asia J. Biega",
            "Gian Antonio Susto"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22102",
        "abstract": "Algorithmic recourse seeks to provide individuals with actionable recommendations that increase their chances of receiving favorable outcomes from automated decision systems (e.g., loan approvals). While prior research has emphasized robustness to model updates, considerably less attention has been given to the temporal dynamics of recourse--particularly in competitive, resource-constrained settings where recommendations shape future applicant pools. In this work, we present a novel time-aware framework for algorithmic recourse, explicitly modeling how candidate populations adapt in response to recommendations. Additionally, we introduce a novel reinforcement learning (RL)-based recourse algorithm that captures the evolving dynamics of the environment to generate recommendations that are both feasible and valid. We design our recommendations to be durable, supporting validity over a predefined time horizon T. This durability allows individuals to confidently reapply after taking time to implement the suggested changes. Through extensive experiments in complex simulation environments, we show that our approach substantially outperforms existing baselines, offering a superior balance between feasibility and long-term validity. Together, these results underscore the importance of incorporating temporal and behavioral dynamics into the design of practical recourse systems.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "226",
        "title": "Large Material Gaussian Model for Relightable 3D Generation",
        "author": [
            "Jingrui Ye",
            "Lingting Zhu",
            "Runze Zhang",
            "Zeyu Hu",
            "Yingda Yin",
            "Lanjiong Li",
            "Lequan Yu",
            "Qingmin Liao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22112",
        "abstract": "The increasing demand for 3D assets across various industries necessitates efficient and automated methods for 3D content creation. Leveraging 3D Gaussian Splatting, recent large reconstruction models (LRMs) have demonstrated the ability to efficiently achieve high-quality 3D rendering by integrating multiview diffusion for generation and scalable transformers for reconstruction. However, existing models fail to produce the material properties of assets, which is crucial for realistic rendering in diverse lighting environments. In this paper, we introduce the Large Material Gaussian Model (MGM), a novel framework designed to generate high-quality 3D content with Physically Based Rendering (PBR) materials, ie, albedo, roughness, and metallic properties, rather than merely producing RGB textures with uncontrolled light baking. Specifically, we first fine-tune a new multiview material diffusion model conditioned on input depth and normal maps. Utilizing the generated multiview PBR images, we explore a Gaussian material representation that not only aligns with 2D Gaussian Splatting but also models each channel of the PBR materials. The reconstructed point clouds can then be rendered to acquire PBR attributes, enabling dynamic relighting by applying various ambient light maps. Extensive experiments demonstrate that the materials produced by our method not only exhibit greater visual appeal compared to baseline methods but also enhance material modeling, thereby enabling practical downstream rendering applications.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "227",
        "title": "SK2Decompile: LLM-based Two-Phase Binary Decompilation from Skeleton to Skin",
        "author": [
            "Hanzhuo Tan",
            "Weihao Li",
            "Xiaolong Tian",
            "Siyi Wang",
            "Jiaming Liu",
            "Jing Li",
            "Yuqun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22114",
        "abstract": "Large Language Models (LLMs) have emerged as a promising approach for binary decompilation. However, the existing LLM-based decompilers still are somewhat limited in effectively presenting a program's source-level structure with its original identifiers. To mitigate this, we introduce SK2Decompile, a novel two-phase approach to decompile from the skeleton (semantic structure) to the skin (identifier) of programs. Specifically, we first apply a Structure Recovery model to translate a program's binary code to an Intermediate Representation (IR) as deriving the program's \"skeleton\", i.e., preserving control flow and data structures while obfuscating all identifiers with generic placeholders. We also apply reinforcement learning to reward the model for producing program structures that adhere to the syntactic and semantic rules expected by compilers. Second, we apply an Identifier Naming model to produce meaningful identifiers which reflect actual program semantics as deriving the program's \"skin\". We train the Identifier Naming model with a separate reinforcement learning objective that rewards the semantic similarity between its predictions and the reference code. Such a two-phase decompilation process facilitates advancing the correctness and readability of decompilation independently. Our evaluations indicate that SK2Decompile, significantly outperforms the SOTA baselines, achieving 21.6% average re-executability rate gain over GPT-5-mini on the HumanEval dataset and 29.4% average R2I improvement over Idioms on the GitHub2025 benchmark.",
        "tags": [
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "228",
        "title": "Learning More with Less: A Dynamic Dual-Level Down-Sampling Framework for Efficient Policy Optimization",
        "author": [
            "Chao Wang",
            "Tao Yang",
            "Hongtao Tian",
            "Yunsheng Shi",
            "Qiyao Ma",
            "Xiaotao Liu",
            "Ting Yao",
            "Wenbo Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22115",
        "abstract": "Critic-free methods like GRPO reduce memory demands by estimating advantages from multiple rollouts but tend to converge slowly, as critical learning signals are diluted by an abundance of uninformative samples and tokens. To tackle this challenge, we propose the \\textbf{Dynamic Dual-Level Down-Sampling (D$^3$S)} framework that prioritizes the most informative samples and tokens across groups to improve the efficient of policy optimization. D$^3$S operates along two levels: (1) the sample-level, which selects a subset of rollouts to maximize advantage variance ($\\text{Var}(A)$). We theoretically proven that this selection is positively correlated with the upper bound of the policy gradient norms, yielding higher policy gradients. (2) the token-level, which prioritizes tokens with a high product of advantage magnitude and policy entropy ($|A_{i,t}|\\times H_{i,t}$), focusing updates on tokens where the policy is both uncertain and impactful. Moreover, to prevent overfitting to high-signal data, D$^3$S employs a dynamic down-sampling schedule inspired by curriculum learning. This schedule starts with aggressive down-sampling to accelerate early learning and gradually relaxes to promote robust generalization. Extensive experiments on Qwen2.5 and Llama3.1 demonstrate that integrating D$^3$S into advanced RL algorithms achieves state-of-the-art performance and generalization while requiring \\textit{fewer} samples and tokens across diverse reasoning benchmarks. Our code is added in the supplementary materials and will be made publicly available.",
        "tags": [
            "GRPO",
            "RL"
        ]
    },
    {
        "id": "229",
        "title": "Universal Legal Article Prediction via Tight Collaboration between Supervised Classification Model and LLM",
        "author": [
            "Xiao Chi",
            "Wenlin Zhong",
            "Yiquan Wu",
            "Wei Wang",
            "Kun Kuang",
            "Fei Wu",
            "Minghui Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22119",
        "abstract": "Legal Article Prediction (LAP) is a critical task in legal text classification, leveraging natural language processing (NLP) techniques to automatically predict relevant legal articles based on the fact descriptions of cases. As a foundational step in legal decision-making, LAP plays a pivotal role in determining subsequent judgments, such as charges and penalties. Despite its importance, existing methods face significant challenges in addressing the complexities of LAP. Supervised classification models (SCMs), such as CNN and BERT, struggle to fully capture intricate fact patterns due to their inherent limitations. Conversely, large language models (LLMs), while excelling in generative tasks, perform suboptimally in predictive scenarios due to the abstract and ID-based nature of legal articles. Furthermore, the diversity of legal systems across jurisdictions exacerbates the issue, as most approaches are tailored to specific countries and lack broader applicability. To address these limitations, we propose Uni-LAP, a universal framework for legal article prediction that integrates the strengths of SCMs and LLMs through tight collaboration. Specifically, in Uni-LAP, the SCM is enhanced with a novel Top-K loss function to generate accurate candidate articles, while the LLM employs syllogism-inspired reasoning to refine the final predictions. We evaluated Uni-LAP on datasets from multiple jurisdictions, and empirical results demonstrate that our approach consistently outperforms existing baselines, showcasing its effectiveness and generalizability.",
        "tags": [
            "BERT",
            "LLM"
        ]
    },
    {
        "id": "230",
        "title": "Multi-stage robust nonlinear model predictive control of a lower-limb exoskeleton robot",
        "author": [
            "Alireza Aliyari",
            "Gholamreza Vossoughi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22120",
        "abstract": "The use of exoskeleton robots is increasing due to the rising number of musculoskeletal injuries. However, their effectiveness depends heavily on the design of control systems. Designing robust controllers is challenging because of uncertainties in human-robot systems. Among various control strategies, Model Predictive Control (MPC) is a powerful approach due to its ability to handle constraints and optimize performance. Previous studies have used linearization-based methods to implement robust MPC on exoskeletons, but these can degrade performance due to nonlinearities in the robot's dynamics. To address this gap, this paper proposes a Robust Nonlinear Model Predictive Control (RNMPC) method, called multi-stage NMPC, to control a two-degree-of-freedom exoskeleton by solving a nonlinear optimization problem. This method uses multiple scenarios to represent system uncertainties. The study focuses on minimizing human-robot interaction forces during the swing phase, particularly when the robot carries unknown loads. Simulations and experimental tests show that the proposed method significantly improves robustness, outperforming non-robust NMPC. It achieves lower tracking errors and interaction forces under various uncertainties. For instance, when a 2 kg unknown payload is combined with external disturbances, the RMS values of thigh and shank interaction forces for multi-stage NMPC are reduced by 77 and 94 percent, respectively, compared to non-robust NMPC.",
        "tags": [
            "MPC",
            "Robotics"
        ]
    },
    {
        "id": "231",
        "title": "Mind the Missing: Variable-Aware Representation Learning for Irregular EHR Time Series using Large Language Models",
        "author": [
            "Jeong Eul Kwon",
            "Joo Heung Yoon",
            "Hyo Kyung Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22121",
        "abstract": "Irregular sampling and high missingness are intrinsic challenges in modeling time series derived from electronic health records (EHRs),where clinical variables are measured at uneven intervals depending on workflow and intervention timing. To address this, we propose VITAL, a variable-aware, large language model (LLM) based framework tailored for learning from irregularly sampled physiological time series. VITAL differentiates between two distinct types of clinical variables: vital signs, which are frequently recorded and exhibit temporal patterns, and laboratory tests, which are measured sporadically and lack temporal structure. It reprograms vital signs into the language space, enabling the LLM to capture temporal context and reason over missing values through explicit encoding. In contrast, laboratory variables are embedded either using representative summary values or a learnable [Not measured] token, depending on their availability. Extensive evaluations on the benchmark datasets from the PhysioNet demonstrate that VITAL outperforms state of the art methods designed for irregular time series. Furthermore, it maintains robust performance under high levels of missingness, which is prevalent in real world clinical scenarios where key variables are often unavailable.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "232",
        "title": "Multilingual Vision-Language Models, A Survey",
        "author": [
            "Andrei-Alexandru Manea",
            "JindÅich LibovickÃ½"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22123",
        "abstract": "This survey examines multilingual vision-language models that process text and images across languages. We review 31 models and 21 benchmarks, spanning encoder-only and generative architectures, and identify a key tension between language neutrality (consistent cross-lingual representations) and cultural awareness (adaptation to cultural contexts). Current training methods favor neutrality through contrastive learning, while cultural awareness depends on diverse data. Two-thirds of evaluation benchmarks use translation-based approaches prioritizing semantic consistency, though recent work incorporates culturally grounded content. We find discrepancies in cross-lingual capabilities and gaps between training objectives and evaluation goals.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "233",
        "title": "FoodSEM: Large Language Model Specialized in Food Named-Entity Linking",
        "author": [
            "Ana Gjorgjevikj",
            "Matej Martinc",
            "Gjorgjina Cenikj",
            "SaÅ¡o DÅ¾eroski",
            "Barbara KorouÅ¡iÄ Seljak",
            "Tome Eftimov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22125",
        "abstract": "This paper introduces FoodSEM, a state-of-the-art fine-tuned open-source large language model (LLM) for named-entity linking (NEL) to food-related ontologies. To the best of our knowledge, food NEL is a task that cannot be accurately solved by state-of-the-art general-purpose (large) language models or custom domain-specific models/systems. Through an instruction-response (IR) scenario, FoodSEM links food-related entities mentioned in a text to several ontologies, including FoodOn, SNOMED-CT, and the Hansard taxonomy. The FoodSEM model achieves state-of-the-art performance compared to related models/systems, with F1 scores even reaching 98% on some ontologies and datasets. The presented comparative analyses against zero-shot, one-shot, and few-shot LLM prompting baselines further highlight FoodSEM's superior performance over its non-fine-tuned version. By making FoodSEM and its related resources publicly available, the main contributions of this article include (1) publishing a food-annotated corpora into an IR format suitable for LLM fine-tuning/evaluation, (2) publishing a robust model to advance the semantic understanding of text in the food domain, and (3) providing a strong baseline on food NEL for future benchmarking.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "234",
        "title": "Multi-Agent Path Finding via Offline RL and LLM Collaboration",
        "author": [
            "Merve Atasever",
            "Matthew Hong",
            "Mihir Nitin Kulkarni",
            "Qingpei Li",
            "Jyotirmoy V. Deshmukh"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22130",
        "abstract": "Multi-Agent Path Finding (MAPF) poses a significant and challenging problem critical for applications in robotics and logistics, particularly due to its combinatorial complexity and the partial observability inherent in realistic environments. Decentralized reinforcement learning methods commonly encounter two substantial difficulties: first, they often yield self-centered behaviors among agents, resulting in frequent collisions, and second, their reliance on complex communication modules leads to prolonged training times, sometimes spanning weeks. To address these challenges, we propose an efficient decentralized planning framework based on the Decision Transformer (DT), uniquely leveraging offline reinforcement learning to substantially reduce training durations from weeks to mere hours. Crucially, our approach effectively handles long-horizon credit assignment and significantly improves performance in scenarios with sparse and delayed rewards. Furthermore, to overcome adaptability limitations inherent in standard RL methods under dynamic environmental changes, we integrate a large language model (GPT-4o) to dynamically guide agent policies. Extensive experiments in both static and dynamically changing environments demonstrate that our DT-based approach, augmented briefly by GPT-4o, significantly enhances adaptability and performance.",
        "tags": [
            "GPT",
            "LLM",
            "RL",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "235",
        "title": "R-Capsule: Compressing High-Level Plans for Efficient Large Language Model Reasoning",
        "author": [
            "Hongyu Shan",
            "Mingyang Song",
            "Chang Dai",
            "Di Liang",
            "Han Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22131",
        "abstract": "Chain-of-Thought (CoT) prompting helps Large Language Models (LLMs) tackle complex reasoning by eliciting explicit step-by-step rationales. However, CoT's verbosity increases latency and memory usage and may propagate early errors across long chains. We propose the Reasoning Capsule (R-Capsule), a framework that aims to combine the efficiency of latent reasoning with the transparency of explicit CoT. The core idea is to compress the high-level plan into a small set of learned latent tokens (a Reasoning Capsule) while keeping execution steps lightweight or explicit. This hybrid approach is inspired by the Information Bottleneck (IB) principle, where we encourage the capsule to be approximately minimal yet sufficient for the task. Minimality is encouraged via a low-capacity bottleneck, which helps improve efficiency. Sufficiency is encouraged via a dual objective: a primary task loss for answer accuracy and an auxiliary plan-reconstruction loss that encourages the capsule to faithfully represent the original textual plan. The reconstruction objective helps ground the latent space, thereby improving interpretability and reducing the use of uninformative shortcuts. Our framework strikes a balance between efficiency, accuracy, and interpretability, thereby reducing the visible token footprint of reasoning while maintaining or improving accuracy on complex benchmarks. Our codes are available at: https://anonymous.4open.science/r/Reasoning-Capsule-7BE0",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "236",
        "title": "Self-Supervised Point Cloud Completion based on Multi-View Augmentations of Single Partial Point Cloud",
        "author": [
            "Jingjing Lu",
            "Huilong Pi",
            "Yunchuan Qin",
            "Zhuo Tang",
            "Ruihui Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22132",
        "abstract": "Point cloud completion aims to reconstruct complete shapes from partial observations. Although current methods have achieved remarkable performance, they still have some limitations: Supervised methods heavily rely on ground truth, which limits their generalization to real-world datasets due to the synthetic-to-real domain gap. Unsupervised methods require complete point clouds to compose unpaired training data, and weakly-supervised methods need multi-view observations of the object. Existing self-supervised methods frequently produce unsatisfactory predictions due to the limited capabilities of their self-supervised signals. To overcome these challenges, we propose a novel self-supervised point cloud completion method. We design a set of novel self-supervised signals based on multi-view augmentations of the single partial point cloud. Additionally, to enhance the model's learning ability, we first incorporate Mamba into self-supervised point cloud completion task, encouraging the model to generate point clouds with better quality. Experiments on synthetic and real-world datasets demonstrate that our method achieves state-of-the-art results.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "237",
        "title": "Bridging Draft Policy Misalignment: Group Tree Optimization for Speculative Decoding",
        "author": [
            "Shijing Hu",
            "Jingyang Li",
            "Zhihui Lu",
            "Pan Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22134",
        "abstract": "Speculative decoding accelerates large language model (LLM) inference by letting a lightweight draft model propose multiple tokens that the target model verifies in parallel. Yet existing training objectives optimize only a single greedy draft path, while decoding follows a tree policy that re-ranks and verifies multiple branches. This draft policy misalignment limits achievable speedups. We introduce Group Tree Optimization (GTO), which aligns training with the decoding-time tree policy through two components: (i) Draft Tree Reward, a sampling-free objective equal to the expected acceptance length of the draft tree under the target model, directly measuring decoding performance; (ii) Group-based Draft Policy Training, a stable optimization scheme that contrasts trees from the current and a frozen reference draft model, forming debiased group-standardized advantages and applying a PPO-style surrogate along the longest accepted sequence for robust updates. We further prove that increasing our Draft Tree Reward provably improves acceptance length and speedup. Across dialogue (MT-Bench), code (HumanEval), and math (GSM8K), and multiple LLMs (e.g., LLaMA-3.1-8B, LLaMA-3.3-70B, Vicuna-1.3-13B, DeepSeek-R1-Distill-LLaMA-8B), GTO increases acceptance length by 7.4% and yields an additional 7.7% speedup over prior state-of-the-art EAGLE-3. By bridging draft policy misalignment, GTO offers a practical, general solution for efficient LLM inference.",
        "tags": [
            "DeepSeek",
            "LLM",
            "LLaMA",
            "PPO",
            "Vicuna"
        ]
    },
    {
        "id": "238",
        "title": "Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach",
        "author": [
            "Seoyoung Lee",
            "Seonbin Yoon",
            "Seongbeen Lee",
            "Hyesoo Kim",
            "Joo Yong Sim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22137",
        "abstract": "GUI task automation streamlines repetitive tasks, but existing LLM or VLM-based planner-executor agents suffer from brittle generalization, high latency, and limited long-horizon coherence. Their reliance on single-shot reasoning or static plans makes them fragile under UI changes or complex tasks. Log2Plan addresses these limitations by combining a structured two-level planning framework with a task mining approach over user behavior logs, enabling robust and adaptable GUI automation. Log2Plan constructs high-level plans by mapping user commands to a structured task dictionary, enabling consistent and generalizable automation. To support personalization and reuse, it employs a task mining approach from user behavior logs that identifies user-specific patterns. These high-level plans are then grounded into low-level action sequences by interpreting real-time GUI context, ensuring robust execution across varying interfaces. We evaluated Log2Plan on 200 real-world tasks, demonstrating significant improvements in task success rate and execution time. Notably, it maintains over 60.0% success rate even on long-horizon task sequences, highlighting its robustness in complex, multi-step workflows.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "239",
        "title": "From Long to Lean: Performance-aware and Adaptive Chain-of-Thought Compression via Multi-round Refinement",
        "author": [
            "Jianzhi Yan",
            "Le Liu",
            "Youcheng Pan",
            "Shiwei Chen",
            "Zike Yuan",
            "Yang Xiang",
            "Buzhou Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22144",
        "abstract": "Chain-of-Thought (CoT) reasoning improves performance on complex tasks but introduces significant inference latency due to verbosity. We propose Multiround Adaptive Chain-of-Thought Compression (MACC), a framework that leverages the token elasticity phenomenon--where overly small token budgets can paradoxically increase output length--to progressively compress CoTs via multiround refinement. This adaptive strategy allows MACC to determine the optimal compression depth for each input. Our method achieves an average accuracy improvement of 5.6 percent over state-of-the-art baselines, while also reducing CoT length by an average of 47 tokens and significantly lowering latency. Furthermore, we show that test-time performance--accuracy and token length--can be reliably predicted using interpretable features like perplexity and compression rate on the training set. Evaluated across different models, our method enables efficient model selection and forecasting without repeated fine-tuning, demonstrating that CoT compression is both effective and predictable. Our code will be released in https://github.com/Leon221220/MACC.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "240",
        "title": "DemoGrasp: Universal Dexterous Grasping from a Single Demonstration",
        "author": [
            "Haoqi Yuan",
            "Ziye Huang",
            "Ye Wang",
            "Chuan Mao",
            "Chaoyi Xu",
            "Zongqing Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22149",
        "abstract": "Universal grasping with multi-fingered dexterous hands is a fundamental challenge in robotic manipulation. While recent approaches successfully learn closed-loop grasping policies using reinforcement learning (RL), the inherent difficulty of high-dimensional, long-horizon exploration necessitates complex reward and curriculum design, often resulting in suboptimal solutions across diverse objects. We propose DemoGrasp, a simple yet effective method for learning universal dexterous grasping. We start from a single successful demonstration trajectory of grasping a specific object and adapt to novel objects and poses by editing the robot actions in this trajectory: changing the wrist pose determines where to grasp, and changing the hand joint angles determines how to grasp. We formulate this trajectory editing as a single-step Markov Decision Process (MDP) and use RL to optimize a universal policy across hundreds of objects in parallel in simulation, with a simple reward consisting of a binary success term and a robot-table collision penalty. In simulation, DemoGrasp achieves a 95% success rate on DexGraspNet objects using the Shadow Hand, outperforming previous state-of-the-art methods. It also shows strong transferability, achieving an average success rate of 84.6% across diverse dexterous hand embodiments on six unseen object datasets, while being trained on only 175 objects. Through vision-based imitation learning, our policy successfully grasps 110 unseen real-world objects, including small, thin items. It generalizes to spatial, background, and lighting changes, supports both RGB and depth inputs, and extends to language-guided grasping in cluttered scenes.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "241",
        "title": "Collusion-Driven Impersonation Attack on Channel-Resistant RF Fingerprinting",
        "author": [
            "Zhou Xu",
            "Guyue Li",
            "Zhe Peng",
            "Aiqun Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22154",
        "abstract": "Radio frequency fingerprint (RFF) is a promising device identification technology, with recent research shifting from robustness to security due to growing concerns over vulnerabilities. To date, while the security of RFF against basic spoofing such as MAC address tampering has been validated, its resilience to advanced mimicry remains unknown. To address this gap, we propose a collusion-driven impersonation attack that achieves RF-level mimicry, successfully breaking RFF identification systems across diverse environments. Specifically, the attacker synchronizes with a colluding receiver to match the centralized logarithmic power spectrum (CLPS) of the legitimate transmitter; once the colluder deems the CLPS identical, the victim receiver will also accept the forged fingerprint, completing RF-level spoofing. Given that the distribution of CLPS features is relatively concentrated and has a clear underlying structure, we design a spoofed signal generation network that integrates a variational autoencoder (VAE) with a multi-objective loss function to enhance the similarity and deceptive capability of the generated samples. We carry out extensive simulations, validating cross-channel attacks in environments that incorporate standard channel variations including additive white Gaussian noise (AWGN), multipath fading, and Doppler shift. The results indicate that the proposed attack scheme essentially maintains a success rate of over 95% under different channel conditions, revealing the effectiveness of this attack.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "242",
        "title": "Context Parametrization with Compositional Adapters",
        "author": [
            "Josip JukiÄ",
            "Martin Tutek",
            "Jan Å najder"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22158",
        "abstract": "Large language models (LLMs) often seamlessly adapt to new tasks through in-context learning (ICL) or supervised fine-tuning (SFT). However, both of these approaches face key limitations: ICL is inefficient when handling many demonstrations, and SFT incurs training overhead while sacrificing flexibility. Mapping instructions or demonstrations from context directly into adapter parameters offers an appealing alternative. While prior work explored generating adapters based on a single input context, it has overlooked the need to integrate multiple chunks of information. To address this gap, we introduce CompAs, a meta-learning framework that translates context into adapter parameters with a compositional structure. Adapters generated this way can be merged algebraically, enabling instructions, demonstrations, or retrieved passages to be seamlessly combined without reprocessing long prompts. Critically, this approach yields three benefits: lower inference cost, robustness to long-context instability, and establishes a principled solution when input exceeds the model's context window. Furthermore, CompAs encodes information into adapter parameters in a reversible manner, enabling recovery of input context through a decoder, facilitating safety and security. Empirical results on diverse multiple-choice and extractive question answering tasks show that CompAs outperforms ICL and prior generator-based methods, especially when scaling to more inputs. Our work establishes composable adapter generation as a practical and efficient alternative for scaling LLM deployment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "243",
        "title": "Pushing Toward the Simplex Vertices: A Simple Remedy for Code Collapse in Smoothed Vector Quantization",
        "author": [
            "Takashi Morita"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22161",
        "abstract": "Vector quantization, which discretizes a continuous vector space into a finite set of representative vectors (a codebook), has been widely adopted in modern machine learning. Despite its effectiveness, vector quantization poses a fundamental challenge: the non-differentiable quantization step blocks gradient backpropagation. Smoothed vector quantization addresses this issue by relaxing the hard assignment of a codebook vector into a weighted combination of codebook entries, represented as the matrix product of a simplex vector and the codebook. Effective smoothing requires two properties: (1) smoothed quantizers should remain close to a onehot vector, ensuring tight approximation, and (2) all codebook entries should be utilized, preventing code collapse. Existing methods typically address these desiderata separately. By contrast, the present study introduces a simple and intuitive regularization that promotes both simultaneously by minimizing the distance between each simplex vertex and its $K$-nearest smoothed quantizers. Experiments on representative benchmarks, including discrete image autoencoding and contrastive speech representation learning, demonstrate that the proposed method achieves more reliable codebook utilization and improves performance compared to prior approaches.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "244",
        "title": "Lightweight error mitigation strategies for post-training N:M activation sparsity in LLMs",
        "author": [
            "Shirin Alanova",
            "Kristina Kazistova",
            "Ekaterina Galaeva",
            "Alina Kostromina",
            "Vladimir Smirnov",
            "Redko Dmitry",
            "Alexey Dontsov",
            "Maxim Zhelnin",
            "Evgeny Burnaev",
            "Egor Shvetsov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22166",
        "abstract": "The demand for efficient large language model (LLM) inference has intensified the focus on sparsification techniques. While semi-structured (N:M) pruning is well-established for weights, its application to activation pruning remains underexplored despite its potential for dynamic, input-adaptive compression and reductions in I/O overhead. This work presents a comprehensive analysis of methods for post-training N:M activation pruning in LLMs. Across multiple LLMs, we demonstrate that pruning activations enables superior preservation of generative capabilities compared to weight pruning at equivalent sparsity levels. We evaluate lightweight, plug-and-play error mitigation techniques and pruning criteria, establishing strong hardware-friendly baselines that require minimal calibration. Furthermore, we explore sparsity patterns beyond NVIDIA's standard 2:4, showing that the 16:32 pattern achieves performance nearly on par with unstructured sparsity. However, considering the trade-off between flexibility and hardware implementation complexity, we focus on the 8:16 pattern as a superior candidate. Our findings provide both effective practical methods for activation pruning and a motivation for future hardware to support more flexible sparsity patterns. Our code is available https://anonymous.4open.science/r/Structured-Sparse-Activations-Inference-EC3C/README.md .",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "245",
        "title": "DragGANSpace: Latent Space Exploration and Control for GANs",
        "author": [
            "Kirsten Odendaal",
            "Neela Kaushik",
            "Spencer Halverson"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22169",
        "abstract": "This work integrates StyleGAN, DragGAN and Principal Component Analysis (PCA) to enhance the latent space efficiency and controllability of GAN-generated images. Style-GAN provides a structured latent space, DragGAN enables intuitive image manipulation, and PCA reduces dimensionality and facilitates cross-model alignment for more streamlined and interpretable exploration of latent spaces. We apply our techniques to the Animal Faces High Quality (AFHQ) dataset, and find that our approach of integrating PCA-based dimensionality reduction with the Drag-GAN framework for image manipulation retains performance while improving optimization efficiency. Notably, introducing PCA into the latent W+ layers of DragGAN can consistently reduce the total optimization time while maintaining good visual quality and even boosting the Structural Similarity Index Measure (SSIM) of the optimized image, particularly in shallower latent spaces (W+ layers = 3). We also demonstrate capability for aligning images generated by two StyleGAN models trained on similar but distinct data domains (AFHQ-Dog and AFHQ-Cat), and show that we can control the latent space of these aligned images to manipulate the images in an intuitive and interpretable manner. Our findings highlight the possibility for efficient and interpretable latent space control for a wide range of image synthesis and editing applications.",
        "tags": [
            "GAN",
            "StyleGAN"
        ]
    },
    {
        "id": "246",
        "title": "Leveraging LLM Agents for Automated Video Game Testing",
        "author": [
            "Chengjia Wang",
            "Lanling Tang",
            "Ming Yuan",
            "Jiongchi Yu",
            "Xiaofei Xie",
            "Jiajun Bu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22170",
        "abstract": "Testing MMORPGs (Massively Multiplayer Online Role-Playing Games) is a critical yet labor-intensive task in game development due to their complexity and frequent updating nature. Traditional automated game testing approaches struggle to achieve high state coverage and efficiency in these rich, open-ended environments, while existing LLM-based game-playing approaches are limited to shallow reasoning ability in understanding complex game state-action spaces and long-complex tasks. To address these challenges, we propose TITAN, an effective LLM-driven agent framework for intelligent MMORPG testing. TITAN incorporates four key components to: (1) perceive and abstract high-dimensional game states, (2) proactively optimize and prioritize available actions, (3) enable long-horizon reasoning with action trace memory and reflective self-correction, and (4) employ LLM-based oracles to detect potential functional and logic bugs with diagnostic reports.\nWe implement the prototype of TITAN and evaluate it on two large-scale commercial MMORPGs spanning both PC and mobile platforms. In our experiments, TITAN achieves significantly higher task completion rates (95%) and bug detection performance compared to existing automated game testing approaches. An ablation study further demonstrates that each core component of TITAN contributes substantially to its overall performance. Notably, TITAN detects four previously unknown bugs that prior testing approaches fail to identify. We provide an in-depth discussion of these results, which offer guidance for new avenues of advancing intelligent, general-purpose testing systems. Moreover, TITAN has been deployed in eight real-world game QA pipelines, underscoring its practical impact as an LLM-driven game testing framework.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "247",
        "title": "When Does Reasoning Matter? A Controlled Study of Reasoning's Contribution to Model Performance",
        "author": [
            "Nicolas Boizard",
            "Hippolyte Gisserot-Boukhlef",
            "Kevin El-Haddad",
            "CÃ©line Hudelot",
            "Pierre Colombo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22193",
        "abstract": "Large Language Models (LLMs) with reasoning capabilities have achieved state-of-the-art performance on a wide range of tasks. Despite its empirical success, the tasks and model scales at which reasoning becomes effective, as well as its training and inference costs, remain underexplored. In this work, we rely on a synthetic data distillation framework to conduct a large-scale supervised study. We compare Instruction Fine-Tuning (IFT) and reasoning models of varying sizes, on a wide range of math-centric and general-purpose tasks, evaluating both multiple-choice and open-ended formats. Our analysis reveals that reasoning consistently improves model performance, often matching or surpassing significantly larger IFT systems. Notably, while IFT remains Pareto-optimal in training and inference costs, reasoning models become increasingly valuable as model size scales, overcoming IFT performance limits on reasoning-intensive and open-ended tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "248",
        "title": "Actions as Language: Fine-Tuning VLMs into VLAs Without Catastrophic Forgetting",
        "author": [
            "Asher J. Hancock",
            "Xindi Wu",
            "Lihan Zha",
            "Olga Russakovsky",
            "Anirudha Majumdar"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22195",
        "abstract": "Fine-tuning vision-language models (VLMs) on robot teleoperation data to create vision-language-action (VLA) models is a promising paradigm for training generalist policies, but it suffers from a fundamental tradeoff: learning to produce actions often diminishes the VLM's foundational reasoning and multimodal understanding, hindering generalization to novel scenarios, instruction following, and semantic understanding. We argue that this catastrophic forgetting is due to a distribution mismatch between the VLM's internet-scale pretraining corpus and the robotics fine-tuning data. Inspired by this observation, we introduce VLM2VLA: a VLA training paradigm that first resolves this mismatch at the data level by representing low-level actions with natural language. This alignment makes it possible to train VLAs solely with Low-Rank Adaptation (LoRA), thereby minimally modifying the VLM backbone and averting catastrophic forgetting. As a result, the VLM can be fine-tuned on robot teleoperation data without fundamentally altering the underlying architecture and without expensive co-training on internet-scale VLM datasets. Through extensive Visual Question Answering (VQA) studies and over 800 real-world robotics experiments, we demonstrate that VLM2VLA preserves the VLM's core capabilities, enabling zero-shot generalization to novel tasks that require open-world semantic reasoning and multilingual instruction following.",
        "tags": [
            "LoRA",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "249",
        "title": "MimicDreamer: Aligning Human and Robot Demonstrations for Scalable VLA Training",
        "author": [
            "Haoyun Li",
            "Ivan Zhang",
            "Runqi Ouyang",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Zhiqin Yang",
            "Zhentao Zhang",
            "Boyuan Wang",
            "Chaojun Ni",
            "Wenkang Qin",
            "Xinze Chen",
            "Yun Ye",
            "Guan Huang",
            "Zhenbo Song",
            "Xingang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22199",
        "abstract": "Vision Language Action (VLA) models derive their generalization capability from diverse training data, yet collecting embodied robot interaction data remains prohibitively expensive. In contrast, human demonstration videos are far more scalable and cost-efficient to collect, and recent studies confirm their effectiveness in training VLA models. However, a significant domain gap persists between human videos and robot-executed videos, including unstable camera viewpoints, visual discrepancies between human hands and robotic arms, and differences in motion dynamics. To bridge this gap, we propose MimicDreamer, a framework that turns fast, low-cost human demonstrations into robot-usable supervision by jointly aligning vision, viewpoint, and actions to directly support policy training. For visual alignment, we propose H2R Aligner, a video diffusion model that generates high-fidelity robot demonstration videos by transferring motion from human manipulation footage. For viewpoint stabilization, EgoStabilizer is proposed, which canonicalizes egocentric videos via homography and inpaints occlusions and distortions caused by warping. For action alignment, we map human hand trajectories to the robot frame and apply a constrained inverse kinematics solver to produce feasible, low-jitter joint commands with accurate pose tracking. Empirically, VLA models trained purely on our synthesized human-to-robot videos achieve few-shot execution on real robots. Moreover, scaling training with human data significantly boosts performance compared to models trained solely on real robot data; our approach improves the average success rate by 14.7\\% across six representative manipulation tasks.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "250",
        "title": "Library Hallucinations in LLMs: Risk Analysis Grounded in Developer Queries",
        "author": [
            "Lukas Twist",
            "Jie M. Zhang",
            "Mark Harman",
            "Helen Yannakoudakis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22202",
        "abstract": "Large language models (LLMs) are increasingly used to generate code, yet they continue to hallucinate, often inventing non-existent libraries. Such library hallucinations are not just benign errors: they can mislead developers, break builds, and expose systems to supply chain threats such as slopsquatting. Despite increasing awareness of these risks, little is known about how real-world prompt variations affect hallucination rates. Therefore, we present the first systematic study of how user-level prompt variations impact library hallucinations in LLM-generated code. We evaluate six diverse LLMs across two hallucination types: library name hallucinations (invalid imports) and library member hallucinations (invalid calls from valid libraries). We investigate how realistic user language extracted from developer forums and how user errors of varying degrees (one- or multi-character misspellings and completely fake names/members) affect LLM hallucination rates. Our findings reveal systemic vulnerabilities: one-character misspellings in library names trigger hallucinations in up to 26% of tasks, fake library names are accepted in up to 99% of tasks, and time-related prompts lead to hallucinations in up to 84% of tasks. Prompt engineering shows promise for mitigating hallucinations, but remains inconsistent and LLM-dependent. Our results underscore the fragility of LLMs to natural prompt variation and highlight the urgent need for safeguards against library-related hallucinations and their potential exploitation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "251",
        "title": "From Watch to Imagine: Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment",
        "author": [
            "Ke Ye",
            "Jiaming Zhou",
            "Yuanfeng Qiu",
            "Jiayi Liu",
            "Shihui Zhou",
            "Kun-Yu Lin",
            "Junwei Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22205",
        "abstract": "Generalizing to long-horizon manipulation tasks in a zero-shot setting remains a central challenge in robotics. Current multimodal foundation based approaches, despite their capabilities, typically fail to decompose high-level commands into executable action sequences from static visual input alone. To address this challenge, we introduce Super-Mimic, a hierarchical framework that enables zero-shot robotic imitation by directly inferring procedural intent from unscripted human demonstration videos. Our framework is composed of two sequential modules. First, a Human Intent Translator (HIT) parses the input video using multimodal reasoning to produce a sequence of language-grounded subtasks. These subtasks then condition a Future Dynamics Predictor (FDP), which employs a generative model that synthesizes a physically plausible video rollout for each step. The resulting visual trajectories are dynamics-aware, explicitly modeling crucial object interactions and contact points to guide the low-level controller. We validate this approach through extensive experiments on a suite of long-horizon manipulation tasks, where Super-Mimic significantly outperforms state-of-the-art zero-shot methods by over 20\\%. These results establish that coupling video-driven intent parsing with prospective dynamics modeling is a highly effective strategy for developing general-purpose robotic systems.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "252",
        "title": "The Outputs of Large Language Models are Meaningless",
        "author": [
            "Anandi Hattiangadi",
            "Anders J. Schoubye"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22206",
        "abstract": "In this paper, we offer a simple argument for the conclusion that the outputs of large language models (LLMs) are meaningless. Our argument is based on two key premises: (a) that certain kinds of intentions are needed in order for LLMs' outputs to have literal meanings, and (b) that LLMs cannot plausibly have the right kinds of intentions. We defend this argument from various types of responses, for example, the semantic externalist argument that deference can be assumed to take the place of intentions and the semantic internalist argument that meanings can be defined purely in terms of intrinsic relations between concepts, such as conceptual roles. We conclude the paper by discussing why, even if our argument is sound, the outputs of LLMs nevertheless seem meaningful and can be used to acquire true beliefs and even knowledge.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "253",
        "title": "Question-Driven Analysis and Synthesis: Building Interpretable Thematic Trees with LLMs for Text Clustering and Controllable Generation",
        "author": [
            "Tiago Fernandes Tavares"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22211",
        "abstract": "Unsupervised analysis of text corpora is challenging, especially in data-scarce domains where traditional topic models struggle. While these models offer a solution, they typically describe clusters with lists of keywords that require significant manual effort to interpret and often lack semantic coherence. To address this critical interpretability gap, we introduce Recursive Thematic Partitioning (RTP), a novel framework that leverages Large Language Models (LLMs) to interactively build a binary tree. Each node in the tree is a natural language question that semantically partitions the data, resulting in a fully interpretable taxonomy where the logic of each cluster is explicit. Our experiments demonstrate that RTP's question-driven hierarchy is more interpretable than the keyword-based topics from a strong baseline like BERTopic. Furthermore, we establish the quantitative utility of these clusters by showing they serve as powerful features in downstream classification tasks, particularly when the data's underlying themes correlate with the task labels. RTP introduces a new paradigm for data exploration, shifting the focus from statistical pattern discovery to knowledge-driven thematic analysis. Furthermore, we demonstrate that the thematic paths from the RTP tree can serve as structured, controllable prompts for generative models. This transforms our analytical framework into a powerful tool for synthesis, enabling the consistent imitation of specific characteristics discovered in the source corpus.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "254",
        "title": "Impact of Collective Behaviors of Autonomous Vehicles on Urban Traffic Dynamics: A Multi-Agent Reinforcement Learning Approach",
        "author": [
            "Ahmet Onur Akman",
            "Anastasia Psarou",
            "ZoltÃ¡n GyÃ¶rgy Varga",
            "Grzegorz JamrÃ³z",
            "RafaÅ Kucharski"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22216",
        "abstract": "This study examines the potential impact of reinforcement learning (RL)-enabled autonomous vehicles (AV) on urban traffic flow in a mixed traffic environment. We focus on a simplified day-to-day route choice problem in a multi-agent setting. We consider a city network where human drivers travel through their chosen routes to reach their destinations in minimum travel time. Then, we convert one-third of the population into AVs, which are RL agents employing Deep Q-learning algorithm. We define a set of optimization targets, or as we call them behaviors, namely selfish, collaborative, competitive, social, altruistic, and malicious. We impose a selected behavior on AVs through their rewards. We run our simulations using our in-house developed RL framework PARCOUR. Our simulations reveal that AVs optimize their travel times by up to 5\\%, with varying impacts on human drivers' travel times depending on the AV behavior. In all cases where AVs adopt a self-serving behavior, they achieve shorter travel times than human drivers. Our findings highlight the complexity differences in learning tasks of each target behavior. We demonstrate that the multi-agent RL setting is applicable for collective routing on traffic networks, though their impact on coexisting parties greatly varies with the behaviors adopted.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "255",
        "title": "VizGen: Data Exploration and Visualization from Natural Language via a Multi-Agent AI Architecture",
        "author": [
            "Sandaru Fernando",
            "Imasha Jayarathne",
            "Sithumini Abeysekara",
            "Shanuja Sithamparanthan",
            "Thushari Silva",
            "Deshan Jayawardana"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22218",
        "abstract": "Data visualization is essential for interpreting complex datasets, yet traditional tools often require technical expertise, limiting accessibility. VizGen is an AI-assisted graph generation system that empowers users to create meaningful visualizations using natural language. Leveraging advanced NLP and LLMs like Claude 3.7 Sonnet and Gemini 2.0 Flash, it translates user queries into SQL and recommends suitable graph types. Built on a multi-agent architecture, VizGen handles SQL generation, graph creation, customization, and insight extraction. Beyond visualization, it analyzes data for patterns, anomalies, and correlations, and enhances user understanding by providing explanations enriched with contextual information gathered from the internet. The system supports real-time interaction with SQL databases and allows conversational graph refinement, making data analysis intuitive and accessible. VizGen democratizes data visualization by bridging the gap between technical complexity and user-friendly design.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "256",
        "title": "StableToken: A Noise-Robust Semantic Speech Tokenizer for Resilient SpeechLLMs",
        "author": [
            "Yuhan Song",
            "Linhao Zhang",
            "Chuhan Wu",
            "Aiwei Liu",
            "Wei Jia",
            "Houfeng Wang",
            "Xiao Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22220",
        "abstract": "Prevalent semantic speech tokenizers, designed to capture linguistic content, are surprisingly fragile. We find they are not robust to meaning-irrelevant acoustic perturbations; even at high Signal-to-Noise Ratios (SNRs) where speech is perfectly intelligible, their output token sequences can change drastically, increasing the learning burden for downstream LLMs. This instability stems from two flaws: a brittle single-path quantization architecture and a distant training signal indifferent to intermediate token stability. To address this, we introduce StableToken, a tokenizer that achieves stability through a consensus-driven mechanism. Its multi-branch architecture processes audio in parallel, and these representations are merged via a powerful bit-wise voting mechanism to form a single, stable token sequence. StableToken sets a new state-of-the-art in token stability, drastically reducing Unit Edit Distance (UED) under diverse noise conditions. This foundational stability translates directly to downstream benefits, significantly improving the robustness of SpeechLLMs on a variety of tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "257",
        "title": "Polysemous Language Gaussian Splatting via Matching-based Mask Lifting",
        "author": [
            "Jiayu Ding",
            "Xinpeng Liu",
            "Zhiyi Pan",
            "Shiqiang Long",
            "Ge Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22225",
        "abstract": "Lifting 2D open-vocabulary understanding into 3D Gaussian Splatting (3DGS) scenes is a critical challenge. However, mainstream methods suffer from three key flaws: (i) their reliance on costly per-scene retraining prevents plug-and-play application; (ii) their restrictive monosemous design fails to represent complex, multi-concept semantics; and (iii) their vulnerability to cross-view semantic inconsistencies corrupts the final semantic representation. To overcome these limitations, we introduce MUSplat, a training-free framework that abandons feature optimization entirely. Leveraging a pre-trained 2D segmentation model, our pipeline generates and lifts multi-granularity 2D masks into 3D, where we estimate a foreground probability for each Gaussian point to form initial object groups. We then optimize the ambiguous boundaries of these initial groups using semantic entropy and geometric opacity. Subsequently, by interpreting the object's appearance across its most representative viewpoints, a Vision-Language Model (VLM) distills robust textual features that reconciles visual inconsistencies, enabling open-vocabulary querying via semantic matching. By eliminating the costly per-scene training process, MUSplat reduces scene adaptation time from hours to mere minutes. On benchmark tasks for open-vocabulary 3D object selection and semantic segmentation, MUSplat outperforms established training-based frameworks while simultaneously addressing their monosemous limitations.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "258",
        "title": "UrbanFeel: A Comprehensive Benchmark for Temporal and Perceptual Understanding of City Scenes through Human Perspective",
        "author": [
            "Jun He",
            "Yi Lin",
            "Zilong Huang",
            "Jiacong Yin",
            "Junyan Ye",
            "Yuchuan Zhou",
            "Weijia Li",
            "Xiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22228",
        "abstract": "Urban development impacts over half of the global population, making human-centered understanding of its structural and perceptual changes essential for sustainable development. While Multimodal Large Language Models (MLLMs) have shown remarkable capabilities across various domains, existing benchmarks that explore their performance in urban environments remain limited, lacking systematic exploration of temporal evolution and subjective perception of urban environment that aligns with human perception. To address these limitations, we propose UrbanFeel, a comprehensive benchmark designed to evaluate the performance of MLLMs in urban development understanding and subjective environmental perception. UrbanFeel comprises 14.3K carefully constructed visual questions spanning three cognitively progressive dimensions: Static Scene Perception, Temporal Change Understanding, and Subjective Environmental Perception. We collect multi-temporal single-view and panoramic street-view images from 11 representative cities worldwide, and generate high-quality question-answer pairs through a hybrid pipeline of spatial clustering, rule-based generation, model-assisted prompting, and manual annotation. Through extensive evaluation of 20 state-of-the-art MLLMs, we observe that Gemini-2.5 Pro achieves the best overall performance, with its accuracy approaching human expert levels and narrowing the average gap to just 1.5\\%. Most models perform well on tasks grounded in scene understanding. In particular, some models even surpass human annotators in pixel-level change detection. However, performance drops notably in tasks requiring temporal reasoning over urban development. Additionally, in the subjective perception dimension, several models reach human-level or even higher consistency in evaluating dimension such as beautiful and safety.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "259",
        "title": "Fairness-Aware Reinforcement Learning (FAReL): A Framework for Transparent and Balanced Sequential Decision-Making",
        "author": [
            "Alexandra Cimpean",
            "Nicole Orzan",
            "Catholijn Jonker",
            "Pieter Libin",
            "Ann NowÃ©"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22232",
        "abstract": "Equity in real-world sequential decision problems can be enforced using fairness-aware methods. Therefore, we require algorithms that can make suitable and transparent trade-offs between performance and the desired fairness notions. As the desired performance-fairness trade-off is hard to specify a priori, we propose a framework where multiple trade-offs can be explored. Insights provided by the reinforcement learning algorithm regarding the obtainable performance-fairness trade-offs can then guide stakeholders in selecting the most appropriate policy. To capture fairness, we propose an extended Markov decision process, $f$MDP, that explicitly encodes individuals and groups. Given this $f$MDP, we formalise fairness notions in the context of sequential decision problems and formulate a fairness framework that computes fairness measures over time. We evaluate our framework in two scenarios with distinct fairness requirements: job hiring, where strong teams must be composed while treating applicants equally, and fraud detection, where fraudulent transactions must be detected while ensuring the burden on customers is fairly distributed. We show that our framework learns policies that are more fair across multiple scenarios, with only minor loss in performance reward. Moreover, we observe that group and individual fairness notions do not necessarily imply one another, highlighting the benefit of our framework in settings where both fairness types are desired. Finally, we provide guidelines on how to apply this framework across different problem settings.",
        "tags": [
            "Detection",
            "RL"
        ]
    },
    {
        "id": "260",
        "title": "FeatBench: Evaluating Coding Agents on Feature Implementation for Vibe Coding",
        "author": [
            "Haorui Chen",
            "Chengze Li",
            "Jia Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22237",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has given rise to a novel software development paradigm known as \"vibe coding,\" where users interact with coding agents through high-level natural language. However, existing evaluation benchmarks for code generation inadequately assess an agent's vibe coding capabilities. Existing benchmarks are misaligned, as they either require code-level specifications or focus narrowly on issue-solving, neglecting the critical scenario of feature implementation within the vibe coding paradiam. To address this gap, we propose FeatBench, a novel benchmark for vibe coding that focuses on feature implementation. Our benchmark is distinguished by several key features: 1. Pure Natural Language Prompts. Task inputs consist solely of abstract natural language descriptions, devoid of any code or structural hints. 2. A Rigorous & Evolving Data Collection Process. FeatBench is built on a multi-level filtering pipeline to ensure quality and a fully automated pipeline to evolve the benchmark, mitigating data contamination. 3. Comprehensive Test Cases. Each task includes Fail-to-Pass (F2P) and Pass-to-Pass (P2P) tests to verify correctness and prevent regressions. 4. Diverse Application Domains. The benchmark includes repositories from diverse domains to ensure it reflects real-world scenarios. We evaluate two state-of-the-art agent frameworks with four leading LLMs on FeatBench. Our evaluation reveals that feature implementation within the vibe coding paradigm is a significant challenge, with the highest success rate of only 29.94%. Our analysis also reveals a tendency for \"aggressive implementation,\" a strategy that paradoxically leads to both critical failures and superior software design. We release FeatBench, our automated collection pipeline, and all experimental results to facilitate further community research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "261",
        "title": "FLEXI: Benchmarking Full-duplex Human-LLM Speech Interaction",
        "author": [
            "Yuan Ge",
            "Saihan Chen",
            "Jingqi Xiao",
            "Xiaoqian Liu",
            "Tong Xiao",
            "Yan Xiang",
            "Zhengtao Yu",
            "Jingbo Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22243",
        "abstract": "Full-Duplex Speech-to-Speech Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling real-time spoken dialogue systems. However, benchmarking and modeling these models remains a fundamental challenge. We introduce FLEXI, the first benchmark for full-duplex LLM-human spoken interaction that explicitly incorporates model interruption in emergency scenarios. FLEXI systematically evaluates the latency, quality, and conversational effectiveness of real-time dialogue through six diverse human-LLM interaction scenarios, revealing significant gaps between open source and commercial models in emergency awareness, turn terminating, and interaction latency. Finally, we suggest that next token-pair prediction offers a promising path toward achieving truly seamless and human-like full-duplex interaction.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "262",
        "title": "FlashEdit: Decoupling Speed, Structure, and Semantics for Precise Image Editing",
        "author": [
            "Junyi Wu",
            "Zhiteng Li",
            "Haotong Qin",
            "Xiaohong Liu",
            "Linghe Kong",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22244",
        "abstract": "Text-guided image editing with diffusion models has achieved remarkable quality but suffers from prohibitive latency, hindering real-world applications. We introduce FlashEdit, a novel framework designed to enable high-fidelity, real-time image editing. Its efficiency stems from three key innovations: (1) a One-Step Inversion-and-Editing (OSIE) pipeline that bypasses costly iterative processes; (2) a Background Shield (BG-Shield) technique that guarantees background preservation by selectively modifying features only within the edit region; and (3) a Sparsified Spatial Cross-Attention (SSCA) mechanism that ensures precise, localized edits by suppressing semantic leakage to the background. Extensive experiments demonstrate that FlashEdit maintains superior background consistency and structural integrity, while performing edits in under 0.2 seconds, which is an over 150$\\times$ speedup compared to prior multi-step methods. Our code will be made publicly available at https://github.com/JunyiWuCode/FlashEdit.",
        "tags": [
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "263",
        "title": "Safety Compliance: Rethinking LLM Safety Reasoning through the Lens of Compliance",
        "author": [
            "Wenbin Hu",
            "Huihao Jing",
            "Haochen Shi",
            "Haoran Li",
            "Yangqiu Song"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22250",
        "abstract": "The proliferation of Large Language Models (LLMs) has demonstrated remarkable capabilities, elevating the critical importance of LLM safety. However, existing safety methods rely on ad-hoc taxonomy and lack a rigorous, systematic protection, failing to ensure safety for the nuanced and complex behaviors of modern LLM systems. To address this problem, we solve LLM safety from legal compliance perspectives, named safety compliance. In this work, we posit relevant established legal frameworks as safety standards for defining and measuring safety compliance, including the EU AI Act and GDPR, which serve as core legal frameworks for AI safety and data security in Europe. To bridge the gap between LLM safety and legal compliance, we first develop a new benchmark for safety compliance by generating realistic LLM safety scenarios seeded with legal statutes. Subsequently, we align Qwen3-8B using Group Policy Optimization (GRPO) to construct a safety reasoner, Compliance Reasoner, which effectively aligns LLMs with legal standards to mitigate safety risks. Our comprehensive experiments demonstrate that the Compliance Reasoner achieves superior performance on the new benchmark, with average improvements of +10.45% for the EU AI Act and +11.85% for GDPR.",
        "tags": [
            "GRPO",
            "LLM"
        ]
    },
    {
        "id": "264",
        "title": "Beyond Textual Context: Structural Graph Encoding with Adaptive Space Alignment to alleviate the hallucination of LLMs",
        "author": [
            "Yifang Zhang",
            "Pengfei Duan",
            "Yiwen Yang",
            "Shengwu Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22251",
        "abstract": "Currently, the main approach for Large Language Models (LLMs) to tackle the hallucination issue is incorporating Knowledge Graphs(KGs).However, LLMs typically treat KGs as plain text, extracting only semantic information and limiting their use of the crucial structural aspects of KGs. Another challenge is the gap between the embedding spaces of KGs encoders and LLMs text embeddings, which hinders the effective integration of structured knowledge. To overcome these obstacles, we put forward the SSKG-LLM, an innovative model architecture that is designed to efficiently integrate both the Structural and Semantic information of KGs into the reasoning processes of LLMs. SSKG-LLM incorporates the Knowledge Graph Retrieval (KGR) module and the Knowledge Graph Encoding (KGE) module to preserve semantics while utilizing structure. Then, the Knowledge Graph Adaptation (KGA) module is incorporated to enable LLMs to understand KGs embeddings. We conduct extensive experiments and provide a detailed analysis to explore how incorporating the structural information of KGs can enhance the factual reasoning abilities of LLMs. Our code are available at https://github.com/yfangZhang/SSKG-LLM.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "265",
        "title": "Evaluating LLMs for Combinatorial Optimization: One-Phase and Two-Phase Heuristics for 2D Bin-Packing",
        "author": [
            "Syed Mahbubul Huq",
            "Daniel Brito",
            "Daniel Sikar",
            "Rajesh Mojumder"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22255",
        "abstract": "This paper presents an evaluation framework for assessing Large Language Models' (LLMs) capabilities in combinatorial optimization, specifically addressing the 2D bin-packing problem. We introduce a systematic methodology that combines LLMs with evolutionary algorithms to generate and refine heuristic solutions iteratively. Through comprehensive experiments comparing LLM generated heuristics against traditional approaches (Finite First-Fit and Hybrid First-Fit), we demonstrate that LLMs can produce more efficient solutions while requiring fewer computational resources. Our evaluation reveals that GPT-4o achieves optimal solutions within two iterations, reducing average bin usage from 16 to 15 bins while improving space utilization from 0.76-0.78 to 0.83. This work contributes to understanding LLM evaluation in specialized domains and establishes benchmarks for assessing LLM performance in combinatorial optimization tasks.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "266",
        "title": "Secure and Efficient Access Control for Computer-Use Agents via Context Space",
        "author": [
            "Haochen Gong",
            "Chenxiao Li",
            "Rui Chang",
            "Wenbo Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22256",
        "abstract": "Large language model (LLM)-based computer-use agents represent a convergence of AI and OS capabilities, enabling natural language to control system- and application-level functions. However, due to LLMs' inherent uncertainty issues, granting agents control over computers poses significant security risks. When agent actions deviate from user intentions, they can cause irreversible consequences. Existing mitigation approaches, such as user confirmation and LLM-based dynamic action validation, still suffer from limitations in usability, security, and performance. To address these challenges, we propose CSAgent, a system-level, static policy-based access control framework for computer-use agents. To bridge the gap between static policy and dynamic context and user intent, CSAgent introduces intent- and context-aware policies, and provides an automated toolchain to assist developers in constructing and refining them. CSAgent enforces these policies through an optimized OS service, ensuring that agent actions can only be executed under specific user intents and contexts. CSAgent supports protecting agents that control computers through diverse interfaces, including API, CLI, and GUI. We implement and evaluate CSAgent, which successfully defends against more than 99.36% of attacks while introducing only 6.83% performance overhead.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "267",
        "title": "Wavelet-Induced Rotary Encodings: RoPE Meets Graphs",
        "author": [
            "Isaac Reid",
            "Arijit Sehanobish",
            "Cedrik HÃ¶fs",
            "Bruno Mlodozeniec",
            "Leonhard Vulpius",
            "Federico Barbero",
            "Adrian Weller",
            "Krzysztof Choromanski",
            "Richard E. Turner",
            "Petar VeliÄkoviÄ"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22259",
        "abstract": "We introduce WIRE: Wavelet-Induced Rotary Encodings. WIRE extends Rotary Position Encodings (RoPE), a popular algorithm in LLMs and ViTs, to graph-structured data. We demonstrate that WIRE is more general than RoPE, recovering the latter in the special case of grid graphs. WIRE also enjoys a host of desirable theoretical properties, including equivariance under node ordering permutation, compatibility with linear attention, and (under select assumptions) asymptotic dependence on graph resistive distance. We test WIRE on a range of synthetic and real-world tasks, including identifying monochromatic subgraphs, semantic segmentation of point clouds, and more standard graph benchmarks. We find it to be effective in settings where the underlying graph structure is important.",
        "tags": [
            "LLM",
            "RoPE",
            "Segmentation"
        ]
    },
    {
        "id": "268",
        "title": "Erase or Hide? Suppressing Spurious Unlearning Neurons for Robust Unlearning",
        "author": [
            "Nakyeong Yang",
            "Dong-Kyum Kim",
            "Jea Kwon",
            "Minsung Kim",
            "Kyomin Jung",
            "Meeyoung Cha"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22263",
        "abstract": "Large language models trained on web-scale data can memorize private or sensitive knowledge, raising significant privacy risks. Although some unlearning methods mitigate these risks, they remain vulnerable to \"relearning\" during subsequent training, allowing a substantial portion of forgotten knowledge to resurface. In this paper, we show that widely used unlearning methods cause shallow alignment: instead of faithfully erasing target knowledge, they generate spurious unlearning neurons that amplify negative influence to hide it. To overcome this limitation, we introduce Ssiuu, a new class of unlearning methods that employs attribution-guided regularization to prevent spurious negative influence and faithfully remove target knowledge. Experimental results confirm that our method reliably erases target knowledge and outperforms strong baselines across two practical retraining scenarios: (1) adversarial injection of private data, and (2) benign attack using an instruction-following benchmark. Our findings highlight the necessity of robust and faithful unlearning methods for safe deployment of language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "269",
        "title": "Human Autonomy and Sense of Agency in Human-Robot Interaction: A Systematic Literature Review",
        "author": [
            "Felix Glawe",
            "Tim Schmeckel",
            "Philipp Brauner",
            "Martina Ziefle"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22271",
        "abstract": "Human autonomy and sense of agency are increasingly recognised as critical for user well-being, motivation, and the ethical deployment of robots in human-robot interaction (HRI). Given the rapid development of artificial intelligence, robot capabilities and their potential to function as colleagues and companions are growing. This systematic literature review synthesises 22 empirical studies selected from an initial pool of 728 articles published between 2011 and 2024. Articles were retrieved from major scientific databases and identified based on empirical focus and conceptual relevance, namely, how to preserve and promote human autonomy and sense of agency in HRI. Derived through thematic synthesis, five clusters of potentially influential factors are revealed: robot adaptiveness, communication style, anthropomorphism, presence of a robot and individual differences. Measured through psychometric scales or the intentional binding paradigm, perceptions of autonomy and agency varied across industrial, educational, healthcare, care, and hospitality settings. The review underscores the theoretical differences between both concepts, but their yet entangled use in HRI. Despite increasing interest, the current body of empirical evidence remains limited and fragmented, underscoring the necessity for standardised definitions, more robust operationalisations, and further exploratory and qualitative research. By identifying existing gaps and highlighting emerging trends, this review contributes to the development of human-centered, autonomy-supportive robot design strategies that uphold ethical and psychological principles, ultimately supporting well-being in human-robot interaction.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "270",
        "title": "Fine-Grained Uncertainty Decomposition in Large Language Models: A Spectral Approach",
        "author": [
            "Nassim Walha",
            "Sebastian G. Gruber",
            "Thomas Decker",
            "Yinchong Yang",
            "Alireza Javanmardi",
            "Eyke HÃ¼llermeier",
            "Florian Buettner"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22272",
        "abstract": "As Large Language Models (LLMs) are increasingly integrated in diverse applications, obtaining reliable measures of their predictive uncertainty has become critically important. A precise distinction between aleatoric uncertainty, arising from inherent ambiguities within input data, and epistemic uncertainty, originating exclusively from model limitations, is essential to effectively address each uncertainty source. In this paper, we introduce Spectral Uncertainty, a novel approach to quantifying and decomposing uncertainties in LLMs. Leveraging the Von Neumann entropy from quantum information theory, Spectral Uncertainty provides a rigorous theoretical foundation for separating total uncertainty into distinct aleatoric and epistemic components. Unlike existing baseline methods, our approach incorporates a fine-grained representation of semantic similarity, enabling nuanced differentiation among various semantic interpretations in model responses. Empirical evaluations demonstrate that Spectral Uncertainty outperforms state-of-the-art methods in estimating both aleatoric and total uncertainty across diverse models and benchmark datasets.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "271",
        "title": "GS-2M: Gaussian Splatting for Joint Mesh Reconstruction and Material Decomposition",
        "author": [
            "Dinh Minh Nguyen",
            "Malte Avenhaus",
            "Thomas Lindemeier"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22276",
        "abstract": "We propose a unified solution for mesh reconstruction and material decomposition from multi-view images based on 3D Gaussian Splatting, referred to as GS-2M. Previous works handle these tasks separately and struggle to reconstruct highly reflective surfaces, often relying on priors from external models to enhance the decomposition results. Conversely, our method addresses these two problems by jointly optimizing attributes relevant to the quality of rendered depth and normals, maintaining geometric details while being resilient to reflective surfaces. Although contemporary works effectively solve these tasks together, they often employ sophisticated neural components to learn scene properties, which hinders their performance at scale. To further eliminate these neural components, we propose a novel roughness supervision strategy based on multi-view photometric variation. When combined with a carefully designed loss and optimization process, our unified framework produces reconstruction results comparable to state-of-the-art methods, delivering triangle meshes and their associated material components for downstream tasks. We validate the effectiveness of our approach with widely used datasets from previous works and qualitative comparisons with state-of-the-art surface reconstruction methods.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "272",
        "title": "Unlocking the Power of Mixture-of-Experts for Task-Aware Time Series Analytics",
        "author": [
            "Xingjian Wu",
            "Zhengyu Li",
            "Hanyin Cheng",
            "Xiangfei Qiu",
            "Jilin Hu",
            "Chenjuan Guo",
            "Bin Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22279",
        "abstract": "Time Series Analysis is widely used in various real-world applications such as weather forecasting, financial fraud detection, imputation for missing data in IoT systems, and classification for action recognization. Mixture-of-Experts (MoE), as a powerful architecture, though demonstrating effectiveness in NLP, still falls short in adapting to versatile tasks in time series analytics due to its task-agnostic router and the lack of capability in modeling channel correlations. In this study, we propose a novel, general MoE-based time series framework called PatchMoE to support the intricate ``knowledge'' utilization for distinct tasks, thus task-aware. Based on the observation that hierarchical representations often vary across tasks, e.g., forecasting vs. classification, we propose a Recurrent Noisy Gating to utilize the hierarchical information in routing, thus obtaining task-sepcific capability. And the routing strategy is operated on time series tokens in both temporal and channel dimensions, and encouraged by a meticulously designed Temporal \\& Channel Load Balancing Loss to model the intricate temporal and channel correlations. Comprehensive experiments on five downstream tasks demonstrate the state-of-the-art performance of PatchMoE.",
        "tags": [
            "Detection",
            "MoE"
        ]
    },
    {
        "id": "273",
        "title": "MesaTask: Towards Task-Driven Tabletop Scene Generation via 3D Spatial Reasoning",
        "author": [
            "Jinkun Hao",
            "Naifu Liang",
            "Zhen Luo",
            "Xudong Xu",
            "Weipeng Zhong",
            "Ran Yi",
            "Yichen Jin",
            "Zhaoyang Lyu",
            "Feng Zheng",
            "Lizhuang Ma",
            "Jiangmiao Pang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22281",
        "abstract": "The ability of robots to interpret human instructions and execute manipulation tasks necessitates the availability of task-relevant tabletop scenes for training. However, traditional methods for creating these scenes rely on time-consuming manual layout design or purely randomized layouts, which are limited in terms of plausibility or alignment with the tasks. In this paper, we formulate a novel task, namely task-oriented tabletop scene generation, which poses significant challenges due to the substantial gap between high-level task instructions and the tabletop scenes. To support research on such a challenging task, we introduce MesaTask-10K, a large-scale dataset comprising approximately 10,700 synthetic tabletop scenes with manually crafted layouts that ensure realistic layouts and intricate inter-object relations. To bridge the gap between tasks and scenes, we propose a Spatial Reasoning Chain that decomposes the generation process into object inference, spatial interrelation reasoning, and scene graph construction for the final 3D layout. We present MesaTask, an LLM-based framework that utilizes this reasoning chain and is further enhanced with DPO algorithms to generate physically plausible tabletop scenes that align well with given task descriptions. Exhaustive experiments demonstrate the superior performance of MesaTask compared to baselines in generating task-conforming tabletop scenes with realistic layouts. Project page is at https://mesatask.github.io/",
        "tags": [
            "3D",
            "DPO",
            "LLM"
        ]
    },
    {
        "id": "274",
        "title": "Conditional Denoising Diffusion Autoencoders for Wireless Semantic Communications",
        "author": [
            "Mehdi Letafati",
            "Samad Ali",
            "Matti Latva-aho"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22282",
        "abstract": "Semantic communication (SemCom) systems aim to learn the mapping from low-dimensional semantics to high-dimensional ground-truth. While this is more akin to a \"domain translation\" problem, existing frameworks typically emphasize on channel-adaptive neural encoding-decoding schemes, lacking full exploration of signal distribution. Moreover, such methods so far have employed autoencoder-based architectures, where the encoding is tightly coupled to a matched decoder, causing scalability issues in practice. To address these gaps, diffusion autoencoder models are proposed for wireless SemCom. The goal is to learn a \"semantic-to-clean\" mapping, from the semantic space to the ground-truth probability distribution. A neural encoder at semantic transmitter extracts the high-level semantics, and a conditional diffusion model (CDiff) at the semantic receiver exploits the source distribution for signal-space denoising, while the received semantic latents are incorporated as the conditioning input to \"steer\" the decoding process towards the semantics intended by the transmitter. It is analytically proved that the proposed decoder model is a consistent estimator of the ground-truth data. Furthermore, extensive simulations over CIFAR-10 and MNIST datasets are provided along with design insights, highlighting the performance compared to legacy autoencoders and variational autoencoders (VAE). Simulations are further extended to the multi-user SemCom, identifying the dominating factors in a more realistic setup.",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "275",
        "title": "Rule-Based Reinforcement Learning for Document Image Classification with Vision Language Models",
        "author": [
            "Michael Jungo",
            "Andreas Fischer"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22283",
        "abstract": "Rule-based reinforcement learning has been gaining popularity ever since DeepSeek-R1 has demonstrated its success through simple verifiable rewards. In the domain of document analysis, reinforcement learning is not as prevalent, even though many downstream tasks may benefit from the emerging properties of reinforcement learning, particularly the enhanced reason capabilities. We study the effects of rule-based reinforcement learning with the task of Document Image Classification which is one of the most commonly studied downstream tasks in document analysis. We find that reinforcement learning tends to have better generalisation capabilities to out-of-distritbution data, which we examine in three different scenarios, namely out-of-distribution images, unseen classes and different modalities. Our code is available at https://github.com/jungomi/vision-finetune.",
        "tags": [
            "DeepSeek",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "276",
        "title": "Structured Sparse Transition Matrices to Enable State Tracking in State-Space Models",
        "author": [
            "Aleksandar TerziÄ",
            "Nicolas Menet",
            "Michael Hersche",
            "Thomas Hofmann",
            "Abbas Rahimi"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22284",
        "abstract": "Modern state-space models (SSMs) often utilize transition matrices which enable efficient computation but pose restrictions on the model's expressivity, as measured in terms of the ability to emulate finite-state automata (FSA). While unstructured transition matrices are optimal in terms of expressivity, they come at a prohibitively high compute and memory cost even for moderate state sizes. We propose a structured sparse parametrization of transition matrices in SSMs that enables FSA state tracking with optimal state size and depth, while keeping the computational cost of the recurrence comparable to that of diagonal SSMs. Our method, PD-SSM, parametrizes the transition matrix as the product of a column one-hot matrix ($P$) and a complex-valued diagonal matrix ($D$). Consequently, the computational cost of parallel scans scales linearly with the state size. Theoretically, the model is BIBO-stable and can emulate any $N$-state FSA with one layer of dimension $N$ and a linear readout of size $N \\times N$, significantly improving on all current structured SSM guarantees. Experimentally, the model significantly outperforms a wide collection of modern SSM variants on various FSA state tracking tasks. On multiclass time-series classification, the performance is comparable to that of neural controlled differential equations, a paradigm explicitly built for time-series analysis. Finally, we integrate PD-SSM into a hybrid Transformer-SSM architecture and demonstrate that the model can effectively track the states of a complex FSA in which transitions are encoded as a set of variable-length English sentences. The code is available at https://github.com/IBM/expressive-sparse-state-space-model",
        "tags": [
            "SSMs",
            "Transformer"
        ]
    },
    {
        "id": "277",
        "title": "Leveraging Large Language Models for Robot-Assisted Learning of Morphological Structures in Preschool Children with Language Vulnerabilities",
        "author": [
            "Stina Sundstedt",
            "Mattias Wingren",
            "Susanne HÃ¤gglund",
            "Daniel Ventus"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22287",
        "abstract": "Preschool children with language vulnerabilities -- such as developmental language disorders or immigration related language challenges -- often require support to strengthen their expressive language skills. Based on the principle of implicit learning, speech-language therapists (SLTs) typically embed target morphological structures (e.g., third person -s) into everyday interactions or game-based learning activities. Educators are recommended by SLTs to do the same. This approach demands precise linguistic knowledge and real-time production of various morphological forms (e.g., \"Daddy wears these when he drives to work\"). The task becomes even more demanding when educators or parent also must keep children engaged and manage turn-taking in a game-based activity. In the TalBot project our multiprofessional team have developed an application in which the Furhat conversational robot plays the word retrieval game \"Alias\" with children to improve language skills. Our application currently employs a large language model (LLM) to manage gameplay, dialogue, affective responses, and turn-taking. Our next step is to further leverage the capacity of LLMs so the robot can generate and deliver specific morphological targets during the game. We hypothesize that a robot could outperform humans at this task. Novel aspects of this approach are that the robot could ultimately serve as a model and tutor for both children and professionals and that using LLM capabilities in this context would support basic communication needs for children with language vulnerabilities. Our long-term goal is to create a robust LLM-based Robot-Assisted Language Learning intervention capable of teaching a variety of morphological structures across different languages.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "278",
        "title": "IMU-Preintegrated Radar Factors for Asynchronous Radar-LiDAR-Inertial SLAM",
        "author": [
            "Johan Hatleskog",
            "Morten Nissov",
            "Kostas Alexis"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22288",
        "abstract": "Fixed-lag Radar-LiDAR-Inertial smoothers conventionally create one factor graph node per measurement to compensate for the lack of time synchronization between radar and LiDAR. For a radar-LiDAR sensor pair with equal rates, this strategy results in a state creation rate of twice the individual sensor frequencies. This doubling of the number of states per second yields high optimization costs, inhibiting real-time performance on resource-constrained hardware. We introduce IMU-preintegrated radar factors that use high-rate inertial data to propagate the most recent LiDAR state to the radar measurement timestamp. This strategy maintains the node creation rate at the LiDAR measurement frequency. Assuming equal sensor rates, this lowers the number of nodes by 50 % and consequently the computational costs. Experiments on a single board computer (which has 4 cores each of 2.2 GHz A73 and 2 GHz A53 with 8 GB RAM) show that our method preserves the absolute pose error of a conventional baseline while simultaneously lowering the aggregated factor graph optimization time by up to 56 %.",
        "tags": [
            "SLAM"
        ]
    },
    {
        "id": "279",
        "title": "Jailbreaking on Text-to-Video Models via Scene Splitting Strategy",
        "author": [
            "Wonjun Lee",
            "Haon Park",
            "Doehyeon Lee",
            "Bumsub Ham",
            "Suhyun Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22292",
        "abstract": "Along with the rapid advancement of numerous Text-to-Video (T2V) models, growing concerns have emerged regarding their safety risks. While recent studies have explored vulnerabilities in models like LLMs, VLMs, and Text-to-Image (T2I) models through jailbreak attacks, T2V models remain largely unexplored, leaving a significant safety gap. To address this gap, we introduce SceneSplit, a novel black-box jailbreak method that works by fragmenting a harmful narrative into multiple scenes, each individually benign. This approach manipulates the generative output space, the abstract set of all potential video outputs for a given prompt, using the combination of scenes as a powerful constraint to guide the final outcome. While each scene individually corresponds to a wide and safe space where most outcomes are benign, their sequential combination collectively restricts this space, narrowing it to an unsafe region and significantly increasing the likelihood of generating a harmful video. This core mechanism is further enhanced through iterative scene manipulation, which bypasses the safety filter within this constrained unsafe region. Additionally, a strategy library that reuses successful attack patterns further improves the attack's overall effectiveness and robustness. To validate our method, we evaluate SceneSplit across 11 safety categories on T2V models. Our results show that it achieves a high average Attack Success Rate (ASR) of 77.2% on Luma Ray2, 84.1% on Hailuo, and 78.2% on Veo2, significantly outperforming the existing baseline. Through this work, we demonstrate that current T2V safety mechanisms are vulnerable to attacks that exploit narrative structure, providing new insights for understanding and improving the safety of T2V models.",
        "tags": [
            "LLM",
            "Text-to-Image",
            "Text-to-Video",
            "VLM"
        ]
    },
    {
        "id": "280",
        "title": "Aurora: Towards Universal Generative Multimodal Time Series Forecasting",
        "author": [
            "Xingjian Wu",
            "Jianxin Jin",
            "Wanghui Qiu",
            "Peng Chen",
            "Yang Shu",
            "Bin Yang",
            "Chenjuan Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22295",
        "abstract": "Cross-domain generalization is very important in Time Series Forecasting because similar historical information may lead to distinct future trends due to the domain-specific characteristics. Recent works focus on building unimodal time series foundation models and end-to-end multimodal supervised models. Since domain-specific knowledge is often contained in modalities like texts, the former lacks the explicit utilization of them, thus hindering the performance. The latter is tailored for end-to-end scenarios and does not support zero-shot inference for cross-domain scenarios. In this work, we introduce Aurora, a Multimodal Time Series Foundation Model, which supports multimodal inputs and zero-shot inference. Pretrained on Corss-domain Multimodal Time Series Corpus, Aurora can adaptively extract and focus on key domain knowledge contained in corrsponding text or image modalities, thus possessing strong Cross-domain generalization capability. Through tokenization, encoding, and distillation, Aurora can extract multimodal domain knowledge as guidance and then utilizes a Modality-Guided Multi-head Self-Attention to inject them into the modeling of temporal representations. In the decoding phase, the multimodal representations are used to generate the conditions and prototypes of future tokens, contributing to a novel Prototype-Guided Flow Matching for generative probabilistic forecasting. Comprehensive experiments on well-recognized benchmarks, including TimeMMD, TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art performance of Aurora on both unimodal and multimodal scenarios.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "281",
        "title": "Beyond Detection -- Orchestrating Human-Robot-Robot Assistance via an Internet of Robotic Things Paradigm",
        "author": [
            "Joseph Hunt",
            "Koyo Fujii",
            "Aly Magassouba",
            "Praminda Caleb-Solly"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22296",
        "abstract": "Hospital patient falls remain a critical and costly challenge worldwide. While conventional fall prevention systems typically rely on post-fall detection or reactive alerts, they also often suffer from high false positive rates and fail to address the underlying patient needs that lead to bed-exit attempts. This paper presents a novel system architecture that leverages the Internet of Robotic Things (IoRT) to orchestrate human-robot-robot interaction for proactive and personalized patient assistance. The system integrates a privacy-preserving thermal sensing model capable of real-time bed-exit prediction, with two coordinated robotic agents that respond dynamically based on predicted intent and patient input. This orchestrated response could not only reduce fall risk but also attend to the patient's underlying motivations for movement, such as thirst, discomfort, or the need for assistance, before a hazardous situation arises. Our contributions with this pilot study are three-fold: (1) a modular IoRT-based framework enabling distributed sensing, prediction, and multi-robot coordination; (2) a demonstration of low-resolution thermal sensing for accurate, privacy-preserving preemptive bed-exit detection; and (3) results from a user study and systematic error analysis that inform the design of situationally aware, multi-agent interactions in hospital settings. The findings highlight how interactive and connected robotic systems can move beyond passive monitoring to deliver timely, meaningful assistance, empowering safer, more responsive care environments.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "282",
        "title": "Large Language Models as Nondeterministic Causal Models",
        "author": [
            "Sander Beckers"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22297",
        "abstract": "Recent work by Chatzi et al. and Ravfogel et al. has developed, for the first time, a method for generating counterfactuals of probabilistic Large Language Models. Such counterfactuals tell us what would - or might - have been the output of an LLM if some factual prompt ${\\bf x}$ had been ${\\bf x}^*$ instead. The ability to generate such counterfactuals is an important necessary step towards explaining, evaluating, and comparing, the behavior of LLMs. I argue, however, that the existing method rests on an ambiguous interpretation of LLMs: it does not interpret LLMs literally, for the method involves the assumption that one can change the implementation of an LLM's sampling process without changing the LLM itself, nor does it interpret LLMs as intended, for the method involves explicitly representing a nondeterministic LLM as a deterministic causal model. I here present a much simpler method for generating counterfactuals that is based on an LLM's intended interpretation by representing it as a nondeterministic causal model instead. The advantage of my simpler method is that it is directly applicable to any black-box LLM without modification, as it is agnostic to any implementation details. The advantage of the existing method, on the other hand, is that it directly implements the generation of a specific type of counterfactuals that is useful for certain purposes, but not for others. I clarify how both methods relate by offering a theoretical foundation for reasoning about counterfactuals in LLMs based on their intended semantics, thereby laying the groundwork for novel application-specific methods for generating counterfactuals.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "283",
        "title": "HEAPr: Hessian-based Efficient Atomic Expert Pruning in Output Space",
        "author": [
            "Ke Li",
            "Zheng Yang",
            "Zhongbin Zhou",
            "Feng Xue",
            "Zhonglin Jiang",
            "Wenxiao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22299",
        "abstract": "Mixture-of-Experts (MoE) architectures in large language models (LLMs) deliver exceptional performance and reduced inference costs compared to dense LLMs. However, their large parameter counts result in prohibitive memory requirements, limiting practical deployment. While existing pruning methods primarily focus on expert-level pruning, this coarse granularity often leads to substantial accuracy degradation. In this work, we introduce HEAPr, a novel pruning algorithm that decomposes experts into smaller, indivisible atomic experts, enabling more precise and flexible atomic expert pruning. To measure the importance of each atomic expert, we leverage second-order information based on principles similar to Optimal Brain Surgeon (OBS) theory. To address the computational and storage challenges posed by second-order information, HEAPr exploits the inherent properties of atomic experts to transform the second-order information from expert parameters into that of atomic expert parameters, and further simplifies it to the second-order information of atomic expert outputs. This approach reduces the space complexity from $O(d^4)$, where d is the model's dimensionality, to $O(d^2)$. HEAPr requires only two forward passes and one backward pass on a small calibration set to compute the importance of atomic experts. Extensive experiments on MoE models, including DeepSeek MoE and Qwen MoE family, demonstrate that HEAPr outperforms existing expert-level pruning methods across a wide range of compression ratios and benchmarks. Specifically, HEAPr achieves nearly lossless compression at compression ratios of 20% ~ 25% in most models, while also reducing FLOPs nearly by 20%. The code can be found at \\href{https://github.com/LLIKKE/HEAPr}{https://github.com/LLIKKE/HEAPr}.",
        "tags": [
            "DeepSeek",
            "LLM",
            "MoE",
            "Qwen"
        ]
    },
    {
        "id": "284",
        "title": "HiGS: History-Guided Sampling for Plug-and-Play Enhancement of Diffusion Models",
        "author": [
            "Seyedmorteza Sadat",
            "Farnood Salehi",
            "Romann M. Weber"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22300",
        "abstract": "While diffusion models have made remarkable progress in image generation, their outputs can still appear unrealistic and lack fine details, especially when using fewer number of neural function evaluations (NFEs) or lower guidance scales. To address this issue, we propose a novel momentum-based sampling technique, termed history-guided sampling (HiGS), which enhances quality and efficiency of diffusion sampling by integrating recent model predictions into each inference step. Specifically, HiGS leverages the difference between the current prediction and a weighted average of past predictions to steer the sampling process toward more realistic outputs with better details and structure. Our approach introduces practically no additional computation and integrates seamlessly into existing diffusion frameworks, requiring neither extra training nor fine-tuning. Extensive experiments show that HiGS consistently improves image quality across diverse models and architectures and under varying sampling budgets and guidance scales. Moreover, using a pretrained SiT model, HiGS achieves a new state-of-the-art FID of 1.61 for unguided ImageNet generation at 256$\\times$256 with only 30 sampling steps (instead of the standard 250). We thus present HiGS as a plug-and-play enhancement to standard diffusion sampling that enables faster generation with higher fidelity.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "285",
        "title": "Adaptive Policy Backbone via Shared Network",
        "author": [
            "Bumgeun Park",
            "Donghwan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22310",
        "abstract": "Reinforcement learning (RL) has achieved impressive results across domains, yet learning an optimal policy typically requires extensive interaction data, limiting practical deployment. A common remedy is to leverage priors, such as pre-collected datasets or reference policies, but their utility degrades under task mismatch between training and deployment. While prior work has sought to address this mismatch, it has largely been restricted to in-distribution settings. To address this challenge, we propose Adaptive Policy Backbone (APB), a meta-transfer RL method that inserts lightweight linear layers before and after a shared backbone, thereby enabling parameter-efficient fine-tuning (PEFT) while preserving prior knowledge during adaptation. Our results show that APB improves sample efficiency over standard RL and adapts to out-of-distribution (OOD) tasks where existing meta-RL baselines typically fail.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "286",
        "title": "PRIME: Planning and Retrieval-Integrated Memory for Enhanced Reasoning",
        "author": [
            "Hieu Tran",
            "Zonghai Yao",
            "Nguyen Luong Tran",
            "Zhichao Yang",
            "Feiyun Ouyang",
            "Shuo Han",
            "Razieh Rahimi",
            "Hong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22315",
        "abstract": "Inspired by the dual-process theory of human cognition from \\textit{Thinking, Fast and Slow}, we introduce \\textbf{PRIME} (Planning and Retrieval-Integrated Memory for Enhanced Reasoning), a multi-agent reasoning framework that dynamically integrates \\textbf{System 1} (fast, intuitive thinking) and \\textbf{System 2} (slow, deliberate thinking). PRIME first employs a Quick Thinking Agent (System 1) to generate a rapid answer; if uncertainty is detected, it then triggers a structured System 2 reasoning pipeline composed of specialized agents for \\textit{planning}, \\textit{hypothesis generation}, \\textit{retrieval}, \\textit{information integration}, and \\textit{decision-making}. This multi-agent design faithfully mimics human cognitive processes and enhances both efficiency and accuracy. Experimental results with LLaMA 3 models demonstrate that PRIME enables open-source LLMs to perform competitively with state-of-the-art closed-source models like GPT-4 and GPT-4o on benchmarks requiring multi-hop and knowledge-grounded reasoning. This research establishes PRIME as a scalable solution for improving LLMs in domains requiring complex, knowledge-intensive reasoning.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "287",
        "title": "NIFTY: a Non-Local Image Flow Matching for Texture Synthesis",
        "author": [
            "Pierrick Chatillon",
            "Julien Rabin",
            "David TschumperlÃ©"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22318",
        "abstract": "This paper addresses the problem of exemplar-based texture synthesis. We introduce NIFTY, a hybrid framework that combines recent insights on diffusion models trained with convolutional neural networks, and classical patch-based texture optimization techniques. NIFTY is a non-parametric flow-matching model built on non-local patch matching, which avoids the need for neural network training while alleviating common shortcomings of patch-based methods, such as poor initialization or visual artifacts. Experimental results demonstrate the effectiveness of the proposed approach compared to representative methods from the literature. Code is available at https://github.com/PierrickCh/Nifty.git",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "288",
        "title": "Progressive Weight Loading: Accelerating Initial Inference and Gradually Boosting Performance on Resource-Constrained Environments",
        "author": [
            "Hyunwoo Kim",
            "Junha Lee",
            "Mincheol Choi",
            "Jeonghwan Lee",
            "Jaeshin Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22319",
        "abstract": "Deep learning models have become increasingly large and complex, resulting in higher memory consumption and computational demands. Consequently, model loading times and initial inference latency have increased, posing significant challenges in mobile and latency-sensitive environments where frequent model loading and unloading are required, which directly impacts user experience. While Knowledge Distillation (KD) offers a solution by compressing large teacher models into smaller student ones, it often comes at the cost of reduced performance. To address this trade-off, we propose Progressive Weight Loading (PWL), a novel technique that enables fast initial inference by first deploying a lightweight student model, then incrementally replacing its layers with those of a pre-trained teacher model. To support seamless layer substitution, we introduce a training method that not only aligns intermediate feature representations between student and teacher layers, but also improves the overall output performance of the student model. Our experiments on VGG, ResNet, and ViT architectures demonstrate that models trained with PWL maintain competitive distillation performance and gradually improve accuracy as teacher layers are loaded-matching the final accuracy of the full teacher model without compromising initial inference speed. This makes PWL particularly suited for dynamic, resource-constrained deployments where both responsiveness and performance are critical.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "289",
        "title": "RAPID^3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformer",
        "author": [
            "Wangbo Zhao",
            "Yizeng Han",
            "Zhiwei Tang",
            "Jiasheng Tang",
            "Pengfei Zhou",
            "Kai Wang",
            "Bohan Zhuang",
            "Zhangyang Wang",
            "Fan Wang",
            "Yang You"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22323",
        "abstract": "Diffusion Transformers (DiTs) excel at visual generation yet remain hampered by slow sampling. Existing training-free accelerators - step reduction, feature caching, and sparse attention - enhance inference speed but typically rely on a uniform heuristic or a manually designed adaptive strategy for all images, leaving quality on the table. Alternatively, dynamic neural networks offer per-image adaptive acceleration, but their high fine-tuning costs limit broader applicability. To address these limitations, we introduce RAPID3: Tri-Level Reinforced Acceleration Policies for Diffusion Transformers, a framework that delivers image-wise acceleration with zero updates to the base generator. Specifically, three lightweight policy heads - Step-Skip, Cache-Reuse, and Sparse-Attention - observe the current denoising state and independently decide their corresponding speed-up at each timestep. All policy parameters are trained online via Group Relative Policy Optimization (GRPO) while the generator remains frozen. Meanwhile, an adversarially learned discriminator augments the reward signal, discouraging reward hacking by boosting returns only when generated samples stay close to the original model's distribution. Across state-of-the-art DiT backbones, including Stable Diffusion 3 and FLUX, RAPID3 achieves nearly 3x faster sampling with competitive generation quality.",
        "tags": [
            "DiT",
            "Diffusion",
            "FLUX",
            "GRPO",
            "Transformer"
        ]
    },
    {
        "id": "290",
        "title": "Can Synthetic Query Rewrites Capture User Intent Better than Humans in Retrieval-Augmented Generation?",
        "author": [
            "JiaYing Zheng",
            "HaiNan Zhang",
            "Liang Pang",
            "YongXin Tong",
            "ZhiMing Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22325",
        "abstract": "Multi-turn RAG systems often face queries with colloquial omissions and ambiguous references, posing significant challenges for effective retrieval and generation. Traditional query rewriting relies on human annotators to clarify queries, but due to limitations in annotators' expressive ability and depth of understanding, manually rewritten queries often diverge from those needed in real-world RAG systems, resulting in a gap between user intent and system response. We observe that high-quality synthetic queries can better bridge this gap, achieving superior performance in both retrieval and generation compared to human rewrites. This raises an interesting question: Can rewriting models trained on synthetic queries better capture user intent than human annotators? In this paper, we propose SynRewrite, a synthetic data-driven query rewriting model to generate high-quality synthetic rewrites more aligned with user intent. To construct training data, we prompt GPT-4o with dialogue history, current queries, positive documents, and answers to synthesize high-quality rewrites. A Flan-T5 model is then finetuned on this dataset to map dialogue history and queries to synthetic rewrites. Finally, we further enhance the rewriter using the generator's feedback through the DPO algorithm to boost end-task performance. Experiments on TopiOCQA and QRECC datasets show that SynRewrite consistently outperforms human rewrites in both retrieval and generation tasks. Our results demonstrate that synthetic rewrites can serve as a scalable and effective alternative to human annotations.",
        "tags": [
            "DPO",
            "GPT",
            "RAG"
        ]
    },
    {
        "id": "291",
        "title": "Spectral Collapse Drives Loss of Plasticity in Deep Continual Learning",
        "author": [
            "Naicheng He",
            "Kaicheng Guo",
            "Arjun Prakash",
            "Saket Tiwari",
            "Ruo Yu Tao",
            "Tyrone Serapio",
            "Amy Greenwald",
            "George Konidaris"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22335",
        "abstract": "We investigate why deep neural networks suffer from \\emph{loss of plasticity} in deep continual learning, failing to learn new tasks without reinitializing parameters. We show that this failure is preceded by Hessian spectral collapse at new-task initialization, where meaningful curvature directions vanish and gradient descent becomes ineffective. To characterize the necessary condition for successful training, we introduce the notion of $\\tau$-trainability and show that current plasticity preserving algorithms can be unified under this framework. Targeting spectral collapse directly, we then discuss the Kronecker factored approximation of the Hessian, which motivates two regularization enhancements: maintaining high effective feature rank and applying $L2$ penalties. Experiments on continual supervised and reinforcement learning tasks confirm that combining these two regularizers effectively preserves plasticity.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "292",
        "title": "Advancing Natural Language Formalization to First Order Logic with Fine-tuned LLMs",
        "author": [
            "Felix Vossel",
            "Till Mossakowski",
            "BjÃ¶rn Gehrke"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22338",
        "abstract": "Automating the translation of natural language to first-order logic (FOL) is crucial for knowledge representation and formal methods, yet remains challenging. We present a systematic evaluation of fine-tuned LLMs for this task, comparing architectures (encoder-decoder vs. decoder-only) and training strategies. Using the MALLS and Willow datasets, we explore techniques like vocabulary extension, predicate conditioning, and multilingual training, introducing metrics for exact match, logical equivalence, and predicate alignment. Our fine-tuned Flan-T5-XXL achieves 70% accuracy with predicate lists, outperforming GPT-4o and even the DeepSeek-R1-0528 model with CoT reasoning ability as well as symbolic systems like ccg2lambda. Key findings show: (1) predicate availability boosts performance by 15-20%, (2) T5 models surpass larger decoder-only LLMs, and (3) models generalize to unseen logical arguments (FOLIO dataset) without specific training. While structural logic translation proves robust, predicate extraction emerges as the main bottleneck.",
        "tags": [
            "CoT",
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "293",
        "title": "CircuitSense: A Hierarchical Circuit System Benchmark Bridging Visual Comprehension and Symbolic Reasoning in Engineering Design Process",
        "author": [
            "Arman Akbari",
            "Jian Gao",
            "Yifei Zou",
            "Mei Yang",
            "Jinru Duan",
            "Dmitrii Torbunov",
            "Yanzhi Wang",
            "Yihui Ren",
            "Xuan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22339",
        "abstract": "Engineering design operates through hierarchical abstraction from system specifications to component implementations, requiring visual understanding coupled with mathematical reasoning at each level. While Multi-modal Large Language Models (MLLMs) excel at natural image tasks, their ability to extract mathematical models from technical diagrams remains unexplored. We present \\textbf{CircuitSense}, a comprehensive benchmark evaluating circuit understanding across this hierarchy through 8,006+ problems spanning component-level schematics to system-level block diagrams. Our benchmark uniquely examines the complete engineering workflow: Perception, Analysis, and Design, with a particular emphasis on the critical but underexplored capability of deriving symbolic equations from visual inputs. We introduce a hierarchical synthetic generation pipeline consisting of a grid-based schematic generator and a block diagram generator with auto-derived symbolic equation labels. Comprehensive evaluation of six state-of-the-art MLLMs, including both closed-source and open-source models, reveals fundamental limitations in visual-to-mathematical reasoning. Closed-source models achieve over 85\\% accuracy on perception tasks involving component recognition and topology identification, yet their performance on symbolic derivation and analytical reasoning falls below 19\\%, exposing a critical gap between visual parsing and symbolic reasoning. Models with stronger symbolic reasoning capabilities consistently achieve higher design task accuracy, confirming the fundamental role of mathematical understanding in circuit synthesis and establishing symbolic reasoning as the key metric for engineering competence.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "294",
        "title": "Transformers Can Learn Connectivity in Some Graphs but Not Others",
        "author": [
            "Amit Roy",
            "Abulhair Saparov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22343",
        "abstract": "Reasoning capability is essential to ensure the factual correctness of the responses of transformer-based Large Language Models (LLMs), and robust reasoning about transitive relations is instrumental in many settings, such as causal inference. Hence, it is essential to investigate the capability of transformers in the task of inferring transitive relations (e.g., knowing A causes B and B causes C, then A causes C). The task of inferring transitive relations is equivalent to the task of connectivity in directed graphs (e.g., knowing there is a path from A to B, and there is a path from B to C, then there is a path from A to C). Past research focused on whether transformers can learn to infer transitivity from in-context examples provided in the input prompt. However, transformers' capability to infer transitive relations from training examples and how scaling affects the ability is unexplored. In this study, we seek to answer this question by generating directed graphs to train transformer models of varying sizes and evaluate their ability to infer transitive relations for various graph sizes. Our findings suggest that transformers are capable of learning connectivity on \"grid-like'' directed graphs where each node can be embedded in a low-dimensional subspace, and connectivity is easily inferable from the embeddings of the nodes. We find that the dimensionality of the underlying grid graph is a strong predictor of transformers' ability to learn the connectivity task, where higher-dimensional grid graphs pose a greater challenge than low-dimensional grid graphs. In addition, we observe that increasing the model scale leads to increasingly better generalization to infer connectivity over grid graphs. However, if the graph is not a grid graph and contains many disconnected components, transformers struggle to learn the connectivity task, especially when the number of components is large.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "295",
        "title": "The InviTE Corpus: Annotating Invectives in Tudor English Texts for Computational Modeling",
        "author": [
            "Sophie Spliethoff",
            "Sanne Hoeken",
            "Silke Schwandt",
            "Sina ZarrieÃ",
            "Ãzge AlaÃ§am"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22345",
        "abstract": "In this paper, we aim at the application of Natural Language Processing (NLP) techniques to historical research endeavors, particularly addressing the study of religious invectives in the context of the Protestant Reformation in Tudor England. We outline a workflow spanning from raw data, through pre-processing and data selection, to an iterative annotation process. As a result, we introduce the InviTE corpus -- a corpus of almost 2000 Early Modern English (EModE) sentences, which are enriched with expert annotations regarding invective language throughout 16th-century England. Subsequently, we assess and compare the performance of fine-tuned BERT-based models and zero-shot prompted instruction-tuned large language models (LLMs), which highlights the superiority of models pre-trained on historical data and fine-tuned to invective detection.",
        "tags": [
            "BERT",
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "296",
        "title": "RoboView-Bias: Benchmarking Visual Bias in Embodied Agents for Robotic Manipulation",
        "author": [
            "Enguang Liu",
            "Siyuan Liang",
            "Liming Lu",
            "Xiyu Zeng",
            "Xiaochun Cao",
            "Aishan Liu",
            "Shuchao Pang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22356",
        "abstract": "The safety and reliability of embodied agents rely on accurate and unbiased visual perception. However, existing benchmarks mainly emphasize generalization and robustness under perturbations, while systematic quantification of visual bias remains scarce. This gap limits a deeper understanding of how perception influences decision-making stability. To address this issue, we propose RoboView-Bias, the first benchmark specifically designed to systematically quantify visual bias in robotic manipulation, following a principle of factor isolation. Leveraging a structured variant-generation framework and a perceptual-fairness validation protocol, we create 2,127 task instances that enable robust measurement of biases induced by individual visual factors and their interactions. Using this benchmark, we systematically evaluate three representative embodied agents across two prevailing paradigms and report three key findings: (i) all agents exhibit significant visual biases, with camera viewpoint being the most critical factor; (ii) agents achieve their highest success rates on highly saturated colors, indicating inherited visual preferences from underlying VLMs; and (iii) visual biases show strong, asymmetric coupling, with viewpoint strongly amplifying color-related bias. Finally, we demonstrate that a mitigation strategy based on a semantic grounding layer substantially reduces visual bias by approximately 54.5\\% on MOKA. Our results highlight that systematic analysis of visual bias is a prerequisite for developing safe and reliable general-purpose embodied agents.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "297",
        "title": "CHRONOBERG: Capturing Language Evolution and Temporal Awareness in Foundation Models",
        "author": [
            "Niharika Hegde",
            "Subarnaduti Paul",
            "Lars Joel-Frey",
            "Manuel Brack",
            "Kristian Kersting",
            "Martin Mundt",
            "Patrick Schramowski"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22360",
        "abstract": "Large language models (LLMs) excel at operating at scale by leveraging social media and various data crawled from the web. Whereas existing corpora are diverse, their frequent lack of long-term temporal structure may however limit an LLM's ability to contextualize semantic and normative evolution of language and to capture diachronic variation. To support analysis and training for the latter, we introduce CHRONOBERG, a temporally structured corpus of English book texts spanning 250 years, curated from Project Gutenberg and enriched with a variety of temporal annotations. First, the edited nature of books enables us to quantify lexical semantic change through time-sensitive Valence-Arousal-Dominance (VAD) analysis and to construct historically calibrated affective lexicons to support temporally grounded interpretation. With the lexicons at hand, we demonstrate a need for modern LLM-based tools to better situate their detection of discriminatory language and contextualization of sentiment across various time-periods. In fact, we show how language models trained sequentially on CHRONOBERG struggle to encode diachronic shifts in meaning, emphasizing the need for temporally aware training and evaluation pipelines, and positioning CHRONOBERG as a scalable resource for the study of linguistic change and temporal generalization. Disclaimer: This paper includes language and display of samples that could be offensive to readers. Open Access: Chronoberg is available publicly on HuggingFace at ( https://huggingface.co/datasets/spaul25/Chronoberg). Code is available at (https://github.com/paulsubarna/Chronoberg).",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "298",
        "title": "Investigating Faithfulness in Large Audio Language Models",
        "author": [
            "Lovenya Jain",
            "Pooneh Mousavi",
            "Mirco Ravanelli",
            "Cem Subakan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22363",
        "abstract": "Faithfulness measures whether chain-of-thought (CoT) representations accurately reflect a model's decision process and can be used as reliable explanations. Prior work has shown that CoTs from text-based LLMs are often unfaithful. This question has not been explored for large audio-language models (LALMs), where faithfulness is critical for safety-sensitive applications. Reasoning in LALMs is also more challenging, as models must first extract relevant clues from audio before reasoning over them. In this paper, we investigate the faithfulness of CoTs produced by several LALMs by applying targeted interventions, including paraphrasing, filler token injection, early answering, and introducing mistakes, on two challenging reasoning datasets: SAKURA and MMAR. After going through the aforementioned interventions across several datasets and tasks, our experiments suggest that, LALMs generally produce CoTs that appear to be faithful to their underlying decision processes.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "299",
        "title": "Exploratory Semantic Reliability Analysis of Wind Turbine Maintenance Logs using Large Language Models",
        "author": [
            "Max Malyi",
            "Jonathan Shek",
            "Andre Biscaya"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22366",
        "abstract": "A wealth of operational intelligence is locked within the unstructured free-text of wind turbine maintenance logs, a resource largely inaccessible to traditional quantitative reliability analysis. While machine learning has been applied to this data, existing approaches typically stop at classification, categorising text into predefined labels. This paper addresses the gap in leveraging modern large language models (LLMs) for more complex reasoning tasks. We introduce an exploratory framework that uses LLMs to move beyond classification and perform deep semantic analysis. We apply this framework to a large industrial dataset to execute four analytical workflows: failure mode identification, causal chain inference, comparative site analysis, and data quality auditing. The results demonstrate that LLMs can function as powerful \"reliability co-pilots,\" moving beyond labelling to synthesise textual information and generate actionable, expert-level hypotheses. This work contributes a novel and reproducible methodology for using LLMs as a reasoning tool, offering a new pathway to enhance operational intelligence in the wind energy sector by unlocking insights previously obscured in unstructured data.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "300",
        "title": "What Is The Political Content in LLMs' Pre- and Post-Training Data?",
        "author": [
            "Tanise Ceron",
            "Dmitry Nikolaev",
            "Dominik Stammbach",
            "Debora Nozza"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22367",
        "abstract": "Large language models (LLMs) are known to generate politically biased text, yet how such biases arise remains unclear. A crucial step toward answering this question is the analysis of training data, whose political content remains largely underexplored in current LLM research. To address this gap, we present in this paper an analysis of the pre- and post-training corpora of OLMO2, the largest fully open-source model released together with its complete dataset. From these corpora, we draw large random samples, automatically annotate documents for political orientation, and analyze their source domains and content. We then assess how political content in the training data correlates with models' stance on specific policy issues. Our analysis shows that left-leaning documents predominate across datasets, with pre-training corpora containing significantly more politically engaged content than post-training data. We also find that left- and right-leaning documents frame similar topics through distinct values and sources of legitimacy. Finally, the predominant stance in the training data strongly correlates with models' political biases when evaluated on policy issues. These findings underscore the need to integrate political content analysis into future data curation pipelines as well as in-depth documentation of filtering strategies for transparency.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "301",
        "title": "Role-Aware Multi-modal federated learning system for detecting phishing webpages",
        "author": [
            "Bo Wang",
            "Imran Khan",
            "Martin White",
            "Natalia Beloff"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22369",
        "abstract": "We present a federated, multi-modal phishing website detector that supports URL, HTML, and IMAGE inputs without binding clients to a fixed modality at inference: any client can invoke any modality head trained elsewhere. Methodologically, we propose role-aware bucket aggregation on top of FedProx, inspired by Mixture-of-Experts and FedMM. We drop learnable routing and use hard gating (selecting the IMAGE/HTML/URL expert by sample modality), enabling separate aggregation of modality-specific parameters to isolate cross-embedding conflicts and stabilize convergence. On TR-OP, the Fusion head reaches Acc 97.5% with FPR 2.4% across two data types; on the image subset (ablation) it attains Acc 95.5% with FPR 5.9%. For text, we use GraphCodeBERT for URLs and an early three-way embedding for raw, noisy HTML. On WebPhish (HTML) we obtain Acc 96.5% / FPR 1.8%; on TR-OP (raw HTML) we obtain Acc 95.1% / FPR 4.6%. Results indicate that bucket aggregation with hard-gated experts enables stable federated training under strict privacy, while improving the usability and flexibility of multi-modal phishing detection.",
        "tags": [
            "Detection",
            "MoE"
        ]
    },
    {
        "id": "302",
        "title": "Effectiveness of Large Multimodal Models in Detecting Disinformation: Experimental Results",
        "author": [
            "Yasmina Kheddache",
            "Marc Lalonde"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22377",
        "abstract": "The proliferation of disinformation, particularly in multimodal contexts combining text and images, presents a significant challenge across digital platforms. This study investigates the potential of large multimodal models (LMMs) in detecting and mitigating false information. We propose to approach multimodal disinformation detection by leveraging the advanced capabilities of the GPT-4o model. Our contributions include: (1) the development of an optimized prompt incorporating advanced prompt engineering techniques to ensure precise and consistent evaluations; (2) the implementation of a structured framework for multimodal analysis, including a preprocessing methodology for images and text to comply with the model's token limitations; (3) the definition of six specific evaluation criteria that enable a fine-grained classification of content, complemented by a self-assessment mechanism based on confidence levels; (4) a comprehensive performance analysis of the model across multiple heterogeneous datasets Gossipcop, Politifact, Fakeddit, MMFakeBench, and AMMEBA highlighting GPT-4o's strengths and limitations in disinformation detection; (5) an investigation of prediction variability through repeated testing, evaluating the stability and reliability of the model's classifications; and (6) the introduction of confidence-level and variability-based evaluation methods. These contributions provide a robust and reproducible methodological framework for automated multimodal disinformation analysis.",
        "tags": [
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "303",
        "title": "Zero-Effort Image-to-Music Generation: An Interpretable RAG-based VLM Approach",
        "author": [
            "Zijian Zhao",
            "Dian Jin",
            "Zijing Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22378",
        "abstract": "Recently, Image-to-Music (I2M) generation has garnered significant attention, with potential applications in fields such as gaming, advertising, and multi-modal art creation. However, due to the ambiguous and subjective nature of I2M tasks, most end-to-end methods lack interpretability, leaving users puzzled about the generation results. Even methods based on emotion mapping face controversy, as emotion represents only a singular aspect of art. Additionally, most learning-based methods require substantial computational resources and large datasets for training, hindering accessibility for common users. To address these challenges, we propose the first Vision Language Model (VLM)-based I2M framework that offers high interpretability and low computational cost. Specifically, we utilize ABC notation to bridge the text and music modalities, enabling the VLM to generate music using natural language. We then apply multi-modal Retrieval-Augmented Generation (RAG) and self-refinement techniques to allow the VLM to produce high-quality music without external training. Furthermore, we leverage the generated motivations in text and the attention maps from the VLM to provide explanations for the generated results in both text and image modalities. To validate our method, we conduct both human studies and machine evaluations, where our method outperforms others in terms of music quality and music-image consistency, indicating promising results. Our code is available at https://github.com/RS2002/Image2Music .",
        "tags": [
            "RAG",
            "VLM"
        ]
    },
    {
        "id": "304",
        "title": "GPT-4 for Occlusion Order Recovery",
        "author": [
            "Kaziwa Saleh",
            "Zhyar Rzgar K Rostam",
            "SÃ¡ndor SzÃ©nÃ¡si",
            "ZoltÃ¡n VÃ¡mossy"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22383",
        "abstract": "Occlusion remains a significant challenge for current vision models to robustly interpret complex and dense real-world images and scenes. To address this limitation and to enable accurate prediction of the occlusion order relationship between objects, we propose leveraging the advanced capability of a pre-trained GPT-4 model to deduce the order. By providing a specifically designed prompt along with the input image, GPT-4 can analyze the image and generate order predictions. The response can then be parsed to construct an occlusion matrix which can be utilized in assisting with other occlusion handling tasks and image understanding. We report the results of evaluating the model on COCOA and InstaOrder datasets. The results show that by using semantic context, visual patterns, and commonsense knowledge, the model can produce more accurate order predictions. Unlike baseline methods, the model can reason about occlusion relationships in a zero-shot fashion, which requires no annotated training data and can easily be integrated into occlusion handling frameworks.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "305",
        "title": "SpinGPT: A Large-Language-Model Approach to Playing Poker Correctly",
        "author": [
            "Narada Maugin",
            "Tristan Cazenave"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22387",
        "abstract": "The Counterfactual Regret Minimization (CFR) algorithm and its variants have enabled the development of pokerbots capable of beating the best human players in heads-up (1v1) cash games and competing with them in six-player formats. However, CFR's computational complexity rises exponentially with the number of players. Furthermore, in games with three or more players, following Nash equilibrium no longer guarantees a non-losing outcome. These limitations, along with others, significantly restrict the applicability of CFR to the most popular formats: tournaments. Motivated by the recent success of Large Language Models (LLM) in chess and Diplomacy, we present SpinGPT, the first LLM tailored to Spin & Go, a popular three-player online poker format. SpinGPT is trained in two stages: (1) Supervised Fine-Tuning on 320k high-stakes expert decisions; (2) Reinforcement Learning on 270k solver-generated hands. Our results show that SpinGPT matches the solver's actions in 78% of decisions (tolerant accuracy). With a simple deep-stack heuristic, it achieves 13.4 +/- 12.9 BB/100 versus Slumbot in heads-up over 30,000 hands (95% CI). These results suggest that LLMs could be a new way to deal with multi-player imperfect-information games like poker.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "306",
        "title": "Do LLM Agents Know How to Ground, Recover, and Assess? A Benchmark for Epistemic Competence in Information-Seeking Agents",
        "author": [
            "Jiaqi Shao",
            "Yuxiang Lin",
            "Munish Prasad Lohani",
            "Yufeng Miao",
            "Bing Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22391",
        "abstract": "Recent work has explored training Large Language Model (LLM) search agents with reinforcement learning (RL) for open-domain question answering (QA). However, most evaluations focus solely on final answer accuracy, overlooking how these agents reason with and act on external evidence. We introduce SeekBench, the first benchmark for evaluating the \\textit{epistemic competence} of LLM search agents through step-level analysis of their response traces. SeekBench comprises 190 expert-annotated traces with over 1,800 response steps generated by LLM search agents, each enriched with evidence annotations for granular analysis of whether agents (1) generate reasoning steps grounded in observed evidence, (2) adaptively reformulate searches to recover from low-quality results, and (3) have proper calibration to correctly assess whether the current evidence is sufficient for providing an answer.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "307",
        "title": "ReLAM: Learning Anticipation Model for Rewarding Visual Robotic Manipulation",
        "author": [
            "Nan Tang",
            "Jing-Cheng Pang",
            "Guanlin Li",
            "Chao Qian",
            "Yang Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22402",
        "abstract": "Reward design remains a critical bottleneck in visual reinforcement learning (RL) for robotic manipulation. In simulated environments, rewards are conventionally designed based on the distance to a target position. However, such precise positional information is often unavailable in real-world visual settings due to sensory and perceptual limitations. In this study, we propose a method that implicitly infers spatial distances through keypoints extracted from images. Building on this, we introduce Reward Learning with Anticipation Model (ReLAM), a novel framework that automatically generates dense, structured rewards from action-free video demonstrations. ReLAM first learns an anticipation model that serves as a planner and proposes intermediate keypoint-based subgoals on the optimal path to the final goal, creating a structured learning curriculum directly aligned with the task's geometric objectives. Based on the anticipated subgoals, a continuous reward signal is provided to train a low-level, goal-conditioned policy under the hierarchical reinforcement learning (HRL) framework with provable sub-optimality bound. Extensive experiments on complex, long-horizon manipulation tasks show that ReLAM significantly accelerates learning and achieves superior performance compared to state-of-the-art methods.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "308",
        "title": "MoveFM-R: Advancing Mobility Foundation Models via Language-driven Semantic Reasoning",
        "author": [
            "Fanjin Meng",
            "Yuan Yuan",
            "Jingtao Ding",
            "Jie Feng",
            "Chonghua Han",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22403",
        "abstract": "Mobility Foundation Models (MFMs) have advanced the modeling of human movement patterns, yet they face a ceiling due to limitations in data scale and semantic understanding. While Large Language Models (LLMs) offer powerful semantic reasoning, they lack the innate understanding of spatio-temporal statistics required for generating physically plausible mobility trajectories. To address these gaps, we propose MoveFM-R, a novel framework that unlocks the full potential of mobility foundation models by leveraging language-driven semantic reasoning capabilities. It tackles two key challenges: the vocabulary mismatch between continuous geographic coordinates and discrete language tokens, and the representation gap between the latent vectors of MFMs and the semantic world of LLMs. MoveFM-R is built on three core innovations: a semantically enhanced location encoding to bridge the geography-language gap, a progressive curriculum to align the LLM's reasoning with mobility patterns, and an interactive self-reflection mechanism for conditional trajectory generation. Extensive experiments demonstrate that MoveFM-R significantly outperforms existing MFM-based and LLM-based baselines. It also shows robust generalization in zero-shot settings and excels at generating realistic trajectories from natural language instructions. By synthesizing the statistical power of MFMs with the deep semantic understanding of LLMs, MoveFM-R pioneers a new paradigm that enables a more comprehensive, interpretable, and powerful modeling of human mobility. The implementation of MoveFM-R is available online at https://anonymous.4open.science/r/MoveFM-R-CDE7/.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "309",
        "title": "EMMA: Generalizing Real-World Robot Manipulation via Generative Visual Transfer",
        "author": [
            "Zhehao Dong",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Yirui Wang",
            "Yang Wang",
            "Yukun Zhou",
            "Boyuan Wang",
            "Chaojun Ni",
            "Runqi Ouyang",
            "Wenkang Qin",
            "Xinze Chen",
            "Yun Ye",
            "Guan Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22407",
        "abstract": "Vision-language-action (VLA) models increasingly rely on diverse training data to achieve robust generalization. However, collecting large-scale real-world robot manipulation data across varied object appearances and environmental conditions remains prohibitively time-consuming and expensive. To overcome this bottleneck, we propose Embodied Manipulation Media Adaptation (EMMA), a VLA policy enhancement framework that integrates a generative data engine with an effective training pipeline. We introduce DreamTransfer, a diffusion Transformer-based framework for generating multi-view consistent, geometrically grounded embodied manipulation videos. DreamTransfer enables text-controlled visual editing of robot videos, transforming foreground, background, and lighting conditions without compromising 3D structure or geometrical plausibility. Furthermore, we explore hybrid training with real and generated data, and introduce AdaMix, a hard-sample-aware training strategy that dynamically reweights training batches to focus optimization on perceptually or kinematically challenging samples. Extensive experiments show that videos generated by DreamTransfer significantly outperform prior video generation methods in multi-view consistency, geometric fidelity, and text-conditioning accuracy. Crucially, VLAs trained with generated data enable robots to generalize to unseen object categories and novel visual domains using only demonstrations from a single appearance. In real-world robotic manipulation tasks with zero-shot visual domains, our approach achieves over a 200% relative performance gain compared to training on real data alone, and further improves by 13% with AdaMix, demonstrating its effectiveness in boosting policy generalization.",
        "tags": [
            "3D",
            "DiT",
            "Diffusion",
            "Robotics",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "310",
        "title": "LucidFlux: Caption-Free Universal Image Restoration via a Large-Scale Diffusion Transformer",
        "author": [
            "Song Fei",
            "Tian Ye",
            "Lujia Wang",
            "Lei Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22414",
        "abstract": "Universal image restoration (UIR) aims to recover images degraded by unknown mixtures while preserving semantics -- conditions under which discriminative restorers and UNet-based diffusion priors often oversmooth, hallucinate, or drift. We present LucidFlux, a caption-free UIR framework that adapts a large diffusion transformer (Flux.1) without image captions. LucidFlux introduces a lightweight dual-branch conditioner that injects signals from the degraded input and a lightly restored proxy to respectively anchor geometry and suppress artifacts. Then, a timestep- and layer-adaptive modulation schedule is designed to route these cues across the backbone's hierarchy, in order to yield coarse-to-fine and context-aware updates that protect the global structure while recovering texture. After that, to avoid the latency and instability of text prompts or MLLM captions, we enforce caption-free semantic alignment via SigLIP features extracted from the proxy. A scalable curation pipeline further filters large-scale data for structure-rich supervision. Across synthetic and in-the-wild benchmarks, LucidFlux consistently outperforms strong open-source and commercial baselines, and ablation studies verify the necessity of each component. LucidFlux shows that, for large DiTs, when, where, and what to condition on -- rather than adding parameters or relying on text prompts -- is the governing lever for robust and caption-free universal image restoration in the wild.",
        "tags": [
            "DiT",
            "Diffusion",
            "FLUX",
            "Transformer"
        ]
    },
    {
        "id": "311",
        "title": "Explaining multimodal LLMs via intra-modal token interactions",
        "author": [
            "Jiawei Liang",
            "Ruoyu Chen",
            "Xianghao Jiao",
            "Siyuan Liang",
            "Shiming Liu",
            "Qunli Zhang",
            "Zheng Hu",
            "Xiaochun Cao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22415",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved remarkable success across diverse vision-language tasks, yet their internal decision-making mechanisms remain insufficiently understood. Existing interpretability research has primarily focused on cross-modal attribution, identifying which image regions the model attends to during output generation. However, these approaches often overlook intra-modal dependencies. In the visual modality, attributing importance to isolated image patches ignores spatial context due to limited receptive fields, resulting in fragmented and noisy explanations. In the textual modality, reliance on preceding tokens introduces spurious activations. Failing to effectively mitigate these interference compromises attribution fidelity. To address these limitations, we propose enhancing interpretability by leveraging intra-modal interaction. For the visual branch, we introduce \\textit{Multi-Scale Explanation Aggregation} (MSEA), which aggregates attributions over multi-scale inputs to dynamically adjust receptive fields, producing more holistic and spatially coherent visual explanations. For the textual branch, we propose \\textit{Activation Ranking Correlation} (ARC), which measures the relevance of contextual tokens to the current token via alignment of their top-$k$ prediction rankings. ARC leverages this relevance to suppress spurious activations from irrelevant contexts while preserving semantically coherent ones. Extensive experiments across state-of-the-art MLLMs and benchmark datasets demonstrate that our approach consistently outperforms existing interpretability methods, yielding more faithful and fine-grained explanations of model behavior.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "312",
        "title": "Learning-Based Collaborative Control for Bi-Manual Tactile-Reactive Grasping",
        "author": [
            "Leonel Giacobbe",
            "Jingdao Chen",
            "Chuangchuang Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22421",
        "abstract": "Grasping is a core task in robotics with various applications. However, most current implementations are primarily designed for rigid items, and their performance drops considerably when handling fragile or deformable materials that require real-time feedback. Meanwhile, tactile-reactive grasping focuses on a single agent, which limits their ability to grasp and manipulate large, heavy objects. To overcome this, we propose a learning-based, tactile-reactive multi-agent Model Predictive Controller (MPC) for grasping a wide range of objects with different softness and shapes, beyond the capabilities of preexisting single-agent implementations. Our system uses two Gelsight Mini tactile sensors [1] to extract real-time information on object texture and stiffness. This rich tactile feedback is used to estimate contact dynamics and object compliance in real time, enabling the system to adapt its control policy to diverse object geometries and stiffness profiles. The learned controller operates in a closed loop, leveraging tactile encoding to predict grasp stability and adjust force and position accordingly. Our key technical contributions include a multi-agent MPC formulation trained on real contact interactions, a tactile-data driven method for inferring grasping states, and a coordination strategy that enables collaborative control. By combining tactile sensing and a learning-based multi-agent MPC, our method offers a robust, intelligent solution for collaborative grasping in complex environments, significantly advancing the capabilities of multi-agent systems. Our approach is validated through extensive experiments against independent PD and MPC baselines. Our pipeline outperforms the baselines regarding success rates in achieving and maintaining stable grasps across objects of varying sizes and stiffness.",
        "tags": [
            "MPC",
            "Robotics"
        ]
    },
    {
        "id": "313",
        "title": "TreeMind: Automatically Reproducing Android Bug Reports via LLM-empowered Monte Carlo Tree Search",
        "author": [
            "Zhengyu Chen",
            "Zhaoyi Meng",
            "Wenxiang Zhao",
            "Wansen Wang",
            "Haoyang Zhao",
            "Jiahao Zhan",
            "Jie Cui",
            "Hong Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22431",
        "abstract": "Automatically reproducing Android app crashes from textual bug reports is challenging, particularly when the reports are incomplete and the modern UI exhibits high combinatorial complexity. Existing approaches based on reinforcement learning or large language models (LLMs) exhibit limitations in such scenarios. They struggle to infer unobserved steps and reconstruct the underlying user action sequences to navigate the vast UI interaction space, primarily due to limited goal-directed reasoning and planning. We present TreeMind, a novel technique that integrates LLMs with a customized Monte Carlo Tree Search (MCTS) algorithm to achieve strategic UI exploration in bug reproduction. To the best of our knowledge, this is the first work to combine external decision-making with LLM semantic reasoning for reliable bug reproduction. We formulate the reproduction task as a target-driven search problem, leveraging MCTS as the core planning mechanism to iteratively refine action sequences. To enhance MCTS with semantic reasoning, we introduce two LLM-guided agents with distinct roles: Expander generates top-k promising actions based on the current UI state and exploration history, while Simulator estimates the likelihood that each action leads toward successful reproduction. By incorporating multi-modal UI inputs and advanced prompting techniques, TreeMind conducts feedback-aware navigation that identifies missing but essential user actions and incrementally reconstructs the reproduction paths. We evaluate TreeMind on a dataset of 93 real-world Android bug reports from three widely-used benchmarks. Experimental results show that it significantly outperforms four state-of-the-art baselines in reproduction success rate. A real-world case study indicates that integrating LLM reasoning with MCTS-based planning is a compelling direction for automated bug reproduction.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "314",
        "title": "An Ontology for Unified Modeling of Tasks, Actions, Environments, and Capabilities in Personal Service Robotics",
        "author": [
            "Margherita Martorana",
            "Francesca Urgese",
            "Ilaria Tiddi",
            "Stefan Schlobach"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22434",
        "abstract": "Personal service robots are increasingly used in domestic settings to assist older adults and people requiring support. Effective operation involves not only physical interaction but also the ability to interpret dynamic environments, understand tasks, and choose appropriate actions based on context. This requires integrating both hardware components (e.g. sensors, actuators) and software systems capable of reasoning about tasks, environments, and robot capabilities. Frameworks such as the Robot Operating System (ROS) provide open-source tools that help connect low-level hardware with higher-level functionalities. However, real-world deployments remain tightly coupled to specific platforms. As a result, solutions are often isolated and hard-coded, limiting interoperability, reusability, and knowledge sharing. Ontologies and knowledge graphs offer a structured way to represent tasks, environments, and robot capabilities. Existing ontologies, such as the Socio-physical Model of Activities (SOMA) and the Descriptive Ontology for Linguistic and Cognitive Engineering (DOLCE), provide models for activities, spatial relationships, and reasoning structures. However, they often focus on specific domains and do not fully capture the connection between environment, action, robot capabilities, and system-level integration. In this work, we propose the Ontology for roBOts and acTions (OntoBOT), which extends existing ontologies to provide a unified representation of tasks, actions, environments, and capabilities. Our contributions are twofold: (1) we unify these aspects into a cohesive ontology to support formal reasoning about task execution, and (2) we demonstrate its generalizability by evaluating competency questions across four embodied agents - TIAGo, HSR, UR3, and Stretch - showing how OntoBOT enables context-aware reasoning, task-oriented execution, and knowledge sharing in service robotics.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "315",
        "title": "Chimera: Diagnosing Shortcut Learning in Visual-Language Understanding",
        "author": [
            "Ziheng Chi",
            "Yifan Hou",
            "Chenxi Pang",
            "Shaobo Cui",
            "Mubashara Akhtar",
            "Mrinmaya Sachan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22437",
        "abstract": "Diagrams convey symbolic information in a visual format rather than a linear stream of words, making them especially challenging for AI models to process. While recent evaluations suggest that vision-language models (VLMs) perform well on diagram-related benchmarks, their reliance on knowledge, reasoning, or modality shortcuts raises concerns about whether they genuinely understand and reason over diagrams. To address this gap, we introduce Chimera, a comprehensive test suite comprising 7,500 high-quality diagrams sourced from Wikipedia; each diagram is annotated with its symbolic content represented by semantic triples along with multi-level questions designed to assess four fundamental aspects of diagram comprehension: entity recognition, relation understanding, knowledge grounding, and visual reasoning. We use Chimera to measure the presence of three types of shortcuts in visual question answering: (1) the visual-memorization shortcut, where VLMs rely on memorized visual patterns; (2) the knowledge-recall shortcut, where models leverage memorized factual knowledge instead of interpreting the diagram; and (3) the Clever-Hans shortcut, where models exploit superficial language patterns or priors without true comprehension. We evaluate 15 open-source VLMs from 7 model families on Chimera and find that their seemingly strong performance largely stems from shortcut behaviors: visual-memorization shortcuts have slight impact, knowledge-recall shortcuts play a moderate role, and Clever-Hans shortcuts contribute significantly. These findings expose critical limitations in current VLMs and underscore the need for more robust evaluation protocols that benchmark genuine comprehension of complex visual inputs (e.g., diagrams) rather than question-answering shortcuts.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "316",
        "title": "Learning to Ball: Composing Policies for Long-Horizon Basketball Moves",
        "author": [
            "Pei Xu",
            "Zhen Wu",
            "Ruocheng Wang",
            "Vishnu Sarukkai",
            "Kayvon Fatahalian",
            "Ioannis Karamouzas",
            "Victor Zordan",
            "C. Karen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22442",
        "abstract": "Learning a control policy for a multi-phase, long-horizon task, such as basketball maneuvers, remains challenging for reinforcement learning approaches due to the need for seamless policy composition and transitions between skills. A long-horizon task typically consists of distinct subtasks with well-defined goals, separated by transitional subtasks with unclear goals but critical to the success of the entire task. Existing methods like the mixture of experts and skill chaining struggle with tasks where individual policies do not share significant commonly explored states or lack well-defined initial and terminal states between different phases. In this paper, we introduce a novel policy integration framework to enable the composition of drastically different motor skills in multi-phase long-horizon tasks with ill-defined intermediate states. Based on that, we further introduce a high-level soft router to enable seamless and robust transitions between the subtasks. We evaluate our framework on a set of fundamental basketball skills and challenging transitions. Policies trained by our approach can effectively control the simulated character to interact with the ball and accomplish the long-horizon task specified by real-time user commands, without relying on ball trajectory references.",
        "tags": [
            "MoE",
            "RL"
        ]
    },
    {
        "id": "317",
        "title": "Guiding Evolution of Artificial Life Using Vision-Language Models",
        "author": [
            "Nikhil Baid",
            "Hannah Erlebach",
            "Paul Hellegouarch",
            "Frederico Wieser"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22447",
        "abstract": "Foundation models (FMs) have recently opened up new frontiers in the field of artificial life (ALife) by providing powerful tools to automate search through ALife simulations. Previous work aligns ALife simulations with natural language target prompts using vision-language models (VLMs). We build on Automated Search for Artificial Life (ASAL) by introducing ASAL++, a method for open-ended-like search guided by multimodal FMs. We use a second FM to propose new evolutionary targets based on a simulation's visual history. This induces an evolutionary trajectory with increasingly complex targets.\nWe explore two strategies: (1) evolving a simulation to match a single new prompt at each iteration (Evolved Supervised Targets: EST) and (2) evolving a simulation to match the entire sequence of generated prompts (Evolved Temporal Targets: ETT). We test our method empirically in the Lenia substrate using Gemma-3 to propose evolutionary targets, and show that EST promotes greater visual novelty, while ETT fosters more coherent and interpretable evolutionary sequences.\nOur results suggest that ASAL++ points towards new directions for FM-driven ALife discovery with open-ended characteristics.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "318",
        "title": "Detecting (Un)answerability in Large Language Models with Linear Directions",
        "author": [
            "Maor Juliet Lavi",
            "Tova Milo",
            "Mor Geva"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22449",
        "abstract": "Large language models (LLMs) often respond confidently to questions even when they lack the necessary information, leading to hallucinated answers. In this work, we study the problem of (un)answerability detection, focusing on extractive question answering (QA) where the model should determine if a passage contains sufficient information to answer a given question. We propose a simple approach for identifying a direction in the model's activation space that captures unanswerability and uses it for classification. This direction is selected by applying activation additions during inference and measuring their impact on the model's abstention behavior. We show that projecting hidden activations onto this direction yields a reliable score for (un)answerability classification. Experiments on two open-weight LLMs and four extractive QA benchmarks show that our method effectively detects unanswerable questions and generalizes better across datasets than existing prompt-based and classifier-based approaches. Moreover, the obtained directions extend beyond extractive QA to unanswerability that stems from factors, such as lack of scientific consensus and subjectivity. Last, causal interventions show that adding or ablating the directions effectively controls the abstention behavior of the model.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "319",
        "title": "Overclocking Electrostatic Generative Models",
        "author": [
            "Daniil Shlenskii",
            "Alexander Korotin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22454",
        "abstract": "Electrostatic generative models such as PFGM++ have recently emerged as a powerful framework, achieving state-of-the-art performance in image synthesis. PFGM++ operates in an extended data space with auxiliary dimensionality $D$, recovering the diffusion model framework as $D\\to\\infty$, while yielding superior empirical results for finite $D$. Like diffusion models, PFGM++ relies on expensive ODE simulations to generate samples, making it computationally costly. To address this, we propose Inverse Poisson Flow Matching (IPFM), a novel distillation framework that accelerates electrostatic generative models across all values of $D$. Our IPFM reformulates distillation as an inverse problem: learning a generator whose induced electrostatic field matches that of the teacher. We derive a tractable training objective for this problem and show that, as $D \\to \\infty$, our IPFM closely recovers Score Identity Distillation (SiD), a recent method for distilling diffusion models. Empirically, our IPFM produces distilled generators that achieve near-teacher or even superior sample quality using only a few function evaluations. Moreover, we observe that distillation converges faster for finite $D$ than in the $D \\to \\infty$ (diffusion) limit, which is consistent with prior findings that finite-$D$ PFGM++ models exhibit more favorable optimization and sampling properties.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "ODE"
        ]
    },
    {
        "id": "320",
        "title": "GeoSketch: A Neural-Symbolic Approach to Geometric Multimodal Reasoning with Auxiliary Line Construction and Affine Transformation",
        "author": [
            "Shichao Weng",
            "Zhiqiang Wang",
            "Yuhua Zhou",
            "Rui Lu",
            "Ting Liu",
            "Zhiyang Teng",
            "Xiaozhang Liu",
            "Hanmeng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22460",
        "abstract": "Geometric Problem Solving (GPS) poses a unique challenge for Multimodal Large Language Models (MLLMs), requiring not only the joint interpretation of text and diagrams but also iterative visuospatial reasoning. While existing approaches process diagrams as static images, they lack the capacity for dynamic manipulation - a core aspect of human geometric reasoning involving auxiliary line construction and affine transformations. We present GeoSketch, a neural-symbolic framework that recasts geometric reasoning as an interactive perception-reasoning-action loop. GeoSketch integrates: (1) a Perception module that abstracts diagrams into structured logic forms, (2) a Symbolic Reasoning module that applies geometric theorems to decide the next deductive step, and (3) a Sketch Action module that executes operations such as drawing auxiliary lines or applying transformations, thereby updating the diagram in a closed loop. To train this agent, we develop a two-stage pipeline: supervised fine-tuning on 2,000 symbolic-curated trajectories followed by reinforcement learning with dense, symbolic rewards to enhance robustness and strategic exploration. To evaluate this paradigm, we introduce the GeoSketch Benchmark, a high-quality set of 390 geometry problems requiring auxiliary construction or affine transformations. Experiments on strong MLLM baselines demonstrate that GeoSketch significantly improves stepwise reasoning accuracy and problem-solving success over static perception methods. By unifying hierarchical decision-making, executable visual actions, and symbolic verification, GeoSketch advances multimodal reasoning from static interpretation to dynamic, verifiable interaction, establishing a new foundation for solving complex visuospatial problems.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "321",
        "title": "MDAR: A Multi-scene Dynamic Audio Reasoning Benchmark",
        "author": [
            "Hui Li",
            "Changhao Jiang",
            "Hongyu Wang",
            "Ming Zhang",
            "Jiajun Sun",
            "Zhixiong Yang",
            "Yifei Cao",
            "Shihan Dou",
            "Xiaoran Fan",
            "Baoyu Fan",
            "Tao Ji",
            "Tao Gui",
            "Qi Zhang",
            "Xuanjing Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22461",
        "abstract": "The ability to reason from audio, including speech, paralinguistic cues, environmental sounds, and music, is essential for AI agents to interact effectively in real-world scenarios. Existing benchmarks mainly focus on static or single-scene settings and do not fully capture scenarios where multiple speakers, unfolding events, and heterogeneous audio sources interact. To address these challenges, we introduce MDAR, a benchmark for evaluating models on complex, multi-scene, and dynamically evolving audio reasoning tasks. MDAR comprises 3,000 carefully curated question-answer pairs linked to diverse audio clips, covering five categories of complex reasoning and spanning three question types. We benchmark 26 state-of-the-art audio language models on MDAR and observe that they exhibit limitations in complex reasoning tasks. On single-choice questions, Qwen2.5-Omni (open-source) achieves 76.67% accuracy, whereas GPT-4o Audio (closed-source) reaches 68.47%; however, GPT-4o Audio substantially outperforms Qwen2.5-Omni on the more challenging multiple-choice and open-ended tasks. Across all three question types, no model achieves 80% performance. These findings underscore the unique challenges posed by MDAR and its value as a benchmark for advancing audio reasoning http://research.Code and benchmark can be found at https://github.com/luckyerr/MDAR.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "322",
        "title": "IIET: Efficient Numerical Transformer via Implicit Iterative Euler Method",
        "author": [
            "Xinyu Liu",
            "Bei Li",
            "Jiahao Liu",
            "Junhao Ruan",
            "Kechen Jiao",
            "Hongyin Tang",
            "Jingang Wang",
            "Xiao Tong",
            "Jingbo Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22463",
        "abstract": "High-order numerical methods enhance Transformer performance in tasks like NLP and CV, but introduce a performance-efficiency trade-off due to increased computational overhead. Our analysis reveals that conventional efficiency techniques, such as distillation, can be detrimental to the performance of these models, exemplified by PCformer. To explore more optimizable ODE-based Transformer architectures, we propose the \\textbf{I}terative \\textbf{I}mplicit \\textbf{E}uler \\textbf{T}ransformer \\textbf{(IIET)}, which simplifies high-order methods using an iterative implicit Euler approach. This simplification not only leads to superior performance but also facilitates model compression compared to PCformer. To enhance inference efficiency, we introduce \\textbf{I}teration \\textbf{I}nfluence-\\textbf{A}ware \\textbf{D}istillation \\textbf{(IIAD)}. Through a flexible threshold, IIAD allows users to effectively balance the performance-efficiency trade-off. On lm-evaluation-harness, IIET boosts average accuracy by 2.65\\% over vanilla Transformers and 0.8\\% over PCformer. Its efficient variant, E-IIET, significantly cuts inference overhead by 55\\% while retaining 99.4\\% of the original task accuracy. Moreover, the most efficient IIET variant achieves an average performance gain exceeding 1.6\\% over vanilla Transformer with comparable speed.",
        "tags": [
            "ODE",
            "Transformer"
        ]
    },
    {
        "id": "323",
        "title": "Uncertainty-Aware Multi-Robot Task Allocation With Strongly Coupled Inter-Robot Rewards",
        "author": [
            "Ben Rossano",
            "Jaein Lim",
            "Jonathan P. How"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22469",
        "abstract": "This paper proposes a task allocation algorithm for teams of heterogeneous robots in environments with uncertain task requirements. We model these requirements as probability distributions over capabilities and use this model to allocate tasks such that robots with complementary skills naturally position near uncertain tasks, proactively mitigating task failures without wasting resources. We introduce a market-based approach that optimizes the joint team objective while explicitly capturing coupled rewards between robots, offering a polynomial-time solution in decentralized settings with strict communication assumptions. Comparative experiments against benchmark algorithms demonstrate the effectiveness of our approach and highlight the challenges of incorporating coupled rewards in a decentralized formulation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "324",
        "title": "Evaluating the Limits of Large Language Models in Multilingual Legal Reasoning",
        "author": [
            "Antreas Ioannou",
            "Andreas Shiamishis",
            "Nora Hollenstein",
            "Nezihe Merve GÃ¼rel"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22472",
        "abstract": "In an era dominated by Large Language Models (LLMs), understanding their capabilities and limitations, especially in high-stakes fields like law, is crucial. While LLMs such as Meta's LLaMA, OpenAI's ChatGPT, Google's Gemini, DeepSeek, and other emerging models are increasingly integrated into legal workflows, their performance in multilingual, jurisdictionally diverse, and adversarial contexts remains insufficiently explored. This work evaluates LLaMA and Gemini on multilingual legal and non-legal benchmarks, and assesses their adversarial robustness in legal tasks through character and word-level perturbations. We use an LLM-as-a-Judge approach for human-aligned evaluation. We moreover present an open-source, modular evaluation pipeline designed to support multilingual, task-diverse benchmarking of any combination of LLMs and datasets, with a particular focus on legal tasks, including classification, summarization, open questions, and general reasoning. Our findings confirm that legal tasks pose significant challenges for LLMs with accuracies often below 50% on legal reasoning benchmarks such as LEXam, compared to over 70% on general-purpose tasks like XNLI. In addition, while English generally yields more stable results, it does not always lead to higher accuracy. Prompt sensitivity and adversarial vulnerability is also shown to persist across languages. Finally, a correlation is found between the performance of a language and its syntactic similarity to English. We also observe that LLaMA is weaker than Gemini, with the latter showing an average advantage of about 24 percentage points across the same task. Despite improvements in newer LLMs, challenges remain in deploying them reliably for critical, multilingual legal applications.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "325",
        "title": "NeLLCom-Lex: A Neural-agent Framework to Study the Interplay between Lexical Systems and Language Use",
        "author": [
            "Yuqing Zhang",
            "Ecesu Ãrker",
            "Tessa Verhoef",
            "Gemma Boleda",
            "Arianna Bisazza"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22479",
        "abstract": "Lexical semantic change has primarily been investigated with observational and experimental methods; however, observational methods (corpus analysis, distributional semantic modeling) cannot get at causal mechanisms, and experimental paradigms with humans are hard to apply to semantic change due to the extended diachronic processes involved. This work introduces NeLLCom-Lex, a neural-agent framework designed to simulate semantic change by first grounding agents in a real lexical system (e.g. English) and then systematically manipulating their communicative needs. Using a well-established color naming task, we simulate the evolution of a lexical system within a single generation, and study which factors lead agents to: (i) develop human-like naming behavior and lexicons, and (ii) change their behavior and lexicons according to their communicative needs. Our experiments with different supervised and reinforcement learning pipelines show that neural agents trained to 'speak' an existing language can reproduce human-like patterns in color naming to a remarkable extent, supporting the further use of NeLLCom-Lex to elucidate the mechanisms of semantic change.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "326",
        "title": "Exploring Solution Divergence and Its Effect on Large Language Model Problem Solving",
        "author": [
            "Hang Li",
            "Kaiqi Yang",
            "Yucheng Chu",
            "Hui Liu",
            "Jiliang Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22480",
        "abstract": "Large language models (LLMs) have been widely used for problem-solving tasks. Most recent work improves their performance through supervised fine-tuning (SFT) with labeled data or reinforcement learning (RL) from task feedback. In this paper, we study a new perspective: the divergence in solutions generated by LLMs for a single problem. We show that higher solution divergence is positively related to better problem-solving abilities across various models. Based on this finding, we propose solution divergence as a novel metric that can support both SFT and RL strategies. We test this idea on three representative problem domains and find that using solution divergence consistently improves success rates. These results suggest that solution divergence is a simple but effective tool for advancing LLM training and evaluation.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "327",
        "title": "OFMU: Optimization-Driven Framework for Machine Unlearning",
        "author": [
            "Sadia Asif",
            "Mohammad Mohammadi Amiri"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22483",
        "abstract": "Large language models deployed in sensitive applications increasingly require the ability to unlearn specific knowledge, such as user requests, copyrighted materials, or outdated information, without retraining from scratch to ensure regulatory compliance, user privacy, and safety. This task, known as machine unlearning, aims to remove the influence of targeted data (forgetting) while maintaining performance on the remaining data (retention). A common approach is to formulate this as a multi-objective problem and reduce it to a single-objective problem via scalarization, where forgetting and retention losses are combined using a weighted sum. However, this often results in unstable training dynamics and degraded model utility due to conflicting gradient directions. To address these challenges, we propose OFMU, a penalty-based bi-level optimization framework that explicitly prioritizes forgetting while preserving retention through a hierarchical structure. Our method enforces forgetting via an inner maximization step that incorporates a similarity-aware penalty to decorrelate the gradients of the forget and retention objectives, and restores utility through an outer minimization step. To ensure scalability, we develop a two-loop algorithm with provable convergence guarantees under both convex and non-convex regimes. We further provide a rigorous theoretical analysis of convergence rates and show that our approach achieves better trade-offs between forgetting efficacy and model utility compared to prior methods. Extensive experiments across vision and language benchmarks demonstrate that OFMU consistently outperforms existing unlearning methods in both forgetting efficacy and retained utility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "328",
        "title": "Group Critical-token Policy Optimization for Autoregressive Image Generation",
        "author": [
            "Guohui Zhang",
            "Hu Yu",
            "Xiaoxiao Ma",
            "JingHao Zhang",
            "Yaning Pan",
            "Mingde Yao",
            "Jie Xiao",
            "Linjiang Huang",
            "Feng Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22485",
        "abstract": "Recent studies have extended Reinforcement Learning with Verifiable Rewards (RLVR) to autoregressive (AR) visual generation and achieved promising progress. However, existing methods typically apply uniform optimization across all image tokens, while the varying contributions of different image tokens for RLVR's training remain unexplored. In fact, the key obstacle lies in how to identify more critical image tokens during AR generation and implement effective token-wise optimization for them. To tackle this challenge, we propose $\\textbf{G}$roup $\\textbf{C}$ritical-token $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{GCPO}$), which facilitates effective policy optimization on critical tokens. We identify the critical tokens in RLVR-based AR generation from three perspectives, specifically: $\\textbf{(1)}$ Causal dependency: early tokens fundamentally determine the later tokens and final image effect due to unidirectional dependency; $\\textbf{(2)}$ Entropy-induced spatial structure: tokens with high entropy gradients correspond to image structure and bridges distinct visual regions; $\\textbf{(3)}$ RLVR-focused token diversity: tokens with low visual similarity across a group of sampled images contribute to richer token-level diversity. For these identified critical tokens, we further introduce a dynamic token-wise advantage weight to encourage exploration, based on confidence divergence between the policy model and reference model. By leveraging 30\\% of the image tokens, GCPO achieves better performance than GRPO with full tokens. Extensive experiments on multiple text-to-image benchmarks for both AR models and unified multimodal models demonstrate the effectiveness of GCPO for AR visual generation.",
        "tags": [
            "GRPO",
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": "329",
        "title": "JGU Mainz's Submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: MT and QA",
        "author": [
            "Hossain Shaikh Saadi",
            "Minh Duc Bui",
            "Mario Sanz-Guerrero",
            "Katharina von der Wense"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22490",
        "abstract": "This paper presents the JGU Mainz submission to the WMT25 Shared Task on LLMs with Limited Resources for Slavic Languages: Machine Translation and Question Answering, focusing on Ukrainian, Upper Sorbian, and Lower Sorbian. For each language, we jointly fine-tune a Qwen2.5-3B-Instruct model for both tasks with parameter-efficient finetuning. Our pipeline integrates additional translation and multiple-choice question answering (QA) data. For Ukrainian QA, we further use retrieval-augmented generation. We also apply ensembling for QA in Upper and Lower Sorbian. Experiments show that our models outperform the baseline on both tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "330",
        "title": "Ontological foundations for contrastive explanatory narration of robot plans",
        "author": [
            "Alberto Olivares-Alarcos",
            "Sergi Foix",
            "JÃºlia BorrÃ s",
            "Gerard Canal",
            "Guillem AlenyÃ "
        ],
        "pdf": "https://arxiv.org/pdf/2509.22493",
        "abstract": "Mutual understanding of artificial agents' decisions is key to ensuring a trustworthy and successful human-robot interaction. Hence, robots are expected to make reasonable decisions and communicate them to humans when needed. In this article, the focus is on an approach to modeling and reasoning about the comparison of two competing plans, so that robots can later explain the divergent result. First, a novel ontological model is proposed to formalize and reason about the differences between competing plans, enabling the classification of the most appropriate one (e.g., the shortest, the safest, the closest to human preferences, etc.). This work also investigates the limitations of a baseline algorithm for ontology-based explanatory narration. To address these limitations, a novel algorithm is presented, leveraging divergent knowledge between plans and facilitating the construction of contrastive narratives. Through empirical evaluation, it is observed that the explanations excel beyond the baseline method.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "331",
        "title": "Where MLLMs Attend and What They Rely On: Explaining Autoregressive Token Generation",
        "author": [
            "Ruoyu Chen",
            "Xiaoqing Guo",
            "Kangwei Liu",
            "Siyuan Liang",
            "Shiming Liu",
            "Qunli Zhang",
            "Hua Zhang",
            "Xiaochun Cao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22496",
        "abstract": "Multimodal large language models (MLLMs) have demonstrated remarkable capabilities in aligning visual inputs with natural language outputs. Yet, the extent to which generated tokens depend on visual modalities remains poorly understood, limiting interpretability and reliability. In this work, we present EAGLE, a lightweight black-box framework for explaining autoregressive token generation in MLLMs. EAGLE attributes any selected tokens to compact perceptual regions while quantifying the relative influence of language priors and perceptual evidence. The framework introduces an objective function that unifies sufficiency (insight score) and indispensability (necessity score), optimized via greedy search over sparsified image regions for faithful and efficient attribution. Beyond spatial attribution, EAGLE performs modality-aware analysis that disentangles what tokens rely on, providing fine-grained interpretability of model decisions. Extensive experiments across open-source MLLMs show that EAGLE consistently outperforms existing methods in faithfulness, localization, and hallucination diagnosis, while requiring substantially less GPU memory. These results highlight its effectiveness and practicality for advancing the interpretability of MLLMs. The code is available at https://github.com/RuoyuChen10/EAGLE.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "332",
        "title": "HELIOS: Hierarchical Exploration for Language-grounded Interaction in Open Scenes",
        "author": [
            "Katrina Ashton",
            "Chahyon Ku",
            "Shrey Shah",
            "Wen Jiang",
            "Kostas Daniilidis",
            "Bernadette Bucher"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22498",
        "abstract": "Language-specified mobile manipulation tasks in novel environments simultaneously face challenges interacting with a scene which is only partially observed, grounding semantic information from language instructions to the partially observed scene, and actively updating knowledge of the scene with new observations. To address these challenges, we propose HELIOS, a hierarchical scene representation and associated search objective to perform language specified pick and place mobile manipulation tasks. We construct 2D maps containing the relevant semantic and occupancy information for navigation while simultaneously actively constructing 3D Gaussian representations of task-relevant objects. We fuse observations across this multi-layered representation while explicitly modeling the multi-view consistency of the detections of each object. In order to efficiently search for the target object, we formulate an objective function balancing exploration of unobserved or uncertain regions with exploitation of scene semantic information. We evaluate HELIOS on the OVMM benchmark in the Habitat simulator, a pick and place benchmark in which perception is challenging due to large and complex scenes with comparatively small target objects. HELIOS achieves state-of-the-art results on OVMM. As our approach is zero-shot, HELIOS can also transfer to the real world without requiring additional data, as we illustrate by demonstrating it in a real world office environment on a Spot robot.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "333",
        "title": "InfiAgent: Self-Evolving Pyramid Agent Framework for Infinite Scenarios",
        "author": [
            "Chenglin Yu",
            "Yang Yu",
            "Songmiao Wang",
            "Yucheng Wang",
            "Yifan Yang",
            "Jinjia Li",
            "Ming Li",
            "Hongxia Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22502",
        "abstract": "Large Language Model (LLM) agents have demonstrated remarkable capabilities in organizing and executing complex tasks, and many such agents are now widely used in various application scenarios. However, developing these agents requires carefully designed workflows, carefully crafted prompts, and iterative tuning, which requires LLM techniques and domain-specific expertise. These hand-crafted limitations hinder the scalability and cost-effectiveness of LLM agents across a wide range of industries. To address these challenges, we propose \\textbf{InfiAgent}, a Pyramid-like DAG-based Multi-Agent Framework that can be applied to \\textbf{infi}nite scenarios, which introduces several key innovations: a generalized \"agent-as-a-tool\" mechanism that automatically decomposes complex agents into hierarchical multi-agent systems; a dual-audit mechanism that ensures the quality and stability of task completion; an agent routing function that enables efficient task-agent matching; and an agent self-evolution mechanism that autonomously restructures the agent DAG based on new tasks, poor performance, or optimization opportunities. Furthermore, InfiAgent's atomic task design supports agent parallelism, significantly improving execution efficiency. This framework evolves into a versatile pyramid-like multi-agent system capable of solving a wide range of problems. Evaluations on multiple benchmarks demonstrate that InfiAgent achieves 9.9\\% higher performance compared to ADAS (similar auto-generated agent framework), while a case study of the AI research assistant InfiHelper shows that it generates scientific papers that have received recognition from human reviewers at top-tier IEEE conferences.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "334",
        "title": "Estimating the Empowerment of Language Model Agents",
        "author": [
            "Jinyeop Song",
            "Jeff Gore",
            "Max Kleiman-Weiner"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22504",
        "abstract": "As language model (LM) agents become more capable and gain broader access to real-world tools, there is a growing need for scalable evaluation frameworks of agentic capability. However, conventional benchmark-centric evaluations are costly to design and require human designers to come up with valid tasks that translate into insights about general model capabilities. In this work, we propose information-theoretic evaluation based on empowerment, the mutual information between an agent's actions and future states, as an open-ended method for evaluating LM agents. We introduce EELMA (Estimating Empowerment of Language Model Agents), an algorithm for approximating effective empowerment from multi-turn text interactions. We validate EELMA on both language games and scaled-up realistic web-browsing scenarios. We find that empowerment strongly correlates with average task performance, characterize the impact of environmental complexity and agentic factors such as chain-of-thought, model scale, and memory length on estimated empowerment, and that high empowerment states and actions are often pivotal moments for general capabilities. Together, these results demonstrate empowerment as an appealing general-purpose metric for evaluating and monitoring LM agents in complex, open-ended settings.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "335",
        "title": "Representing LLMs in Prompt Semantic Task Space",
        "author": [
            "Idan Kashani",
            "Avi Mendelson",
            "Yaniv Nemcovsky"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22506",
        "abstract": "Large language models (LLMs) achieve impressive results over various tasks, and ever-expanding public repositories contain an abundance of pre-trained models. Therefore, identifying the best-performing LLM for a given task is a significant challenge. Previous works have suggested learning LLM representations to address this. However, these approaches present limited scalability and require costly retraining to encompass additional models and datasets. Moreover, the produced representation utilizes distinct spaces that cannot be easily interpreted. This work presents an efficient, training-free approach to representing LLMs as linear operators within the prompts' semantic task space, thus providing a highly interpretable representation of the models' application. Our method utilizes closed-form computation of geometrical properties and ensures exceptional scalability and real-time adaptability to dynamically expanding repositories. We demonstrate our approach on success prediction and model selection tasks, achieving competitive or state-of-the-art results with notable performance in out-of-sample scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "336",
        "title": "We Think, Therefore We Align LLMs to Helpful, Harmless and Honest Before They Go Wrong",
        "author": [
            "Gautam Siddharth Kashyap",
            "Mark Dras",
            "Usman Naseem"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22510",
        "abstract": "Alignment of Large Language Models (LLMs) along multiple objectives-helpfulness, harmlessness, and honesty (HHH)-is critical for safe and reliable deployment. Prior work has used steering vector-small control signals injected into hidden states-to guide LLM outputs, typically via one-to-one (1-to-1) Transformer decoders. In this setting, optimizing a single alignment objective can inadvertently overwrite representations learned for other objectives, leading to catastrophic forgetting. More recent approaches extend steering vectors via one-to-many (1-to-N) Transformer decoders. While this alleviates catastrophic forgetting, naive multi-branch designs optimize each objective independently, which can cause inference fragmentation-outputs across HHH objectives may become inconsistent. We propose Adaptive Multi-Branch Steering (AMBS), a two-stage 1-to-N framework for unified and efficient multi-objective alignment. In Stage I, post-attention hidden states of the Transformer layer are computed once to form a shared representation. In Stage II, this representation is cloned into parallel branches and steered via a policy-reference mechanism, enabling objective-specific control while maintaining cross-objective consistency. Empirical evaluations on Alpaca, BeaverTails, and TruthfulQA show that AMBS consistently improves HHH alignment across multiple 7B LLM backbones. For example, on DeepSeek-7B, AMBS improves average alignment scores by +32.4% and reduces unsafe outputs by 11.0% compared to a naive 1-to-N baseline, while remaining competitive with state-of-the-art methods.",
        "tags": [
            "DeepSeek",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "337",
        "title": "AxLLM: accelerator architecture for large language models with computation reuse capability",
        "author": [
            "Soroush Ahadi",
            "Mehdi Modarressi",
            "Masoud Daneshtalab"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22512",
        "abstract": "Large language models demand massive computational power and memory resources, posing significant challenges for efficient deployment. While quantization has been widely explored to reduce model size and computation, this paper demonstrates an additional benefit: quantization increases parameter locality, creating opportunities for computation reuse. Building on this insight, we propose AxLLM, a hardware accelerator architecture designed for quantized models. Axllm introduces a novel redundancy elimination technique that caches and reuses multiplication results for repeated weight values, substantially reducing redundant operations. The architecture features dual multiply and reuse pipelines, efficiently supporting both base models and LoRA fine-tuned models without altering parameters, retraining, or requiring offline preprocessing. Experimental results show that AxLLM achieves up to 90% reduction in computations, delivering 28% lower energy consumption and a 1.7x speedup over baseline execution. These results highlight Axllm as a scalable and efficient solution for accelerating LLMs on specialized hardware.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "338",
        "title": "TrueGradeAI: Retrieval-Augmented and Bias-Resistant AI for Transparent and Explainable Digital Assessments",
        "author": [
            "Rakesh Thakur",
            "Shivaansh Kaushik",
            "Gauri Chopra",
            "Harsh Rohilla"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22516",
        "abstract": "This paper introduces TrueGradeAI, an AI-driven digital examination framework designed to overcome the shortcomings of traditional paper-based assessments, including excessive paper usage, logistical complexity, grading delays, and evaluator bias. The system preserves natural handwriting by capturing stylus input on secure tablets and applying transformer-based optical character recognition for transcription. Evaluation is conducted through a retrieval-augmented pipeline that integrates faculty solutions, cache layers, and external references, enabling a large language model to assign scores with explicit, evidence-linked reasoning. Unlike prior tablet-based exam systems that primarily digitize responses, TrueGradeAI advances the field by incorporating explainable automation, bias mitigation, and auditable grading trails. By uniting handwriting preservation with scalable and transparent evaluation, the framework reduces environmental costs, accelerates feedback cycles, and progressively builds a reusable knowledge base, while actively working to mitigate grading bias and ensure fairness in assessment.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "339",
        "title": "REMA: A Unified Reasoning Manifold Framework for Interpreting Large Language Model",
        "author": [
            "Bo Li",
            "Guanzhi Deng",
            "Ronghao Chen",
            "Junrong Yue",
            "Shuo Zhang",
            "Qinghua Zhao",
            "Linqi Song",
            "Lijie Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22518",
        "abstract": "Understanding how Large Language Models (LLMs) perform complex reasoning and their failure mechanisms is a challenge in interpretability research. To provide a measurable geometric analysis perspective, we define the concept of the Reasoning Manifold, a latent low-dimensional geometric structure formed by the internal representations corresponding to all correctly reasoned generations. This structure can be conceptualized as the embodiment of the effective thinking paths that the model has learned to successfully solve a given task. Based on this concept, we build REMA, a framework that explains the origins of failures by quantitatively comparing the spatial relationships of internal model representations corresponding to both erroneous and correct reasoning samples. Specifically, REMA first quantifies the geometric deviation of each erroneous representation by calculating its k-nearest neighbors distance to the approximated manifold formed by correct representations, thereby providing a unified failure signal. It then localizes the divergence points where these deviations first become significant by tracking this deviation metric across the model's layers and comparing it against a baseline of internal fluctuations from correct representations, thus identifying where the reasoning chain begins to go off-track. Our extensive experiments on diverse language and multimodal models and tasks demonstrate the low-dimensional nature of the reasoning manifold and the high separability between erroneous and correct reasoning representations. The results also validate the effectiveness of the REMA framework in analyzing the origins of reasoning failures. This research connects abstract reasoning failures to measurable geometric deviations in representations, providing new avenues for in-depth understanding and diagnosis of the internal computational processes of black-box models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "340",
        "title": "JointDiff: Bridging Continuous and Discrete in Multi-Agent Trajectory Generation",
        "author": [
            "Guillem Capellera",
            "Luis Ferraz",
            "Antonio Rubio",
            "Alexandre Alahi",
            "Antonio Agudo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22522",
        "abstract": "Generative models often treat continuous data and discrete events as separate processes, creating a gap in modeling complex systems where they interact synchronously. To bridge this gap, we introduce JointDiff, a novel diffusion framework designed to unify these two processes by simultaneously generating continuous spatio-temporal data and synchronous discrete events. We demonstrate its efficacy in the sports domain by simultaneously modeling multi-agent trajectories and key possession events. This joint modeling is validated with non-controllable generation and two novel controllable generation scenarios: weak-possessor-guidance, which offers flexible semantic control over game dynamics through a simple list of intended ball possessors, and text-guidance, which enables fine-grained, language-driven generation. To enable the conditioning with these guidance signals, we introduce CrossGuid, an effective conditioning operation for multi-agent domains. We also share a new unified sports benchmark enhanced with textual descriptions for soccer and football datasets. JointDiff achieves state-of-the-art performance, demonstrating that joint modeling is crucial for building realistic and controllable generative models for interactive systems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "341",
        "title": "Color Names in Vision-Language Models",
        "author": [
            "Alexandra Gomez-Villa",
            "Pablo HernÃ¡ndez-CÃ¡mara",
            "Muhammad Atif Butt",
            "Valero Laparra",
            "Jesus Malo",
            "Javier Vazquez-Corral"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22524",
        "abstract": "Color serves as a fundamental dimension of human visual perception and a primary means of communicating about objects and scenes. As vision-language models (VLMs) become increasingly prevalent, understanding whether they name colors like humans is crucial for effective human-AI interaction. We present the first systematic evaluation of color naming capabilities across VLMs, replicating classic color naming methodologies using 957 color samples across five representative models. Our results show that while VLMs achieve high accuracy on prototypical colors from classical studies, performance drops significantly on expanded, non-prototypical color sets. We identify 21 common color terms that consistently emerge across all models, revealing two distinct approaches: constrained models using predominantly basic terms versus expansive models employing systematic lightness modifiers. Cross-linguistic analysis across nine languages demonstrates severe training imbalances favoring English and Chinese, with hue serving as the primary driver of color naming decisions. Finally, ablation studies reveal that language model architecture significantly influences color naming independent of visual processing capabilities.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "342",
        "title": "EfficientDepth: A Fast and Detail-Preserving Monocular Depth Estimation Model",
        "author": [
            "Andrii Litvynchuk",
            "Ivan Livinsky",
            "Anand Ravi",
            "Nima Kalantari",
            "Andrii Tsarov"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22527",
        "abstract": "Monocular depth estimation (MDE) plays a pivotal role in various computer vision applications, such as robotics, augmented reality, and autonomous driving. Despite recent advancements, existing methods often fail to meet key requirements for 3D reconstruction and view synthesis, including geometric consistency, fine details, robustness to real-world challenges like reflective surfaces, and efficiency for edge devices. To address these challenges, we introduce a novel MDE system, called EfficientDepth, which combines a transformer architecture with a lightweight convolutional decoder, as well as a bimodal density head that allows the network to estimate detailed depth maps. We train our model on a combination of labeled synthetic and real images, as well as pseudo-labeled real images, generated using a high-performing MDE method. Furthermore, we employ a multi-stage optimization strategy to improve training efficiency and produce models that emphasize geometric consistency and fine detail. Finally, in addition to commonly used objectives, we introduce a loss function based on LPIPS to encourage the network to produce detailed depth maps. Experimental results demonstrate that EfficientDepth achieves performance comparable to or better than existing state-of-the-art models, with significantly reduced computational resources.",
        "tags": [
            "3D",
            "Depth Estimation",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "343",
        "title": "Boosting Pointer Analysis With Large Language Model-Enhanced Allocation Function Detection",
        "author": [
            "Baijun Cheng",
            "Kailong Wang",
            "Ling Shi",
            "Haoyu Wang",
            "Peng Di",
            "Yao Guo",
            "Ding Li",
            "Xiangqun Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22530",
        "abstract": "Pointer analysis is foundational for many static analysis tasks, yet its effectiveness is often hindered by imprecise modeling of heap allocations, particularly in C/C++ programs where user-defined allocation functions (AFs) are pervasive. Existing approaches largely overlook these custom allocators, leading to coarse aliasing and reduced analysis precision. In this paper, we present AFD, a novel technique that enhances pointer analysis by automatically identifying and modeling custom allocation functions. AFD employs a hybrid approach: it uses value-flow analysis to detect straightforward wrappers and leverages Large Language Models (LLMs) to reason about more complex allocation patterns with side effects. This targeted enhancement enables precise modeling of heap objects at each call site, achieving context-sensitivity-like benefits without the associated overhead. We evaluate AFD on 15 real-world C projects, identifying over 600 custom AFs. Integrating AFD into a baseline pointer analysis yields a 26x increase in modeled heap objects and a 39% reduction in alias set sizes, with only 1.4x runtime overhead. Furthermore, our enhanced analysis improves indirect call resolution and uncovers 17 previously undetected memory bugs. These results demonstrate that precise modeling of custom allocation functions offers a scalable and practical path to improving pointer analysis in large software systems.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "344",
        "title": "InfiR2: A Comprehensive FP8 Training Recipe for Reasoning-Enhanced Language Models",
        "author": [
            "Wenjun Wang",
            "Shuo Cai",
            "Congkai Xie",
            "Mingfa Feng",
            "Yiming Zhang",
            "Zhen Li",
            "Kejing Yang",
            "Ming Li",
            "Jiannong Cao",
            "Yuan Xie",
            "Hongxia Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22536",
        "abstract": "The immense computational cost of training Large Language Models (LLMs) presents a major barrier to innovation. While FP8 training offers a promising solution with significant theoretical efficiency gains, its widespread adoption has been hindered by the lack of a comprehensive, open-source training recipe. To bridge this gap, we introduce an end-to-end FP8 training recipe that seamlessly integrates continual pre-training and supervised fine-tuning. Our methodology employs a fine-grained, hybrid-granularity quantization strategy to maintain numerical fidelity while maximizing computational efficiency. Through extensive experiments, including the continue pre-training of models on a 160B-token corpus, we demonstrate that our recipe is not only remarkably stable but also essentially lossless, achieving performance on par with the BF16 baseline across a suite of reasoning benchmarks. Crucially, this is achieved with substantial efficiency improvements, including up to a 22% reduction in training time, a 14% decrease in peak memory usage, and a 19% increase in throughput. Our results establish FP8 as a practical and robust alternative to BF16, and we will release the accompanying code to further democratize large-scale model training.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "345",
        "title": "The Emergence of Altruism in Large-Language-Model Agents Society",
        "author": [
            "Haoyang Li",
            "Xiao Jia",
            "Zhanzhan Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22537",
        "abstract": "Leveraging Large Language Models (LLMs) for social simulation is a frontier in computational social science. Understanding the social logics these agents embody is critical to this attempt. However, existing research has primarily focused on cooperation in small-scale, task-oriented games, overlooking how altruism, which means sacrificing self-interest for collective benefit, emerges in large-scale agent societies. To address this gap, we introduce a Schelling-variant urban migration model that creates a social dilemma, compelling over 200 LLM agents to navigate an explicit conflict between egoistic (personal utility) and altruistic (system utility) goals. Our central finding is a fundamental difference in the social tendencies of LLMs. We identify two distinct archetypes: \"Adaptive Egoists\", which default to prioritizing self-interest but whose altruistic behaviors significantly increase under the influence of a social norm-setting message board; and \"Altruistic Optimizers\", which exhibit an inherent altruistic logic, consistently prioritizing collective benefit even at a direct cost to themselves. Furthermore, to qualitatively analyze the cognitive underpinnings of these decisions, we introduce a method inspired by Grounded Theory to systematically code agent reasoning. In summary, this research provides the first evidence of intrinsic heterogeneity in the egoistic and altruistic tendencies of different LLMs. We propose that for social simulation, model selection is not merely a matter of choosing reasoning capability, but of choosing an intrinsic social action logic. While \"Adaptive Egoists\" may offer a more suitable choice for simulating complex human societies, \"Altruistic Optimizers\" are better suited for modeling idealized pro-social actors or scenarios where collective welfare is the primary consideration.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "346",
        "title": "Does AI Coaching Prepare us for Workplace Negotiations?",
        "author": [
            "Veda Duddu",
            "Jash Rajesh Parekh",
            "Andy Mao",
            "Hanyi Min",
            "Ziang Xiao",
            "Vedant Das Swain",
            "Koustuv Saha"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22545",
        "abstract": "Workplace negotiations are undermined by psychological barriers, which can even derail well-prepared tactics. AI offers personalized and always -- available negotiation coaching, yet its effectiveness for negotiation preparedness remains unclear. We built Trucey, a prototype AI coach grounded in Brett's negotiation model. We conducted a between-subjects experiment (N=267), comparing Trucey, ChatGPT, and a traditional negotiation Handbook, followed by in-depth interviews (N=15). While Trucey showed the strongest reductions in fear relative to both comparison conditions, the Handbook outperformed both AIs in usability and psychological empowerment. Interviews revealed that the Handbook's comprehensive, reviewable content was crucial for participants' confidence and preparedness. In contrast, although participants valued AI's rehearsal capability, its guidance often felt verbose and fragmented -- delivered in bits and pieces that required additional effort -- leaving them uncertain or overwhelmed. These findings challenge assumptions of AI superiority and motivate hybrid designs that integrate structured, theory-driven content with targeted rehearsal, clear boundaries, and adaptive scaffolds to address psychological barriers and support negotiation preparedness.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "347",
        "title": "Think Socially via Cognitive Reasoning",
        "author": [
            "Jinfeng Zhou",
            "Zheyu Chen",
            "Shuai Wang",
            "Quanyu Dai",
            "Zhenhua Dong",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22546",
        "abstract": "LLMs trained for logical reasoning excel at step-by-step deduction to reach verifiable answers. However, this paradigm is ill-suited for navigating social situations, which induce an interpretive process of analyzing ambiguous cues that rarely yield a definitive outcome. To bridge this gap, we introduce Cognitive Reasoning, a paradigm modeled on human social cognition. It formulates the interpretive process into a structured cognitive flow of interconnected cognitive units (e.g., observation or attribution), which combine adaptively to enable effective social thinking and responses. We then propose CogFlow, a complete framework that instills this capability in LLMs. CogFlow first curates a dataset of cognitive flows by simulating the associative and progressive nature of human thought via tree-structured planning. After instilling the basic cognitive reasoning capability via supervised fine-tuning, CogFlow adopts reinforcement learning to enable the model to improve itself via trial and error, guided by a multi-objective reward that optimizes both cognitive flow and response quality. Extensive experiments show that CogFlow effectively enhances the social cognitive capabilities of LLMs, and even humans, leading to more effective social decision-making.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "348",
        "title": "JanusVLN: Decoupling Semantics and Spatiality with Dual Implicit Memory for Vision-Language Navigation",
        "author": [
            "Shuang Zeng",
            "Dekang Qi",
            "Xinyuan Chang",
            "Feng Xiong",
            "Shichao Xie",
            "Xiaolong Wu",
            "Shiyi Liang",
            "Mu Xu",
            "Xing Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22548",
        "abstract": "Vision-and-Language Navigation requires an embodied agent to navigate through unseen environments, guided by natural language instructions and a continuous video stream. Recent advances in VLN have been driven by the powerful semantic understanding of Multimodal Large Language Models. However, these methods typically rely on explicit semantic memory, such as building textual cognitive maps or storing historical visual frames. This type of method suffers from spatial information loss, computational redundancy, and memory bloat, which impede efficient navigation. Inspired by the implicit scene representation in human navigation, analogous to the left brain's semantic understanding and the right brain's spatial cognition, we propose JanusVLN, a novel VLN framework featuring a dual implicit neural memory that models spatial-geometric and visual-semantic memory as separate, compact, and fixed-size neural representations. This framework first extends the MLLM to incorporate 3D prior knowledge from the spatial-geometric encoder, thereby enhancing the spatial reasoning capabilities of models based solely on RGB input. Then, the historical key-value caches from the spatial-geometric and visual-semantic encoders are constructed into a dual implicit memory. By retaining only the KVs of tokens in the initial and sliding window, redundant computation is avoided, enabling efficient incremental updates. Extensive experiments demonstrate that JanusVLN outperforms over 20 recent methods to achieve SOTA performance. For example, the success rate improves by 10.5-35.5 compared to methods using multiple data types as input and by 3.6-10.8 compared to methods using more RGB training data. This indicates that the proposed dual implicit neural memory, as a novel paradigm, explores promising new directions for future VLN research. Ours project page: https://miv-xjtu.github.io/JanusVLN.github.io/.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "349",
        "title": "An Intention-driven Lane Change Framework Considering Heterogeneous Dynamic Cooperation in Mixed-traffic Environment",
        "author": [
            "Xiaoyun Qiu",
            "Haichao Liu",
            "Yue Pan",
            "Jun Ma",
            "Xinhu Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22550",
        "abstract": "In mixed-traffic environments, where autonomous vehicles (AVs) interact with diverse human-driven vehicles (HVs), unpredictable intentions and heterogeneous behaviors make safe and efficient lane change maneuvers highly challenging. Existing methods often oversimplify these interactions by assuming uniform patterns. We propose an intention-driven lane change framework that integrates driving-style recognition, cooperation-aware decision-making, and coordinated motion planning. A deep learning classifier trained on the NGSIM dataset identifies human driving styles in real time. A cooperation score with intrinsic and interactive components estimates surrounding drivers' intentions and quantifies their willingness to cooperate with the ego vehicle. Decision-making combines behavior cloning with inverse reinforcement learning to determine whether a lane change should be initiated. For trajectory generation, model predictive control is integrated with IRL-based intention inference to produce collision-free and socially compliant maneuvers. Experiments show that the proposed model achieves 94.2\\% accuracy and 94.3\\% F1-score, outperforming rule-based and learning-based baselines by 4-15\\% in lane change recognition. These results highlight the benefit of modeling inter-driver heterogeneity and demonstrate the potential of the framework to advance context-aware and human-like autonomous driving in complex traffic environments.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "350",
        "title": "StepORLM: A Self-Evolving Framework With Generative Process Supervision For Operations Research Language Models",
        "author": [
            "Chenyu Zhou",
            "Tianyi Xu",
            "Jianghao Lin",
            "Dongdong Ge"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22558",
        "abstract": "Large Language Models (LLMs) have shown promising capabilities for solving Operations Research (OR) problems. While reinforcement learning serves as a powerful paradigm for LLM training on OR problems, existing works generally face two key limitations. First, outcome reward suffers from the credit assignment problem, where correct final answers can reinforce flawed reasoning. Second, conventional discriminative process supervision is myopic, failing to evaluate the interdependent steps of OR modeling holistically. To this end, we introduce StepORLM, a novel self-evolving framework with generative process supervision. At its core, StepORLM features a co-evolutionary loop where a policy model and a generative process reward model (GenPRM) iteratively improve on each other. This loop is driven by a dual-feedback mechanism: definitive, outcome-based verification from an external solver, and nuanced, holistic process evaluation from the GenPRM. The combined signal is used to align the policy via Weighted Direct Preference Optimization (W-DPO) and simultaneously refine the GenPRM. Our resulting 8B-parameter StepORLM establishes a new state-of-the-art across six benchmarks, significantly outperforming vastly larger generalist models, agentic methods, and specialized baselines. Moreover, the co-evolved GenPRM is able to act as a powerful and universally applicable process verifier, substantially boosting the inference scaling performance of both our own model and other existing LLMs.",
        "tags": [
            "DPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "351",
        "title": "Activation Function Design Sustains Plasticity in Continual Learning",
        "author": [
            "Lute Lillo",
            "Nick Cheney"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22562",
        "abstract": "In independent, identically distributed (i.i.d.) training regimes, activation functions have been benchmarked extensively, and their differences often shrink once model size and optimization are tuned. In continual learning, however, the picture is different: beyond catastrophic forgetting, models can progressively lose the ability to adapt (referred to as loss of plasticity) and the role of the non-linearity in this failure mode remains underexplored. We show that activation choice is a primary, architecture-agnostic lever for mitigating plasticity loss. Building on a property-level analysis of negative-branch shape and saturation behavior, we introduce two drop-in nonlinearities (Smooth-Leaky and Randomized Smooth-Leaky) and evaluate them in two complementary settings: (i) supervised class-incremental benchmarks and (ii) reinforcement learning with non-stationary MuJoCo environments designed to induce controlled distribution and dynamics shifts. We also provide a simple stress protocol and diagnostics that link the shape of the activation to the adaptation under change. The takeaway is straightforward: thoughtful activation design offers a lightweight, domain-general way to sustain plasticity in continual learning without extra capacity or task-specific tuning.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "352",
        "title": "Retrieval-Augmented Guardrails for AI-Drafted Patient-Portal Messages: Error Taxonomy Construction and Large-Scale Evaluation",
        "author": [
            "Wenyuan Chen",
            "Fateme Nateghi Haredasht",
            "Kameron C. Black",
            "Francois Grolleau",
            "Emily Alsentzer",
            "Jonathan H. Chen",
            "Stephen P. Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22565",
        "abstract": "Asynchronous patient-clinician messaging via EHR portals is a growing source of clinician workload, prompting interest in large language models (LLMs) to assist with draft responses. However, LLM outputs may contain clinical inaccuracies, omissions, or tone mismatches, making robust evaluation essential. Our contributions are threefold: (1) we introduce a clinically grounded error ontology comprising 5 domains and 59 granular error codes, developed through inductive coding and expert adjudication; (2) we develop a retrieval-augmented evaluation pipeline (RAEC) that leverages semantically similar historical message-response pairs to improve judgment quality; and (3) we provide a two-stage prompting architecture using DSPy to enable scalable, interpretable, and hierarchical error detection. Our approach assesses the quality of drafts both in isolation and with reference to similar past message-response pairs retrieved from institutional archives. Using a two-stage DSPy pipeline, we compared baseline and reference-enhanced evaluations on over 1,500 patient messages. Retrieval context improved error identification in domains such as clinical completeness and workflow appropriateness. Human validation on 100 messages demonstrated superior agreement (concordance = 50% vs. 33%) and performance (F1 = 0.500 vs. 0.256) of context-enhanced labels vs. baseline, supporting the use of our RAEC pipeline as AI guardrails for patient messaging.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "353",
        "title": "From Parameters to Behavior: Unsupervised Compression of the Policy Space",
        "author": [
            "Davide Tenedini",
            "Riccardo Zamboni",
            "Mirco Mutti",
            "Marcello Restelli"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22566",
        "abstract": "Despite its recent successes, Deep Reinforcement Learning (DRL) is notoriously sample-inefficient. We argue that this inefficiency stems from the standard practice of optimizing policies directly in the high-dimensional and highly redundant parameter space $\\Theta$. This challenge is greatly compounded in multi-task settings. In this work, we develop a novel, unsupervised approach that compresses the policy parameter space $\\Theta$ into a low-dimensional latent space $\\mathcal{Z}$. We train a generative model $g:\\mathcal{Z}\\to\\Theta$ by optimizing a behavioral reconstruction loss, which ensures that the latent space is organized by functional similarity rather than proximity in parameterization. We conjecture that the inherent dimensionality of this manifold is a function of the environment's complexity, rather than the size of the policy network. We validate our approach in continuous control domains, showing that the parameterization of standard policy networks can be compressed up to five orders of magnitude while retaining most of its expressivity. As a byproduct, we show that the learned manifold enables task-specific adaptation via Policy Gradient operating in the latent space $\\mathcal{Z}$.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "354",
        "title": "UniMIC: Token-Based Multimodal Interactive Coding for Human-AI Collaboration",
        "author": [
            "Qi Mao",
            "Tinghan Yang",
            "Jiahao Li",
            "Bin Li",
            "Libiao Jin",
            "Yan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22570",
        "abstract": "The rapid progress of Large Multimodal Models (LMMs) and cloud-based AI agents is transforming human-AI collaboration into bidirectional, multimodal interaction. However, existing codecs remain optimized for unimodal, one-way communication, resulting in repeated degradation under conventional compress-transmit-reconstruct pipelines. To address this limitation, we propose UniMIC, a Unified token-based Multimodal Interactive Coding framework that bridges edge devices and cloud AI agents. Instead of transmitting raw pixels or plain text, UniMIC employs compact tokenized representations as the communication medium, enabling efficient low-bitrate transmission while maintaining compatibility with LMMs. To further enhance compression, lightweight Transformer-based entropy models with scenario-specific designs-generic, masked, and text-conditioned-effectively minimize inter-token redundancy. Extensive experiments on text-to-image generation, text-guided inpainting, outpainting, and visual question answering show that UniMIC achieves substantial bitrate savings and remains robust even at ultra-low bitrates (<0.05bpp), without compromising downstream task performance. These results establish UniMIC as a practical and forward-looking paradigm for next-generation multimodal interactive communication.",
        "tags": [
            "Inpainting",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "355",
        "title": "Dynamic Experts Search: Enhancing Reasoning in Mixture-of-Experts LLMs at Test Time",
        "author": [
            "Yixuan Han",
            "Fan Ma",
            "Ruijie Quan",
            "Yi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22572",
        "abstract": "Test-Time Scaling (TTS) enhances the reasoning ability of large language models (LLMs) by allocating additional computation during inference. However, existing approaches primarily rely on output-level sampling while overlooking the role of model architecture. In mainstream Mixture-of-Experts (MoE) LLMs, we observe that varying the number of activated experts yields complementary solution sets with stable accuracy, revealing a new and underexplored source of diversity. Motivated by this observation, we propose Dynamic Experts Search (DES), a TTS strategy that elevates expert activation into a controllable dimension of the search space. DES integrates two key components: (1) Dynamic MoE, which enables direct control of expert counts during inference to generate diverse reasoning trajectories without additional cost; and (2) Expert Configuration Inheritance, which preserves consistent expert counts within a reasoning path while varying them across runs, thereby balancing stability and diversity throughout the search. Extensive experiments across MoE architectures, verifiers and reasoning benchmarks (i.e., math, code and knowledge) demonstrate that DES reliably outperforms TTS baselines, enhancing accuracy and stability without additional cost. These results highlight DES as a practical and scalable form of architecture-aware TTS, illustrating how structural flexibility in modern LLMs can advance reasoning.",
        "tags": [
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "356",
        "title": "MINT-RVAE: Multi-Cues Intention Prediction of Human-Robot Interaction using Human Pose and Emotion Information from RGB-only Camera Data",
        "author": [
            "Farida Mohsen",
            "Ali Safa"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22573",
        "abstract": "Efficiently detecting human intent to interact with ubiquitous robots is crucial for effective human-robot interaction (HRI) and collaboration. Over the past decade, deep learning has gained traction in this field, with most existing approaches relying on multimodal inputs, such as RGB combined with depth (RGB-D), to classify time-sequence windows of sensory data as interactive or non-interactive. In contrast, we propose a novel RGB-only pipeline for predicting human interaction intent with frame-level precision, enabling faster robot responses and improved service quality. A key challenge in intent prediction is the class imbalance inherent in real-world HRI datasets, which can hinder the model's training and generalization. To address this, we introduce MINT-RVAE, a synthetic sequence generation method, along with new loss functions and training strategies that enhance generalization on out-of-sample data. Our approach achieves state-of-the-art performance (AUROC: 0.95) outperforming prior works (AUROC: 0.90-0.912), while requiring only RGB input and supporting precise frame onset prediction. Finally, to support future research, we openly release our new dataset with frame-level labeling of human interaction intent.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "357",
        "title": "Machine learning approaches to seismic event classification in the Ostrava region",
        "author": [
            "Marek Pecha",
            "Michael Skotnica",
            "Jana RuÅ¡ajovÃ¡",
            "Bohdan Rieznikov",
            "VÃ­t Wandrol",
            "MarkÃ©ta RÃ¶snerovÃ¡",
            "JaromÃ­r KnejzlÃ­k"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22574",
        "abstract": "The northeastern region of the Czech Republic is among the most seismically active areas in the country. The most frequent seismic events are mining-induced since there used to be strong mining activity in the past. However, natural tectonic events may also occur. In addition, seismic stations often record explosions in quarries in the region. Despite the cessation of mining activities, mine-induced seismic events still occur. Therefore, a rapid differentiation between tectonic and anthropogenic events is still important.\nThe region is currently monitored by the OKC seismic station in Ostrava-KrÃ¡snÃ© Pole built in 1983 which is a part of the Czech Regional Seismic Network. The station has been providing digital continuous waveform data at 100 Hz since 2007. In the years 1992--2002, the region was co-monitored by the Seismic Polygon FrenÅ¡tÃ¡t (SPF) which consisted of five seismic stations using a triggered STA/LTA system.\nIn this study, we apply and compare machine learning methods to the SPF dataset, which contains labeled records of tectonic and mining-induced events. For binary classification, a Long Short-Term Memory recurrent neural network and XGBoost achieved an F1-score of 0.94 -- 0.95, demonstrating the potential of modern machine learning techniques for rapid event characterization.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "358",
        "title": "EPO: Entropy-regularized Policy Optimization for LLM Agents Reinforcement Learning",
        "author": [
            "Xu Wujiang",
            "Wentian Zhao",
            "Zhenting Wang",
            "Li Yu-Jhe",
            "Jin Can",
            "Jin Mingyu",
            "Mei Kai",
            "Wan Kun",
            "Metaxas Dimitris"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22576",
        "abstract": "Training LLM agents in multi-turn environments with sparse rewards, where completing a single task requires 30+ turns of interaction within an episode, presents a fundamental challenge for reinforcement learning. We identify a critical failure mode unique to this setting: the exploration-exploitation cascade failure. This cascade begins with early-stage policy premature convergence, where sparse feedback causes agents to commit to flawed, low-entropy strategies. Subsequently, agents enter late-stage policy collapse, where conventional entropy regularization becomes counterproductive, promoting chaotic exploration that destabilizes training. We propose Entropy-regularized Policy Optimization (EPO), a general framework that breaks this failure cycle through three synergistic mechanisms: (1) adopting entropy regularization in multi-turn settings to enhance exploration, (2) an entropy smoothing regularizer that bounds policy entropy within historical averages to prevent abrupt fluctuations, and (3) adaptive phase-based weighting that balances exploration and exploitation across training. Our analysis justifies that EPO guarantees monotonically decreasing entropy variance while maintaining convergence. EPO achieves up to 152% performance improvement on ScienceWorld and up to 19.8% on ALFWorld. Our work demonstrates that multi-turn sparse-reward settings require fundamentally different entropy control than traditional RL, with broad implications for LLM agent training.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "359",
        "title": "EgoDemoGen: Novel Egocentric Demonstration Generation Enables Viewpoint-Robust Manipulation",
        "author": [
            "Yuan Xu",
            "Jiabing Yang",
            "Xiaofeng Wang",
            "Yixiang Chen",
            "Zheng Zhu",
            "Bowen Fang",
            "Guan Huang",
            "Xinze Chen",
            "Yun Ye",
            "Qiang Zhang",
            "Peiyan Li",
            "Xiangnan Wu",
            "Kai Wang",
            "Bing Zhan",
            "Shuo Lu",
            "Jing Liu",
            "Nianfeng Liu",
            "Yan Huang",
            "Liang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22578",
        "abstract": "Imitation learning based policies perform well in robotic manipulation, but they often degrade under *egocentric viewpoint shifts* when trained from a single egocentric viewpoint. To address this issue, we present **EgoDemoGen**, a framework that generates *paired* novel egocentric demonstrations by retargeting actions in the novel egocentric frame and synthesizing the corresponding egocentric observation videos with proposed generative video repair model **EgoViewTransfer**, which is conditioned by a novel-viewpoint reprojected scene video and a robot-only video rendered from the retargeted joint actions. EgoViewTransfer is finetuned from a pretrained video generation model using self-supervised double reprojection strategy. We evaluate EgoDemoGen on both simulation (RoboTwin2.0) and real-world robot. After training with a mixture of EgoDemoGen-generated novel egocentric demonstrations and original standard egocentric demonstrations, policy success rate improves **absolutely** by **+17.0%** for standard egocentric viewpoint and by **+17.7%** for novel egocentric viewpoints in simulation. On real-world robot, the **absolute** improvements are **+18.3%** and **+25.8%**. Moreover, performance continues to improve as the proportion of EgoDemoGen-generated demonstrations increases, with diminishing returns. These results demonstrate that EgoDemoGen provides a practical route to egocentric viewpoint-robust robotic manipulation.",
        "tags": [
            "Robotics",
            "Video Generation"
        ]
    },
    {
        "id": "360",
        "title": "Fine-Grained Detection of Context-Grounded Hallucinations Using LLMs",
        "author": [
            "Yehonatan Pesiakhovsky",
            "Zorik Gekhman",
            "Yosi Mass",
            "Liat Ein-Dor",
            "Roi Reichart"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22582",
        "abstract": "Context-grounded hallucinations are cases where model outputs contain information not verifiable against the source text. We study the applicability of LLMs for localizing such hallucinations, as a more practical alternative to existing complex evaluation pipelines. In the absence of established benchmarks for meta-evaluation of hallucinations localization, we construct one tailored to LLMs, involving a challenging human annotation of over 1,000 examples. We complement the benchmark with an LLM-based evaluation protocol, verifying its quality in a human evaluation. Since existing representations of hallucinations limit the types of errors that can be expressed, we propose a new representation based on free-form textual descriptions, capturing the full range of possible errors. We conduct a comprehensive study, evaluating four large-scale LLMs, which highlights the benchmark's difficulty, as the best model achieves an F1 score of only 0.67. Through careful analysis, we offer insights into optimal prompting strategies for the task and identify the main factors that make it challenging for LLMs: (1) a tendency to incorrectly flag missing details as inconsistent, despite being instructed to check only facts in the output; and (2) difficulty with outputs containing factually correct information absent from the source - and thus not verifiable - due to alignment with the model's parametric knowledge.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "361",
        "title": "Orochi: Versatile Biomedical Image Processor",
        "author": [
            "Gaole Dai",
            "Chenghao Zhou",
            "Yu Zhou",
            "Rongyu Zhang",
            "Yuan Zhang",
            "Chengkai Hou",
            "Tiejun Huang",
            "Jianxu Chen",
            "Shanghang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22583",
        "abstract": "Deep learning has emerged as a pivotal tool for accelerating research in the life sciences, with the low-level processing of biomedical images (e.g., registration, fusion, restoration, super-resolution) being one of its most critical applications. Platforms such as ImageJ (Fiji) and napari have enabled the development of customized plugins for various models. However, these plugins are typically based on models that are limited to specific tasks and datasets, making them less practical for biologists. To address this challenge, we introduce Orochi, the first application-oriented, efficient, and versatile image processor designed to overcome these limitations. Orochi is pre-trained on patches/volumes extracted from the raw data of over 100 publicly available studies using our Random Multi-scale Sampling strategy. We further propose Task-related Joint-embedding Pre-Training (TJP), which employs biomedical task-related degradation for self-supervision rather than relying on Masked Image Modelling (MIM), which performs poorly in downstream tasks such as registration. To ensure computational efficiency, we leverage Mamba's linear computational complexity and construct Multi-head Hierarchy Mamba. Additionally, we provide a three-tier fine-tuning framework (Full, Normal, and Light) and demonstrate that Orochi achieves comparable or superior performance to current state-of-the-art specialist models, even with lightweight parameter-efficient options. We hope that our study contributes to the development of an all-in-one workflow, thereby relieving biologists from the overwhelming task of selecting among numerous models.",
        "tags": [
            "Mamba",
            "Super Resolution"
        ]
    },
    {
        "id": "362",
        "title": "ArabJobs: A Multinational Corpus of Arabic Job Ads",
        "author": [
            "Mo El-Haj"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22589",
        "abstract": "ArabJobs is a publicly available corpus of Arabic job advertisements collected from Egypt, Jordan, Saudi Arabia, and the United Arab Emirates. Comprising over 8,500 postings and more than 550,000 words, the dataset captures linguistic, regional, and socio-economic variation in the Arab labour market. We present analyses of gender representation and occupational structure, and highlight dialectal variation across ads, which offers opportunities for future research. We also demonstrate applications such as salary estimation and job category normalisation using large language models, alongside benchmark tasks for gender bias detection and profession classification. The findings show the utility of ArabJobs for fairness-aware Arabic NLP and labour market research. The dataset is publicly available on GitHub: https://github.com/drelhaj/ArabJobs.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "363",
        "title": "Transport Based Mean Flows for Generative Modeling",
        "author": [
            "Elaheh Akbari",
            "Ping He",
            "Ahmadreza Moradipari",
            "Yikun Bai",
            "Soheil Kolouri"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22592",
        "abstract": "Flow-matching generative models have emerged as a powerful paradigm for continuous data generation, achieving state-of-the-art results across domains such as images, 3D shapes, and point clouds. Despite their success, these models suffer from slow inference due to the requirement of numerous sequential sampling steps. Recent work has sought to accelerate inference by reducing the number of sampling steps. In particular, Mean Flows offer a one-step generation approach that delivers substantial speedups while retaining strong generative performance. Yet, in many continuous domains, Mean Flows fail to faithfully approximate the behavior of the original multi-step flow-matching process. In this work, we address this limitation by incorporating optimal transport-based sampling strategies into the Mean Flow framework, enabling one-step generators that better preserve the fidelity and diversity of the original multi-step flow process. Experiments on controlled low-dimensional settings and on high-dimensional tasks such as image generation, image-to-image translation, and point cloud generation demonstrate that our approach achieves superior inference accuracy in one-step generative modeling.",
        "tags": [
            "3D",
            "Flow Matching"
        ]
    },
    {
        "id": "364",
        "title": "Learn the Ropes, Then Trust the Wins: Self-imitation with Progressive Exploration for Agentic Reinforcement Learning",
        "author": [
            "Yulei Qin",
            "Xiaoyu Tan",
            "Zhengbao He",
            "Gang Li",
            "Haojia Lin",
            "Zongyi Li",
            "Zihan Xu",
            "Yuchen Shi",
            "Siqi Cai",
            "Renting Rui",
            "Shaofei Cai",
            "Yuzheng Cai",
            "Xuan Zhang",
            "Sheng Ye",
            "Ke Li",
            "Xing Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22601",
        "abstract": "Reinforcement learning (RL) is the dominant paradigm for sharpening strategic tool use capabilities of LLMs on long-horizon, sparsely-rewarded agent tasks, yet it faces a fundamental challenge of exploration-exploitation trade-off. Existing studies stimulate exploration through the lens of policy entropy, but such mechanical entropy maximization is prone to RL training instability due to the multi-turn distribution shifting. In this paper, we target the progressive exploration-exploitation balance under the guidance of the agent own experiences without succumbing to either entropy collapsing or runaway divergence. We propose SPEAR, a curriculum-based self-imitation learning (SIL) recipe for training agentic LLMs. It extends the vanilla SIL framework, where a replay buffer stores self-generated promising trajectories for off-policy update, by gradually steering the policy evolution within a well-balanced range of entropy across stages. Specifically, our approach incorporates a curriculum to manage the exploration process, utilizing intrinsic rewards to foster skill-level exploration and facilitating action-level exploration through SIL. At first, the auxiliary tool call reward plays a critical role in the accumulation of tool-use skills, enabling broad exposure to the unfamiliar distributions of the environment feedback with an upward entropy trend. As training progresses, self-imitation gets strengthened to exploit existing successful patterns from replayed experiences for comparative action-level exploration, accelerating solution iteration without unbounded entropy growth. To further stabilize training, we recalibrate the advantages of experiences in the replay buffer to address the potential policy drift. Reugularizations such as the clipping of tokens with high covariance between probability and advantage are introduced to the trajectory-level entropy control to curb over-confidence.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "365",
        "title": "Quantile Advantage Estimation for Entropy-Safe Reasoning",
        "author": [
            "Junkang Wu",
            "Kexin Huang",
            "Jiancan Wu",
            "An Zhang",
            "Xiang Wang",
            "Xiangnan He"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22611",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) strengthens LLM reasoning, but training often oscillates between {entropy collapse} and {entropy explosion}. We trace both hazards to the mean baseline used in value-free RL (e.g., GRPO and DAPO), which improperly penalizes negative-advantage samples under reward outliers. We propose {Quantile Advantage Estimation} (QAE), replacing the mean with a group-wise K-quantile baseline. QAE induces a response-level, two-regime gate: on hard queries (p <= 1 - K) it reinforces rare successes, while on easy queries (p > 1 - K) it targets remaining failures. Under first-order softmax updates, we prove {two-sided entropy safety}, giving lower and upper bounds on one-step entropy change that curb explosion and prevent collapse. Empirically, this minimal modification stabilizes entropy, sparsifies credit assignment (with tuned K, roughly 80% of responses receive zero advantage), and yields sustained pass@1 gains on Qwen3-8B/14B-Base across AIME 2024/2025 and AMC 2023. These results identify {baseline design} -- rather than token-level heuristics -- as the primary mechanism for scaling RLVR.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "366",
        "title": "Benefits and Pitfalls of Reinforcement Learning for Language Model Planning: A Theoretical Perspective",
        "author": [
            "Siwei Wang",
            "Yifei Shen",
            "Haoran Sun",
            "Shi Feng",
            "Shang-Hua Teng",
            "Li Dong",
            "Yaru Hao",
            "Wei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22613",
        "abstract": "Recent reinforcement learning (RL) methods have substantially enhanced the planning capabilities of Large Language Models (LLMs), yet the theoretical basis for their effectiveness remains elusive. In this work, we investigate RL's benefits and limitations through a tractable graph-based abstraction, focusing on policy gradient (PG) and Q-learning methods. Our theoretical analyses reveal that supervised fine-tuning (SFT) may introduce co-occurrence-based spurious solutions, whereas RL achieves correct planning primarily through exploration, underscoring exploration's role in enabling better generalization. However, we also show that PG suffers from diversity collapse, where output diversity decreases during training and persists even after perfect accuracy is attained. By contrast, Q-learning provides two key advantages: off-policy learning and diversity preservation at convergence. We further demonstrate that careful reward design is necessary to prevent reward hacking in Q-learning. Finally, applying our framework to the real-world planning benchmark Blocksworld, we confirm that these behaviors manifest in practice.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "367",
        "title": "Vision-Language Alignment from Compressed Image Representations using 2D Gaussian Splatting",
        "author": [
            "Yasmine Omri",
            "Connor Ding",
            "Tsachy Weissman",
            "Thierry Tambe"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22615",
        "abstract": "Modern vision language pipelines are driven by RGB vision encoders trained on massive image text corpora. While these pipelines have enabled impressive zero shot capabilities and strong transfer across tasks, they still inherit two structural inefficiencies from the pixel domain: (i) transmitting dense RGB images from edge devices to the cloud is energy intensive and costly, and (ii) patch based tokenization explodes sequence length, stressing attention budgets and context limits. We explore 2D Gaussian Splatting (2DGS) as an alternative visual substrate for alignment: a compact, spatially adaptive representation that parameterizes images by a set of colored anisotropic Gaussians. We develop a scalable 2DGS pipeline with structured initialization, luminance aware pruning, and batched CUDA kernels, achieving over 90x faster fitting and about 97% GPU utilization compared to prior implementations. We further adapt contrastive language image pretraining (CLIP) to 2DGS by reusing a frozen RGB-based transformer backbone with a lightweight splat aware input stem and a perceiver resampler, training only about 7% of the total parameters. On large DataComp subsets, GS encoders yield meaningful zero shot ImageNet-1K performance while compressing inputs 3 to 20x relative to pixels. While accuracy currently trails RGB encoders, our results establish 2DGS as a viable multimodal substrate, pinpoint architectural bottlenecks, and open a path toward representations that are both semantically powerful and transmission efficient for edge cloud learning.",
        "tags": [
            "CLIP",
            "Gaussian Splatting",
            "Transformer"
        ]
    },
    {
        "id": "368",
        "title": "Voting-Bloc Entropy: A New Metric for DAO Decentralization",
        "author": [
            "AndrÃ©s FÃ¡brega",
            "Amy Zhao",
            "Jay Yu",
            "James Austgen",
            "Sarah Allen",
            "Kushal Babel",
            "Mahimna Kelkar",
            "Ari Juels"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22620",
        "abstract": "Decentralized Autonomous Organizations (DAOs) use smart contracts to foster communities working toward common goals. Existing definitions of decentralization, however -- the 'D' in DAO -- fall short of capturing the key properties characteristic of diverse and equitable participation. This work proposes a new framework for measuring DAO decentralization called Voting-Bloc Entropy (VBE, pronounced ''vibe''). VBE is based on the idea that voters with closely aligned interests act as a centralizing force and should be modeled as such. VBE formalizes this notion by measuring the similarity of participants' utility functions across a set of voting rounds. Unlike prior, ad hoc definitions of decentralization, VBE derives from first principles: We introduce a simple (yet powerful) reinforcement learning-based conceptual model for voting, that in turn implies VBE. We first show VBE's utility as a theoretical tool. We prove a number of results about the (de)centralizing effects of vote delegation, proposal bundling, bribery, etc. that are overlooked in previous notions of DAO decentralization. Our results lead to practical suggestions for enhancing DAO decentralization. We also show how VBE can be used empirically by presenting measurement studies and VBE-based governance experiments. We make the tools we developed for these results available to the community in the form of open-source artifacts in order to facilitate future study of DAO decentralization.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "369",
        "title": "LongLive: Real-time Interactive Long Video Generation",
        "author": [
            "Shuai Yang",
            "Wei Huang",
            "Ruihang Chu",
            "Yicheng Xiao",
            "Yuyang Zhao",
            "Xianbang Wang",
            "Muyang Li",
            "Enze Xie",
            "Yingcong Chen",
            "Yao Lu",
            "Song Han",
            "Yukang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22622",
        "abstract": "We present LongLive, a frame-level autoregressive (AR) framework for real-time and interactive long video generation. Long video generation presents challenges in both efficiency and quality. Diffusion and Diffusion-Forcing models can produce high-quality videos but suffer from low efficiency due to bidirectional attention. Causal attention AR models support KV caching for faster inference, but often degrade in quality on long videos due to memory challenges during long-video training. In addition, beyond static prompt-based generation, interactive capabilities, such as streaming prompt inputs, are critical for dynamic content creation, enabling users to guide narratives in real time. This interactive requirement significantly increases complexity, especially in ensuring visual consistency and semantic coherence during prompt transitions. To address these challenges, LongLive adopts a causal, frame-level AR design that integrates a KV-recache mechanism that refreshes cached states with new prompts for smooth, adherent switches; streaming long tuning to enable long video training and to align training and inference (train-long-test-long); and short window attention paired with a frame-level attention sink, shorten as frame sink, preserving long-range consistency while enabling faster generation. With these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model to minute-long generation in just 32 GPU-days. At inference, LongLive sustains 20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both short and long videos. LongLive supports up to 240-second videos on a single H100 GPU. LongLive further supports INT8-quantized inference with only marginal quality loss.",
        "tags": [
            "CLIP",
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "370",
        "title": "A Theoretical Analysis of Discrete Flow Matching Generative Models",
        "author": [
            "Maojiang Su",
            "Mingcheng Lu",
            "Jerry Yao-Chieh Hu",
            "Shang Wu",
            "Zhao Song",
            "Alex Reneau",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22623",
        "abstract": "We provide a theoretical analysis for end-to-end training Discrete Flow Matching (DFM) generative models. DFM is a promising discrete generative modeling framework that learns the underlying generative dynamics by training a neural network to approximate the transformative velocity field. Our analysis establishes a clear chain of guarantees by decomposing the final distribution estimation error. We first prove that the total variation distance between the generated and target distributions is controlled by the risk of the learned velocity field. We then bound this risk by analyzing its two primary sources: (i) Approximation Error, where we quantify the capacity of the Transformer architecture to represent the true velocity, and (ii) Estimation Error, where we derive statistical convergence rates that bound the error from training on a finite dataset. By composing these results, we provide the first formal proof that the distribution generated by a trained DFM model provably converges to the true data distribution as the training set size increases.",
        "tags": [
            "Flow Matching",
            "Transformer"
        ]
    },
    {
        "id": "371",
        "title": "SPARK: Synergistic Policy And Reward Co-Evolving Framework",
        "author": [
            "Ziyu Liu",
            "Yuhang Zang",
            "Shengyuan Ding",
            "Yuhang Cao",
            "Xiaoyi Dong",
            "Haodong Duan",
            "Dahua Lin",
            "Jiaqi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22624",
        "abstract": "Recent Large Language Models (LLMs) and Large Vision-Language Models (LVLMs) increasingly use Reinforcement Learning (RL) for post-pretraining, such as RL with Verifiable Rewards (RLVR) for objective tasks and RL from Human Feedback (RLHF) for subjective tasks. However, RLHF incurs high costs and potential reward-policy mismatch due to reliance on human preferences, while RLVR still wastes supervision by discarding rollouts and correctness signals after each update. To address these challenges, we introduce the Synergistic Policy And Reward Co-Evolving Framework (SPARK), an efficient, on-policy, and stable method that builds on RLVR. Instead of discarding rollouts and correctness data, SPARK recycles this valuable information to simultaneously train the model itself as a generative reward model. This auxiliary training uses a mix of objectives, such as pointwise reward score, pairwise comparison, and evaluation conditioned on further-reflection responses, to teach the model to evaluate and improve its own responses. Our process eliminates the need for a separate reward model and costly human preference data. SPARK creates a positive co-evolving feedback loop: improved reward accuracy yields better policy gradients, which in turn produce higher-quality rollouts that further refine the reward model. Our unified framework supports test-time scaling via self-reflection without external reward models and their associated costs. We show that SPARK achieves significant performance gains on multiple LLM and LVLM models and multiple reasoning, reward models, and general benchmarks. For example, SPARK-VL-7B achieves an average 9.7% gain on 7 reasoning benchmarks, 12.1% on 2 reward benchmarks, and 1.5% on 8 general benchmarks over the baselines, demonstrating robustness and broad generalization.",
        "tags": [
            "LLM",
            "RL",
            "RLHF",
            "VLM"
        ]
    },
    {
        "id": "372",
        "title": "CCNeXt: An Effective Self-Supervised Stereo Depth Estimation Approach",
        "author": [
            "Alexandre Lopes",
            "Roberto Souza",
            "Helio Pedrini"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22627",
        "abstract": "Depth Estimation plays a crucial role in recent applications in robotics, autonomous vehicles, and augmented reality. These scenarios commonly operate under constraints imposed by computational power. Stereo image pairs offer an effective solution for depth estimation since it only needs to estimate the disparity of pixels in image pairs to determine the depth in a known rectified system. Due to the difficulty in acquiring reliable ground-truth depth data across diverse scenarios, self-supervised techniques emerge as a solution, particularly when large unlabeled datasets are available. We propose a novel self-supervised convolutional approach that outperforms existing state-of-the-art Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) while balancing computational cost. The proposed CCNeXt architecture employs a modern CNN feature extractor with a novel windowed epipolar cross-attention module in the encoder, complemented by a comprehensive redesign of the depth estimation decoder. Our experiments demonstrate that CCNeXt achieves competitive metrics on the KITTI Eigen Split test data while being 10.18$\\times$ faster than the current best model and achieves state-of-the-art results in all metrics in the KITTI Eigen Split Improved Ground Truth and Driving Stereo datasets when compared to recently proposed techniques. To ensure complete reproducibility, our project is accessible at \\href{https://github.com/alelopes/CCNext}{\\texttt{https://github.com/alelopes/CCNext}}.",
        "tags": [
            "Depth Estimation",
            "Robotics"
        ]
    },
    {
        "id": "373",
        "title": "UML-CoT: Structured Reasoning and Planning with Unified Modeling Language for Robotic Room Cleaning",
        "author": [
            "Hongyu Chen",
            "Guangrun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22628",
        "abstract": "Chain-of-Thought (CoT) prompting improves reasoning in large language models (LLMs), but its reliance on unstructured text limits interpretability and executability in embodied tasks. Prior work has explored structured CoTs using scene or logic graphs, yet these remain fundamentally limited: they model only low-order relations, lack constructs like inheritance or behavioral abstraction, and provide no standardized semantics for sequential or conditional planning. We propose UML-CoT, a structured reasoning and planning framework that leverages Unified Modeling Language (UML) to generate symbolic CoTs and executable action plans. UML class diagrams capture compositional object semantics, while activity diagrams model procedural control flow. Our three-stage training pipeline combines supervised fine-tuning with Group Relative Policy Optimization (GRPO), including reward learning from answer-only data. We evaluate UML-CoT on MRoom-30k, a new benchmark of cluttered room-cleaning scenarios. UML-CoT outperforms unstructured CoTs in interpretability, planning coherence, and execution success, highlighting UML as a more expressive and actionable structured reasoning formalism.",
        "tags": [
            "CoT",
            "GRPO",
            "LLM"
        ]
    },
    {
        "id": "374",
        "title": "StateX: Enhancing RNN Recall via Post-training State Expansion",
        "author": [
            "Xingyu Shen",
            "Yingfa Chen",
            "Zhen Leng Thai",
            "Xu Han",
            "Zhiyuan Liu",
            "Maosong Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22630",
        "abstract": "While Transformer-based models have demonstrated remarkable language modeling performance, their high complexities result in high costs when processing long contexts. In contrast, recurrent neural networks (RNNs) such as linear attention and state space models have gained popularity due to their constant per-token complexities. However, these recurrent models struggle with tasks that require accurate recall of contextual information from long contexts, because all contextual information is compressed into a constant-size recurrent state. Previous works have shown that recall ability is positively correlated with the recurrent state size, yet directly training RNNs with larger recurrent states results in high training costs. In this paper, we introduce StateX, a training pipeline for efficiently expanding the states of pre-trained RNNs through post-training. For two popular classes of RNNs, linear attention and state space models, we design post-training architectural modifications to scale up the state size with no or negligible increase in model parameters. Experiments on models up to 1.3B parameters demonstrate that StateX efficiently enhances the recall and in-context learning ability of RNNs without incurring high post-training costs or compromising other capabilities.",
        "tags": [
            "RNN",
            "SSMs",
            "Transformer"
        ]
    },
    {
        "id": "375",
        "title": "Training-Free Synthetic Data Generation with Dual IP-Adapter Guidance",
        "author": [
            "Luc Boudier",
            "Loris Manganelli",
            "Eleftherios Tsonis",
            "Nicolas Dufour",
            "Vicky Kalogeiton"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22635",
        "abstract": "Few-shot image classification remains challenging due to the limited availability of labeled examples. Recent approaches have explored generating synthetic training data using text-to-image diffusion models, but often require extensive model fine-tuning or external information sources. We present a novel training-free approach, called DIPSY, that leverages IP-Adapter for image-to-image translation to generate highly discriminative synthetic images using only the available few-shot examples. DIPSY introduces three key innovations: (1) an extended classifier-free guidance scheme that enables independent control over positive and negative image conditioning; (2) a class similarity-based sampling strategy that identifies effective contrastive examples; and (3) a simple yet effective pipeline that requires no model fine-tuning or external captioning and filtering. Experiments across ten benchmark datasets demonstrate that our approach achieves state-of-the-art or comparable performance, while eliminating the need for generative model adaptation or reliance on external tools for caption generation and image filtering. Our results highlight the effectiveness of leveraging dual image prompting with positive-negative guidance for generating class-discriminative features, particularly for fine-grained classification tasks.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "376",
        "title": "Scale-Wise VAR is Secretly Discrete Diffusion",
        "author": [
            "Amandeep Kumar",
            "Nithin Gopalakrishnan Nair",
            "Vishal M. Patel"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22636",
        "abstract": "Autoregressive (AR) transformers have emerged as a powerful paradigm for visual generation, largely due to their scalability, computational efficiency and unified architecture with language and vision. Among them, next scale prediction Visual Autoregressive Generation (VAR) has recently demonstrated remarkable performance, even surpassing diffusion-based models. In this work, we revisit VAR and uncover a theoretical insight: when equipped with a Markovian attention mask, VAR is mathematically equivalent to a discrete diffusion. We term this reinterpretation as Scalable Visual Refinement with Discrete Diffusion (SRDD), establishing a principled bridge between AR transformers and diffusion models. Leveraging this new perspective, we show how one can directly import the advantages of diffusion such as iterative refinement and reduce architectural inefficiencies into VAR, yielding faster convergence, lower inference cost, and improved zero-shot reconstruction. Across multiple datasets, we show that the diffusion based perspective of VAR leads to consistent gains in efficiency and generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "377",
        "title": "Variational Reasoning for Language Models",
        "author": [
            "Xiangxin Zhou",
            "Zichen Liu",
            "Haonan Wang",
            "Chao Du",
            "Min Lin",
            "Chongxuan Li",
            "Liang Wang",
            "Tianyu Pang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22637",
        "abstract": "We introduce a variational reasoning framework for language models that treats thinking traces as latent variables and optimizes them through variational inference. Starting from the evidence lower bound (ELBO), we extend it to a multi-trace objective for tighter bounds and propose a forward-KL formulation that stabilizes the training of the variational posterior. We further show that rejection sampling finetuning and binary-reward RL, including GRPO, can be interpreted as local forward-KL objectives, where an implicit weighting by model accuracy naturally arises from the derivation and reveals a previously unnoticed bias toward easier questions. We empirically validate our method on the Qwen 2.5 and Qwen 3 model families across a wide range of reasoning tasks. Overall, our work provides a principled probabilistic perspective that unifies variational inference with RL-style methods and yields stable objectives for improving the reasoning ability of language models. Our code is available at https://github.com/sail-sg/variational-reasoning.",
        "tags": [
            "GRPO",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": "378",
        "title": "Language Models Can Learn from Verbal Feedback Without Scalar Rewards",
        "author": [
            "Renjie Luo",
            "Zichen Liu",
            "Xiangyan Liu",
            "Chao Du",
            "Min Lin",
            "Wenhu Chen",
            "Wei Lu",
            "Tianyu Pang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22638",
        "abstract": "LLMs are often trained with RL from human or AI feedback, yet such methods typically compress nuanced feedback into scalar rewards, discarding much of their richness and inducing scale imbalance. We propose treating verbal feedback as a conditioning signal. Inspired by language priors in text-to-image generation, which enable novel outputs from unseen prompts, we introduce the feedback-conditional policy (FCP). FCP learns directly from response-feedback pairs, approximating the feedback-conditional posterior through maximum likelihood training on offline data. We further develop an online bootstrapping stage where the policy generates under positive conditions and receives fresh feedback to refine itself. This reframes feedback-driven learning as conditional generation rather than reward optimization, offering a more expressive way for LLMs to directly learn from verbal feedback. Our code is available at https://github.com/sail-sg/feedback-conditional-policy.",
        "tags": [
            "LLM",
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": "379",
        "title": "Death of the Novel(ty): Beyond n-Gram Novelty as a Metric for Textual Creativity",
        "author": [
            "Arkadiy Saakyan",
            "Najoung Kim",
            "Smaranda Muresan",
            "Tuhin Chakrabarty"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22641",
        "abstract": "N-gram novelty is widely used to evaluate language models' ability to generate text outside of their training data. More recently, it has also been adopted as a metric for measuring textual creativity. However, theoretical work on creativity suggests that this approach may be inadequate, as it does not account for creativity's dual nature: novelty (how original the text is) and appropriateness (how sensical and pragmatic it is). We investigate the relationship between this notion of creativity and n-gram novelty through 7542 expert writer annotations (n=26) of novelty, pragmaticality, and sensicality via close reading of human and AI-generated text. We find that while n-gram novelty is positively associated with expert writer-judged creativity, ~91% of top-quartile expressions by n-gram novelty are not judged as creative, cautioning against relying on n-gram novelty alone. Furthermore, unlike human-written text, higher n-gram novelty in open-source LLMs correlates with lower pragmaticality. In an exploratory study with frontier close-source models, we additionally confirm that they are less likely to produce creative expressions than humans. Using our dataset, we test whether zero-shot, few-shot, and finetuned models are able to identify creative expressions (a positive aspect of writing) and non-pragmatic ones (a negative aspect). Overall, frontier LLMs exhibit performance much higher than random but leave room for improvement, especially struggling to identify non-pragmatic expressions. We further find that LLM-as-a-Judge novelty scores from the best-performing model were predictive of expert writer preferences.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "380",
        "title": "WoW: Towards a World omniscient World model Through Embodied Interaction",
        "author": [
            "Xiaowei Chi",
            "Peidong Jia",
            "Chun-Kai Fan",
            "Xiaozhu Ju",
            "Weishi Mi",
            "Kevin Zhang",
            "Zhiyuan Qin",
            "Wanxin Tian",
            "Kuangzhi Ge",
            "Hao Li",
            "Zezhong Qian",
            "Anthony Chen",
            "Qiang Zhou",
            "Yueru Jia",
            "Jiaming Liu",
            "Yong Dai",
            "Qingpo Wuwu",
            "Chengyu Bai",
            "Yu-Kai Wang",
            "Ying Li",
            "Lizhang Chen",
            "Yong Bao",
            "Zhiyuan Jiang",
            "Jiacheng Zhu",
            "Kai Tang",
            "Ruichuan An",
            "Yulin Luo",
            "Qiuxuan Feng",
            "Siyuan Zhou",
            "Chi-min Chan",
            "Chengkai Hou",
            "Wei Xue",
            "Sirui Han",
            "Yike Guo",
            "Shanghang Zhang",
            "Jian Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22642",
        "abstract": "Humans develop an understanding of intuitive physics through active interaction with the world. This approach is in stark contrast to current video models, such as Sora, which rely on passive observation and therefore struggle with grasping physical causality. This observation leads to our central hypothesis: authentic physical intuition of the world model must be grounded in extensive, causally rich interactions with the real world. To test this hypothesis, we present WoW, a 14-billion-parameter generative world model trained on 2 million robot interaction trajectories. Our findings reveal that the model's understanding of physics is a probabilistic distribution of plausible outcomes, leading to stochastic instabilities and physical hallucinations. Furthermore, we demonstrate that this emergent capability can be actively constrained toward physical realism by SOPHIA, where vision-language model agents evaluate the DiT-generated output and guide its refinement by iteratively evolving the language instructions. In addition, a co-trained Inverse Dynamics Model translates these refined plans into executable robotic actions, thus closing the imagination-to-action loop. We establish WoWBench, a new benchmark focused on physical consistency and causal reasoning in video, where WoW achieves state-of-the-art performance in both human and autonomous evaluation, demonstrating strong ability in physical causality, collision dynamics, and object permanence. Our work provides systematic evidence that large-scale, real-world interaction is a cornerstone for developing physical intuition in AI. Models, data, and benchmarks will be open-sourced.",
        "tags": [
            "DiT",
            "Robotics",
            "Sora"
        ]
    },
    {
        "id": "381",
        "title": "WebGen-Agent: Enhancing Interactive Website Generation with Multi-Level Feedback and Step-Level Reinforcement Learning",
        "author": [
            "Zimu Lu",
            "Houxing Ren",
            "Yunqiao Yang",
            "Ke Wang",
            "Zhuofan Zong",
            "Junting Pan",
            "Mingjie Zhan",
            "Hongsheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22644",
        "abstract": "Agent systems powered by large language models (LLMs) have demonstrated impressive performance on repository-level code-generation tasks. However, for tasks such as website codebase generation, which depend heavily on visual effects and user-interaction feedback, current code agents rely only on simple code execution for feedback and verification. This approach fails to capture the actual quality of the generated code. In this paper, we propose WebGen-Agent, a novel website-generation agent that leverages comprehensive and multi-level visual feedback to iteratively generate and refine the website codebase. Detailed and expressive text descriptions and suggestions regarding the screenshots and GUI-agent testing of the websites are generated by a visual language model (VLM), together with scores that quantify their quality. The screenshot and GUI-agent scores are further integrated with a backtracking and select-best mechanism, enhancing the performance of the agent. Utilizing the accurate visual scores inherent in the WebGen-Agent workflow, we further introduce \\textit{Step-GRPO with Screenshot and GUI-agent Feedback} to improve the ability of LLMs to act as the reasoning engine of WebGen-Agent. By using the screenshot and GUI-agent scores at each step as the reward in Step-GRPO, we provide a dense and reliable process supervision signal, which effectively improves the model's website-generation ability. On the WebGen-Bench dataset, WebGen-Agent increases the accuracy of Claude-3.5-Sonnet from 26.4% to 51.9% and its appearance score from 3.0 to 3.9, outperforming the previous state-of-the-art agent system. Additionally, our Step-GRPO training approach increases the accuracy of Qwen2.5-Coder-7B-Instruct from 38.9% to 45.4% and raises the appearance score from 3.4 to 3.7.",
        "tags": [
            "GRPO",
            "LLM",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "382",
        "title": "Hierarchical Representation Matching for CLIP-based Class-Incremental Learning",
        "author": [
            "Zhen-Hao Wen",
            "Yan Wang",
            "Ji Feng",
            "Han-Jia Ye",
            "De-Chuan Zhan",
            "Da-Wei Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22645",
        "abstract": "Class-Incremental Learning (CIL) aims to endow models with the ability to continuously adapt to evolving data streams. Recent advances in pre-trained vision-language models (e.g., CLIP) provide a powerful foundation for this task. However, existing approaches often rely on simplistic templates, such as \"a photo of a [CLASS]\", which overlook the hierarchical nature of visual concepts. For example, recognizing \"cat\" versus \"car\" depends on coarse-grained cues, while distinguishing \"cat\" from \"lion\" requires fine-grained details. Similarly, the current feature mapping in CLIP relies solely on the representation from the last layer, neglecting the hierarchical information contained in earlier layers. In this work, we introduce HiErarchical Representation MAtchiNg (HERMAN) for CLIP-based CIL. Our approach leverages LLMs to recursively generate discriminative textual descriptors, thereby augmenting the semantic space with explicit hierarchical cues. These descriptors are matched to different levels of the semantic hierarchy and adaptively routed based on task-specific requirements, enabling precise discrimination while alleviating catastrophic forgetting in incremental tasks. Extensive experiments on multiple benchmarks demonstrate that our method consistently achieves state-of-the-art performance.",
        "tags": [
            "CLIP",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "383",
        "title": "Learning Human-Perceived Fakeness in AI-Generated Videos via Multimodal LLMs",
        "author": [
            "Xingyu Fu",
            "Siyi Liu",
            "Yinuo Xu",
            "Pan Lu",
            "Guangqiuse Hu",
            "Tianbo Yang",
            "Taran Anantasagar",
            "Christopher Shen",
            "Yikai Mao",
            "Yuanzhe Liu",
            "Keyush Shah",
            "Chung Un Lee",
            "Yejin Choi",
            "James Zou",
            "Dan Roth",
            "Chris Callison-Burch"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22646",
        "abstract": "Can humans identify AI-generated (fake) videos and provide grounded reasons? While video generation models have advanced rapidly, a critical dimension -- whether humans can detect deepfake traces within a generated video, i.e., spatiotemporal grounded visual artifacts that reveal a video as machine generated -- has been largely overlooked. We introduce DeeptraceReward, the first fine-grained, spatially- and temporally- aware benchmark that annotates human-perceived fake traces for video generation reward. The dataset comprises 4.3K detailed annotations across 3.3K high-quality generated videos. Each annotation provides a natural-language explanation, pinpoints a bounding-box region containing the perceived trace, and marks precise onset and offset timestamps. We consolidate these annotations into 9 major categories of deepfake traces that lead humans to identify a video as AI-generated, and train multimodal language models (LMs) as reward models to mimic human judgments and localizations. On DeeptraceReward, our 7B reward model outperforms GPT-5 by 34.7% on average across fake clue identification, grounding, and explanation. Interestingly, we observe a consistent difficulty gradient: binary fake v.s. real classification is substantially easier than fine-grained deepfake trace detection; within the latter, performance degrades from natural language explanations (easiest), to spatial grounding, to temporal labeling (hardest). By foregrounding human-perceived deepfake traces, DeeptraceReward provides a rigorous testbed and training signal for socially aware and trustworthy video generation.",
        "tags": [
            "Detection",
            "GPT",
            "LLM",
            "Video Generation"
        ]
    },
    {
        "id": "384",
        "title": "CapRL: Stimulating Dense Image Caption Capabilities via Reinforcement Learning",
        "author": [
            "Long Xing",
            "Xiaoyi Dong",
            "Yuhang Zang",
            "Yuhang Cao",
            "Jianze Liang",
            "Qidong Huang",
            "Jiaqi Wang",
            "Feng Wu",
            "Dahua Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22647",
        "abstract": "Image captioning is a fundamental task that bridges the visual and linguistic domains, playing a critical role in pre-training Large Vision-Language Models (LVLMs). Current state-of-the-art captioning models are typically trained with Supervised Fine-Tuning (SFT), a paradigm that relies on expensive, non-scalable data annotated by humans or proprietary models. This approach often leads to models that memorize specific ground-truth answers, limiting their generality and ability to generate diverse, creative descriptions. To overcome the limitation of SFT, we propose applying the Reinforcement Learning with Verifiable Rewards (RLVR) paradigm to the open-ended task of image captioning. A primary challenge, however, is designing an objective reward function for the inherently subjective nature of what constitutes a \"good\" caption. We introduce Captioning Reinforcement Learning (CapRL), a novel training framework that redefines caption quality through its utility: a high-quality caption should enable a non-visual language model to accurately answer questions about the corresponding image. CapRL employs a decoupled two-stage pipeline where an LVLM generates a caption, and the objective reward is derived from the accuracy of a separate, vision-free LLM answering Multiple-Choice Questions based solely on that caption. As the first study to apply RLVR to the subjective image captioning task, we demonstrate that CapRL significantly enhances multiple settings. Pretraining on the CapRL-5M caption dataset annotated by CapRL-3B results in substantial gains across 12 benchmarks. Moreover, within the Prism Framework for caption quality evaluation, CapRL achieves performance comparable to Qwen2.5-VL-72B, while exceeding the baseline by an average margin of 8.4%. Code is available here: https://github.com/InternLM/CapRL.",
        "tags": [
            "LLM",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "385",
        "title": "RefAM: Attention Magnets for Zero-Shot Referral Segmentation",
        "author": [
            "Anna Kukleva",
            "Enis Simsar",
            "Alessio Tonioni",
            "Muhammad Ferjad Naeem",
            "Federico Tombari",
            "Jan Eric Lenssen",
            "Bernt Schiele"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22650",
        "abstract": "Most existing approaches to referring segmentation achieve strong performance only through fine-tuning or by composing multiple pre-trained models, often at the cost of additional training and architectural modifications. Meanwhile, large-scale generative diffusion models encode rich semantic information, making them attractive as general-purpose feature extractors. In this work, we introduce a new method that directly exploits features, attention scores, from diffusion transformers for downstream tasks, requiring neither architectural modifications nor additional training. To systematically evaluate these features, we extend benchmarks with vision-language grounding tasks spanning both images and videos. Our key insight is that stop words act as attention magnets: they accumulate surplus attention and can be filtered to reduce noise. Moreover, we identify global attention sinks (GAS) emerging in deeper layers and show that they can be safely suppressed or redirected onto auxiliary tokens, leading to sharper and more accurate grounding maps. We further propose an attention redistribution strategy, where appended stop words partition background activations into smaller clusters, yielding sharper and more localized heatmaps. Building on these findings, we develop RefAM, a simple training-free grounding framework that combines cross-attention maps, GAS handling, and redistribution. Across zero-shot referring image and video segmentation benchmarks, our approach consistently outperforms prior methods, establishing a new state of the art without fine-tuning or additional components.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "386",
        "title": "VoiceAssistant-Eval: Benchmarking AI Assistants across Listening, Speaking, and Viewing",
        "author": [
            "Ke Wang",
            "Houxing Ren",
            "Zimu Lu",
            "Mingjie Zhan",
            "Hongsheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22651",
        "abstract": "The growing capabilities of large language models and multimodal systems have spurred interest in voice-first AI assistants, yet existing benchmarks are inadequate for evaluating the full range of these systems' capabilities. We introduce VoiceAssistant-Eval, a comprehensive benchmark designed to assess AI assistants across listening, speaking, and viewing. VoiceAssistant-Eval comprises 10,497 curated examples spanning 13 task categories. These tasks include natural sounds, music, and spoken dialogue for listening; multi-turn dialogue, role-play imitation, and various scenarios for speaking; and highly heterogeneous images for viewing. To demonstrate its utility, we evaluate 21 open-source models and GPT-4o-Audio, measuring the quality of the response content and speech, as well as their consistency. The results reveal three key findings: (1) proprietary models do not universally outperform open-source models; (2) most models excel at speaking tasks but lag in audio understanding; and (3) well-designed smaller models can rival much larger ones. Notably, the mid-sized Step-Audio-2-mini (7B) achieves more than double the listening accuracy of LLaMA-Omni2-32B-Bilingual. However, challenges remain: multimodal (audio plus visual) input and role-play voice imitation tasks are difficult for current models, and significant gaps persist in robustness and safety alignment. VoiceAssistant-Eval identifies these gaps and establishes a rigorous framework for evaluating and guiding the development of next-generation AI assistants. Code and data will be released at https://mathllm.github.io/VoiceAssistantEval/ .",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "387",
        "title": "Pixel Motion Diffusion is What We Need for Robot Control",
        "author": [
            "E-Ro Nguyen",
            "Yichi Zhang",
            "Kanchana Ranasinghe",
            "Xiang Li",
            "Michael S. Ryoo"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22652",
        "abstract": "We present DAWN (Diffusion is All We Need for robot control), a unified diffusion-based framework for language-conditioned robotic manipulation that bridges high-level motion intent and low-level robot action via structured pixel motion representation. In DAWN, both the high-level and low-level controllers are modeled as diffusion processes, yielding a fully trainable, end-to-end system with interpretable intermediate motion abstractions. DAWN achieves state-of-the-art results on the challenging CALVIN benchmark, demonstrating strong multi-task performance, and further validates its effectiveness on MetaWorld. Despite the substantial domain gap between simulation and reality and limited real-world data, we demonstrate reliable real-world transfer with only minimal finetuning, illustrating the practical viability of diffusion-based motion abstractions for robotic control. Our results show the effectiveness of combining diffusion modeling with motion-centric representations as a strong baseline for scalable and robust robot learning. Project page: https://nero1342.github.io/DAWN/",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "388",
        "title": "See, Point, Fly: A Learning-Free VLM Framework for Universal Unmanned Aerial Navigation",
        "author": [
            "Chih Yao Hu",
            "Yang-Sen Lin",
            "Yuna Lee",
            "Chih-Hai Su",
            "Jie-Ying Lee",
            "Shr-Ruei Tsai",
            "Chin-Yang Lin",
            "Kuan-Wen Chen",
            "Tsung-Wei Ke",
            "Yu-Lun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22653",
        "abstract": "We present See, Point, Fly (SPF), a training-free aerial vision-and-language navigation (AVLN) framework built atop vision-language models (VLMs). SPF is capable of navigating to any goal based on any type of free-form instructions in any kind of environment. In contrast to existing VLM-based approaches that treat action prediction as a text generation task, our key insight is to consider action prediction for AVLN as a 2D spatial grounding task. SPF harnesses VLMs to decompose vague language instructions into iterative annotation of 2D waypoints on the input image. Along with the predicted traveling distance, SPF transforms predicted 2D waypoints into 3D displacement vectors as action commands for UAVs. Moreover, SPF also adaptively adjusts the traveling distance to facilitate more efficient navigation. Notably, SPF performs navigation in a closed-loop control manner, enabling UAVs to follow dynamic targets in dynamic environments. SPF sets a new state of the art in DRL simulation benchmark, outperforming the previous best method by an absolute margin of 63%. In extensive real-world evaluations, SPF outperforms strong baselines by a large margin. We also conduct comprehensive ablation studies to highlight the effectiveness of our design choice. Lastly, SPF shows remarkable generalization to different VLMs. Project page: https://spf-web.pages.dev",
        "tags": [
            "3D",
            "VLM"
        ]
    },
    {
        "id": "389",
        "title": "Accurate typhoon intensity forecasts using a non-iterative spatiotemporal transformer model",
        "author": [
            "Hongyu Qu",
            "Hongxiong Xu",
            "Lin Dong",
            "Chunyi Xiang",
            "Gaozhen Nie"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21349",
        "abstract": "Accurate forecasting of tropical cyclone (TC) intensity - particularly during periods of rapid intensification and rapid weakening - remains a challenge for operational meteorology, with high-stakes implications for disaster preparedness and infrastructure resilience. Recent advances in machine learning have yielded notable progress in TC prediction; however, most existing systems provide forecasts that degrade rapidly in extreme regimes and lack long-range consistency. Here we introduce TIFNet, a transformer-based forecasting model that generates non-iterative, 5-day intensity trajectories by integrating high-resolution global forecasts with a historical-evolution fusion mechanism. Trained on reanalysis data and fine-tuned with operational data, TIFNet consistently outperforms operational numerical models across all forecast horizons, delivering robust improvements across weak, strong, and super typhoon categories. In rapid intensity change regimes - long regarded as the most difficult to forecast - TIFNet reduces forecast error by 29-43% relative to current operational baselines. These results represent a substantial advance in artificial-intelligence-based TC intensity forecasting, especially under extreme conditions where traditional models consistently underperform.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "390",
        "title": "Multi-Speaker DOA Estimation in Binaural Hearing Aids using Deep Learning and Speaker Count Fusion",
        "author": [
            "Farnaz Jazaeri",
            "Homayoun Kamkar-Parsi",
            "FranÃ§ois Grondin",
            "Martin Bouchard"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21382",
        "abstract": "For extracting a target speaker voice, direction-of-arrival (DOA) estimation is crucial for binaural hearing aids operating in noisy, multi-speaker environments. Among the solutions developed for this task, a deep learning convolutional recurrent neural network (CRNN) model leveraging spectral phase differences and magnitude ratios between microphone signals is a popular option. In this paper, we explore adding source-count information for multi-sources DOA estimation. The use of dual-task training with joint multi-sources DOA estimation and source counting is first considered. We then consider using the source count as an auxiliary feature in a standalone DOA estimation system, where the number of active sources (0, 1, or 2+) is integrated into the CRNN architecture through early, mid, and late fusion strategies. Experiments using real binaural recordings are performed. Results show that the dual-task training does not improve DOA estimation performance, although it benefits source-count prediction. However, a ground-truth (oracle) source count used as an auxiliary feature significantly enhances standalone DOA estimation performance, with late fusion yielding up to 14% higher average F1-scores over the baseline CRNN. This highlights the potential of using source-count estimation for robust DOA estimation in binaural hearing aids.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "391",
        "title": "Discovering alternative solutions beyond the simplicity bias in recurrent neural networks",
        "author": [
            "William Qian",
            "Cengiz Pehlevan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21504",
        "abstract": "Training recurrent neural networks (RNNs) to perform neuroscience-style tasks has become a popular way to generate hypotheses for how neural circuits in the brain might perform computations. Recent work has demonstrated that task-trained RNNs possess a strong simplicity bias. In particular, this inductive bias often causes RNNs trained on the same task to collapse on effectively the same solution, typically comprised of fixed-point attractors or other low-dimensional dynamical motifs. While such solutions are readily interpretable, this collapse proves counterproductive for the sake of generating a set of genuinely unique hypotheses for how neural computations might be performed. Here we propose Iterative Neural Similarity Deflation (INSD), a simple method to break this inductive bias. By penalizing linear predictivity of neural activity produced by standard task-trained RNNs, we find an alternative class of solutions to classic neuroscience-style RNN tasks. These solutions appear distinct across a battery of analysis techniques, including representational similarity metrics, dynamical systems analysis, and the linear decodability of task-relevant variables. Moreover, these alternative solutions can sometimes achieve superior performance in difficult or out-of-distribution task regimes. Our findings underscore the importance of moving beyond the simplicity bias to uncover richer and more varied models of neural computation.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "392",
        "title": "A comprehensive equivalent circuit model for high overtone bulk acoustic resonators (HBARs)",
        "author": [
            "Vikrant J. Gokhale",
            "Brian P. Downey"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21640",
        "abstract": "This paper presents a new and comprehensive equivalent circuit model for high overtone bulk acoustic resonators (HBARs). HBARs demonstrate several very sharp resonance modes distributed nearly periodically over a very wide frequency range. This spectrum response of HBARs offers unique advantages but poses significant modeling challenges. The proposed circuit incorporates and models the unique physical components of the HBAR: piezoelectric transducer, substrate (a perfectly periodic multimode cavity), piezoelectric coupling, and critically, the imperfectly matched transducer-substrate interface which imparts characteristic aperiodicity to the HBAR mode spectrum. By judicious use of fixed, periodic, or tightly constrained virtual lumped-element branches, and sets of branches, the model retains clear and intuitive links to the physical device, while reducing the complexity needed for fitting dense, broadband datasets. We demonstrate the validity and power of this model by simultaneously fitting measured data for 61 modes of a GaN/NbN/sapphire HBAR over a span of 1 GHz, and extracting modal parameters such as quality factors and coupling coefficients. We show that this new model is compact and yet scalable: by leveraging the inherent internal relationships in an HBAR, the model can be easily expanded to include multiple transducer overtones and envelopes, multiple distinct transducers, and spurious modes. In addition to fitting measured datasets, the new model can also be used to easily analyze various perturbations to the nominal state of the HBAR. We expect the new model to be useful for the design of classical HBAR-based oscillators, filters, and sensors, and for the integration of HBARs into quantum circuits.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "393",
        "title": "SADA: Safe and Adaptive Inference with Multiple Black-Box Predictions",
        "author": [
            "Jiawei Shan",
            "Yiming Dong",
            "Jiwei Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21707",
        "abstract": "Real-world applications often face scarce labeled data due to the high cost and time requirements of gold-standard experiments, whereas unlabeled data are typically abundant. With the growing adoption of machine learning techniques, it has become increasingly feasible to generate multiple predicted labels using a variety of models and algorithms, including deep learning, large language models, and generative AI. In this paper, we propose a novel approach that safely and adaptively aggregates multiple black-box predictions with unknown quality while preserving valid statistical inference. Our method provides two key guarantees: (i) it never performs worse than using the labeled data alone, regardless of the quality of the predictions; and (ii) if any one of the predictions (without knowing which one) perfectly fits the ground truth, the algorithm adaptively exploits this to achieve either a faster convergence rate or the semiparametric efficiency bound. We demonstrate the effectiveness of the proposed algorithm through experiments on both synthetic and benchmark datasets.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "394",
        "title": "Optimizing the non-Clifford-count in unitary synthesis using Reinforcement Learning",
        "author": [
            "David Kremer",
            "Ali Javadi-Abhari",
            "Priyanka Mukhopadhyay"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21709",
        "abstract": "An efficient implementation of unitary operators is important in order to practically realize the computational advantages claimed by quantum algorithms over their classical counterparts. In this paper we study the potential of using reinforcement learning (RL) in order to synthesize quantum circuits, while optimizing the T-count and CS-count, of unitaries that are exactly implementable by the Clifford+T and Clifford+CS gate sets, respectively. In general, the complexity of existing algorithms depend exponentially on the number of qubits and the non-Clifford-count of unitaries. We have designed our RL framework to work with channel representation of unitaries, that enables us to perform matrix operations efficiently, using integers only. We have also incorporated pruning heuristics and a canonicalization of operators, in order to reduce the search complexity. As a result, compared to previous works, we are able to implement significantly larger unitaries, in less time, with much better success rate and improvement factor. Our results for Clifford+T synthesis on two qubits achieve close-to-optimal decompositions for up to 100 T gates, 5 times more than previous RL algorithms and to the best of our knowledge, the largest instances achieved with any method to date. Our RL algorithm is able to recover previously-known optimal linear complexity algorithm for T-count-optimal decomposition of 1 qubit unitaries. For 2-qubit Clifford+CS unitaries, our algorithm achieves a linear complexity, something that could only be accomplished by a previous algorithm using $SO(6)$ representation.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "395",
        "title": "Error Analysis of Discrete Flow with Generator Matching",
        "author": [
            "Zhengyan Wan",
            "Yidong Ouyang",
            "Qiang Yao",
            "Liyan Xie",
            "Fang Fang",
            "Hongyuan Zha",
            "Guang Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21906",
        "abstract": "Discrete flow models offer a powerful framework for learning distributions over discrete state spaces and have demonstrated superior performance compared to the discrete diffusion model. However, their convergence properties and error analysis remain largely unexplored. In this work, we develop a unified framework grounded in stochastic calculus theory to systematically investigate the theoretical properties of discrete flow. Specifically, we derive the KL divergence of two path measures regarding two continuous-time Markov chains (CTMCs) with different transition rates by developing a novel Girsanov-type theorem, and provide a comprehensive analysis that encompasses the error arising from transition rate estimation and early stopping, where the first type of error has rarely been analyzed by existing works. Unlike discrete diffusion models, discrete flow incurs no truncation error caused by truncating the time horizon in the noising process. Building on generator matching and uniformization, we establish non-asymptotic error bounds for distribution estimation. Our results provide the first error analysis for discrete flow models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "396",
        "title": "AUV: Teaching Audio Universal Vector Quantization with Single Nested Codebook",
        "author": [
            "Yushen Chen",
            "Kai Hu",
            "Long Zhou",
            "Shulin Feng",
            "Xusheng Yang",
            "Hangting Chen",
            "Xie Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2509.21968",
        "abstract": "We propose AUV, a unified neural audio codec with a single codebook, which enables a favourable reconstruction of speech and further extends to general audio, including vocal, music, and sound. AUV is capable of tackling any 16 kHz mixed-domain audio segment at bit rates around 700 bps. To accomplish this, we guide the matryoshka codebook with nested domain-specific partitions, assigned with corresponding teacher models to perform distillation, all in a single-stage training. A conformer-style encoder-decoder architecture with STFT features as audio representation is employed, yielding better audio quality. Comprehensive evaluations demonstrate that AUV exhibits comparable audio reconstruction ability to state-of-the-art domain-specific single-layer quantizer codecs, showcasing the potential of audio universal vector quantization with a single codebook. The pre-trained model and demo samples are available at https://swivid.github.io/AUV/.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "397",
        "title": "Speak Your Mind: The Speech Continuation Task as a Probe of Voice-Based Model Bias",
        "author": [
            "Shree Harsha Bokkahalli Satish",
            "Harm Lameris",
            "Olivier Perrotin",
            "Gustav Eje Henter",
            "Ãva SzÃ©kely"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22061",
        "abstract": "Speech Continuation (SC) is the task of generating a coherent extension of a spoken prompt while preserving both semantic context and speaker identity. Because SC is constrained to a single audio stream, it offers a more direct setting for probing biases in speech foundation models than dialogue does. In this work we present the first systematic evaluation of bias in SC, investigating how gender and phonation type (breathy, creaky, end-creak) affect continuation behaviour. We evaluate three recent models: SpiritLM (base and expressive), VAE-GSLM, and SpeechGPT across speaker similarity, voice quality preservation, and text-based bias metrics. Results show that while both speaker similarity and coherence remain a challenge, textual evaluations reveal significant model and gender interactions: once coherence is sufficiently high (for VAE-GSLM), gender effects emerge on text-metrics such as agency and sentence polarity. In addition, continuations revert toward modal phonation more strongly for female prompts than for male ones, revealing a systematic voice-quality bias. These findings highlight SC as a controlled probe of socially relevant representational biases in speech foundation models, and suggest that it will become an increasingly informative diagnostic as continuation quality improves.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "398",
        "title": "Universal Inverse Distillation for Matching Models with Real-Data Supervision (No GANs)",
        "author": [
            "Nikita Kornilov",
            "David Li",
            "Tikhon Mavrin",
            "Aleksei Leonov",
            "Nikita Gushchin",
            "Evgeny Burnaev",
            "Iaroslav Koshelev",
            "Alexander Korotin"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22459",
        "abstract": "While achieving exceptional generative quality, modern diffusion, flow, and other matching models suffer from slow inference, as they require many steps of iterative generation. Recent distillation methods address this by training efficient one-step generators under the guidance of a pre-trained teacher model. However, these methods are often constrained to only one specific framework, e.g., only to diffusion or only to flow models. Furthermore, these methods are naturally data-free, and to benefit from the usage of real data, it is required to use an additional complex adversarial training with an extra discriminator model. In this paper, we present RealUID, a universal distillation framework for all matching models that seamlessly incorporates real data into the distillation procedure without GANs. Our RealUID approach offers a simple theoretical foundation that covers previous distillation methods for Flow Matching and Diffusion models, and is also extended to their modifications, such as Bridge Matching and Stochastic Interpolants.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "399",
        "title": "CausalKANs: interpretable treatment effect estimation with Kolmogorov-Arnold networks",
        "author": [
            "Alejandro AlmodÃ³var",
            "Patricia A. ApellÃ¡niz",
            "Santiago Zazo",
            "Juan Parras"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22467",
        "abstract": "Deep neural networks achieve state-of-the-art performance in estimating heterogeneous treatment effects, but their opacity limits trust and adoption in sensitive domains such as medicine, economics, and public policy. Building on well-established and high-performing causal neural architectures, we propose causalKANs, a framework that transforms neural estimators of conditional average treatment effects (CATEs) into Kolmogorov--Arnold Networks (KANs). By incorporating pruning and symbolic simplification, causalKANs yields interpretable closed-form formulas while preserving predictive accuracy. Experiments on benchmark datasets demonstrate that causalKANs perform on par with neural baselines in CATE error metrics, and that even simple KAN variants achieve competitive performance, offering a favorable accuracy--interpretability trade-off. By combining reliability with analytic accessibility, causalKANs provide auditable estimators supported by closed-form expressions and interpretable plots, enabling trustworthy individualized decision-making in high-stakes settings. We release the code for reproducibility at https://github.com/aalmodovares/causalkans .",
        "tags": [
            "KAN"
        ]
    },
    {
        "id": "400",
        "title": "Linear Causal Representation Learning by Topological Ordering, Pruning, and Disentanglement",
        "author": [
            "Hao Chen",
            "Lin Liu",
            "Yu Guang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22553",
        "abstract": "Causal representation learning (CRL) has garnered increasing interests from the causal inference and artificial intelligence community, due to its capability of disentangling potentially complex data-generating mechanism into causally interpretable latent features, by leveraging the heterogeneity of modern datasets. In this paper, we further contribute to the CRL literature, by focusing on the stylized linear structural causal model over the latent features and assuming a linear mixing function that maps latent features to the observed data or measurements. Existing linear CRL methods often rely on stringent assumptions, such as accessibility to single-node interventional data or restrictive distributional constraints on latent features and exogenous measurement noise. However, these prerequisites can be challenging to satisfy in certain scenarios. In this work, we propose a novel linear CRL algorithm that, unlike most existing linear CRL methods, operates under weaker assumptions about environment heterogeneity and data-generating distributions while still recovering latent causal features up to an equivalence class. We further validate our new algorithm via synthetic experiments and an interpretability analysis of large language models (LLMs), demonstrating both its superiority over competing methods in finite samples and its potential in integrating causality into AI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "401",
        "title": "Towards Efficient Online Exploration for Reinforcement Learning with Human Feedback",
        "author": [
            "Gen Li",
            "Yuling Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2509.22633",
        "abstract": "Reinforcement learning with human feedback (RLHF), which learns a reward model from human preference data and then optimizes a policy to favor preferred responses, has emerged as a central paradigm for aligning large language models (LLMs) with human preferences. In this paper, we investigate exploration principles for online RLHF, where one seeks to adaptively collect new preference data to refine both the reward model and the policy in a data-efficient manner. By examining existing optimism-based exploration algorithms, we identify a drawback in their sampling protocol: they tend to gather comparisons that fail to reduce the most informative uncertainties in reward differences, and we prove lower bounds showing that such methods can incur linear regret over exponentially long horizons. Motivated by this insight, we propose a new exploration scheme that directs preference queries toward reducing uncertainty in reward differences most relevant to policy improvement. Under a multi-armed bandit model of RLHF, we establish regret bounds of order $T^{(\\beta+1)/(\\beta+2)}$, where $\\beta>0$ is a hyperparameter that balances reward maximization against mitigating distribution shift. To our knowledge, this is the first online RLHF algorithm with regret scaling polynomially in all model parameters.",
        "tags": [
            "LLM",
            "RL",
            "RLHF"
        ]
    }
]