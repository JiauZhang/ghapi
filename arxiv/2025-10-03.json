[
    {
        "id": "1",
        "title": "Methodological Framework for Quantifying Semantic Test Coverage in RAG Systems",
        "author": [
            "Noah Broestl",
            "Adel Nasser Abdalla",
            "Rajprakash Bale",
            "Hersh Gupta",
            "Max Struever"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00001",
        "abstract": "Reliably determining the performance of Retrieval-Augmented Generation (RAG) systems depends on comprehensive test questions. While a proliferation of evaluation frameworks for LLM-powered applications exists, current practices lack a systematic method to ensure these test sets adequately cover the underlying knowledge base, leaving developers with significant blind spots. To address this, we present a novel, applied methodology to quantify the semantic coverage of RAG test questions against their underlying documents. Our approach leverages existing technologies, including vector embeddings and clustering algorithms, to create a practical framework for validating test comprehensiveness. Our methodology embeds document chunks and test questions into a unified vector space, enabling the calculation of multiple coverage metrics: basic proximity, content-weighted coverage, and multi-topic question coverage. Furthermore, we incorporate outlier detection to filter irrelevant questions, allowing for the refinement of test sets. Experimental evidence from two distinct use cases demonstrates that our framework effectively quantifies test coverage, identifies specific content areas with inadequate representation, and provides concrete recommendations for generating new, high-value test questions. This work provides RAG developers with essential tools to build more robust test suites, thereby improving system reliability and extending to applications such as identifying misaligned documents.",
        "tags": [
            "Detection",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "2",
        "title": "IA aplicada al anÃ¡lisis del conflicto IrÃ¡n-Israel: Mapeo de discursos en YouTube",
        "author": [
            "Alvaro Vallejo RamÃ­rez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00021",
        "abstract": "Purpose. This study analyzes the digital representation of the Iran-Israel conflict that occurred in June 2025, based on 120,000 comments posted on YouTube. It sought to identify discursive positions regarding the actors involved and to examine how media and algorithmic biases shape digital conversations. Methodology. A mixed-methods design with triangulation was adopted. In the quantitative phase, natural language processing techniques and machine learning models (BERT and XLM-RoBERTa) were used to classify comments into ten categories. In the qualitative phase, a critical analysis of media context and ideological narratives was conducted, complemented by manual annotation and supervised training. This strategy enabled the integration of statistical robustness with contextual understanding. Results and conclusions. The findings reveal a clear overrepresentation of pro-Palestinian and anti-United States/Israel discourses, while pro-United States and anti-Palestinian positions were marginal. Iran, usually rendered invisible in global media, emerged as a central actor in the digital conversation during the conflict, suggesting a narrative shift away from previous hegemonic frameworks. Likewise, the results confirm the influence of algorithmic biases in amplifying certain discourses while limiting others. Original contributions. This work combines computational analysis and philosophical critique for the study of digital controversies, providing a methodological framework replicable in geopolitical contexts. It is one of the first Spanish-language studies to map, through artificial intelligence and critical analysis, discourses on an international conflict on YouTube, highlighting asymmetries and narrative disputes that are often overlooked.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "3",
        "title": "Learning to Lead Themselves: Agentic AI in MAS using MARL",
        "author": [
            "Ansh Kamthan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00022",
        "abstract": "As autonomous systems move from prototypes to real deployments, the ability of multiple agents to make decentralized, cooperative decisions becomes a core requirement. This paper examines how agentic artificial intelligence, agents that act independently, adaptively and proactively can improve task allocation and coordination in multi-agent systems, with primary emphasis on drone delivery and secondary relevance to warehouse automation. We formulate the problem in a cooperative multi-agent reinforcement learning setting and implement a lightweight multi-agent Proximal Policy Optimization, called IPPO, approach in PyTorch under a centralized-training, decentralized-execution paradigm. Experiments are conducted in PettingZoo environment, where multiple homogeneous drones or agents must self-organize to cover distinct targets without explicit communication.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "4",
        "title": "ToolBrain: A Flexible Reinforcement Learning Framework for Agentic Tools",
        "author": [
            "Quy Minh Le",
            "Minh Sao Khue Luu",
            "Khanh-Tung Tran",
            "Duc-Hai Nguyen",
            "Hoang-Quoc-Viet Pham",
            "Quan Le",
            "Hoang Thanh Lam",
            "Hoang D. Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00023",
        "abstract": "Effective tool use is essential for agentic AI, yet training agents to utilize tools remains challenging due to manually designed rewards, limited training data, and poor multi-tool selection, resulting in slow adaptation, wasted computational resources, and suboptimal performance. We introduce ToolBrain, a lightweight and user-friendly framework for coaching tool use in agentic models with flexible reinforcement learning (RL), easing the barriers for researchers and practitioners to adapt LLM-based agents to specific domains. It supports a wide range of training strategies, including RL algorithms such as GRPO and DPO, as well as supervised learning. ToolBrain enables custom reward callables directly on an agent's execution traces or simply utilizes an automated LLM-as-a-judge system for reward generation. It is packed with useful capabilities, including knowledge distillation from large to small models for efficient development, automatic task generation from tool descriptions, seamless tool retrieval, efficient fine-tuning pipelines with QLoRA through Unsloth, and quantized inference via bitsandbytes. We demonstrate ToolBrain through diverse use cases, such as training a CodeAct agent to autonomously execute email search tasks, showing fast, targeted improvements (up to 30.0%) in tool-use skills while keeping the codebase simple and extensible in Agentic AI. Our framework is publicly available at https://toolbrain.org.",
        "tags": [
            "DPO",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "5",
        "title": "EpidemIQs: Prompt-to-Paper LLM Agents for Epidemic Modeling and Analysis",
        "author": [
            "Mohammad Hossein Samaei",
            "Faryad Darabi Sahneh",
            "Lee W. Cohnstaedt",
            "Caterina Scoglio"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00024",
        "abstract": "Large Language Models (LLMs) offer new opportunities to automate complex interdisciplinary research domains. Epidemic modeling, characterized by its complexity and reliance on network science, dynamical systems, epidemiology, and stochastic simulations, represents a prime candidate for leveraging LLM-driven automation. We introduce \\textbf{EpidemIQs}, a novel multi-agent LLM framework that integrates user inputs and autonomously conducts literature review, analytical derivation, network modeling, mechanistic modeling, stochastic simulations, data visualization and analysis, and finally documentation of findings in a structured manuscript. We introduced two types of agents: a scientist agent for planning, coordination, reflection, and generation of final results, and a task-expert agent to focus exclusively on one specific duty serving as a tool to the scientist agent. The framework consistently generated complete reports in scientific article format. Specifically, using GPT 4.1 and GPT 4.1 mini as backbone LLMs for scientist and task-expert agents, respectively, the autonomous process completed with average total token usage 870K at a cost of about \\$1.57 per study, achieving a 100\\% completion success rate through our experiments. We evaluate EpidemIQs across different epidemic scenarios, measuring computational cost, completion success rate, and AI and human expert reviews of generated reports. We compare EpidemIQs to the single-agent LLM, which has the same system prompts and tools, iteratively planning, invoking tools, and revising outputs until task completion. The comparison shows consistently higher performance of the proposed framework across five different scenarios. EpidemIQs represents a step forward in accelerating scientific research by significantly reducing costs and turnaround time of discovery processes, and enhancing accessibility to advanced modeling tools.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "Rethinking RoPE Scaling in Quantized LLM: Theory, Outlier, and Channel-Band Analysis with Weight Rescaling",
        "author": [
            "Ye Qiao",
            "Haocheng Xu",
            "Xiaofan Zhang",
            "Sitao Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00028",
        "abstract": "Extending the context window support of large language models (LLMs) is crucial for tasks with long-distance dependencies. RoPE-based interpolation and extrapolation methods, such as linear scaling and frequency-aware schemes, enable longer input length support without retraining, while post-training quantization (PTQ) makes deployment practical. However, we show that combining RoPE position interpolation (PI) with PTQ degrades accuracy due to coupled effects including long-context aliasing, dynamic-range dilation, anisotropy from axis-aligned quantizers vs. rotated RoPE pairs, and outlier shifting that produces position-dependent logit noise. We provide, to the best of our knowledge, the first systematic analysis of the PI+PTQ approach and introduce two practical diagnostics: interpolation pressure (per-band sensitivity to phase scaling) and tail-inflation ratios (outlier shift from short to long contexts). Following the analysis results, we propose Q-ROAR (Quantization, RoPE-interpolation, and Outlier Aware Rescaling), a weight-only, interpolation-aware stabilization of PI for quantized LLMs. Q-ROAR groups RoPE dimensions into a small number of frequency bands and performs a lightweight search over per-band scales for Key and Query weights (with an optional symmetric variant to preserve logit scale). The search is guided by our diagnostics and uses a tiny long-context development dataset, requiring no fine-tuning to the model, no architecture or kernel changes, and no additional deployment overhead. Empirically, Q-ROAR reduces the model's perplexity on long-context workloads by more than 14%, while preserving short-context performance, inference throughput, and compatibility with existing LLM system stacks.",
        "tags": [
            "LLM",
            "RoPE"
        ]
    },
    {
        "id": "7",
        "title": "VibeCodeHPC: An Agent-Based Iterative Prompting Auto-Tuner for HPC Code Generation Using LLMs",
        "author": [
            "Shun-ichiro Hayashi",
            "Koki Morita",
            "Daichi Mukunoki",
            "Tetsuya Hoshino",
            "Takahiro Katagiri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00031",
        "abstract": "We propose VibeCodeHPC, an automatic tuning system for HPC programs based on multi-agent LLMs for code generation. VibeCodeHPC tunes programs through multi-agent role allocation and iterative prompt refinement. We describe the system configuration with four roles: Project Manager (PM), System Engineer (SE), Programmer (PG), and Continuous Delivery (CD). We introduce dynamic agent deployment and activity monitoring functions to facilitate effective multi-agent collaboration. In our case study, we convert and optimize CPU-based matrix-matrix multiplication code written in C to GPU code using CUDA. The multi-agent configuration of VibeCodeHPC achieved higher-quality code generation per unit time compared to a solo-agent configuration. Additionally, the dynamic agent deployment and activity monitoring capabilities facilitated more effective identification of requirement violations and other issues.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "8",
        "title": "On Robustness of Vision-Language-Action Model against Multi-Modal Perturbations",
        "author": [
            "Jianing Guo",
            "Zhenhong Wu",
            "Chang Tu",
            "Yiyao Ma",
            "Xiangqi Kong",
            "Zhiqian Liu",
            "Jiaming Ji",
            "Shuning Zhang",
            "Yuanpei Chen",
            "Kai Chen",
            "Xianglong Liu",
            "Qi Dou",
            "Yaodong Yang",
            "Huijie Zhao",
            "Weifeng Lv",
            "Simin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00037",
        "abstract": "In Vision-Language-Action (VLA) models, robustness to real-world perturbations is critical for deployment. Existing methods target simple visual disturbances, overlooking the broader multi-modal perturbations that arise in actions, instructions, environments, and observations. Here, we first evaluate the robustness of mainstream VLAs under 17 perturbations across four modalities. We find (1) actions as the most fragile modality, (2) Existing visual-robust VLA do not gain robustness in other modality, and (3) pi0 demonstrates superior robustness with a diffusion-based action head. To build multi-modal robust VLAs, we propose RobustVLA against perturbations in VLA inputs and outputs. For output robustness, we perform offline robust optimization against worst-case action noise that maximizes mismatch in flow matching objective. This can be seen as adversarial training, label smoothing, and outlier penalization. For input robustness, we enforce consistent actions across input variations that preserve task semantics. To account for multiple perturbations, we formulate robustness as a multi-armed bandit problem and apply an upper confidence bound algorithm to automatically identify the most harmful noise. Experiments on LIBERO demonstrate our RobustVLA delivers absolute gains over baselines of 12.6% on the pi0 backbone and 10.4% on the OpenVLA backbone across all 17 perturbations, achieving 50.6x faster inference than existing visual-robust VLAs, and a 10.4% gain under mixed perturbations. Our RobustVLA is particularly effective on real-world FR5 robot with limited demonstrations, showing absolute gains by 65.6% under perturbations of four modalities.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "Robotics"
        ]
    },
    {
        "id": "9",
        "title": "DexBench: Benchmarking LLMs for Personalized Decision Making in Diabetes Management",
        "author": [
            "Maria Ana Cardei",
            "Josephine Lamp",
            "Mark Derdzinski",
            "Karan Bhatia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00038",
        "abstract": "We present DexBench, the first benchmark designed to evaluate large language model (LLM) performance across real-world decision-making tasks faced by individuals managing diabetes in their daily lives. Unlike prior health benchmarks that are either generic, clinician-facing or focused on clinical tasks (e.g., diagnosis, triage), DexBench introduces a comprehensive evaluation framework tailored to the unique challenges of prototyping patient-facing AI solutions in diabetes, glucose management, metabolic health and related domains. Our benchmark encompasses 7 distinct task categories, reflecting the breadth of real-world questions individuals with diabetes ask, including basic glucose interpretation, educational queries, behavioral associations, advanced decision making and long term planning. Towards this end, we compile a rich dataset comprising one month of time-series data encompassing glucose traces and metrics from continuous glucose monitors (CGMs) and behavioral logs (e.g., eating and activity patterns) from 15,000 individuals across three different diabetes populations (type 1, type 2, pre-diabetes/general health and wellness). Using this data, we generate a total of 360,600 personalized, contextual questions across the 7 tasks. We evaluate model performance on these tasks across 5 metrics: accuracy, groundedness, safety, clarity and actionability. Our analysis of 8 recent LLMs reveals substantial variability across tasks and metrics; no single model consistently outperforms others across all dimensions. By establishing this benchmark, we aim to advance the reliability, safety, effectiveness and practical utility of AI solutions in diabetes care.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "10",
        "title": "AutoPK: Leveraging LLMs and a Hybrid Similarity Metric for Advanced Retrieval of Pharmacokinetic Data from Complex Tables and Documents",
        "author": [
            "Hossein Sholehrasa",
            "Amirhossein Ghanaatian",
            "Doina Caragea",
            "Lisa A. Tell",
            "Jim E. Riviere",
            "Majid Jaberi-Douraki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00039",
        "abstract": "Pharmacokinetics (PK) plays a critical role in drug development and regulatory decision-making for human and veterinary medicine, directly affecting public health through drug safety and efficacy assessments. However, PK data are often embedded in complex, heterogeneous tables with variable structures and inconsistent terminologies, posing significant challenges for automated PK data retrieval and standardization. AutoPK, a novel two-stage framework for accurate and scalable extraction of PK data from complex scientific tables. In the first stage, AutoPK identifies and extracts PK parameter variants using large language models (LLMs), a hybrid similarity metric, and LLM-based validation. The second stage filters relevant rows, converts the table into a key-value text format, and uses an LLM to reconstruct a standardized table. Evaluated on a real-world dataset of 605 PK tables, including captions and footnotes, AutoPK shows significant improvements in precision and recall over direct LLM baselines. For instance, AutoPK with LLaMA 3.1-70B achieved an F1-score of 0.92 on half-life and 0.91 on clearance parameters, outperforming direct use of LLaMA 3.1-70B by margins of 0.10 and 0.21, respectively. Smaller models such as Gemma 3-27B and Phi 3-12B with AutoPK achieved 2-7 fold F1 gains over their direct use, with Gemma's hallucination rates reduced from 60-95% down to 8-14%. Notably, AutoPK enabled open-source models like Gemma 3-27B to outperform commercial systems such as GPT-4o Mini on several PK parameters. AutoPK enables scalable and high-confidence PK data extraction, making it well-suited for critical applications in veterinary pharmacology, drug safety monitoring, and public health decision-making, while addressing heterogeneous table structures and terminology and demonstrating generalizability across key PK parameters. Code and data: https://github.com/hosseinsholehrasa/AutoPK",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "11",
        "title": "Uncovering Intrinsic Capabilities: A Paradigm for Data Curation in Vision-Language Models",
        "author": [
            "Junjie Li",
            "Ziao Wang",
            "Jianghong Ma",
            "Xiaofeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00040",
        "abstract": "Large vision-language models (VLMs) achieve strong benchmark performance, but controlling their behavior through instruction tuning remains difficult. Reducing the budget of instruction tuning dataset often causes regressions, as heuristic strategies treat models as black boxes and overlook the latent capabilities that govern learning. We introduce Capability-Attributed Data Curation (CADC), a framework that shifts curation from task-specific heuristics to intrinsic capability analysis. CADC discovers intrinsic capabilities in an unsupervised manner from gradient-based learning trajectories, attributes training data to these capabilities via influence estimation, and curates capability-aware curricula through balanced selection and staged sequencing. This transforms black-box instruction tuning into a controllable, capability-driven process. With as little as 5% of the original data, CADC surpasses full-data training on multimodal benchmarks. These results validate intrinsic capabilities as the fundamental building blocks of model learning and establish CADC as a principle paradigm for instruction data curation.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "12",
        "title": "Culture In a Frame: C$^3$B as a Comic-Based Benchmark for Multimodal Culturally Awareness",
        "author": [
            "Yuchen Song",
            "Andong Chen",
            "Wenxin Zhu",
            "Kehai Chen",
            "Xuefeng Bai",
            "Muyun Yang",
            "Tiejun Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00041",
        "abstract": "Cultural awareness capabilities has emerged as a critical capability for Multimodal Large Language Models (MLLMs). However, current benchmarks lack progressed difficulty in their task design and are deficient in cross-lingual tasks. Moreover, current benchmarks often use real-world images. Each real-world image typically contains one culture, making these benchmarks relatively easy for MLLMs. Based on this, we propose C$^3$B ($\\textbf{C}$omics $\\textbf{C}$ross-$\\textbf{C}$ultural $\\textbf{B}$enchmark), a novel multicultural, multitask and multilingual cultural awareness capabilities benchmark. C$^3$B comprises over 2000 images and over 18000 QA pairs, constructed on three tasks with progressed difficulties, from basic visual recognition to higher-level cultural conflict understanding, and finally to cultural content generation. We conducted evaluations on 11 open-source MLLMs, revealing a significant performance gap between MLLMs and human performance. The gap demonstrates that C$^3$B poses substantial challenges for current MLLMs, encouraging future research to advance the cultural awareness capabilities of MLLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "13",
        "title": "Beyond the Prompt: Gender Bias in Text-to-Image Models, with a Case Study on Hospital Professions",
        "author": [
            "Franck Vandewiele",
            "Remi Synave",
            "Samuel Delepoulle",
            "Remi Cozot"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00045",
        "abstract": "Text-to-image (TTI) models are increasingly used in professional, educational, and creative contexts, yet their outputs often embed and amplify social biases. This paper investigates gender representation in six state-of-the-art open-weight models: HunyuanImage 2.1, HiDream-I1-dev, Qwen-Image, FLUX.1-dev, Stable-Diffusion 3.5 Large, and Stable-Diffusion-XL. Using carefully designed prompts, we generated 100 images for each combination of five hospital-related professions (cardiologist, hospital director, nurse, paramedic, surgeon) and five portrait qualifiers (\"\", corporate, neutral, aesthetic, beautiful).\nOur analysis reveals systematic occupational stereotypes: all models produced nurses exclusively as women and surgeons predominantly as men. However, differences emerge across models: Qwen-Image and SDXL enforce rigid male dominance, HiDream-I1-dev shows mixed outcomes, and FLUX.1-dev skews female in most roles. HunyuanImage 2.1 and Stable-Diffusion 3.5 Large also reproduce gender stereotypes but with varying degrees of sensitivity to prompt formulation. Portrait qualifiers further modulate gender balance, with terms like corporate reinforcing male depictions and beautiful favoring female ones. Sensitivity varies widely: Qwen-Image remains nearly unaffected, while FLUX.1-dev, SDXL, and SD3.5 show strong prompt dependence.\nThese findings demonstrate that gender bias in TTI models is both systematic and model-specific. Beyond documenting disparities, we argue that prompt wording plays a critical role in shaping demographic outcomes. The results underscore the need for bias-aware design, balanced defaults, and user guidance to prevent the reinforcement of occupational stereotypes in generative AI.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Qwen",
            "SDXL",
            "Text-to-Image"
        ]
    },
    {
        "id": "14",
        "title": "Reinforcement Learning-Based Prompt Template Stealing for Text-to-Image Models",
        "author": [
            "Xiaotian Zou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00046",
        "abstract": "Multimodal Large Language Models (MLLMs) have transformed text-to-image workflows, allowing designers to create novel visual concepts with unprecedented speed. This progress has given rise to a thriving prompt trading market, where curated prompts that induce trademark styles are bought and sold. Although commercially attractive, prompt trading also introduces a largely unexamined security risk: the prompts themselves can be stolen.\nIn this paper, we expose this vulnerability and present RLStealer, a reinforcement learning based prompt inversion framework that recovers its template from only a small set of example images. RLStealer treats template stealing as a sequential decision making problem and employs multiple similarity based feedback signals as reward functions to effectively explore the prompt space. Comprehensive experiments on publicly available benchmarks demonstrate that RLStealer gets state-of-the-art performance while reducing the total attack cost to under 13% of that required by existing baselines. Our further analysis confirms that RLStealer can effectively generalize across different image styles to efficiently steal unseen prompt templates. Our study highlights an urgent security threat inherent in prompt trading and lays the groundwork for developing protective standards in the emerging MLLMs marketplace.",
        "tags": [
            "LLM",
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": "15",
        "title": "Explanation-Driven Counterfactual Testing for Faithfulness in Vision-Language Model Explanations",
        "author": [
            "Sihao Ding",
            "Santosh Vasa",
            "Aditi Ramadwar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00047",
        "abstract": "Vision-Language Models (VLMs) often produce fluent Natural Language Explanations (NLEs) that sound convincing but may not reflect the causal factors driving predictions. This mismatch of plausibility and faithfulness poses technical and governance risks. We introduce Explanation-Driven Counterfactual Testing (EDCT), a fully automated verification procedure for a target VLM that treats the model's own explanation as a falsifiable hypothesis. Given an image-question pair, EDCT: (1) obtains the model's answer and NLE, (2) parses the NLE into testable visual concepts, (3) generates targeted counterfactual edits via generative inpainting, and (4) computes a Counterfactual Consistency Score (CCS) using LLM-assisted analysis of changes in both answers and explanations. Across 120 curated OK-VQA examples and multiple VLMs, EDCT uncovers substantial faithfulness gaps and provides regulator-aligned audit artifacts indicating when cited concepts fail causal tests.",
        "tags": [
            "Inpainting",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "16",
        "title": "Object-AVEdit: An Object-level Audio-Visual Editing Model",
        "author": [
            "Youquan Fu",
            "Ruiyang Si",
            "Hongfa Wang",
            "Dongzhan Zhou",
            "Jiacheng Sun",
            "Ping Luo",
            "Di Hu",
            "Hongyuan Zhang",
            "Xuelong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00050",
        "abstract": "There is a high demand for audio-visual editing in video post-production and the film making field. While numerous models have explored audio and video editing, they struggle with object-level audio-visual operations. Specifically, object-level audio-visual editing requires the ability to perform object addition, replacement, and removal across both audio and visual modalities, while preserving the structural information of the source instances during the editing process. In this paper, we present \\textbf{Object-AVEdit}, achieving the object-level audio-visual editing based on the inversion-regeneration paradigm. To achieve the object-level controllability during editing, we develop a word-to-sounding-object well-aligned audio generation model, bridging the gap in object-controllability between audio and current video generation models. Meanwhile, to achieve the better structural information preservation and object-level editing effect, we propose an inversion-regeneration holistically-optimized editing algorithm, ensuring both information retention during the inversion and better regeneration effect. Extensive experiments demonstrate that our editing model achieved advanced results in both audio-video object-level editing tasks with fine audio-visual semantic alignment. In addition, our developed audio generation model also achieved advanced performance. More results on our project page: https://gewu-lab.github.io/Object_AVEdit-website/.",
        "tags": [
            "Video Editing",
            "Video Generation"
        ]
    },
    {
        "id": "17",
        "title": "HiDe: Rethinking The Zoom-IN method in High Resolution MLLMs via Hierarchical Decoupling",
        "author": [
            "Xianjie Liu",
            "Yiman Hu",
            "Yixiong Zou",
            "Liang Wu",
            "Jian Xu",
            "Bo Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00054",
        "abstract": "Multimodal Large Language Models (MLLMs) have made significant strides in visual understanding tasks. However, their performance on high-resolution images remains suboptimal. While existing approaches often attribute this limitation to perceptual constraints and argue that MLLMs struggle to recognize small objects, leading them to use \"zoom in\" strategies for better detail, our analysis reveals a different cause: the main issue is not object size, but rather caused by complex background interference. We systematically analyze this \"zoom in\" operation through a series of decoupling experiments and propose the Hierarchical Decoupling Framework (HiDe), a training-free framework that uses Token-wise Attention Decoupling (TAD) to decouple the question tokens and identify the key information tokens, then leverages their attention weights to achieve precise alignment with the target visual regions. Subsequently, it employs Layout-Preserving Decoupling (LPD) to decouple these regions from the background and reconstructs a compact representation that preserves essential spatial layouts while eliminating background interference. HiDe sets a new SOTA on V*Bench, HRBench4K, and HRBench8K, boosting Qwen2.5-VL 7B and InternVL3 8B to SOTA (92.1% and 91.6% on V*Bench), even surpassing RL methods. After optimization, HiDe uses 75% less memory than the previous training-free approach. Code is provided in https://github.com/Tennine2077/HiDe.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "18",
        "title": "Less is More: Lean yet Powerful Vision-Language Model for Autonomous Driving",
        "author": [
            "Sheng Yang",
            "Tong Zhan",
            "Guancheng Chen",
            "Yanfeng Lu",
            "Jian Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00060",
        "abstract": "In this work, we reconceptualize autonomous driving as a generalized language and formulate the trajectory planning task as next waypoint prediction. We introduce Max-V1, a novel framework for one-stage end-to-end autonomous driving. Our framework presents a single-pass generation paradigm that aligns with the inherent sequentiality of driving. This approach leverages the generative capacity of the VLM (Vision-Language Model) to enable end-to-end trajectory prediction directly from front-view camera input. The efficacy of this method is underpinned by a principled supervision strategy derived from statistical modeling. This provides a well-defined learning objective, which makes the framework highly amenable to master complex driving policies through imitation learning from large-scale expert demonstrations. Empirically, our method achieves the state-of-the-art performance on the nuScenes dataset, delivers an overall improvement of over 30% compared to prior baselines. Furthermore, it exhibits superior generalization performance on cross-domain datasets acquired from diverse vehicles, demonstrating notable potential for cross-vehicle robustness and adaptability. Due to these empirical strengths, this work introduces a model enabling fundamental driving behaviors, laying the foundation for the development of more capable self-driving agents. Code will be available upon publication.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "19",
        "title": "Intelligent 5S Audit: Application of Artificial Intelligence for Continuous Improvement in the Automotive Industry",
        "author": [
            "Rafael da Silva Maciel",
            "Lucio Veraldo Jr"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00067",
        "abstract": "The evolution of the 5S methodology with the support of artificial intelligence techniques represents a significant opportunity to improve industrial organization audits in the automotive chain, making them more objective, efficient and aligned with Industry 4.0 standards. This work developed an automated 5S audit system based on large-scale language models (LLM), capable of assessing the five senses (Seiri, Seiton, Seiso, Seiketsu, Shitsuke) in a standardized way through intelligent image analysis. The system's reliability was validated using Cohen's concordance coefficient (kappa = 0.75), showing strong alignment between the automated assessments and the corresponding human audits. The results indicate that the proposed solution contributes significantly to continuous improvement in automotive manufacturing environments, speeding up the audit process by 50% of the traditional time and maintaining the consistency of the assessments, with a 99.8% reduction in operating costs compared to traditional manual audits. The methodology presented establishes a new paradigm for integrating lean systems with emerging AI technologies, offering scalability for implementation in automotive plants of different sizes.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "20",
        "title": "OIG-Bench: A Multi-Agent Annotated Benchmark for Multimodal One-Image Guides Understanding",
        "author": [
            "Jiancong Xie",
            "Wenjin Wang",
            "Zhuomeng Zhang",
            "Zihan Liu",
            "Qi Liu",
            "Ke Feng",
            "Zixun Sun",
            "Yuedong Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00069",
        "abstract": "Recent advances in Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities. However, evaluating their capacity for human-like understanding in One-Image Guides remains insufficiently explored. One-Image Guides are a visual format combining text, imagery, and symbols to present reorganized and structured information for easier comprehension, which are specifically designed for human viewing and inherently embody the characteristics of human perception and understanding. Here, we present OIG-Bench, a comprehensive benchmark focused on One-Image Guide understanding across diverse domains. To reduce the cost of manual annotation, we developed a semi-automated annotation pipeline in which multiple intelligent agents collaborate to generate preliminary image descriptions, assisting humans in constructing image-text pairs. With OIG-Bench, we have conducted a comprehensive evaluation of 29 state-of-the-art MLLMs, including both proprietary and open-source models. The results show that Qwen2.5-VL-72B performs the best among the evaluated models, with an overall accuracy of 77%. Nevertheless, all models exhibit notable weaknesses in semantic understanding and logical reasoning, indicating that current MLLMs still struggle to accurately interpret complex visual-text relationships. In addition, we also demonstrate that the proposed multi-agent annotation system outperforms all MLLMs in image captioning, highlighting its potential as both a high-quality image description generator and a valuable tool for future dataset construction. Datasets are available at https://github.com/XiejcSYSU/OIG-Bench.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "21",
        "title": "Geo-R1: Unlocking VLM Geospatial Reasoning with Cross-View Reinforcement Learning",
        "author": [
            "Chenhui Xu",
            "Fuxun Yu",
            "Michael J. Bianco",
            "Jacob Kovarskiy",
            "Raphael Tang",
            "Qi Zhang",
            "Zirui Xu",
            "Will LeVine",
            "Brandon Dubbs",
            "Heming Liao",
            "Cassandra Burgess",
            "Suvam Bag",
            "Jay Patravali",
            "Rupanjali Kukal",
            "Mikael Figueroa",
            "Rishi Madhok",
            "Nikolaos Karianakis",
            "Jinjun Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00072",
        "abstract": "We introduce Geo-R1, a reasoning-centric post-training framework that unlocks geospatial reasoning in vision-language models by combining thinking scaffolding and elevating. In the scaffolding stage, Geo-R1 instills a ``geospatial thinking paradigm\" via supervised fine-tuning on synthetic chain-of-thought exemplars, enabling models to connect visual cues with geographic priors without costly human reasoning annotations. In the elevating stage, it uses GRPO-based reinforcement learning on a weakly-supervised cross-view pairing proxy. This design supplies a verifiable and scalable reward signal: teaching models to capture and reconcile features across modalities, and harnessing reasoning for accurate prediction. Geo-R1 extends geospatial modeling from domain pretraining / supervised finetuning to reasoning-first post-training, and achieves state-of-the-art performance across various geospatial reasoning benchmarks. Our model is available at https://huggingface.co/miniHui/Geo-R1.",
        "tags": [
            "CoT",
            "GRPO",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "22",
        "title": "Adaptive and Resource-efficient Agentic AI Systems for Mobile and Embedded Devices: A Survey",
        "author": [
            "Sicong Liu",
            "Weiye Wu",
            "Xiangrui Xu",
            "Teng Li",
            "Bowen Pang",
            "Bin Guo",
            "Zhiwen Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00078",
        "abstract": "Foundation models have reshaped AI by unifying fragmented architectures into scalable backbones with multimodal reasoning and contextual adaptation. In parallel, the long-standing notion of AI agents, defined by the sensing-decision-action loop, is entering a new paradigm: with FMs as their cognitive core, agents transcend rule-based behaviors to achieve autonomy, generalization, and self-reflection. This dual shift is reinforced by real-world demands such as autonomous driving, robotics, virtual assistants, and GUI agents, as well as ecosystem advances in embedded hardware, edge computing, mobile deployment platforms, and communication protocols that together enable large-scale deployment. Yet this convergence collides with reality: while applications demand long-term adaptability and real-time interaction, mobile and edge deployments remain constrained by memory, energy, bandwidth, and latency. This creates a fundamental tension between the growing complexity of FMs and the limited resources of deployment environments. This survey provides the first systematic characterization of adaptive, resource-efficient agentic AI systems. We summarize enabling techniques into elastic inference, test-time adaptation, dynamic multimodal integration, and agentic AI applications, and identify open challenges in balancing accuracy-latency-communication trade-offs and sustaining robustness under distribution shifts. We further highlight future opportunities in algorithm-system co-design, cognitive adaptation, and collaborative edge deployment. By mapping FM structures, cognition, and hardware resources, this work establishes a unified perspective toward scalable, adaptive, and resource-efficient agentic AI. We believe this survey can help readers to understand the connections between enabling technologies while promoting further discussions on the fusion of agentic intelligence and intelligent agents.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "23",
        "title": "Directed Information $Î³$-covering: An Information-Theoretic Framework for Context Engineering",
        "author": [
            "Hai Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00079",
        "abstract": "We introduce \\textbf{Directed Information $\\gamma$-covering}, a simple but general framework for redundancy-aware context engineering. Directed information (DI), a causal analogue of mutual information, measures asymmetric predictiveness between chunks. If $\\operatorname{DI}_{i \\to j} \\ge H(C_j) - \\gamma$, then $C_i$ suffices to represent $C_j$ up to $\\gamma$ bits. Building on this criterion, we formulate context selection as a $\\gamma$-cover problem and propose a greedy algorithm with provable guarantees: it preserves query information within bounded slack, inherits $(1+\\ln n)$ and $(1-1/e)$ approximations from submodular set cover, and enforces a diversity margin. Importantly, building the $\\gamma$-cover is \\emph{query-agnostic}: it incurs no online cost and can be computed once offline and amortized across all queries. Experiments on HotpotQA show that $\\gamma$-covering consistently improves over BM25, a competitive baseline, and provides clear advantages in hard-decision regimes such as context compression and single-slot prompt selection. These results establish DI $\\gamma$-covering as a principled, self-organizing backbone for modern LLM pipelines.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "24",
        "title": "Enhancing Certifiable Semantic Robustness via Robust Pruning of Deep Neural Networks",
        "author": [
            "Hanjiang Hu",
            "Bowei Li",
            "Ziwei Wang",
            "Tianhao Wei",
            "Casidhe Hutchison",
            "Eric Sample",
            "Changliu Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00083",
        "abstract": "Deep neural networks have been widely adopted in many vision and robotics applications with visual inputs. It is essential to verify its robustness against semantic transformation perturbations, such as brightness and contrast. However, current certified training and robustness certification methods face the challenge of over-parameterization, which hinders the tightness and scalability due to the over-complicated neural networks. To this end, we first analyze stability and variance of layers and neurons against input perturbation, showing that certifiable robustness can be indicated by a fundamental Unbiased and Smooth Neuron metric (USN). Based on USN, we introduce a novel neural network pruning method that removes neurons with low USN and retains those with high USN, thereby preserving model expressiveness without over-parameterization. To further enhance this pruning process, we propose a new Wasserstein distance loss to ensure that pruned neurons are more concentrated across layers. We validate our approach through extensive experiments on the challenging robust keypoint detection task, which involves realistic brightness and contrast perturbations, demonstrating that our method achieves superior robustness certification performance and efficiency compared to baselines.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "25",
        "title": "Judging by Appearances? Auditing and Intervening Vision-Language Models for Bail Prediction",
        "author": [
            "Sagnik Basu",
            "Shubham Prakash",
            "Ashish Maruti Barge",
            "Siddharth D Jaiswal",
            "Abhisek Dash",
            "Saptarshi Ghosh",
            "Animesh Mukherjee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00088",
        "abstract": "Large language models (LLMs) have been extensively used for legal judgment prediction tasks based on case reports and crime history. However, with a surge in the availability of large vision language models (VLMs), legal judgment prediction systems can now be made to leverage the images of the criminals in addition to the textual case reports/crime history. Applications built in this way could lead to inadvertent consequences and be used with malicious intent. In this work, we run an audit to investigate the efficiency of standalone VLMs in the bail decision prediction task. We observe that the performance is poor across multiple intersectional groups and models \\textit{wrongly deny bail to deserving individuals with very high confidence}. We design different intervention algorithms by first including legal precedents through a RAG pipeline and then fine-tuning the VLMs using innovative schemes. We demonstrate that these interventions substantially improve the performance of bail prediction. Our work paves the way for the design of smarter interventions on VLMs in the future, before they can be deployed for real-world legal judgment prediction.",
        "tags": [
            "LLM",
            "RAG",
            "VLM"
        ]
    },
    {
        "id": "26",
        "title": "Direct Token Optimization: A Self-contained Approach to Large Language Model Unlearning",
        "author": [
            "Hong kyu Lee",
            "Ruixuan Liu",
            "Li Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00125",
        "abstract": "Machine unlearning is an emerging technique that removes the influence of a subset of training data (forget set) from a model without full retraining, with applications including privacy protection, content moderation, and model correction. The key challenge lies in ensuring that the model completely forgets the knowledge of the forget set without compromising its overall utility. Existing unlearning methods for large language models (LLMs) often utilize auxiliary language models, retain datasets, or even commercial AI services for effective unlearning and maintaining the model utility. However, dependence on these external resources is often impractical and could potentially introduce additional privacy risks. In this work, we propose direct token optimization (DTO), a novel self-contained unlearning approach for LLMs that directly optimizes the token level objectives and eliminates the need for external resources. Given a sequence to unlearn, we identify two categories of tokens: target tokens, which capture critical knowledge for unlearning, and the remaining non-target tokens, which are crucial for maintaining the model utility. The former are used to optimize the unlearning objective, while the latter serve to preserve the model's performance. The experimental results show that the proposed DTO achieves up to 16.8$\\times$ improvement in forget quality on several benchmark datasets than the latest baselines while maintaining a comparable level of model utility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "27",
        "title": "BigBang-Proton Technical Report: Next-Word-Prediction is Scientific Multitask Learner",
        "author": [
            "Hengkui Wu",
            "Liujiang Liu",
            "Jihua He",
            "Qihao Wang",
            "Keke Zhao",
            "Shuyang Hu",
            "Renle Fu",
            "Dahao Liang",
            "Lingyu Zeng",
            "Bruce Liu",
            "Yuan Liu",
            "Jin Zhan",
            "Jiaqiang Niu",
            "Xinglong Jia",
            "Yaqin Hu",
            "Wenjun Ji",
            "Panpan Chi",
            "Ken Chen",
            "Hengyuan Wu",
            "Yingsi Xin",
            "Yongfeng Zhu",
            "Yuexin Wang",
            "Manqi Ruan",
            "Ningtao Bian",
            "Xiaohua Wu",
            "Weipeng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00129",
        "abstract": "We introduce BigBang-Proton, a unified sequence-based architecture for auto-regressive language modeling pretrained on cross-scale, cross-structure, cross-discipline real-world scientific tasks to construct a scientific multi-task learner. BigBang-Proton incorporates three fundamental innovations compared to mainstream general-purpose LLMs: Theory-Experiment Learning paradigm aligns large-scale numerical experimental data with theoretical text corpora; Binary Patch Encoding replaces byte pair encoding(BPE) tokenization; Monte Carlo Attention substitutes traditional transformer architectures. Through next-word-prediction pretraining on cross-discipline scientific datasets of real-world problems mixed with general textual corpus, followed by fine-tuning and inference on downstream tasks, BigBang-Proton demonstrates 100\\% accuracy in up to 50-digit arithmetic addition operations, performance on par with leading specialized models in particle physics jet tagging, matching MAE of specialized models in inter-atomic potential simulation, performance comparable to traditional spatiotemporal models in water quality prediction, and benchmark-exceeding performance in genome modeling. These results prove that language-guided scientific computing can match or exceed the performance of task-specific scientific models while maintaining multitask learning capabilities. We further hypothesize to scale the pretraining to the universe scale as a fundamental step toward developing material world foundational model.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "28",
        "title": "Large Language Models Inference Engines based on Spiking Neural Networks",
        "author": [
            "Adarsha Balaji",
            "Sandeep Madireddy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00133",
        "abstract": "Foundational models based on the transformer architecture are currently the state-of-the-art in general language modeling, as well as in scientific areas such as material science and climate. However, training and deploying these models is computationally challenging as the time and space complexity has a quadratic relation to the input sequence length. Several efforts exploring efficient computational paradigms and model architectures to address these limitations have been made. In this work, we explore spiking neural networks (SNNs) to design transformer models. A challenge in training large-scale SNNs, using existing surrogate learning methods is inefficient and time-consuming. On the other hand, techniques to convert existing transformer-based models to their SNN equivalent are not scalable, as achieving optimal performance comes at the cost of a large number of spike time-steps, i.e. increased latency. To address this, we propose NeurTransformer, a methodology for designing transformer-based SNN for inference using a supervised fine-tuning approach with existing conversion methods. The proposed methodology works by: (1) replacing the self-attention mechanism with a spike-based self-attention (SSA), (2) converting the feed-forward block of the trained transformer model to its equivalent SNN, and (3) fine-tuning the SSA block using SNN-based surrogate learning algorithms. We benchmark the proposed methodology and demonstrate its accuracy and scalability using three variants of the GPT-2 model of increasing model size. We observe that the converted GPT-2 small models demonstrate a 5-12% loss in cosine similarity and a 9.7% reduction in perplexity. Finally, we demonstrate the energy efficiency of the SSA block compared to the ASA block and show between 64.71% and 85.28% reductions in estimated energy consumption when implementing the self-attention mechanism on a digital hardware.",
        "tags": [
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "29",
        "title": "HLTCOE at TREC 2024 NeuCLIR Track",
        "author": [
            "Eugene Yang",
            "Dawn Lawrie",
            "Orion Weller",
            "James Mayfield"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00143",
        "abstract": "The HLTCOE team applied PLAID, an mT5 reranker, GPT-4 reranker, score fusion, and document translation to the TREC 2024 NeuCLIR track. For PLAID we included a variety of models and training techniques -- Translate Distill (TD), Generate Distill (GD) and multi-lingual translate-distill (MTD). TD uses scores from the mT5 model over English MS MARCO query-document pairs to learn how to score query-document pairs where the documents are translated to match the CLIR setting. GD follows TD but uses passages from the collection and queries generated by an LLM for training examples. MTD uses MS MARCO translated into multiple languages, allowing experiments on how to batch the data during training. Finally, for report generation we experimented with system combination over different runs. One family of systems used either GPT-4o or Claude-3.5-Sonnet to summarize the retrieved results from a series of decomposed sub-questions. Another system took the output from those two models and verified/combined them with Claude-3.5-Sonnet. The other family used GPT4o and GPT3.5Turbo to extract and group relevant facts from the retrieved documents based on the decomposed queries. The resulting submissions directly concatenate the grouped facts to form the report and their documents of origin as the citations. The team submitted runs to all NeuCLIR tasks: CLIR and MLIR news tasks as well as the technical documents task and the report generation task.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "30",
        "title": "Which Rewards Matter? Reward Selection for Reinforcement Learning under Limited Feedback",
        "author": [
            "Shreyas Chaudhari",
            "Renhao Zhang",
            "Philip S. Thomas",
            "Bruno Castro da Silva"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00144",
        "abstract": "The ability of reinforcement learning algorithms to learn effective policies is determined by the rewards available during training. However, for practical problems, obtaining large quantities of reward labels is often infeasible due to computational or financial constraints, particularly when relying on human feedback. When reinforcement learning must proceed with limited feedback -- only a fraction of samples get rewards labeled -- a fundamental question arises: which samples should be labeled to maximize policy performance? We formalize this problem of reward selection for reinforcement learning from limited feedback (RLLF), introducing a new problem formulation that facilitates the study of strategies for selecting impactful rewards. Two types of selection strategies are investigated: (i) heuristics that rely on reward-free information such as state visitation and partial value functions, and (ii) strategies pre-trained using auxiliary evaluative feedback. We find that critical subsets of rewards are those that (1) guide the agent along optimal trajectories, and (2) support recovery toward near-optimal behavior after deviations. Effective selection methods yield near-optimal policies with significantly fewer reward labels than full supervision, establishing reward selection as a powerful paradigm for scaling reinforcement learning in feedback-limited settings.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "31",
        "title": "RoboPilot: Generalizable Dynamic Robotic Manipulation with Dual-thinking Modes",
        "author": [
            "Xinyi Liu",
            "Mohammadreza Fani Sani",
            "Zewei Zhou",
            "Julius Wirbel",
            "Bahram Zarrin",
            "Roberto Galeazzi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00154",
        "abstract": "Despite rapid progress in autonomous robotics, executing complex or long-horizon tasks remains a fundamental challenge. Most current approaches follow an open-loop paradigm with limited reasoning and no feedback, resulting in poor robustness to environmental changes and severe error accumulation. We present RoboPilot, a dual-thinking closed-loop framework for robotic manipulation that supports adaptive reasoning for complex tasks in real-world dynamic environments. RoboPilot leverages primitive actions for structured task planning and flexible action generation, while introducing feedback to enable replanning from dynamic changes and execution errors. Chain-of-Thought reasoning further enhances high-level task planning and guides low-level action generation. The system dynamically switches between fast and slow thinking to balance efficiency and accuracy. To systematically evaluate the robustness of RoboPilot in diverse robot manipulation scenarios, we introduce RoboPilot-Bench, a benchmark spanning 21 tasks across 10 categories, including infeasible-task recognition and failure recovery. Experiments show that RoboPilot outperforms state-of-the-art baselines by 25.9\\% in task success rate, and the real-world deployment on an industrial robot further demonstrates its robustness in real-world settings.",
        "tags": [
            "CoT",
            "Robotics"
        ]
    },
    {
        "id": "32",
        "title": "TAMA: Tool-Augmented Multimodal Agent for Procedural Activity Understanding",
        "author": [
            "Kimihiro Hasegawa",
            "Wiradee Imrattanatrai",
            "Masaki Asada",
            "Ken Fukuda",
            "Teruko Mitamura"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00161",
        "abstract": "Procedural activity assistants potentially support humans in a variety of settings, from our daily lives, e.g., cooking or assembling flat-pack furniture, to professional situations, e.g., manufacturing or biological experiments. Despite its potential use cases, the system development tailored for such an assistant is still underexplored. In this paper, we propose a novel framework, called TAMA, a Tool-Augmented Multimodal Agent, for procedural activity understanding. TAMA enables interleaved multimodal reasoning by making use of multimedia-returning tools in a training-free setting. Our experimental result on the multimodal procedural QA dataset, ProMQA-Assembly, shows that our approach can improve the performance of vision-language models, especially GPT-5 and MiMo-VL. Furthermore, our ablation studies provide empirical support for the effectiveness of two features that characterize our framework, multimedia-returning tools and agentic flexible tool selection. We believe our proposed framework and experimental results facilitate the thinking with images paradigm for video and multimodal tasks, let alone the development of procedural activity assistants.",
        "tags": [
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "33",
        "title": "DRBench: A Realistic Benchmark for Enterprise Deep Research",
        "author": [
            "Amirhossein Abaskohi",
            "Tianyi Chen",
            "Miguel MuÃ±oz-MÃ¡rmol",
            "Curtis Fox",
            "Amrutha Varshini Ramesh",
            "Ãtienne Marcotte",
            "Xing Han LÃ¹",
            "Nicolas Chapados",
            "Spandana Gella",
            "Christopher Pal",
            "Alexandre Drouin",
            "Issam H. Laradji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00172",
        "abstract": "We introduce DRBench, a benchmark for evaluating AI agents on complex, open-ended deep research tasks in enterprise settings. Unlike prior benchmarks that focus on simple questions or web-only queries, DRBench evaluates agents on multi-step queries (for example, ``What changes should we make to our product roadmap to ensure compliance with this standard?\") that require identifying supporting facts from both the public web and private company knowledge base. Each task is grounded in realistic user personas and enterprise context, spanning a heterogeneous search space that includes productivity software, cloud file systems, emails, chat conversations, and the open web. Tasks are generated through a carefully designed synthesis pipeline with human-in-the-loop verification, and agents are evaluated on their ability to recall relevant insights, maintain factual accuracy, and produce coherent, well-structured reports. We release 15 deep research tasks across 10 domains, such as Sales, Cybersecurity, and Compliance. We demonstrate the effectiveness of DRBench by evaluating diverse DR agents across open- and closed-source models (such as GPT, Llama, and Qwen) and DR strategies, highlighting their strengths, weaknesses, and the critical path for advancing enterprise deep research. Code is available at https://github.com/ServiceNow/drbench.",
        "tags": [
            "GPT",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "34",
        "title": "Personalized Reasoning: Just-In-Time Personalization and Why LLMs Fail At It",
        "author": [
            "Shuyue Stella Li",
            "Avinandan Bose",
            "Faeze Brahman",
            "Simon Shaolei Du",
            "Pang Wei Koh",
            "Maryam Fazel",
            "Yulia Tsvetkov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00177",
        "abstract": "Current large language model (LLM) development treats task-solving and preference alignment as separate challenges, optimizing first for objective correctness, then for alignment to aggregated human preferences. This paradigm fails in human-facing applications where solving a problem correctly is insufficient if the response mismatches the user's needs. This challenge intensifies in just-in-time scenarios where no prior user interaction history exists due to cold-start conditions or privacy constraints. LLMs need to identify what they don't know about user preferences, strategically elicit preference values through questioning, then adapt their reasoning processes and responses accordingly -- a complicated chain of cognitive processes which we term personalized reasoning. We introduce PREFDISCO, an evaluation methodology that transforms static benchmarks into interactive personalization tasks using psychologically-grounded personas with sparse preferences. Our framework creates scenarios where identical questions require different reasoning chains depending on user context, as optimal explanation approaches vary by individual expertise and preferences while maintaining factual accuracy. Evaluation of 21 frontier models across 10 tasks reveals 29.0% of naive personalization attempts produce worse preference alignment than generic responses, yet generic responses also fail to serve individual user needs effectively. These findings suggest personalized reasoning requires dedicated development rather than emerging naturally. PREFDISCO establishes personalized reasoning as a measurable research frontier and reveals fundamental limitations in current LLMs' interactive capabilities, providing a foundation for developing systems that can adapt to individual users in education, healthcare, and technical domains where personalization is critical.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "35",
        "title": "A Systematic Study of Large Language Models for Task and Motion Planning With PDDLStream",
        "author": [
            "Jorge Mendez-Mendez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00182",
        "abstract": "Using large language models (LLMs) to solve complex robotics problems requires understanding their planning capabilities. Yet while we know that LLMs can plan on some problems, the extent to which these planning capabilities cover the space of robotics tasks is unclear. One promising direction is to integrate the semantic knowledge of LLMs with the formal reasoning of task and motion planning (TAMP). However, the myriad of choices for how to integrate LLMs within TAMP complicates the design of such systems. We develop 16 algorithms that use Gemini 2.5 Flash to substitute key TAMP components. Our zero-shot experiments across 4,950 problems and three domains reveal that the Gemini-based planners exhibit lower success rates and higher planning times than their engineered counterparts. We show that providing geometric details increases the number of task-planning errors compared to pure PDDL descriptions, and that (faster) non-reasoning LLM variants outperform (slower) reasoning variants in most cases, since the TAMP system can direct the LLM to correct its mistakes.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "36",
        "title": "Lattica: A Decentralized Cross-NAT Communication Framework for Scalable AI Inference and Training",
        "author": [
            "Ween Yang",
            "Jason Liu",
            "Suli Wang",
            "Xinyuan Song",
            "Lynn Ai",
            "Eric Yang",
            "Tianyu Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00183",
        "abstract": "The rapid expansion of distributed Artificial Intelligence (AI) workloads beyond centralized data centers creates a demand for new communication substrates. These substrates must operate reliably in heterogeneous and permissionless environments, where Network Address Translators (NATs) and firewalls impose significant constraints. Existing solutions, however, are either designed for controlled data center deployments or implemented as monolithic systems that tightly couple machine learning logic with networking code. To address these limitations, we present Lattica, a decentralized cross-NAT communication framework designed to support distributed AI systems. Lattica integrates three core components. First, it employs a robust suite of NAT traversal mechanisms to establish a globally addressable peer-to-peer mesh. Second, it provides a decentralized data store based on Conflict-free Replicated Data Types (CRDTs), ensuring verifiable and eventually consistent state replication. Third, it incorporates a content discovery layer that leverages distributed hash tables (DHTs) together with an optimized RPC protocol for efficient model synchronization. By integrating these components, Lattica delivers a complete protocol stack for sovereign, resilient, and scalable AI systems that operate independently of centralized intermediaries. It is directly applicable to edge intelligence, collaborative reinforcement learning, and other large-scale distributed machine learning scenarios.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "37",
        "title": "Why Can't Transformers Learn Multiplication? Reverse-Engineering Reveals Long-Range Dependency Pitfalls",
        "author": [
            "Xiaoyan Bai",
            "Itamar Pres",
            "Yuntian Deng",
            "Chenhao Tan",
            "Stuart Shieber",
            "Fernanda ViÃ©gas",
            "Martin Wattenberg",
            "Andrew Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00184",
        "abstract": "Language models are increasingly capable, yet still fail at a seemingly simple task of multi-digit multiplication. In this work, we study why, by reverse-engineering a model that successfully learns multiplication via \\emph{implicit chain-of-thought}, and report three findings: (1) Evidence of long-range structure: Logit attributions and linear probes indicate that the model encodes the necessary long-range dependencies for multi-digit multiplication. (2) Mechanism: the model encodes long-range dependencies using attention to construct a directed acyclic graph to ``cache'' and ``retrieve'' pairwise partial products. (3) Geometry: the model implements partial products in attention heads by forming Minkowski sums between pairs of digits, and digits are represented using a Fourier basis, both of which are intuitive and efficient representations that the standard fine-tuning model lacks. With these insights, we revisit the learning dynamics of standard fine-tuning and find that the model converges to a local optimum that lacks the required long-range dependencies. We further validate this understanding by introducing an auxiliary loss that predicts the ``running sum'' via a linear regression probe, which provides an inductive bias that enables the model to successfully learn multi-digit multiplication. In summary, by reverse-engineering the mechanisms of an implicit chain-of-thought model we uncover a pitfall for learning long-range dependencies in Transformers and provide an example of how the correct inductive bias can address this issue.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "38",
        "title": "Thinkquel: A Model Dedicated to Text-to-dbt Using Synthetic Data and a Span-Aware Objective",
        "author": [
            "Anni Li",
            "Aria Attar",
            "Paul Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00186",
        "abstract": "Transforming natural-language requests into reliable, production-ready data transformations remains challenging: correctness depends on precise schema linking and warehouse-specific SQL dialects, while the strongest supervision available during training--execution success and result matching--are provided only at the sequence level. At the same time, assembling large, execution-validated corpora is costly, and token-level objectives misalign with these global signals, yielding unstable optimization and limited portability. We introduce Thinkquel, a fine-tuned model for producing robust, portable, and execution-validated database queries. Methodologies in Thinkquel integrates a novel synthetic data pipeline, TS-SQL, that leverages dbt as a portable intermediate representation with a span-aware reinforcement learning objective, and Token-Sequence GRPO (TS-GRPO), specifically designed to bridge the gap between token-level training signals and sequence-level execution rewards when finetuning LLMs. On the 500-example TS-SQL test set, Thinkquel (32B) reaches 93.2\\% execution success and 61.8\\% exact-result match with a two-stage SFT curriculum, improving over the base model by 67.2\\% (exec.) and 44.4\\% (match). In Spider (14B) experiments, TS-GRPO increases training stability and speeds convergence of the execution-match reward relative to GRPO and GSPO.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "39",
        "title": "A Novel Robust Control Method Combining DNN-Based NMPC Approximation and PI Control: Application to Exoskeleton Squat Movements",
        "author": [
            "Alireza Aliyari",
            "Gholamreza Vossoughi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00188",
        "abstract": "Nonlinear Model Predictive Control (NMPC) is a precise controller, but its heavy computational load often prevents application in robotic systems. Some studies have attempted to approximate NMPC using deep neural networks (NMPC-DNN). However, in the presence of unexpected disturbances or when operating conditions differ from training data, this approach lacks robustness, leading to large tracking errors. To address this issue, for the first time, the NMPC-DNN output is combined with a PI controller (Hybrid NMPC-DNN-PI). The proposed controller is validated by applying it to an exoskeleton robot during squat movement, which has a complex dynamic model and has received limited attention regarding robust nonlinear control design. A human-robot dynamic model with three active joints (ankle, knee, hip) is developed, and more than 5.3 million training samples are used to train the DNN. The results show that, under unseen conditions for the DNN, the tracking error in Hybrid NMPC-DNN-PI is significantly lower compared to NMPC-DNN. Moreover, human joint torques are greatly reduced with the use of the exoskeleton, with RMS values for the studied case reduced by 30.9%, 41.8%, and 29.7% at the ankle, knee, and hip, respectively. In addition, the computational cost of Hybrid NMPC-DNN-PI is 99.93% lower than that of NMPC.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "40",
        "title": "PrunedLoRA: Robust Gradient-Based structured pruning for Low-rank Adaptation in Fine-tuning",
        "author": [
            "Xin Yu",
            "Cong Xie",
            "Ziyu Zhao",
            "Tiantian Fan",
            "Lingzhou Xue",
            "Zhi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00192",
        "abstract": "Low-rank adaptation (LoRA) has become a widely used paradigm for parameter-efficient fine-tuning of large language models, yet its representational capacity often lags behind full fine-tuning. Within the context of LoRA, a key open question is how to obtain expressive low-rank adapters from over-parameterized spaces. We propose \\textit{PrunedLoRA}, a new framework that leverages structured pruning to obtain highly representative low-rank adapters from an over-parameterized initialization. Unlike prior approaches that impose a fixed low-rank budget, PrunedLoRA dynamically prunes less important components during fine-tuning and prevents their reactivation, enabling flexible and adaptive rank allocation. For structured pruning, by minimizing the pruning error for overall loss, we provide fine-grained pruning and recovery updates in a gradient-based pruning strategy with grounded interpretation. We provide the first theoretical analysis of the robustness of structured pruning and provably show that under the impact of weight perturbation, gradient-based pruning is more robust than activation-based pruning with respect to overall loss. Empirically, PrunedLoRA consistently outperforms LoRA and its variants across supervised fine-tuning tasks in mathematical reasoning, code generation, and natural language understanding, and it also demonstrates advantages over existing structured pruning methods across diverse sparsity levels.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "41",
        "title": "GRPO-$Î»$: Credit Assignment improves LLM Reasoning",
        "author": [
            "Prasanna Parthasarathi",
            "Mathieu Reymond",
            "Boxing Chen",
            "Yufei Cui",
            "Sarath Chandar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00194",
        "abstract": "Large language models (LLMs) are increasingly deployed for tasks requiring complex reasoning, prompting significant interest in improving their reasoning abilities through post-training. Especially RL based methods using verifiable reward, like the state-of-the-art GRPO, have shown to tremendously improve reasoning behaviors when applied as post-training methods. However, the lack of an explicit reward or critic model limits GRPO's ability to assign fine-grained credit across token sequences. In this work, we present GRPO-$\\lambda$, a novel extension to GRPO that enhances credit assignment in RL finetuning of LLMs for complex reasoning tasks. We approximate learning from $\\lambda$-return with a reformulation of eligibility traces using token-level log-probabilities applied after each sequence generation, and a novel critic-free approximation of the temporal-difference error. We introduce a few variations for the weighting of the $\\lambda$-return, and their applications to the eligibility-trace, where all the variations provide significant gains over GRPO. We compare GRPO-$\\lambda$ against GRPO by training models from 1.5B to 7B parameters on $4$ different math reasoning datasets. The training plots demonstrate 30-40% improved performance during RL training on both LLaMA-3.1 and Qwen-2.5 architectures. Finally, we show that with GRPO-$\\lambda$, the resulting average performance on AIME24, Math500, OlympiadMath, MinervaMath, and AMC improves over GRPO by over $3$ points and a $4.5$ points improvement on the 7B model.",
        "tags": [
            "GRPO",
            "LLM",
            "LLaMA",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": "42",
        "title": "RouterArena: An Open Platform for Comprehensive Comparison of LLM Routers",
        "author": [
            "Yifan Lu",
            "Rixin Liu",
            "Jiayi Yuan",
            "Xingqi Cui",
            "Shenrun Zhang",
            "Hongyi Liu",
            "Jiarong Xing"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00202",
        "abstract": "Today's LLM ecosystem comprises a wide spectrum of models that differ in size, capability, and cost. No single model is optimal for all scenarios; hence, LLM routers have become essential for selecting the most appropriate model under varying circumstances. However, the rapid emergence of various routers makes choosing the right one increasingly challenging. To address this problem, we need a comprehensive router comparison and a standardized leaderboard, similar to those available for models. In this work, we introduce RouterArena, the first open platform enabling comprehensive comparison of LLM routers. RouterArena has (1) a principally constructed dataset with broad knowledge domain coverage, (2) distinguishable difficulty levels for each domain, (3) an extensive list of evaluation metrics, and (4) an automated framework for leaderboard updates. Leveraging our framework, we have produced the initial leaderboard with detailed metrics comparison as shown in Figure 1. We will make our platform open to the public soon.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "43",
        "title": "LoRAFusion: Efficient LoRA Fine-Tuning for LLMs",
        "author": [
            "Zhanda Zhu",
            "Qidong Su",
            "Yaoyao Ding",
            "Kevin Song",
            "Shang Wang",
            "Gennady Pekhimenko"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00206",
        "abstract": "Low-Rank Adaptation (LoRA) has become the leading Parameter-Efficient Fine-Tuning (PEFT) method for Large Language Models (LLMs), as it significantly reduces GPU memory usage while maintaining competitive fine-tuned model quality on downstream tasks. Despite these benefits, we identify two key inefficiencies in existing LoRA fine-tuning systems. First, they incur substantial runtime overhead due to redundant memory accesses on large activation tensors. Second, they miss the opportunity to concurrently fine-tune multiple independent LoRA adapters that share the same base model on the same set of GPUs. This leads to missed performance gains such as reduced pipeline bubbles, better communication overlap, and improved GPU load balance.\nTo address these issues, we introduce LoRAFusion, an efficient LoRA fine-tuning system for LLMs. At the kernel level, we propose a graph-splitting method that fuses memory-bound operations. This design eliminates unnecessary memory accesses and preserves the performance of compute-bound GEMMs without incurring the cost of recomputation or synchronization. At the scheduling level, LoRAFusion introduces an adaptive batching algorithm for multi-job fine-tuning. It first splits LoRA adapters into groups to intentionally stagger batch execution across jobs, and then solves a bin-packing problem within each group to generate balanced, dependency-aware microbatches. LoRAFusion achieves up to $1.96\\times$ ($1.47\\times$ on average) end-to-end speedup compared to Megatron-LM, and up to $1.46\\times$ ($1.29\\times$ on average) improvement over mLoRA, the state-of-the-art multi-LoRA fine-tuning system. Our fused kernel achieves up to $1.39\\times$ ($1.27\\times$ on average) kernel performance improvement and can directly serve as a plug-and-play replacement in existing LoRA systems. We open-source LoRAFusion at https://github.com/CentML/lorafusion.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "44",
        "title": "FlowMoE: A Scalable Pipeline Scheduling Framework for Distributed Mixture-of-Experts Training",
        "author": [
            "Yunqi Gao",
            "Bing Hu",
            "Mahdi Boloursaz Mashhadi",
            "A-Long Jin",
            "Yanfeng Zhang",
            "Pei Xiao",
            "Rahim Tafazolli",
            "Merouane Debbah"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00207",
        "abstract": "The parameter size of modern large language models (LLMs) can be scaled up via the sparsely-activated Mixture-of-Experts (MoE) technique to avoid excessive increase of the computational costs. To further improve training efficiency, pipelining computation and communication has become a promising solution for distributed MoE training. However, existing work primarily focuses on scheduling tasks within the MoE layer, such as expert computing and all-to-all (A2A) communication, while neglecting other key operations including multi-head attention (MHA) computing, gating, and all-reduce communication. In this paper, we propose FlowMoE, a scalable framework for scheduling multi-type task pipelines. First, FlowMoE constructs a unified pipeline to consistently scheduling MHA computing, gating, expert computing, and A2A communication. Second, FlowMoE introduces a tensor chunk-based priority scheduling mechanism to overlap the all-reduce communication with all computing tasks. We implement FlowMoE as an adaptive and generic framework atop PyTorch. Extensive experiments with 675 typical MoE layers and four real-world MoE models across two GPU clusters demonstrate that our proposed FlowMoE framework outperforms state-of-the-art MoE training frameworks, reducing training time by 13%-57%, energy consumption by 10%-39%, and memory usage by 7%-32%.",
        "tags": [
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "45",
        "title": "Directed-MAML: Meta Reinforcement Learning Algorithm with Task-directed Approximation",
        "author": [
            "Yang Zhang",
            "Huiwen Yan",
            "Mushuang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00212",
        "abstract": "Model-Agnostic Meta-Learning (MAML) is a versatile meta-learning framework applicable to both supervised learning and reinforcement learning (RL). However, applying MAML to meta-reinforcement learning (meta-RL) presents notable challenges. First, MAML relies on second-order gradient computations, leading to significant computational and memory overhead. Second, the nested structure of optimization increases the problem's complexity, making convergence to a global optimum more challenging. To overcome these limitations, we propose Directed-MAML, a novel task-directed meta-RL algorithm. Before the second-order gradient step, Directed-MAML applies an additional first-order task-directed approximation to estimate the effect of second-order gradients, thereby accelerating convergence to the optimum and reducing computational cost. Experimental results demonstrate that Directed-MAML surpasses MAML-based baselines in computational efficiency and convergence speed in the scenarios of CartPole-v1, LunarLander-v2 and two-vehicle intersection crossing. Furthermore, we show that task-directed approximation can be effectively integrated into other meta-learning algorithms, such as First-Order Model-Agnostic Meta-Learning (FOMAML) and Meta Stochastic Gradient Descent(Meta-SGD), yielding improved computational efficiency and convergence speed.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "46",
        "title": "Thoughtbubbles: an Unsupervised Method for Parallel Thinking in Latent Space",
        "author": [
            "Houjun Liu",
            "Shikhar Murty",
            "Christopher D. Manning",
            "RÃ³bert CsordÃ¡s"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00219",
        "abstract": "Current approaches for scaling inference-time compute in transformers rely on training them to emit explicit chain-of-thought tokens before producing an answer. While these methods are powerful, they are limited because they cannot be applied during pretraining and are limited to only serially-generated, natural-language verbalization to scale inference-time compute. In this work, we propose Thoughtbubbles, a transformer variant that natively performs parallel adaptive computation in latent space by learning to fork or delete residual streams. Thus, tokens that require a large amount of computation can form a \"bubble\" of cloned residuals in the middle of the network for additional thinking. Crucially, this behavior is learned during pretraining with only language modeling loss. Thoughtbubbles outperforms both standard decoder LMs as well as non-adaptive parallel computation approaches on OpenWebText and peS2o perplexity and in zero-shot evaluations such as HellaSwag and LAMBADA after pretraining across 150M to 772M parameter scales. The implicit nature of our method enables adaptive computation to be learned starting at pretraining time, paving the way to unify train and test-time behavior for reasoning models.",
        "tags": [
            "CoT",
            "Transformer"
        ]
    },
    {
        "id": "47",
        "title": "TGPO: Temporal Grounded Policy Optimization for Signal Temporal Logic Tasks",
        "author": [
            "Yue Meng",
            "Fei Chen",
            "Chuchu Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00225",
        "abstract": "Learning control policies for complex, long-horizon tasks is a central challenge in robotics and autonomous systems. Signal Temporal Logic (STL) offers a powerful and expressive language for specifying such tasks, but its non-Markovian nature and inherent sparse reward make it difficult to be solved via standard Reinforcement Learning (RL) algorithms. Prior RL approaches focus only on limited STL fragments or use STL robustness scores as sparse terminal rewards. In this paper, we propose TGPO, Temporal Grounded Policy Optimization, to solve general STL tasks. TGPO decomposes STL into timed subgoals and invariant constraints and provides a hierarchical framework to tackle the problem. The high-level component of TGPO proposes concrete time allocations for these subgoals, and the low-level time-conditioned policy learns to achieve the sequenced subgoals using a dense, stage-wise reward signal. During inference, we sample various time allocations and select the most promising assignment for the policy network to rollout the solution trajectory. To foster efficient policy learning for complex STL with multiple subgoals, we leverage the learned critic to guide the high-level temporal search via Metropolis-Hastings sampling, focusing exploration on temporally feasible solutions. We conduct experiments on five environments, ranging from low-dimensional navigation to manipulation, drone, and quadrupedal locomotion. Under a wide range of STL tasks, TGPO significantly outperforms state-of-the-art baselines (especially for high-dimensional and long-horizon cases), with an average of 31.6% improvement in task success rate compared to the best baseline. The code will be available at https://github.com/mengyuest/TGPO",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "48",
        "title": "DualTune: Decoupled Fine-Tuning for On-Device Agentic Systems",
        "author": [
            "Rohan Kadekodi",
            "Zhan Jin",
            "Keisuke Kamahori",
            "Yile Gu",
            "Sean Khatiri",
            "Noah H. Bayindirli",
            "Sergey Gorbunov",
            "Baris Kasikci"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00229",
        "abstract": "The deployment of Large Language Models (LLMs) as agentic orchestrators has revolutionized task automation, but the need for privacy-preserving, cost-effective solutions demands on-device inference capabilities. However, local LLMs consistently underperform compared to frontier models in tool calling scenarios, struggling with both tool selection from large tool sets and accurate argument generation for complex parameter structures. We introduce a methodology that disaggregates a tool-calling task into two distinct subtasks: tool selection and argument generation. We propose \"decoupled fine-tuning\", a novel post-training approach that employs LoRA fine-tuning to create dedicated LoRA adapters for tool selection and tool-specific argument generation using separate loss masking for each of the subtasks. Furthermore, we present DualTune, an inference framework that leverages the LoRA adapters created using decoupled fine-tuning to perform efficient agent orchestration with the help of local models on end-user devices. DualTune decomposes the tool-call generation step into tool selection and argument generation, and dynamically loads the corresponding LoRA adapters to generate tool calls. Additionally, DualTune implements hierarchical orchestration to restrict the number of tools required for tool selection. Our experiments on the MCP-Bench benchmark demonstrate that the Qwen-2.5-7B model trained using decoupled fine-tuning improves the tool calling accuracy of the base model by 46%, and outperforms other local reasoning, non-reasoning and fine-tuned models of similar size in all cases, and models that are 2x larger, in most cases.",
        "tags": [
            "LLM",
            "LoRA",
            "Qwen"
        ]
    },
    {
        "id": "49",
        "title": "The Pitfalls of KV Cache Compression",
        "author": [
            "Alex Chen",
            "Renato Geh",
            "Aditya Grover",
            "Guy Van den Broeck",
            "Daniel Israel"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00231",
        "abstract": "KV cache compression promises increased throughput and efficiency with negligible loss in performance. While the gains in throughput are indisputable and recent literature has indeed shown minimal degradation on particular benchmarks, in general the consequences of compression in realistic scenarios such as multi-instruction prompting have been insufficiently studied. In this paper, we identify several pitfalls practitioners should be aware of when deploying KV cache compressed LLMs. Importantly, we show that certain instructions degrade much more rapidly with compression, effectively causing them to be completely ignored by the LLM. As a practical example of that, we highlight system prompt leakage as a case study, empirically showing the impact of compression on leakage and general instruction following. We show several factors that play a role in prompt leakage: compression method, instruction order, and KV eviction bias. We then propose simple changes to KV cache eviction policies that can reduce the impact of these factors and improve the overall performance in multi-instruction tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "50",
        "title": "BiasFreeBench: a Benchmark for Mitigating Bias in Large Language Model Responses",
        "author": [
            "Xin Xu",
            "Xunzhi He",
            "Churan Zhi",
            "Ruizhe Chen",
            "Julian McAuley",
            "Zexue He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00232",
        "abstract": "Existing studies on bias mitigation methods for large language models (LLMs) use diverse baselines and metrics to evaluate debiasing performance, leading to inconsistent comparisons among them. Moreover, their evaluations are mostly based on the comparison between LLMs' probabilities of biased and unbiased contexts, which ignores the gap between such evaluations and real-world use cases where users interact with LLMs by reading model responses and expect fair and safe outputs rather than LLMs' probabilities. To enable consistent evaluation across debiasing methods and bridge this gap, we introduce BiasFreeBench, an empirical benchmark that comprehensively compares eight mainstream bias mitigation techniques (covering four prompting-based and four training-based methods) on two test scenarios (multi-choice QA and open-ended multi-turn QA) by reorganizing existing datasets into a unified query-response setting. We further introduce a response-level metric, Bias-Free Score, to measure the extent to which LLM responses are fair, safe, and anti-stereotypical. Debiasing performances are systematically compared and analyzed across key dimensions: the prompting vs. training paradigm, model size, and generalization of different training strategies to unseen bias types. We will publicly release our benchmark, aiming to establish a unified testbed for bias mitigation research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "51",
        "title": "Differentiable Autoencoding Neural Operator for Interpretable and Integrable Latent Space Modeling",
        "author": [
            "Siva Viknesh",
            "Amirhossein Arzani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00233",
        "abstract": "Scientific machine learning has enabled the extraction of physical insights from high-dimensional spatiotemporal flow data using linear and nonlinear dimensionality reduction techniques. Despite these advances, achieving interpretability within the latent space remains a challenge. To address this, we propose the DIfferentiable Autoencoding Neural Operator (DIANO), a deterministic autoencoding neural operator framework that constructs physically interpretable latent spaces for both dimensional and geometric reduction, with the provision to enforce differential governing equations directly within the latent space. Built upon neural operators, DIANO compresses high-dimensional input functions into a low-dimensional latent space via spatial coarsening through an encoding neural operator and subsequently reconstructs the original inputs using a decoding neural operator through spatial refinement. We assess DIANO's latent space interpretability and performance in dimensionality reduction against baseline models, including the Convolutional Neural Operator and standard autoencoders. Furthermore, a fully differentiable partial differential equation (PDE) solver is developed and integrated within the latent space, enabling the temporal advancement of both high- and low-fidelity PDEs, thereby embedding physical priors into the latent dynamics. We further investigate various PDE formulations, including the 2D unsteady advection-diffusion and the 3D Pressure-Poisson equation, to examine their influence on shaping the latent flow representations. Benchmark problems considered include flow past a 2D cylinder, flow through a 2D symmetric stenosed artery, and a 3D patient-specific coronary artery. These case studies demonstrate DIANO's capability to solve PDEs within a latent space that facilitates both dimensional and geometrical reduction while allowing latent interpretability.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "52",
        "title": "Debunk the Myth of SFT Generalization",
        "author": [
            "Xiaofeng Lin",
            "Hejian Sang",
            "Zhipeng Wang",
            "Xuezhou Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00237",
        "abstract": "A prevailing view holds that supervised fine-tuning (SFT) memorizes training data and fails to generalize, whereas reinforcement learning (RL) attains broader robustness. We revisit this claim through a systematic evaluation on two decision-making benchmarks, Sokoban and General Points, and arrive at a different conclusion. We show that much of SFT's perceived failure stems from frozen-prompt artifacts: when trained on fixed instruction templates, SFT models cling to training semantics rather than adapting to new ones. Introducing prompt diversity during training breaks this shortcut and yields strong generalization to unseen instruction variants without harming in-distribution performance. Beyond instruction shifts, we ask whether SFT can generalize to strictly harder tasks. Here, chain-of-thought (CoT) supervision provides an algorithmic scaffold that markedly improves transfer to more difficult regimes, such as larger Sokoban grids with additional boxes and arithmetic with out-of-distribution values or five-card compositions that increase combinatorial complexity. Finally, combining prompt diversity with CoT achieves the best of both worlds: robust generalization across both instruction-variant and difficulty-variant settings, matching or surpassing RL baselines on our benchmarks while retaining SFT's simplicity and stability. These findings challenge the narrative that SFT is inherently inferior to RL and support a data-centric perspective: with appropriately curated demonstrations, vanilla SFT can generalize as strongly as RL. Code reproducing the results in the paper can be found at: https://github.com/XiaofengLin7/debunking-sft-generalization.",
        "tags": [
            "CoT",
            "RL"
        ]
    },
    {
        "id": "53",
        "title": "SecureBERT 2.0: Advanced Language Model for Cybersecurity Intelligence",
        "author": [
            "Ehsan Aghaei",
            "Sarthak Jain",
            "Prashanth Arun",
            "Arjun Sambamoorthy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00240",
        "abstract": "Effective analysis of cybersecurity and threat intelligence data demands language models that can interpret specialized terminology, complex document structures, and the interdependence of natural language and source code. Encoder-only transformer architectures provide efficient and robust representations that support critical tasks such as semantic search, technical entity extraction, and semantic analysis, which are key to automated threat detection, incident triage, and vulnerability assessment. However, general-purpose language models often lack the domain-specific adaptation required for high precision. We present SecureBERT 2.0, an enhanced encoder-only language model purpose-built for cybersecurity applications. Leveraging the ModernBERT architecture, SecureBERT 2.0 introduces improved long-context modeling and hierarchical encoding, enabling effective processing of extended and heterogeneous documents, including threat reports and source code artifacts. Pretrained on a domain-specific corpus more than thirteen times larger than its predecessor, comprising over 13 billion text tokens and 53 million code tokens from diverse real-world sources, SecureBERT 2.0 achieves state-of-the-art performance on multiple cybersecurity benchmarks. Experimental results demonstrate substantial improvements in semantic search for threat intelligence, semantic analysis, cybersecurity-specific named entity recognition, and automated vulnerability detection in code within the cybersecurity domain.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "54",
        "title": "Can AI agents understand spoken conversations about data visualizations in online meetings?",
        "author": [
            "Rizul Sharma",
            "Tianyu Jiang",
            "Seokki Lee",
            "Jillian Aurisano"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00245",
        "abstract": "In this short paper, we present work evaluating an AI agent's understanding of spoken conversations about data visualizations in an online meeting scenario. There is growing interest in the development of AI-assistants that support meetings, such as by providing assistance with tasks or summarizing a discussion. The quality of this support depends on a model that understands the conversational dialogue. To evaluate this understanding, we introduce a dual-axis testing framework for diagnosing the AI agent's comprehension of spoken conversations about data. Using this framework, we designed a series of tests to evaluate understanding of a novel corpus of 72 spoken conversational dialogues about data visualizations. We examine diverse pipelines and model architectures, LLM vs VLM, and diverse input formats for visualizations (the chart image, its underlying source code, or a hybrid of both) to see how this affects model performance on our tests. Using our evaluation methods, we found that text-only input modalities achieved the best performance (96%) in understanding discussions of visualizations in online meetings.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "55",
        "title": "TASER: Translation Assessment via Systematic Evaluation and Reasoning",
        "author": [
            "Monishwaran Maheswaran",
            "Marco Carini",
            "Christian Federmann",
            "Tony Diaz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00255",
        "abstract": "We introduce TASER (Translation Assessment via Systematic Evaluation and Reasoning), a metric that uses Large Reasoning Models (LRMs) for automated translation quality assessment. TASER harnesses the explicit reasoning capabilities of LRMs to conduct systematic, step-by-step evaluation of translation quality. We evaluate TASER on the WMT24 Metrics Shared Task across both reference-based and reference-free scenarios, demonstrating state-of-the-art performance. In system-level evaluation, TASER achieves the highest soft pairwise accuracy in both reference-based and reference-free settings, outperforming all existing metrics. At the segment level, TASER maintains competitive performance with our reference-free variant ranking as the top-performing metric among all reference-free approaches. Our experiments reveal that structured prompting templates yield superior results with LRMs compared to the open-ended approaches that proved optimal for traditional LLMs. We evaluate o3, a large reasoning model from OpenAI, with varying reasoning efforts, providing insights into the relationship between reasoning depth and evaluation quality. The explicit reasoning process in LRMs offers interpretability and visibility, addressing a key limitation of existing automated metrics. Our results demonstrate that Large Reasoning Models show a measurable advancement in translation quality assessment, combining improved accuracy with transparent evaluation across diverse language pairs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "56",
        "title": "Delayed Attention Training Improves Length Generalization in Transformer--RNN Hybrids",
        "author": [
            "Buu Phan",
            "Reza Ebrahimi",
            "Sanjay Haresh",
            "Roland Memisevic"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00258",
        "abstract": "We study length generalization in sequence models on a composite problem involving both state tracking and associative recall. Prior work finds that recurrent networks handle state tracking well but struggle with recall, whereas Transformers excel at recall yet fail to extend state-tracking capabilities to longer sequences. Motivated by the complementary strengths of these architectures, we construct hybrid models integrating recurrent and attention-based components, and train them on the combined task to evaluate whether both capabilities can be preserved. Our results reveal that, in such hybrids, the Transformer component tends to exploit shortcut solutions, leading to poor length generalization. We identify this shortcut reliance as a key obstacle and propose a simple yet effective training strategy -- delaying the training of the attention layers -- that mitigates this effect and significantly improves length generalization performance. Our experiments show that this approach enables hybrid models to achieve near-perfect accuracy ($>90\\%$) on hybrid sequences three times longer than those used during training.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "57",
        "title": "Learning Energy-based Variational Latent Prior for VAEs",
        "author": [
            "Debottam Dutta",
            "Chaitanya Amballa",
            "Zhongweiyang Xu",
            "Yu-Lin Wei",
            "Romit Roy Choudhury"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00260",
        "abstract": "Variational Auto-Encoders (VAEs) are known to generate blurry and inconsistent samples. One reason for this is the \"prior hole\" problem. A prior hole refers to regions that have high probability under the VAE's prior but low probability under the VAE's posterior. This means that during data generation, high probability samples from the prior could have low probability under the posterior, resulting in poor quality data. Ideally, a prior needs to be flexible enough to match the posterior while retaining the ability to generate samples fast. Generative models continue to address this tradeoff. This paper proposes to model the prior as an energy-based model (EBM). While EBMs are known to offer the flexibility to match posteriors (and also improving the ELBO), they are traditionally slow in sample generation due to their dependency on MCMC methods. Our key idea is to bring a variational approach to tackle the normalization constant in EBMs, thus bypassing the expensive MCMC approaches. The variational form can be approximated with a sampler network, and we show that such an approach to training priors can be formulated as an alternating optimization problem. Moreover, the same sampler reduces to an implicit variational prior during generation, providing efficient and fast sampling. We compare our Energy-based Variational Latent Prior (EVaLP) method to multiple SOTA baselines and show improvements in image generation quality, reduced prior holes, and better sampling efficiency.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "58",
        "title": "Retrieval-Augmented Generation for Electrocardiogram-Language Models",
        "author": [
            "Xiaoyu Song",
            "William Han",
            "Tony Chen",
            "Chaojing Duan",
            "Michael A. Rosenberg",
            "Emerson Liu",
            "Ding Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00261",
        "abstract": "Interest in generative Electrocardiogram-Language Models (ELMs) is growing, as they can produce textual responses conditioned on ECG signals and textual queries. Unlike traditional classifiers that output label probabilities, ELMs are more versatile, supporting domain-specific tasks (e.g., waveform analysis, diagnosis, prognosis) as well as general tasks (e.g., open-ended questions, dialogue). Retrieval-Augmented Generation (RAG), widely used in Large Language Models (LLMs) to ground LLM outputs in retrieved knowledge, helps reduce hallucinations and improve natural language generation (NLG). However, despite its promise, no open-source implementation or systematic study of RAG pipeline design for ELMs currently exists. To address this gap, we present the first open-source RAG pipeline for ELMs, along with baselines and ablation studies for NLG. Experiments on three public datasets show that ELMs with RAG consistently improves performance over non-RAG baselines and highlights key ELM design considerations. Our code is available at: https://github.com/willxxy/ECG-Bench.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "59",
        "title": "Judging with Confidence: Calibrating Autoraters to Preference Distributions",
        "author": [
            "Zhuohang Li",
            "Xiaowei Li",
            "Chengyu Huang",
            "Guowang Li",
            "Katayoon Goshvadi",
            "Bo Dai",
            "Dale Schuurmans",
            "Paul Zhou",
            "Hamid Palangi",
            "Yiwen Song",
            "Palash Goyal",
            "Murat Kantarcioglu",
            "Bradley A. Malin",
            "Yuan Xue"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00263",
        "abstract": "The alignment of large language models (LLMs) with human values increasingly relies on using other LLMs as automated judges, or ``autoraters''. However, their reliability is limited by a foundational issue: they are trained on discrete preference labels, forcing a single ground truth onto tasks that are often subjective, ambiguous, or nuanced. We argue that a reliable autorater must learn to model the full distribution of preferences defined by a target population. In this paper, we propose a general framework for calibrating probabilistic autoraters to any given preference distribution. We formalize the problem and present two learning methods tailored to different data conditions: 1) a direct supervised fine-tuning for dense, probabilistic labels, and 2) a reinforcement learning approach for sparse, binary labels. Our empirical results show that finetuning autoraters with a distribution-matching objective leads to verbalized probability predictions that are better aligned with the target preference distribution, with improved calibration and significantly lower positional bias, all while preserving performance on objective tasks.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "60",
        "title": "Low Resource Audio Codec Challenge Baseline Systems",
        "author": [
            "Yusuf Ziya Isik",
            "RafaÅ Åaganowski"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00264",
        "abstract": "The Low-Resource Audio Codec (LRAC) Challenge aims to advance neural audio coding for deployment in resource-constrained environments. The first edition focuses on low-resource neural speech codecs that must operate reliably under everyday noise and reverberation, while satisfying strict constraints on computational complexity, latency, and bitrate. Track 1 targets transparency codecs, which aim to preserve the perceptual transparency of input speech under mild noise and reverberation. Track 2 addresses enhancement codecs, which combine coding and compression with denoising and dereverberation. This paper presents the official baseline systems for both tracks in the 2025 LRAC Challenge. The baselines are convolutional neural codec models with Residual Vector Quantization, trained end-to-end using a combination of adversarial and reconstruction objectives. We detail the data filtering and augmentation strategies, model architectures, optimization procedures, and checkpoint selection criteria.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "61",
        "title": "Efficient Layer-wise LLM Fine-tuning for Revision Intention Prediction",
        "author": [
            "Zhexiong Liu",
            "Diane Litman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00268",
        "abstract": "Large Language Models (LLMs) have shown extraordinary success across various text generation tasks; however, their potential for simple yet essential text classification remains underexplored, as LLM pre-training tends to emphasize generation over classification. While LLMs with instruction tuning can transform classification into a generation task, they often struggle to categorize nuanced texts. One such example is text revision, which involves nuanced edits between pairs of texts. Although simply fine-tuning LLMs for revision classification seems plausible, it requires a large amount of revision annotations, which are exceptionally expensive and scarce in the community. To address this issue, we introduce a plug-and-play layer-wise parameter-efficient fine-tuning (PEFT) framework, i.e., IR-Tuning, which fine-tunes a subset of important LLM layers that are dynamically selected based on their gradient norm distribution, while freezing those of redundant layers. Extensive experiments suggest that IR-Tuning surpasses several layer-wise PEFT baselines over diverse text revisions, while achieving fast convergence, low GPU memory consumption, and effectiveness on small revision corpora.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "62",
        "title": "MAGIC-MASK: Multi-Agent Guided Inter-Agent Collaboration with Mask-Based Explainability for Reinforcement Learning",
        "author": [
            "Maisha Maliha",
            "Dean Hougen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00274",
        "abstract": "Understanding the decision-making process of Deep Reinforcement Learning agents remains a key challenge for deploying these systems in safety-critical and multi-agent environments. While prior explainability methods like StateMask, have advanced the identification of critical states, they remain limited by computational cost, exploration coverage, and lack of adaptation to multi-agent settings. To overcome these limitations, we propose a mathematically grounded framework, MAGIC-MASK (Multi-Agent Guided Inter-agent Collaboration with Mask-Based Explainability for Reinforcement Learning), that extends perturbation-based explanation to Multi-Agent Reinforcement Learning. Our method integrates Proximal Policy Optimization, adaptive epsilon-greedy exploration, and lightweight inter-agent collaboration to share masked state information and peer experience. This collaboration enables each agent to perform saliency-guided masking and share reward-based insights with peers, reducing the time required for critical state discovery, improving explanation fidelity, and leading to faster and more robust learning. The core novelty of our approach lies in generalizing explainability from single-agent to multi-agent systems through a unified mathematical formalism built on trajectory perturbation, reward fidelity analysis, and Kullback-Leibler divergence regularization. This framework yields localized, interpretable explanations grounded in probabilistic modeling and multi-agent Markov decision processes. We validate our framework on both single-agent and multi-agent benchmarks, including a multi-agent highway driving environment and Google Research Football, demonstrating that MAGIC-MASK consistently outperforms state-of-the-art baselines in fidelity, learning efficiency, and policy robustness while offering interpretable and transferable explanations.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "63",
        "title": "SafePassage: High-Fidelity Information Extraction with Black Box LLMs",
        "author": [
            "Joe Barrow",
            "Raj Patel",
            "Misha Kharkovski",
            "Ben Davies",
            "Ryan Schmitt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00276",
        "abstract": "Black box large language models (LLMs) make information extraction (IE) easy to configure, but hard to trust. Unlike traditional information extraction pipelines, the information \"extracted\" is not guaranteed to be grounded in the document. To prevent this, this paper introduces the notion of a \"safe passage\": context generated by the LLM that is both grounded in the document and consistent with the extracted information. This is operationalized via a three-step pipeline, SafePassage, which consists of: (1) an LLM extractor that generates structured entities and their contexts from a document, (2) a string-based global aligner, and (3) a scoring model. Results show that using these three parts in conjunction reduces hallucinations by up to 85% on information extraction tasks with minimal risk of flagging non-hallucinations. High agreement between the SafePassage pipeline and human judgments of extraction quality mean that the pipeline can be dually used to evaluate LLMs. Surprisingly, results also show that using a transformer encoder fine-tuned on a small number of task-specific examples can outperform an LLM scoring model at flagging unsafe passages. These annotations can be collected in as little as 1-2 hours.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "64",
        "title": "o-MEGA: Optimized Methods for Explanation Generation and Analysis",
        "author": [
            "Ä½uboÅ¡ KriÅ¡",
            "Jaroslav KopÄan",
            "Qiwei Peng",
            "Andrej Ridzik",
            "Marcel VeselÃ½",
            "Martin Tamajka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00288",
        "abstract": "The proliferation of transformer-based language models has revolutionized NLP domain while simultaneously introduced significant challenges regarding model transparency and trustworthiness. The complexity of achieving explainable systems in this domain is evidenced by the extensive array of explanation methods and evaluation metrics developed by researchers. To address the challenge of selecting optimal explainability approaches, we present \\textbf{\\texttt{o-mega}}, a hyperparameter optimization tool designed to automatically identify the most effective explainable AI methods and their configurations within the semantic matching domain. We evaluate o-mega on a post-claim matching pipeline using a curated dataset of social media posts paired with refuting claims. Our tool systematically explores different explainable methods and their hyperparameters, demonstrating improved transparency in automated fact-checking systems. As a result, such automated optimization of explanation methods can significantly enhance the interpretability of claim-matching models in critical applications such as misinformation detection, contributing to more trustworthy and transparent AI systems.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "65",
        "title": "Free Draft-and-Verification: Toward Lossless Parallel Decoding for Diffusion Large Language Models",
        "author": [
            "Shutong Wu",
            "Jiawei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00294",
        "abstract": "Diffusion Large Language Models (DLLMs) have emerged as a new paradigm of language modeling beyond autoregressive next-token prediction. Thanks to their bidirectional attention mechanism, DLLMs are more capable of capturing the connection of context, and thus show unique advantages in challenges like the famous \"reversal curse\" or learning under data-constrained scenarios. However, this bidirectional nature also brings an obstacle that DLLMs are not inherently compatible with KV Cache, and consequently, the inference efficiency is not competitive compared with autoregressive models. Taking advantage of their inherent capability of multi-token prediction, existing parallel decoding algorithms can speed up the DLLM inference, but at the cost of non-negligible performance degradation. To overcome this challenge, we introduce Free Draft-and-Verification (Freedave), a novel fast sampling algorithm tailored for DLLMs that achieves lossless parallel decoding. Specifically, we propose a pipeline of parallel-decoded candidate generation and verification, which is guaranteed to reproduce the same sequence generated by static sampling, without introducing extra model forward calls. By applying Freedave, the throughput of DLLMs can be boosted up to $2.8\\times$ without performance degradation on math reasoning tasks.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "66",
        "title": "Beyond Token Probes: Hallucination Detection via Activation Tensors with ACT-ViT",
        "author": [
            "Guy Bar-Shalom",
            "Fabrizio Frasca",
            "Yaniv Galron",
            "Yftah Ziser",
            "Haggai Maron"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00296",
        "abstract": "Detecting hallucinations in Large Language Model-generated text is crucial for their safe deployment. While probing classifiers show promise, they operate on isolated layer-token pairs and are LLM-specific, limiting their effectiveness and hindering cross-LLM applications. In this paper, we introduce a novel approach to address these shortcomings. We build on the natural sequential structure of activation data in both axes (layers $\\times$ tokens) and advocate treating full activation tensors akin to images. We design ACT-ViT, a Vision Transformer-inspired model that can be effectively and efficiently applied to activation tensors and supports training on data from multiple LLMs simultaneously. Through comprehensive experiments encompassing diverse LLMs and datasets, we demonstrate that ACT-ViT consistently outperforms traditional probing techniques while remaining extremely efficient for deployment. In particular, we show that our architecture benefits substantially from multi-LLM training, achieves strong zero-shot performance on unseen datasets, and can be transferred effectively to new LLMs through fine-tuning. Full code is available at https://github.com/BarSGuy/ACT-ViT.",
        "tags": [
            "Detection",
            "LLM",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "67",
        "title": "ICL Optimized Fragility",
        "author": [
            "Serena Gomez Wannaz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00300",
        "abstract": "ICL guides are known to improve task-specific performance, but their impact on cross-domain cognitive abilities remains unexplored. This study examines how ICL guides affect reasoning across different knowledge domains using six variants of the GPT-OSS:20b model: one baseline model and five ICL configurations (simple, chain-of-thought, random, appended text, and symbolic language). The models were subjected to 840 tests spanning general knowledge questions, logic riddles, and a mathematical olympiad problem. Statistical analysis (ANOVA) revealed significant behavioral modifications (p less than 0.001) across ICL variants, demonstrating a phenomenon termed \"optimized fragility.\" ICL models achieved 91%-99% accuracy on general knowledge tasks while showing degraded performance on complex reasoning problems, with accuracy dropping to 10-43% on riddles compared to 43% for the baseline model. Notably, no significant differences emerged on the olympiad problem (p=0.2173), suggesting that complex mathematical reasoning remains unaffected by ICL optimization. These findings indicate that ICL guides create systematic trade-offs between efficiency and reasoning flexibility, with important implications for LLM deployment and AI safety.",
        "tags": [
            "CoT",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "68",
        "title": "BiasBusters: Uncovering and Mitigating Tool Selection Bias in Large Language Models",
        "author": [
            "Thierry Blankenstein",
            "Jialin Yu",
            "Zixuan Li",
            "Vassilis Plachouras",
            "Sunando Sengupta",
            "Philip Torr",
            "Yarin Gal",
            "Alasdair Paren",
            "Adel Bibi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00307",
        "abstract": "Agents backed by large language models (LLMs) often rely on external tools drawn from marketplaces where multiple providers offer functionally equivalent options. This raises a critical point concerning fairness: if selection is systematically biased, it can degrade user experience and distort competition by privileging some providers over others. We introduce a benchmark of diverse tool categories, each containing multiple functionally equivalent tools, to evaluate tool-selection bias. Using this benchmark, we test seven models and show that unfairness exists with models either fixating on a single provider or disproportionately preferring earlier-listed tools in context. To investigate the origins of this bias, we conduct controlled experiments examining tool features, metadata (name, description, parameters), and pre-training exposure. We find that: (1) semantic alignment between queries and metadata is the strongest predictor of choice; (2) perturbing descriptions significantly shifts selections; and (3) repeated pre-training exposure to a single endpoint amplifies bias. Finally, we propose a lightweight mitigation that first filters the candidate tools to a relevant subset and then samples uniformly, reducing bias while preserving good task coverage. Our findings highlight tool-selection bias as a key obstacle for the fair deployment of tool-augmented LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "69",
        "title": "CORTEX: Collaborative LLM Agents for High-Stakes Alert Triage",
        "author": [
            "Bowen Wei",
            "Yuan Shen Tay",
            "Howard Liu",
            "Jinhao Pan",
            "Kun Luo",
            "Ziwei Zhu",
            "Chris Jordan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00311",
        "abstract": "Security Operations Centers (SOCs) are overwhelmed by tens of thousands of daily alerts, with only a small fraction corresponding to genuine attacks. This overload creates alert fatigue, leading to overlooked threats and analyst burnout. Classical detection pipelines are brittle and context-poor, while recent LLM-based approaches typically rely on a single model to interpret logs, retrieve context, and adjudicate alerts end-to-end -- an approach that struggles with noisy enterprise data and offers limited transparency. We propose CORTEX, a multi-agent LLM architecture for high-stakes alert triage in which specialized agents collaborate over real evidence: a behavior-analysis agent inspects activity sequences, evidence-gathering agents query external systems, and a reasoning agent synthesizes findings into an auditable decision. To support training and evaluation, we release a dataset of fine-grained SOC investigations from production environments, capturing step-by-step analyst actions and linked tool outputs. Across diverse enterprise scenarios, CORTEX substantially reduces false positives and improves investigation quality over state-of-the-art single-agent LLMs.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "70",
        "title": "DiSC-AMC: Token- and Parameter-Efficient Discretized Statistics In-Context Automatic Modulation Classification",
        "author": [
            "Mohammad Rostami",
            "Atik Faysal",
            "Reihaneh Gh. Roshan",
            "Huaxia Wang",
            "Nikhil Muralidhar",
            "Yu-Dong Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00316",
        "abstract": "Large Language Models (LLMs) can perform Automatic Modulation Classification (AMC) in an open-set manner without LLM fine-tuning when equipped with carefully designed in-context prompts~\\cite{rostami2025plug}. Building on this prior work, we target the practical bottlenecks of long prompt contexts and large model sizes that impede in-the-loop deployment. We present Discretized Statistics in-Context Automatic Modulation Classification (DiSC-AMC), a token- and parameter-efficient variant that: (i) discretizes higher-order statistics and cumulants into compact symbolic tokens, (ii) prunes the exemplar list via a lightweight k-top neural prefilter and filters misleading/low-impact features using rationales extracted from prior LLM responses, and (iii) enforces label-only predictions through a calibrated prompt template. Together, these changes reduce both input/output tokens and the model parameter footprint by more than half while maintaining competitive accuracy. On synthetic AMC with ten modulation types under noise, a 7B \\textit{DeepSeek-R1-Distill-Qwen} baseline achieves 5.2% accuracy, whereas our system, using an approximately 5B-parameter \\textit{Gemini-2.5-Flash}~\\cite{comanici2025gemini} model, attains 45.5% accuracy. These results demonstrate that careful discretization and context selection can cut inference cost by over 2x while preserving the advantages of prompt-based AMC and enabling practical in-the-loop use.",
        "tags": [
            "DeepSeek",
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "71",
        "title": "Which Programming Language and Model Work Best With LLM-as-a-Judge For Code Retrieval?",
        "author": [
            "Lucas Roberts",
            "Denisa Roberts"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00324",
        "abstract": "Code search is an important information retrieval application. Benefits of better code search include faster new developer on-boarding, reduced software maintenance, and ease of understanding for large repositories. Despite improvements in search algorithms and search benchmarks, the domain of code search has lagged behind. One reason is the high cost of human annotation for code queries and answers. While humans may annotate search results in general text QA systems, code annotations require specialized knowledge of a programming language (PL), as well as domain specific software engineering knowledge. In this work we study the use of Large Language Models (LLMs) to retrieve code at the level of functions and to generate annotations for code search results. We compare the impact of the retriever representation (sparse vs. semantic), programming language, and LLM by comparing human annotations across several popular languages (C, Java, Javascript, Go, and Python). We focus on repositories that implement common data structures likely to be implemented in any PLs. For the same human annotations, we compare several LLM-as-a-Judge models to evaluate programming language and other affinities between LLMs. We find that the chosen retriever and PL exhibit affinities that can be leveraged to improve alignment of human and AI relevance determinations, with significant performance implications. We also find differences in representation (sparse vs. semantic) across PLs that impact alignment of human and AI relevance determinations. We propose using transpilers to bootstrap scalable code search benchmark datasets in other PLs and in a case study demonstrate that human-AI relevance agreement rates largely match the (worst case) human-human agreement under study. The application code used in this work is available at \\href{https://github.com/rlucas7/code-searcher/}{this github repo}.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "72",
        "title": "Reasoning-Aware Prompt Orchestration: A Foundation Model for Multi-Agent Language Model Coordination",
        "author": [
            "Hassen Dhrif"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00326",
        "abstract": "The emergence of large language models has enabled sophisticated multi-agent systems, yet coordinating their reasoning capabilities through prompt engineering remains challenging. We present a theoretically-grounded framework for dynamic prompt orchestration that enhances reasoning across multiple specialized agents. This framework addresses three core challenges: logical consistency preservation during agent transitions, reasoning-aware prompt adaptation, and scalable coordination of distributed inference.\nOur approach formalizes agent states using prompt templates, reasoning context vectors, and capability matrices. We prove system convergence to stable coordination patterns when step sizes satisfy $\\alpha < \\frac{1}{2L}$ where $L$ is the Lipschitz constant of the state transition function. We implement this through a distributed architecture that dynamically routes reasoning tasks while maintaining semantic coherence.\nExperimental results on 1,000 synthetic multi-agent conversations demonstrate a 42% reduction in reasoning latency, a 23% improvement in logical consistency measured by ROUGE-L score, and an 89% success rate for task completion without context loss across agent transitions. Ablation studies identify the consensus mechanism as the primary performance driver, while revealing limitations: performance degrades beyond 10 agent transitions, and the system requires 76.5GB memory for 1,000 concurrent agents. These findings establish a new paradigm for scalable reasoning in multi-agent systems, providing theoretical foundations for understanding reasoning emergence across coordinated language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "73",
        "title": "Learning Human Reaching Optimality Principles from Minimal Observation Inverse Reinforcement Learning",
        "author": [
            "Sarmad Mehrdad",
            "Maxime Sabbah",
            "Vincent Bonnet",
            "Ludovic Righetti"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00329",
        "abstract": "This paper investigates the application of Minimal Observation Inverse Reinforcement Learning (MO-IRL) to model and predict human arm-reaching movements with time-varying cost weights. Using a planar two-link biomechanical model and high-resolution motion-capture data from subjects performing a pointing task, we segment each trajectory into multiple phases and learn phase-specific combinations of seven candidate cost functions. MO-IRL iteratively refines cost weights by scaling observed and generated trajectories in the maximum entropy IRL formulation, greatly reducing the number of required demonstrations and convergence time compared to classical IRL approaches. Training on ten trials per posture yields average joint-angle Root Mean Squared Errors (RMSE) of 6.4 deg and 5.6 deg for six- and eight-segment weight divisions, respectively, versus 10.4 deg using a single static weight. Cross-validation on remaining trials and, for the first time, inter-subject validation on an unseen subject's 20 trials, demonstrates comparable predictive accuracy, around 8 deg RMSE, indicating robust generalization. Learned weights emphasize joint acceleration minimization during movement onset and termination, aligning with smoothness principles observed in biological motion. These results suggest that MO-IRL can efficiently uncover dynamic, subject-independent cost structures underlying human motor control, with potential applications for humanoid robots.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "74",
        "title": "Navigating the Synchrony-Stability Frontier in Adaptive Chatbots",
        "author": [
            "T. James Brandt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00339",
        "abstract": "Adaptive chatbots that mimic a user's linguistic style can build rapport and engagement, yet unconstrained mimicry risks an agent that feels unstable or sycophantic. We present a computational evaluation framework that makes the core design tension explicit: balancing moment-to-moment linguistic synchrony against long-term persona stability. Using an 8-dimensional style vector and a closed-loop \"base+delta\" prompting architecture, we simulate and compare explicit adaptation policies - Uncapped, Cap, Exponential Moving Average (EMA), Dead-Band, and Hybrids - on a human-log dataset. Our analysis maps a clear Pareto frontier: bounded policies achieve substantial gains in stability at a modest cost to synchrony. For example, a Hybrid (EMA+Cap) raises stability from 0.542 to 0.878 (+62%) while reducing synchrony by only 17%. We confirm this trade-off through large-scale replications on three public corpora (DailyDialog, Persona-Chat, EmpatheticDialogues) and LLM-in-the-loop validation across two model families. Furthermore, we quantify \"prompt legibility,\" showing that frontier policies reduce instruction churn and cut jarring register flips (major tone changes) from 0.254 to 0.092, yielding systems that are easier to reason about and maintain. Taken together, our framework provides a general evaluation harness for style adaptation; a systematic ablation that identifies Pareto-efficient policies; robust validation across diverse datasets and models; and novel legibility metrics linking policy choices to system maintainability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "75",
        "title": "Cutting the Skip: Training Residual-Free Transformers",
        "author": [
            "Yiping Ji",
            "James Martens",
            "Jianqiao Zheng",
            "Ziqin Zhou",
            "Peyman Moghadam",
            "Xinyu Zhang",
            "Hemanth Saratchandran",
            "Simon Lucey"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00345",
        "abstract": "Transformers have achieved remarkable success across a wide range of applications, a feat often attributed to their scalability. Yet training them without skip (residual) connections remains notoriously difficult. While skips stabilize optimization, they also disrupt the hierarchical structure of representations, raising the long-standing question of whether transformers can be trained efficiently without them. In this work, we address this problem by analyzing the Jacobian of a skipless transformer block, showing why skips improve conditioning and revealing that their stabilization benefits can be recovered through a principled initialization strategy. Building on this insight, we introduce the first method that enables stable and efficient training of skipless transformers without altering the standard architecture. We validate our approach on Vision Transformers (ViTs) in both supervised and self-supervised settings, demonstrating that skipless ViTs trained with our initialization overcome the usual optimization barriers, learn richer hierarchical representations, and outperform strong baselines, that incorporate skip connections, on dense prediction benchmarks. These results show that skip connections are not a fundamental requirement for training ViTs and open new avenues for hierarchical representation learning in vision models.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "76",
        "title": "In-Context Curiosity: Distilling Exploration for Decision-Pretrained Transformers on Bandit Tasks",
        "author": [
            "Huitao Yang",
            "Guanting Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00347",
        "abstract": "As large language models (LLMs) continue to grow in capability, there is increasing interest in incorporating them into decision-making tasks. A common pipeline for this is Decision-Pretrained Transformers (DPTs). However, existing training methods for DPTs often struggle to generalize beyond their pretraining data distribution. To explore mitigation of this limitation, we propose in-context curiosity -- a lightweight, exploration-inspired regularizer for offline pretraining -- and introduce the Prediction-Powered Transformer (PPT) framework. PPT augments DPT with an auxiliary reward predictor, using prediction error as an intrinsic curiosity signal to encourage broader exploration during training. In proof-of-concept experiments on Gaussian multi-armed bandits, PPT shows improved robustness: it moderates the performance degradation observed in DPT when test environments exhibit higher variance in reward, particularly when pretraining data has limited diversity. While the quality of offline data remain fundamental, our preliminary results suggest that curiosity-driven pretraining offers a promising direction for enhancing out-of-distribution generalization in in-context RL agents.",
        "tags": [
            "LLM",
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "77",
        "title": "AReUReDi: Annealed Rectified Updates for Refining Discrete Flows with Multi-Objective Guidance",
        "author": [
            "Tong Chen",
            "Yinuo Zhang",
            "Pranam Chatterjee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00352",
        "abstract": "Designing sequences that satisfy multiple, often conflicting, objectives is a central challenge in therapeutic and biomolecular engineering. Existing generative frameworks largely operate in continuous spaces with single-objective guidance, while discrete approaches lack guarantees for multi-objective Pareto optimality. We introduce AReUReDi (Annealed Rectified Updates for Refining Discrete Flows), a discrete optimization algorithm with theoretical guarantees of convergence to the Pareto front. Building on Rectified Discrete Flows (ReDi), AReUReDi combines Tchebycheff scalarization, locally balanced proposals, and annealed Metropolis-Hastings updates to bias sampling toward Pareto-optimal states while preserving distributional invariance. Applied to peptide and SMILES sequence design, AReUReDi simultaneously optimizes up to five therapeutic properties (including affinity, solubility, hemolysis, half-life, and non-fouling) and outperforms both evolutionary and diffusion-based baselines. These results establish AReUReDi as a powerful, sequence-based framework for multi-property biomolecule generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "78",
        "title": "DiSA-IQL: Offline Reinforcement Learning for Robust Soft Robot Control under Distribution Shifts",
        "author": [
            "Linjin He",
            "Xinda Qi",
            "Dong Chen",
            "Zhaojian Li",
            "Xiaobo Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00358",
        "abstract": "Soft snake robots offer remarkable flexibility and adaptability in complex environments, yet their control remains challenging due to highly nonlinear dynamics. Existing model-based and bio-inspired controllers rely on simplified assumptions that limit performance. Deep reinforcement learning (DRL) has recently emerged as a promising alternative, but online training is often impractical because of costly and potentially damaging real-world interactions. Offline RL provides a safer option by leveraging pre-collected datasets, but it suffers from distribution shift, which degrades generalization to unseen scenarios. To overcome this challenge, we propose DiSA-IQL (Distribution-Shift-Aware Implicit Q-Learning), an extension of IQL that incorporates robustness modulation by penalizing unreliable state-action pairs to mitigate distribution shift. We evaluate DiSA-IQL on goal-reaching tasks across two settings: in-distribution and out-of-distribution evaluation. Simulation results show that DiSA-IQL consistently outperforms baseline models, including Behavior Cloning (BC), Conservative Q-Learning (CQL), and vanilla IQL, achieving higher success rates, smoother trajectories, and improved robustness. The codes are open-sourced to support reproducibility and to facilitate further research in offline RL for soft robot control.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "79",
        "title": "Continual Learning with Query-Only Attention",
        "author": [
            "Gautham Bekal",
            "Ashish Pujari",
            "Scott David Kelly"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00365",
        "abstract": "Continual learning involves learning from a stream of data without repetition of data points, a scenario that is inherently complex due to distributional shift across tasks. We propose a query-only attention mechanism that discards keys and values, yet preserves the core inductive bias of transformer architectures. In continual learning scenarios, this simplified mechanism significantly mitigates both loss of plasticity and catastrophic forgetting, outperforming baselines such as selective re-initialization. We establish a conceptual link between query-only attention, full transformer attention, and model agnostic meta-learning, framing them as instances of meta-learning. We further provide intuition for why query-based models and attention networks help preserve plasticity in continual settings. Finally, through preliminary Hessian spectrum analysis, we observe that models maintaining higher curvature rank across tasks tend to retain plasticity. Our findings suggest that full attention may not be essential for capturing the benefits of meta-learning in continual learning.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "80",
        "title": "The Transformer Cookbook",
        "author": [
            "Andy Yang",
            "Christopher Watson",
            "Anton Xue",
            "Satwik Bhattamishra",
            "Jose Llarena",
            "William Merrill",
            "Emile Dos Santos Ferreira",
            "Anej Svete",
            "David Chiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00368",
        "abstract": "We present the transformer cookbook: a collection of techniques for directly encoding algorithms into a transformer's parameters. This work addresses the steep learning curve of such endeavors, a problem exacerbated by a fragmented literature where key results are scattered across numerous papers. In particular, we synthesize this disparate body of findings into a curated set of recipes that demonstrate how to implement everything from basic arithmetic in feed-forward layers to complex data routing via self-attention. Our mise en place of formulations is for both newcomers seeking an accessible entry point and experts in need of a systematic reference. This unified presentation of transformer constructions provides a foundation for future work spanning theoretical research in computational complexity to empirical investigations in architecture design and interpretability.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "81",
        "title": "Combining Large Language Models and Gradient-Free Optimization for Automatic Control Policy Synthesis",
        "author": [
            "Carlo Bosio",
            "Matteo Guarrera",
            "Alberto Sangiovanni-Vincentelli",
            "Mark W. Mueller"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00373",
        "abstract": "Large Language models (LLMs) have shown promise as generators of symbolic control policies, producing interpretable program-like representations through iterative search. However, these models are not capable of separating the functional structure of a policy from the numerical values it is parametrized by, thus making the search process slow and inefficient. We propose a hybrid approach that decouples structural synthesis from parameter optimization by introducing an additional optimization layer for local parameter search. In our method, the numerical parameters of LLM-generated programs are extracted and optimized numerically to maximize task performance. With this integration, an LLM iterates over the functional structure of programs, while a separate optimization loop is used to find a locally optimal set of parameters accompanying candidate programs. We evaluate our method on a set of control tasks, showing that it achieves higher returns and improved sample efficiency compared to purely LLM-guided search. We show that combining symbolic program synthesis with numerical optimization yields interpretable yet high-performing policies, bridging the gap between language-model-guided design and classical control tuning. Our code is available at https://sites.google.com/berkeley.edu/colmo.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "82",
        "title": "Composer: A Search Framework for Hybrid Neural Architecture Design",
        "author": [
            "Bilge Acun",
            "Prasoon Sinha",
            "Newsha Ardalani",
            "Sangmin Bae",
            "Alicia Golden",
            "Chien-Yu Lin",
            "Meghana Madhyastha",
            "Fei Sun",
            "Neeraja J. Yadwadkar",
            "Carole-Jean Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00379",
        "abstract": "Hybrid model architectures that combine computational primitives (e.g., Attention, MLP) in different ratios have shown promising performance beyond Transformers. Some studies have shown that different interleavings of primitives can affect model quality as well. However, prior works explore the hybrid model architecture design space manually. Due to the large design space and training costs, discovering hybrid models that combine key computational primitives for pre-training is challenging. In this work, we take a principled approach in designing a modular hybrid model architecture search framework -- Composer. Composer explores model architectures at a small scale and extrapolates the top-performing model architectures to a larger scale using our proposed scaling strategies. Using Composer, we discover new hybrid LLM architectures that outperform Llama 3.2. Compared to Llama 3.2 and previous state-of-the-art baselines, the new model architectures consistently reduce validation loss at parameter scales of 350M-3B and improve evaluation accuracy on the downstream tasks by up to 2.8-8.3% (1.1-3.1% on average) while improving both training and inference efficiency.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "83",
        "title": "SAGE-Music: Low-Latency Symbolic Music Generation via Attribute-Specialized Key-Value Head Sharing",
        "author": [
            "Jiaye Tan",
            "Haonan Luo",
            "Linfeng Song",
            "Shuaiqi Chen",
            "Yishan Lyu",
            "Zian Zhong",
            "Roujia Wang",
            "Daniel Jiang",
            "Haoran Zhang",
            "Jiaming Bai",
            "Haoran Cheng",
            "Q. Vera Liao",
            "Hao-Wen Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00395",
        "abstract": "Low-latency symbolic music generation is essential for real-time improvisation and human-AI co-creation. Existing transformer-based models, however, face a trade-off between inference speed and musical quality. Traditional acceleration techniques such as embedding pooling significantly degrade quality, while recently proposed Byte Pair Encoding (BPE) methods - though effective on single-track piano data - suffer large performance drops in multi-track settings, as revealed by our analysis. We propose Attribute-Specialized Key-Value Head Sharing (AS-KVHS), adapted to music's structured symbolic representation, achieving about 30% inference speedup with only a negligible (about 0.4%) quality drop in objective evaluations and slight improvements in subjective listening tests. Our main contributions are (1) the first systematic study of BPE's generalizability in multi-track symbolic music, and (2) the introduction of AS-KVHS for low-latency symbolic music generation. Beyond these, we also release SAGE-Music, an open-source benchmark that matches or surpasses state-of-the-art models in generation quality.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "84",
        "title": "Can Mamba Learn In Context with Outliers? A Theoretical Generalization Analysis",
        "author": [
            "Hongkang Li",
            "Songtao Lu",
            "Xiaodong Cui",
            "Pin-Yu Chen",
            "Meng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00399",
        "abstract": "The Mamba model has gained significant attention for its computational advantages over Transformer-based models, while achieving comparable performance across a wide range of language tasks. Like Transformers, Mamba exhibits in-context learning (ICL) capabilities, i.e., making predictions for new tasks based on a prompt containing input-label pairs and a query, without requiring fine-tuning. Despite its empirical success, the theoretical understanding of Mamba remains limited, largely due to the nonlinearity introduced by its gating mechanism. To the best of our knowledge, this paper presents the first theoretical analysis of the training dynamics of a one-layer Mamba model, which consists of a linear attention component followed by a nonlinear gating layer, and its ICL generalization on unseen binary classification tasks, even when the prompt includes additive outliers. Our analysis shows that Mamba leverages the linear attention layer to select informative context examples and uses the nonlinear gating layer to suppress the influence of outliers. By establishing and comparing to the analysis of linear Transformers under the same setting, we show that although Mamba may require more training iterations to converge, it maintains accurate predictions even when the proportion of outliers exceeds the threshold that a linear Transformer can tolerate. These theoretical findings are supported by empirical experiments.",
        "tags": [
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "85",
        "title": "Physics-Informed Neural Controlled Differential Equations for Scalable Long Horizon Multi-Agent Motion Forecasting",
        "author": [
            "Shounak Sural",
            "Charles Kekeh",
            "Wenliang Liu",
            "Federico Pecora",
            "Mouhacine Benosman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00401",
        "abstract": "Long-horizon motion forecasting for multiple autonomous robots is challenging due to non-linear agent interactions, compounding prediction errors, and continuous-time evolution of dynamics. Learned dynamics of such a system can be useful in various applications such as travel time prediction, prediction-guided planning and generative simulation. In this work, we aim to develop an efficient trajectory forecasting model conditioned on multi-agent goals. Motivated by the recent success of physics-guided deep learning for partially known dynamical systems, we develop a model based on neural Controlled Differential Equations (CDEs) for long-horizon motion forecasting. Unlike discrete-time methods such as RNNs and transformers, neural CDEs operate in continuous time, allowing us to combine physics-informed constraints and biases to jointly model multi-robot dynamics. Our approach, named PINCoDE (Physics-Informed Neural Controlled Differential Equations), learns differential equation parameters that can be used to predict the trajectories of a multi-agent system starting from an initial condition. PINCoDE is conditioned on future goals and enforces physics constraints for robot motion over extended periods of time. We adopt a strategy that scales our model from 10 robots to 100 robots without the need for additional model parameters, while producing predictions with an average ADE below 0.5 m for a 1-minute horizon. Furthermore, progressive training with curriculum learning for our PINCoDE model results in a 2.7X reduction of forecasted pose error over 4 minute horizons compared to analytical models.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "86",
        "title": "AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features",
        "author": [
            "Xudong Zhu",
            "Mohammad Mahdi Khalili",
            "Zhihui Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00404",
        "abstract": "Sparse autoencoders (SAEs) have emerged as powerful techniques for interpretability of large language models (LLMs), aiming to decompose hidden states into meaningful semantic features. While several SAE variants have been proposed, there remains no principled framework to derive SAEs from the original dictionary learning formulation. In this work, we introduce such a framework by unrolling the proximal gradient method for sparse coding. We show that a single-step update naturally recovers common SAE variants, including ReLU, JumpReLU, and TopK. Through this lens, we reveal a fundamental limitation of existing SAEs: their sparsity-inducing regularizers enforce non-negativity, preventing a single feature from representing bidirectional concepts (e.g., male vs. female). This structural constraint fragments semantic axes into separate, redundant features, limiting representational completeness. To address this issue, we propose AbsTopK SAE, a new variant derived from the $\\ell_0$ sparsity constraint that applies hard thresholding over the largest-magnitude activations. By preserving both positive and negative activations, AbsTopK uncovers richer, bidirectional conceptual representations. Comprehensive experiments across four LLMs and seven probing and steering tasks show that AbsTopK improves reconstruction fidelity, enhances interpretability, and enables single features to encode contrasting concepts. Remarkably, AbsTopK matches or even surpasses the Difference-in-Mean method, a supervised approach that requires labeled data for each concept and has been shown in prior work to outperform SAEs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "87",
        "title": "EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations",
        "author": [
            "Jiayi Liu",
            "Jiaming Zhou",
            "Ke Ye",
            "Kun-Yu Lin",
            "Allan Wang",
            "Junwei Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00405",
        "abstract": "Reliable trajectory prediction from an ego-centric perspective is crucial for robotic navigation in human-centric environments. However, existing methods typically assume idealized observation histories, failing to account for the perceptual artifacts inherent in first-person vision, such as occlusions, ID switches, and tracking drift. This discrepancy between training assumptions and deployment reality severely limits model robustness. To bridge this gap, we introduce EgoTraj-Bench, the first real-world benchmark that grounds noisy, first-person visual histories in clean, bird's-eye-view future trajectories, enabling robust learning under realistic perceptual constraints. Building on this benchmark, we propose BiFlow, a dual-stream flow matching model that concurrently denoises historical observations and forecasts future motion by leveraging a shared latent representation. To better model agent intent, BiFlow incorporates our EgoAnchor mechanism, which conditions the prediction decoder on distilled historical features via feature modulation. Extensive experiments show that BiFlow achieves state-of-the-art performance, reducing minADE and minFDE by 10-15% on average and demonstrating superior robustness. We anticipate that our benchmark and model will provide a critical foundation for developing trajectory forecasting systems truly resilient to the challenges of real-world, ego-centric perception.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "88",
        "title": "VLA-RFT: Vision-Language-Action Reinforcement Fine-tuning with Verified Rewards in World Simulators",
        "author": [
            "Hengtao Li",
            "Pengxiang Ding",
            "Runze Suo",
            "Yihao Wang",
            "Zirui Ge",
            "Dongyuan Zang",
            "Kexian Yu",
            "Mingyang Sun",
            "Hongyin Zhang",
            "Donglin Wang",
            "Weihua Su"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00406",
        "abstract": "Vision-Language-Action (VLA) models enable embodied decision-making but rely heavily on imitation learning, leading to compounding errors and poor robustness under distribution shift. Reinforcement learning (RL) can mitigate these issues yet typically demands costly real-world interactions or suffers from sim-to-real gaps. We introduce VLA-RFT, a reinforcement fine-tuning framework that leverages a data-driven world model as a controllable simulator. Trained from real interaction data, the simulator predicts future visual observations conditioned on actions, allowing policy rollouts with dense, trajectory-level rewards derived from goal-achieving references. This design delivers an efficient and action-aligned learning signal, drastically lowering sample requirements. With fewer than 400 fine-tuning steps, VLA-RFT surpasses strong supervised baselines and achieves greater efficiency than simulator-based RL. Moreover, it exhibits strong robustness under perturbed conditions, sustaining stable task execution. Our results establish world-model-based RFT as a practical post-training paradigm to enhance the generalization and robustness of VLA models. For more details, please refer to https://vla-rft.github.io/.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "89",
        "title": "PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents",
        "author": [
            "Zikang Liu",
            "Junyi Li",
            "Wayne Xin Zhao",
            "Dawei Gao",
            "Yaliang Li",
            "Ji-rong Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00413",
        "abstract": "Graphical User Interface (GUI) agents powered by Multimodal Large Language Models (MLLMs) promise human-like interaction with software applications, yet long-horizon tasks remain challenging due to memory limitations. Existing approaches either truncate history or rely on simple textual summaries, which risk losing critical information when past visual details become necessary for future decisions. In this paper, we propose \\textbf{PAL-UI} (\\textbf{P}lanning with \\textbf{A}ctive \\textbf{L}ook-back), a novel framework that enables GUI agents to adaptively retrieve past observations when required. PAL-UI combines a dual-level summarization agent, capturing both observation-level cues and action-level outcomes, with a dedicated retrieval tool that allows the agent to recall specific historical screenshots during planning. We curate a step-level instruction dataset of 8.6K samples from mobile GUI navigation trajectories and train \\textbf{PAL-UI-3B} and \\textbf{PAL-UI-7B} models based on Qwen2.5-VL. Extensive experiments demonstrate that PAL-UI significantly outperforms baseline models and prior methods in mobile GUI navigation tasks, even under data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain generalization, achieving notable improvements in web navigation without additional training. Our work highlights the potential of active memory retrieval for long-horizon planning capabilities of vision-based GUI agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "90",
        "title": "RELATE-Sim: Leveraging Turning Point Theory and LLM Agents to Predict and Understand Long-Term Relationship Dynamics through Interactive Narrative Simulations",
        "author": [
            "Matthew Yue",
            "Zhikun Xu",
            "Vivek Gupta",
            "Thao Ha",
            "Liesal Sharabi",
            "Ben Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00414",
        "abstract": "Most dating technologies optimize for getting together, not staying together. We present RELATE-Sim, a theory-grounded simulator that models how couples behave at consequential turning points-exclusivity talks, conflict-and-repair episodes, relocations-rather than static traits. Two persona-aligned LLM agents (one per partner) interact under a centralized Scene Master that frames each turning point as a compact set of realistic options, advances the narrative, and infers interpretable state changes and an auditable commitment estimate after each scene. On a longitudinal dataset of 71 couples with two-year follow-ups, simulation-aware predictions outperform a personas-only baseline while surfacing actionable markers (e.g., repair attempts acknowledged, clarity shifts) that explain why trajectories diverge. RELATE-Sim pushes the relationship research's focus from matchmaking to maintenance, providing a transparent, extensible platform for understanding and forecasting long-term relationship dynamics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "91",
        "title": "Towards Self-Evolving Benchmarks: Synthesizing Agent Trajectories via Test-Time Exploration under Validate-by-Reproduce Paradigm",
        "author": [
            "Dadi Guo",
            "Tianyi Zhou",
            "Dongrui Liu",
            "Chen Qian",
            "Qihan Ren",
            "Shuai Shao",
            "Zhiyuan Fan",
            "Yi R. Fung",
            "Kun Wang",
            "Linfeng Zhang",
            "Jing Shao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00415",
        "abstract": "Recent advances in large language models (LLMs) and agent system designs have empowered agents with unprecedented levels of capability. However, existing agent benchmarks are showing a trend of rapid ceiling-hitting by newly developed agents, making it difficult to meet the demands for evaluating agent abilities. To address this problem, we propose the Trajectory-based Validated-by-Reproducing Agent-benchmark Complexity Evolution (TRACE) framework. This framework takes an original task from an existing benchmark and encourages agents to freely explore and evolve it into a new task with higher difficulty while recording validatable agent trajectories. The framework proceeds in three stages: (1) evolutionary proposal mining, which provides task evolution proposals through preliminary exploration and divergent thinking; (2) problem formation and free exploration, where proposals are conceptualized into feasible problem candidates and the agents then explore them freely while recording their execution trajectories; and (3) multi-level validation, which ensures that the evolved tasks are accompanied by validatable and reproducible trajectories. Experiments on the GAIA benchmark demonstrate that the TRACE framework consistently enhances task complexity while improving the reliability of correctness through validatable execution trajectories. This work marks a paradigm shift from static, manually curated benchmarks to dynamic, self-evolving evaluation systems, providing a sustainable and challenging runway for agent development.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "92",
        "title": "Learning a Zeroth-Order Optimizer for Fine-Tuning LLMs",
        "author": [
            "Kairun Zhang",
            "Haoyu Li",
            "Yanjun Zhao",
            "Yifan Sun",
            "Huan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00419",
        "abstract": "Zeroth-order optimizers have recently emerged as a practical approach for fine-tuning large language models (LLMs), significantly reducing GPU memory consumption compared to traditional first-order methods. Yet, existing zeroth-order methods rely on hand-crafted, static sampling strategies that are not adaptable to model-specific structures. To address this, we propose ZO Fine-tuner, a learning-based zeroth-order optimizer for LLMs that automatically learns efficient perturbation strategies through a compact and memory-efficient design. Crucially, our approach is motivated by the observation that only a small number of foundation models and their derivatives are widely adopted in practice. Therefore, learning the optimizer once for a given LLM and reusing it across diverse downstream tasks is both feasible and highly desirable. Accordingly, ZO Fine-tuner is designed to scale learning to learn (L2L) to the foundation-model era by supporting one-time training per LLM with minimal overhead. Experiments on 4 LLMs and 7 datasets show that ZO Fine-tuner outperforms prior zeroth-order baselines in 82.1\\% of task-model combinations, thereby demonstrating strong performance and scalability for efficient LLM fine-tuning. Our code is available at https://github.com/ASTRAL-Group/ZO_Fine_tuner.git.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "93",
        "title": "Conflict-Based Search as a Protocol: A Multi-Agent Motion Planning Protocol for Heterogeneous Agents, Solvers, and Independent Tasks",
        "author": [
            "Rishi Veerapaneni",
            "Alvin Tang",
            "Haodong He",
            "Sophia Zhao",
            "Viraj Shah",
            "Yidai Cen",
            "Ziteng Ji",
            "Gabriel Olin",
            "Jon Arrizabalaga",
            "Yorai Shaoul",
            "Jiaoyang Li",
            "Maxim Likhachev"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00425",
        "abstract": "Imagine the future construction site, hospital, office, or even sophisticated household with dozens of robots bought from different manufacturers. How can we enable these different systems to effectively move in a shared environment, given that each robot may have its own independent motion planning system? This work shows how we can get efficient collision-free movements between algorithmically heterogeneous agents by using Conflict-Based Search (Sharon et al. 2015) as a protocol. At its core, the CBS Protocol requires one specific single-agent motion planning API; finding a collision-free path that satisfies certain space-time constraints. Given such an API, CBS uses a central planner to find collision-free paths - independent of how the API is implemented. We show how this protocol enables multi-agent motion planning for a heterogeneous team of agents completing independent tasks with a variety of single-agent planners including: Heuristic Search (e.g., A*), Sampling Based Search (e.g., RRT), Optimization (e.g., Direct Collocation), Diffusion, and Reinforcement Learning.",
        "tags": [
            "Diffusion",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "94",
        "title": "Plug-and-Play Prompt Refinement via Latent Feedback for Diffusion Model Alignment",
        "author": [
            "Suhyeon Lee",
            "Jong Chul Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00430",
        "abstract": "Despite the recent progress, reinforcement learning (RL)-based fine-tuning of diffusion models often struggles with generalization, composability, and robustness against reward hacking. Recent studies have explored prompt refinement as a modular alternative, but most adopt a feed-forward approach that applies a single refined prompt throughout the entire sampling trajectory, thereby failing to fully leverage the sequential nature of reinforcement learning. To address this, here we introduce PromptLoop, a plug-and-play RL framework that incorporates latent feedback into step-wise prompt refinement. Rather than modifying diffusion model weights, a multimodal large language model (MLLM) is trained with RL to iteratively update prompts based on intermediate latent states of diffusion models. This design achieves a structural analogy to the Diffusion RL approach, while retaining the flexibility and generality of prompt-based alignment. Extensive experiments across diverse reward functions and diffusion backbones demonstrate that PromptLoop (i) achieves effective reward optimization, (ii) generalizes seamlessly to unseen models, (iii) composes orthogonally with existing alignment methods, and (iv) mitigates over-optimization and reward hacking.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "95",
        "title": "BindWeave: Subject-Consistent Video Generation via Cross-Modal Integration",
        "author": [
            "Zhaoyang Li",
            "Dongjun Qian",
            "Kai Su",
            "Qishuai Diao",
            "Xiangyang Xia",
            "Chang Liu",
            "Wenfei Yang",
            "Tianzhu Zhang",
            "Zehuan Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00438",
        "abstract": "Diffusion Transformer has shown remarkable abilities in generating high-fidelity videos, delivering visually coherent frames and rich details over extended durations. However, existing video generation models still fall short in subject-consistent video generation due to an inherent difficulty in parsing prompts that specify complex spatial relationships, temporal logic, and interactions among multiple subjects. To address this issue, we propose BindWeave, a unified framework that handles a broad range of subject-to-video scenarios from single-subject cases to complex multi-subject scenes with heterogeneous entities. To bind complex prompt semantics to concrete visual subjects, we introduce an MLLM-DiT framework in which a pretrained multimodal large language model performs deep cross-modal reasoning to ground entities and disentangle roles, attributes, and interactions, yielding subject-aware hidden states that condition the diffusion transformer for high-fidelity subject-consistent video generation. Experiments on the OpenS2V benchmark demonstrate that our method achieves superior performance across subject consistency, naturalness, and text relevance in generated videos, outperforming existing open-source and commercial models.",
        "tags": [
            "DiT",
            "Diffusion",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "96",
        "title": "TokMem: Tokenized Procedural Memory for Large Language Models",
        "author": [
            "Zijun Wu",
            "Yongchang Hao",
            "Lili Mou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00444",
        "abstract": "Large language models rely heavily on prompts to specify tasks, recall knowledge and guide reasoning. However, this reliance is inefficient as prompts must be re-read at each step, scale poorly across tasks, and lack mechanisms for modular reuse. We introduce TokMem, a tokenized procedural memory that stores recurring procedures as compact, trainable embeddings. Each memory token encodes both an address to a procedure and a control signal that steers generation, enabling targeted behavior with constant-size overhead. To support continual adaptation, TokMem keeps the backbone model frozen, allowing new procedures to be added without interfering with existing ones. We evaluate TokMem on 1,000 tasks for atomic recall, and on function-calling tasks for compositional recall, where it consistently outperforms retrieval-augmented generation while avoiding repeated context overhead, and fine-tuning with far fewer parameters. These results establish TokMem as a scalable and modular alternative to prompt engineering and fine-tuning, offering an explicit procedural memory for LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "97",
        "title": "LongCodeZip: Compress Long Context for Code Language Models",
        "author": [
            "Yuling Shi",
            "Yichun Qian",
            "Hongyu Zhang",
            "Beijun Shen",
            "Xiaodong Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00446",
        "abstract": "Code generation under long contexts is becoming increasingly critical as Large Language Models (LLMs) are required to reason over extensive information in the codebase. While recent advances enable code LLMs to process long inputs, high API costs and generation latency remain substantial bottlenecks. Existing context pruning techniques, such as LLMLingua, achieve promising results for general text but overlook code-specific structures and dependencies, leading to suboptimal performance in programming tasks. In this paper, we propose LongCodeZip, a novel plug-and-play code compression framework designed specifically for code LLMs. LongCodeZip employs a dual-stage strategy: (1) coarse-grained compression, which identifies and ranks function-level chunks using conditional perplexity with respect to the instruction, retaining only the most relevant functions; and (2) fine-grained compression, which segments retained functions into blocks based on perplexity and selects an optimal subset under an adaptive token budget to maximize relevance. Evaluations across multiple tasks, including code completion, summarization, and question answering, show that LongCodeZip consistently outperforms baseline methods, achieving up to a 5.6x compression ratio without degrading task performance. By effectively reducing context size while preserving essential information, LongCodeZip enables LLMs to better scale to real-world, large-scale code scenarios, advancing the efficiency and capability of code intelligence applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "98",
        "title": "Enhancing Rating Prediction with Off-the-Shelf LLMs Using In-Context User Reviews",
        "author": [
            "Koki Ryu",
            "Hitomi Yanaka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00449",
        "abstract": "Personalizing the outputs of large language models (LLMs) to align with individual user preferences is an active research area. However, previous studies have mainly focused on classification or ranking tasks and have not considered Likert-scale rating prediction, a regression task that requires both language and mathematical reasoning to be solved effectively. This task has significant industrial applications, but the utilization of LLMs remains underexplored, particularly regarding the capabilities of off-the-shelf LLMs. This study investigates the performance of off-the-shelf LLMs on rating prediction, providing different in-context information. Through comprehensive experiments with eight models across three datasets, we demonstrate that user-written reviews significantly improve the rating prediction performance of LLMs. This result is comparable to traditional methods like matrix factorization, highlighting the potential of LLMs as a promising solution for the cold-start problem. We also find that the reviews for concrete items are more effective than general preference descriptions that are not based on any specific item. Furthermore, we discover that prompting LLMs to first generate a hypothetical review enhances the rating prediction performance. Our code is available at https://github.com/ynklab/rating-prediction-with-reviews.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "99",
        "title": "Cloud Investigation Automation Framework (CIAF): An AI-Driven Approach to Cloud Forensics",
        "author": [
            "Dalal Alharthi",
            "Ivan Roberto Kawaminami Garcia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00452",
        "abstract": "Large Language Models (LLMs) have gained prominence in domains including cloud security and forensics. Yet cloud forensic investigations still rely on manual analysis, making them time-consuming and error-prone. LLMs can mimic human reasoning, offering a pathway to automating cloud log analysis. To address this, we introduce the Cloud Investigation Automation Framework (CIAF), an ontology-driven framework that systematically investigates cloud forensic logs while improving efficiency and accuracy. CIAF standardizes user inputs through semantic validation, eliminating ambiguity and ensuring consistency in log interpretation. This not only enhances data quality but also provides investigators with reliable, standardized information for decision-making. To evaluate security and performance, we analyzed Microsoft Azure logs containing ransomware-related events. By simulating attacks and assessing CIAF's impact, results showed significant improvement in ransomware detection, achieving precision, recall, and F1 scores of 93 percent. CIAF's modular, adaptable design extends beyond ransomware, making it a robust solution for diverse cyberattacks. By laying the foundation for standardized forensic methodologies and informing future AI-driven automation, this work underscores the role of deterministic prompt engineering and ontology-based validation in enhancing cloud forensic investigations. These advancements improve cloud security while paving the way for efficient, automated forensic workflows.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "100",
        "title": "UrbanGraph: Physics-Informed Spatio-Temporal Dynamic Heterogeneous Graphs for Urban Microclimate Prediction",
        "author": [
            "Weilin Xin",
            "Chenyu Huang",
            "Peilin Li",
            "Jing Zhong",
            "Jiawei Yao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00457",
        "abstract": "With rapid urbanization, predicting urban microclimates has become critical, as it affects building energy demand and public health risks. However, existing generative and homogeneous graph approaches fall short in capturing physical consistency, spatial dependencies, and temporal variability. To address this, we introduce UrbanGraph, a physics-informed framework integrating heterogeneous and dynamic spatio-temporal graphs. It encodes key physical processes -- vegetation evapotranspiration, shading, and convective diffusion -- while modeling complex spatial dependencies among diverse urban entities and their temporal evolution. We evaluate UrbanGraph on UMC4/12, a physics-based simulation dataset covering diverse urban configurations and climates. Results show that UrbanGraph improves $R^2$ by up to 10.8% and reduces FLOPs by 17.0% over all baselines, with heterogeneous and dynamic graphs contributing 3.5% and 7.1% gains. Our dataset provides the first high-resolution benchmark for spatio-temporal microclimate modeling, and our method extends to broader urban heterogeneous dynamic computing tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "101",
        "title": "Integrating Offline Pre-Training with Online Fine-Tuning: A Reinforcement Learning Approach for Robot Social Navigation",
        "author": [
            "Run Su",
            "Hao Fu",
            "Shuai Zhou",
            "Yingao Fu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00466",
        "abstract": "Offline reinforcement learning (RL) has emerged as a promising framework for addressing robot social navigation challenges. However, inherent uncertainties in pedestrian behavior and limited environmental interaction during training often lead to suboptimal exploration and distributional shifts between offline training and online deployment. To overcome these limitations, this paper proposes a novel offline-to-online fine-tuning RL algorithm for robot social navigation by integrating Return-to-Go (RTG) prediction into a causal Transformer architecture. Our algorithm features a spatiotem-poral fusion model designed to precisely estimate RTG values in real-time by jointly encoding temporal pedestrian motion patterns and spatial crowd dynamics. This RTG prediction framework mitigates distribution shift by aligning offline policy training with online environmental interactions. Furthermore, a hybrid offline-online experience sampling mechanism is built to stabilize policy updates during fine-tuning, ensuring balanced integration of pre-trained knowledge and real-time adaptation. Extensive experiments in simulated social navigation environments demonstrate that our method achieves a higher success rate and lower collision rate compared to state-of-the-art baselines. These results underscore the efficacy of our algorithm in enhancing navigation policy robustness and adaptability. This work paves the way for more reliable and adaptive robotic navigation systems in real-world applications.",
        "tags": [
            "RL",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "102",
        "title": "Analyzing Latent Concepts in Code Language Models",
        "author": [
            "Arushi Sharma",
            "Vedant Pungliya",
            "Christopher J. Quinn",
            "Ali Jannesari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00476",
        "abstract": "Interpreting the internal behavior of large language models trained on code remains a critical challenge, particularly for applications demanding trust, transparency, and semantic robustness. We propose Code Concept Analysis (CoCoA): a global post-hoc interpretability framework that uncovers emergent lexical, syntactic, and semantic structures in a code language model's representation space by clustering contextualized token embeddings into human-interpretable concept groups. We propose a hybrid annotation pipeline that combines static analysis tool-based syntactic alignment with prompt-engineered large language models (LLMs), enabling scalable labeling of latent concepts across abstraction levels. We analyse the distribution of concepts across layers and across three finetuning tasks. Emergent concept clusters can help identify unexpected latent interactions and be used to identify trends and biases within the model's learned representations. We further integrate LCA with local attribution methods to produce concept-grounded explanations, improving the coherence and interpretability of token-level saliency. Empirical evaluations across multiple models and tasks show that LCA discovers concepts that remain stable under semantic-preserving perturbations (average Cluster Sensitivity Index, CSI = 0.288) and evolve predictably with fine-tuning. In a user study, concept-augmented explanations disambiguate token roles. In a user study on the programming-language classification task, concept-augmented explanations disambiguated token roles and improved human-centric explainability by 37 percentage points compared with token-level attributions using Integrated Gradients.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "103",
        "title": "Wireless Laser Power Transfer for Low-altitude Uncrewed Aerial Vehicle-assisted Internet of Things: Paradigms, Challenges, and Solutions",
        "author": [
            "Chengzhen Li",
            "Likun Zhang",
            "Chuang Zhang",
            "Jiahui Li",
            "Changyuan Zhao",
            "Ruichen Zhang",
            "Geng Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00477",
        "abstract": "Low-altitude uncrewed aerial vehicles (UAVs) have become integral enablers for the Internet of Things (IoT) by offering enhanced coverage, improved connectivity and access to remote areas. A critical challenge limiting their operational capacity lies in the energy constraints of both aerial platforms and ground-based sensors. This paper explores WLPT as a transformative solution for sustainable energy provisioning in UAV-assisted IoT networks. We first systematically investigate the fundamental principles of WLPT and analysis the comparative advantages. Then, we introduce three operational paradigms for system integration, identify key challenges, and discuss corresponding potential solutions. In case study, we propose a multi-agent reinforcement learning framework to address the coordination and optimization challenges in WLPT-enabled UAV-assisted IoT data collection. Simulation results demonstrate that our framework significantly improves energy sustainability and data freshness. Finally, we discuss some future directions.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "104",
        "title": "Vicinity-Guided Discriminative Latent Diffusion for Privacy-Preserving Domain Adaptation",
        "author": [
            "Jing Wang",
            "Wonho Bae",
            "Jiahong Chen",
            "Wenxu Wang",
            "Junhyug Noh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00478",
        "abstract": "Recent work on latent diffusion models (LDMs) has focused almost exclusively on generative tasks, leaving their potential for discriminative transfer largely unexplored. We introduce Discriminative Vicinity Diffusion (DVD), a novel LDM-based framework for a more practical variant of source-free domain adaptation (SFDA): the source provider may share not only a pre-trained classifier but also an auxiliary latent diffusion module, trained once on the source data and never exposing raw source samples. DVD encodes each source feature's label information into its latent vicinity by fitting a Gaussian prior over its k-nearest neighbors and training the diffusion network to drift noisy samples back to label-consistent representations. During adaptation, we sample from each target feature's latent vicinity, apply the frozen diffusion module to generate source-like cues, and use a simple InfoNCE loss to align the target encoder to these cues, explicitly transferring decision boundaries without source access. Across standard SFDA benchmarks, DVD outperforms state-of-the-art methods. We further show that the same latent diffusion module enhances the source classifier's accuracy on in-domain data and boosts performance in supervised classification and domain generalization experiments. DVD thus reinterprets LDMs as practical, privacy-preserving bridges for explicit knowledge transfer, addressing a core challenge in source-free domain adaptation that prior methods have yet to solve.",
        "tags": [
            "Diffusion",
            "LDMs"
        ]
    },
    {
        "id": "105",
        "title": "Expandable Decision-Making States for Multi-Agent Deep Reinforcement Learning in Soccer Tactical Analysis",
        "author": [
            "Kenjiro Ide",
            "Taiga Someya",
            "Kohei Kawaguchi",
            "Keisuke Fujii"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00480",
        "abstract": "Invasion team sports such as soccer produce a high-dimensional, strongly coupled state space as many players continuously interact on a shared field, challenging quantitative tactical analysis. Traditional rule-based analyses are intuitive, while modern predictive machine learning models often perform pattern-matching without explicit agent representations. The problem we address is how to build player-level agent models from data, whose learned values and policies are both tactically interpretable and robust across heterogeneous data sources. Here, we propose Expandable Decision-Making States (EDMS), a semantically enriched state representation that augments raw positions and velocities with relational variables (e.g., scoring of space, pass, and score), combined with an action-masking scheme that gives on-ball and off-ball agents distinct decision sets. Compared to prior work, EDMS maps learned value functions and action policies to human-interpretable tactical concepts (e.g., marking pressure, passing lanes, ball accessibility) instead of raw coordinate features, and aligns agent choices with the rules of play. In the experiments, EDMS with action masking consistently reduced both action-prediction loss and temporal-difference (TD) error compared to the baseline. Qualitative case studies and Q-value visualizations further indicate that EDMS highlights high-risk, high-reward tactical patterns (e.g., fast counterattacks and defensive breakthroughs). We also integrated our approach into an open-source library and demonstrated compatibility with multiple commercial and open datasets, enabling cross-provider evaluation and reproducible experiments.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "106",
        "title": "Make a Video Call with LLM: A Measurement Campaign over Five Mainstream Apps",
        "author": [
            "Jiayang Xu",
            "Xiangjie Huang",
            "Zijie Li",
            "Zili Meng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00481",
        "abstract": "In 2025, Large Language Model (LLM) services have launched a new feature -- AI video chat -- allowing users to interact with AI agents via real-time video communication (RTC), just like chatting with real people. Despite its significance, no systematic study has characterized the performance of existing AI video chat systems. To address this gap, this paper proposes a comprehensive benchmark with carefully designed metrics across four dimensions: quality, latency, internal mechanisms, and system overhead. Using custom testbeds, we further evaluate five mainstream AI video chatbots with this benchmark. This work provides the research community a baseline of real-world performance and identifies unique system bottlenecks. In the meantime, our benchmarking results also open up several research questions for future optimizations of AI video chatbots.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "107",
        "title": "Agent Fine-tuning through Distillation for Domain-specific LLMs in Microdomains",
        "author": [
            "Yawen Xue",
            "Masaya Tsunokake",
            "Yuta Koreeda",
            "Ekant Muljibhai Amin",
            "Takashi Sumiyoshi",
            "Yasuhiro Sogawa"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00482",
        "abstract": "Agentic large language models (LLMs) have become prominent for autonomously interacting with external environments and performing multi-step reasoning tasks. Most approaches leverage these capabilities via in-context learning with few-shot prompts, but this often results in lengthy inputs and higher computational costs. Agent fine-tuning offers an alternative by enabling LLMs to internalize procedural reasoning and domain-specific knowledge through training on relevant data and demonstration trajectories. While prior studies have focused on general domains, their effectiveness in specialized technical microdomains remains unclear. This paper explores agent fine-tuning for domain adaptation within Hitachi's JP1 middleware, a microdomain for specialized IT operations. We fine-tuned LLMs using JP1-specific datasets derived from domain manuals and distilled reasoning trajectories generated by LLMs themselves, enhancing decision making accuracy and search efficiency. During inference, we used an agentic prompt with retrieval-augmented generation and introduced a context-answer extractor to improve information relevance. On JP1 certification exam questions, our method achieved a 14% performance improvement over the base model, demonstrating the potential of agent fine-tuning for domain-specific reasoning in complex microdomains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "108",
        "title": "MathSticks: A Benchmark for Visual Symbolic Compositional Reasoning with Matchstick Puzzles",
        "author": [
            "Yuheng Ji",
            "Huajie Tan",
            "Cheng Chi",
            "Yijie Xu",
            "Yuting Zhao",
            "Enshen Zhou",
            "Huaihai Lyu",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Shanghang Zhang",
            "Xiaolong Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00483",
        "abstract": "We introduce \\textsc{MathSticks}, a benchmark for Visual Symbolic Compositional Reasoning (VSCR), which unifies visual perception, symbolic manipulation, and arithmetic consistency. Each task presents an incorrect matchstick equation that must be corrected by moving one or two sticks under strict conservation rules. The benchmark includes both text-guided and purely visual settings, systematically covering digit scale, move complexity, solution multiplicity, and operator variation, with 1.4M generated instances and a curated test set. Evaluations of 14 vision--language models reveal substantial limitations: closed-source models succeed only on simple cases, open-source models fail in the visual regime, while humans exceed 90\\% accuracy. These findings establish \\textsc{MathSticks} as a rigorous testbed for advancing compositional reasoning across vision and symbols. Our code and dataset are publicly available at https://github.com/Yuheng2000/MathSticks.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "109",
        "title": "Has the Two-Decade-Old Prophecy Come True? Artificial Bad Intelligence Triggered by Merely a Single-Bit Flip in Large Language Models",
        "author": [
            "Yu Yan",
            "Siqi Lu",
            "Yang Gao",
            "Zhaoxuan Li",
            "Ziming Zhao",
            "Qingjun Yuan",
            "Yongjuan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00490",
        "abstract": "Recently, Bit-Flip Attack (BFA) has garnered widespread attention for its ability to compromise software system integrity remotely through hardware fault injection. With the widespread distillation and deployment of large language models (LLMs) into single file .gguf formats, their weight spaces have become exposed to an unprecedented hardware attack surface. This paper is the first to systematically discover and validate the existence of single-bit vulnerabilities in LLM weight files: in mainstream open-source models (e.g., DeepSeek and QWEN) using .gguf quantized formats, flipping just single bit can induce three types of targeted semantic level failures Artificial Flawed Intelligence (outputting factual errors), Artificial Weak Intelligence (degradation of logical reasoning capability), and Artificial Bad Intelligence (generating harmful content).\nBy building an information theoretic weight sensitivity entropy model and a probabilistic heuristic scanning framework called BitSifter, we achieved efficient localization of critical vulnerable bits in models with hundreds of millions of parameters. Experiments show that vulnerabilities are significantly concentrated in the tensor data region, particularly in areas related to the attention mechanism and output layers, which are the most sensitive. A negative correlation was observed between model size and robustness, with smaller models being more susceptible to attacks. Furthermore, a remote BFA chain was designed, enabling semantic-level attacks in real-world environments: At an attack frequency of 464.3 times per second, a single bit can be flipped with 100% success in as little as 31.7 seconds. This causes the accuracy of LLM to plummet from 73.5% to 0%, without requiring high-cost equipment or complex prompt engineering.",
        "tags": [
            "DeepSeek",
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "110",
        "title": "From Human Hands to Robot Arms: Manipulation Skills Transfer via Trajectory Alignment",
        "author": [
            "Han Zhou",
            "Jinjin Cao",
            "Liyuan Ma",
            "Xueji Fang",
            "Guo-jun Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00491",
        "abstract": "Learning diverse manipulation skills for real-world robots is severely bottlenecked by the reliance on costly and hard-to-scale teleoperated demonstrations. While human videos offer a scalable alternative, effectively transferring manipulation knowledge is fundamentally hindered by the significant morphological gap between human and robotic embodiments. To address this challenge and facilitate skill transfer from human to robot, we introduce Traj2Action,a novel framework that bridges this embodiment gap by using the 3D trajectory of the operational endpoint as a unified intermediate representation, and then transfers the manipulation knowledge embedded in this trajectory to the robot's actions. Our policy first learns to generate a coarse trajectory, which forms an high-level motion plan by leveraging both human and robot data. This plan then conditions the synthesis of precise, robot-specific actions (e.g., orientation and gripper state) within a co-denoising framework. Extensive real-world experiments on a Franka robot demonstrate that Traj2Action boosts the performance by up to 27% and 22.25% over $\\pi_0$ baseline on short- and long-horizon real-world tasks, and achieves significant gains as human data scales in robot policy learning. Our project website, featuring code and video demonstrations, is available at https://anonymous.4open.science/w/Traj2Action-4A45/.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "111",
        "title": "Rethinking Reward Models for Multi-Domain Test-Time Scaling",
        "author": [
            "Dong Bok Lee",
            "Seanie Lee",
            "Sangwoo Park",
            "Minki Kang",
            "Jinheon Baek",
            "Dongki Kim",
            "Dominik Wagner",
            "Jiongdao Jin",
            "Heejun Lee",
            "Tobias Bocklet",
            "Jinyu Wang",
            "Jingjing Fu",
            "Sung Ju Hwang",
            "Jiang Bia",
            "Lei Song"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00492",
        "abstract": "The reliability of large language models (LLMs) during test-time scaling is often assessed with \\emph{external verifiers} or \\emph{reward models} that distinguish correct reasoning from flawed logic. Prior work generally assumes that process reward models (PRMs), which score every intermediate reasoning step, outperform outcome reward models (ORMs) that assess only the final answer. This view is based mainly on evidence from narrow, math-adjacent domains. We present the first unified evaluation of four reward model variants, discriminative ORM and PRM (\\DisORM, \\DisPRM) and generative ORM and PRM (\\GenORM, \\GenPRM), across 14 diverse domains. Contrary to conventional wisdom, we find that (i) \\DisORM performs on par with \\DisPRM, (ii) \\GenPRM is not competitive, and (iii) overall, \\GenORM is the most robust, yielding significant and consistent gains across every tested domain. We attribute this to PRM-style stepwise scoring, which inherits label noise from LLM auto-labeling and has difficulty evaluating long reasoning trajectories, including those involving self-correcting reasoning. Our theoretical analysis shows that step-wise aggregation compounds errors as reasoning length grows, and our empirical observations confirm this effect. These findings challenge the prevailing assumption that fine-grained supervision is always better and support generative outcome verification for multi-domain deployment. We publicly release our code, datasets, and checkpoints at \\href{https://github.com/db-Lee/Multi-RM}{\\underline{\\small\\texttt{https://github.com/db-Lee/Multi-RM}}} to facilitate future research in multi-domain settings.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "112",
        "title": "Exploring System 1 and 2 communication for latent reasoning in LLMs",
        "author": [
            "Julian Coda-Forno",
            "Zhuokai Zhao",
            "Qiang Zhang",
            "Dipesh Tamboli",
            "Weiwei Li",
            "Xiangjun Fan",
            "Lizhu Zhang",
            "Eric Schulz",
            "Hsiao-Ping Tseng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00494",
        "abstract": "Should LLM reasoning live in a separate module, or within a single model's forward pass and representational space? We study dual-architecture latent reasoning, where a fluent Base exchanges latent messages with a Coprocessor, and test two hypotheses aimed at improving latent communication over Liu et al. (2024): (H1) increase channel capacity; (H2) learn communication via joint finetuning. Under matched latent-token budgets on GPT-2 and Qwen-3, H2 is consistently strongest while H1 yields modest gains. A unified soft-embedding baseline, a single model with the same forward pass and shared representations, using the same latent-token budget, nearly matches H2 and surpasses H1, suggesting current dual designs mostly add compute rather than qualitatively improving reasoning. Across GSM8K, ProsQA, and a Countdown stress test with increasing branching factor, scaling the latent-token budget beyond small values fails to improve robustness. Latent analyses show overlapping subspaces with limited specialization, consistent with weak reasoning gains. We conclude dual-model latent reasoning remains promising in principle, but likely requires objectives and communication mechanisms that explicitly shape latent spaces for algorithmic planning.",
        "tags": [
            "GPT",
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "113",
        "title": "MOSS-Speech: Towards True Speech-to-Speech Models Without Text Guidance",
        "author": [
            "Xingjian Zhao",
            "Zhe Xu",
            "Luozhijie Jin",
            "Yang Wang",
            "Hanfu Chen",
            "Yaozhou Jiang",
            "Ke Chen",
            "Ruixiao Li",
            "Mingshu Chen",
            "Ruiming Wang",
            "Wenbo Zhang",
            "Yiyang Zhang",
            "Donghua Yu",
            "Yang Gao",
            "Xiaogui Yang",
            "Yitian Gong",
            "Yuanfan Xu",
            "Qinyuan Cheng",
            "Zhaoye Fei",
            "Shimin Li",
            "Yaqian Zhou",
            "Xuanjing Huang",
            "Xipeng Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00499",
        "abstract": "Spoken dialogue systems often rely on cascaded pipelines that transcribe, process, and resynthesize speech. While effective, this design discards paralinguistic cues and limits expressivity. Recent end-to-end methods reduce latency and better preserve these cues, yet still rely on text intermediates, creating a fundamental bottleneck. We present MOSS-Speech, a true speech-to-speech large language model that directly understands and generates speech without relying on text guidance. Our approach combines a modality-based layer-splitting architecture with a frozen pre-training strategy, preserving the reasoning and knowledge of pretrained text LLMs while adding native speech capabilities. Experiments show that our model achieves state-of-the-art results in spoken question answering and delivers comparable speech-to-speech performance relative to existing text-guided systems, while still maintaining competitive text performance. By narrowing the gap between text-guided and direct speech generation, our work establishes a new paradigm for expressive and efficient end-to-end speech interaction.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "114",
        "title": "CodeChemist: Functional Knowledge Transfer for Low-Resource Code Generation via Test-Time Scaling",
        "author": [
            "Kaixin Wang",
            "Tianlin Li",
            "Xiaoyu Zhang",
            "Aishan Liu",
            "Xianglong Liu",
            "Ziqi Liu",
            "Zhiqiang Zhang",
            "Jun Zhou",
            "and Bin Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00501",
        "abstract": "Code Large Language Models (CodeLLMs) are increasingly used in code generation tasks across a wide range of applications. However, their performance is often inconsistent across different programming languages (PLs), with low-resource PLs suffering the most due to limited training data. In this paper, we present CodeChemist, a novel and efficient framework for test-time scaling that enables functional knowledge transfer from high-resource to low-resource PLs using generated test cases. CodeChemist first generates and executes code in high-resource PLs to create test cases that encapsulate functional knowledge. It then uses multi-temperature hedged sampling to generate code snippets in the low-resource PL and selects the best one based on the pass rate of the test cases. Our extensive experiments show that CodeChemist outperforms existing test-time scaling approaches, boosting the performance of code generation for low-resource PLs without requiring any model retraining.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "115",
        "title": "Diffusion Alignment as Variational Expectation-Maximization",
        "author": [
            "Jaewoo Lee",
            "Minsu Kim",
            "Sanghyeok Choi",
            "Inhyuck Song",
            "Sujin Yun",
            "Hyeongyu Kang",
            "Woocheol Shin",
            "Taeyoung Yun",
            "Kiyoung Om",
            "Jinkyoo Park"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00502",
        "abstract": "Diffusion alignment aims to optimize diffusion models for the downstream objective. While existing methods based on reinforcement learning or direct backpropagation achieve considerable success in maximizing rewards, they often suffer from reward over-optimization and mode collapse. We introduce Diffusion Alignment as Variational Expectation-Maximization (DAV), a framework that formulates diffusion alignment as an iterative process alternating between two complementary phases: the E-step and the M-step. In the E-step, we employ test-time search to generate diverse and reward-aligned samples. In the M-step, we refine the diffusion model using samples discovered by the E-step. We demonstrate that DAV can optimize reward while preserving diversity for both continuous and discrete tasks: text-to-image synthesis and DNA sequence design.",
        "tags": [
            "Diffusion",
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": "116",
        "title": "Affordance-Guided Diffusion Prior for 3D Hand Reconstruction",
        "author": [
            "Naru Suzuki",
            "Takehiko Ohkawa",
            "Tatsuro Banno",
            "Jihyun Lee",
            "Ryosuke Furuta",
            "Yoichi Sato"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00506",
        "abstract": "How can we reconstruct 3D hand poses when large portions of the hand are heavily occluded by itself or by objects? Humans often resolve such ambiguities by leveraging contextual knowledge -- such as affordances, where an object's shape and function suggest how the object is typically grasped. Inspired by this observation, we propose a generative prior for hand pose refinement guided by affordance-aware textual descriptions of hand-object interactions (HOI). Our method employs a diffusion-based generative model that learns the distribution of plausible hand poses conditioned on affordance descriptions, which are inferred from a large vision-language model (VLM). This enables the refinement of occluded regions into more accurate and functionally coherent hand poses. Extensive experiments on HOGraspNet, a 3D hand-affordance dataset with severe occlusions, demonstrate that our affordance-guided refinement significantly improves hand pose estimation over both recent regression methods and diffusion-based refinement lacking contextual reasoning.",
        "tags": [
            "3D",
            "Diffusion",
            "Pose Estimation",
            "VLM"
        ]
    },
    {
        "id": "117",
        "title": "Graph2Eval: Automatic Multimodal Task Generation for Agents via Knowledge Graphs",
        "author": [
            "Yurun Chen",
            "Xavier Hu",
            "Yuhan Liu",
            "Ziqi Wang",
            "Zeyi Liao",
            "Lin Chen",
            "Feng Wei",
            "Yuxi Qian",
            "Bo Zheng",
            "Keting Yin",
            "Shengyu Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00507",
        "abstract": "As multimodal LLM-driven agents continue to advance in autonomy and generalization, evaluation based on static datasets can no longer adequately assess their true capabilities in dynamic environments and diverse tasks. Existing LLM-based synthetic data methods are largely designed for LLM training and evaluation, and thus cannot be directly applied to agent tasks that require tool use and interactive capabilities. While recent studies have explored automatic agent task generation with LLMs, most efforts remain limited to text or image analysis, without systematically modeling multi-step interactions in web environments. To address these challenges, we propose Graph2Eval, a knowledge graph-based framework that automatically generates both multimodal document comprehension tasks and web interaction tasks, enabling comprehensive evaluation of agents' reasoning, collaboration, and interactive capabilities. In our approach, knowledge graphs constructed from multi-source external data serve as the task space, where we translate semantic relations into structured multimodal tasks using subgraph sampling, task templates, and meta-paths. A multi-stage filtering pipeline based on node reachability, LLM scoring, and similarity analysis is applied to guarantee the quality and executability of the generated tasks. Furthermore, Graph2Eval supports end-to-end evaluation of multiple agent types (Single-Agent, Multi-Agent, Web Agent) and measures reasoning, collaboration, and interaction capabilities. We instantiate the framework with Graph2Eval-Bench, a curated dataset of 1,319 tasks spanning document comprehension and web interaction scenarios. Experiments show that Graph2Eval efficiently generates tasks that differentiate agent and model performance, revealing gaps in reasoning, collaboration, and web interaction across different settings and offering a new perspective for agent evaluation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "118",
        "title": "Copy-Paste to Mitigate Large Language Model Hallucinations",
        "author": [
            "Yongchao Long",
            "Xian Wu",
            "Yingying Zhang",
            "Xianbin Wen",
            "Yuxi Zhou",
            "Shenda Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00508",
        "abstract": "While Retrieval-Augmented Generation (RAG) enables large language models (LLMs) to generate contextually grounded responses, contextual faithfulness remains challenging as LLMs may not consistently trust provided context, leading to hallucinations that undermine reliability. We observe an inverse correlation between response copying degree and context-unfaithful hallucinations on RAGTruth, suggesting that higher copying degrees reduce hallucinations by fostering genuine contextual belief. We propose CopyPasteLLM, obtained through two-stage high-copying response preference training. We design three prompting methods to enhance copying degree, demonstrating that high-copying responses achieve superior contextual faithfulness and hallucination control. These approaches enable a fully automated pipeline that transforms generated responses into high-copying preference data for training CopyPasteLLM. On FaithEval, ConFiQA and PubMedQA, CopyPasteLLM achieves best performance in both counterfactual and original contexts, remarkably with 12.2% to 24.5% accuracy improvements on FaithEval over the best baseline, while requiring only 365 training samples -- 1/50th of baseline data. To elucidate CopyPasteLLM's effectiveness, we propose the Context-Parameter Copying Capturing algorithm. Interestingly, this reveals that CopyPasteLLM recalibrates reliance on internal parametric knowledge rather than external knowledge during generation. All codes are available at https://github.com/longyongchao/CopyPasteLLM",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "119",
        "title": "JoyAgent-JDGenie: Technical Report on the GAIA",
        "author": [
            "Jiarun Liu",
            "Shiyue Xu",
            "Shangkun Liu",
            "Yang Li",
            "Wen Liu",
            "Min Liu",
            "Xiaoqing Zhou",
            "Hanmin Wang",
            "Shilin Jia",
            "zhen Wang",
            "Shaohua Tian",
            "Hanhao Li",
            "Junbo Zhang",
            "Yongli Yu",
            "Peng Cao",
            "Haofen Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00510",
        "abstract": "Large Language Models are increasingly deployed as autonomous agents for complex real-world tasks, yet existing systems often focus on isolated improvements without a unifying design for robustness and adaptability. We propose a generalist agent architecture that integrates three core components: a collective multi-agent framework combining planning and execution agents with critic model voting, a hierarchical memory system spanning working, semantic, and procedural layers, and a refined tool suite for search, code execution, and multimodal parsing. Evaluated on a comprehensive benchmark, our framework consistently outperforms open-source baselines and approaches the performance of proprietary systems. These results demonstrate the importance of system-level integration and highlight a path toward scalable, resilient, and adaptive AI assistants capable of operating across diverse domains and tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "120",
        "title": "Efficient Multi-modal Large Language Models via Progressive Consistency Distillation",
        "author": [
            "Zichen Wen",
            "Shaobo Wang",
            "Yufa Zhou",
            "Junyuan Zhang",
            "Qintong Zhang",
            "Yifeng Gao",
            "Zhaorun Chen",
            "Bin Wang",
            "Weijia Li",
            "Conghui He",
            "Linfeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00515",
        "abstract": "Visual tokens consume substantial computational resources in multi-modal large models (MLLMs), significantly compromising their efficiency. Recent works have attempted to improve efficiency by compressing visual tokens during training, either through modifications to model components or by introducing additional parameters. However, they often overlook the increased learning difficulty caused by such compression, as the model's parameter space struggles to quickly adapt to the substantial perturbations in the feature space induced by token compression. In this work, we propose to develop Efficient MLLMs via Progressive Consistency Distillation (EPIC), a progressive learning framework. Specifically, by decomposing the feature space perturbations introduced by token compression along the token-wise and layer-wise dimensions, we introduce token consistency distillation and layer consistency distillation, respectively, aiming to reduce the training difficulty by leveraging guidance from a teacher model and following a progressive learning trajectory. Extensive experiments demonstrate the superior effectiveness, robustness, and generalization capabilities of our proposed framework.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "121",
        "title": "Understanding Sensitivity of Differential Attention through the Lens of Adversarial Robustness",
        "author": [
            "Tsubasa Takahashi",
            "Shojiro Yamabe",
            "Futa Waseda",
            "Kento Sasaki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00517",
        "abstract": "Differential Attention (DA) has been proposed as a refinement to standard attention, suppressing redundant or noisy context through a subtractive structure and thereby reducing contextual hallucination. While this design sharpens task-relevant focus, we show that it also introduces a structural fragility under adversarial perturbations. Our theoretical analysis identifies negative gradient alignment-a configuration encouraged by DA's subtraction-as the key driver of sensitivity amplification, leading to increased gradient norms and elevated local Lipschitz constants. We empirically validate this Fragile Principle through systematic experiments on ViT/DiffViT and evaluations of pretrained CLIP/DiffCLIP, spanning five datasets in total. These results demonstrate higher attack success rates, frequent gradient opposition, and stronger local sensitivity compared to standard attention. Furthermore, depth-dependent experiments reveal a robustness crossover: stacking DA layers attenuates small perturbations via depth-dependent noise cancellation, though this protection fades under larger attack budgets. Overall, our findings uncover a fundamental trade-off: DA improves discriminative focus on clean inputs but increases adversarial vulnerability, underscoring the need to jointly design for selectivity and robustness in future attention mechanisms.",
        "tags": [
            "CLIP",
            "ViT"
        ]
    },
    {
        "id": "122",
        "title": "ARIONet: An Advanced Self-supervised Contrastive Representation Network for Birdsong Classification and Future Frame Prediction",
        "author": [
            "Md. Abdur Rahman",
            "Selvarajah Thuseethan",
            "Kheng Cher Yeo",
            "Reem E. Mohamed",
            "Sami Azam"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00522",
        "abstract": "Automated birdsong classification is essential for advancing ecological monitoring and biodiversity studies. Despite recent progress, existing methods often depend heavily on labeled data, use limited feature representations, and overlook temporal dynamics essential for accurate species identification. In this work, we propose a self-supervised contrastive network, ARIONet (Acoustic Representation for Interframe Objective Network), that jointly optimizes contrastive classification and future frame prediction using augmented audio representations. The model simultaneously integrates multiple complementary audio features within a transformer-based encoder model. Our framework is designed with two key objectives: (1) to learn discriminative species-specific representations for contrastive learning through maximizing similarity between augmented views of the same audio segment while pushing apart different samples, and (2) to model temporal dynamics by predicting future audio frames, both without requiring large-scale annotations. We validate our framework on four diverse birdsong datasets, including the British Birdsong Dataset, Bird Song Dataset, and two extended Xeno-Canto subsets (A-M and N-Z). Our method consistently outperforms existing baselines and achieves classification accuracies of 98.41%, 93.07%, 91.89%, and 91.58%, and F1-scores of 97.84%, 94.10%, 91.29%, and 90.94%, respectively. Furthermore, it demonstrates low mean absolute errors and high cosine similarity, up to 95%, in future frame prediction tasks. Extensive experiments further confirm the effectiveness of our self-supervised learning strategy in capturing complex acoustic patterns and temporal dependencies, as well as its potential for real-world applicability in ecological conservation and monitoring.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "123",
        "title": "VIRTUE: Visual-Interactive Text-Image Universal Embedder",
        "author": [
            "Wei-Yao Wang",
            "Kazuya Tateishi",
            "Qiyu Wu",
            "Shusuke Takahashi",
            "Yuki Mitsufuji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00523",
        "abstract": "Multimodal representation learning models have demonstrated successful operation across complex tasks, and the integration of vision-language models (VLMs) has further enabled embedding models with instruction-following capabilities. However, existing embedding models lack visual-interactive capabilities to specify regions of interest from users (e.g., point, bounding box, mask), which have been explored in generative models to broaden their human-interactive applicability. Equipping embedding models with visual interactions not only would unlock new applications with localized grounding of user intent, which remains unexplored, but also enable the models to learn entity-level information within images to complement their global representations for conventional embedding tasks. In this paper, we propose a novel Visual-InteRactive Text-Image Universal Embedder (VIRTUE) that extends the capabilities of the segmentation model and the vision-language model to the realm of representation learning. In VIRTUE, the segmentation model can process visual prompts that pinpoint specific regions within an image, thereby enabling the embedder to handle complex and ambiguous scenarios more precisely. To evaluate the visual-interaction ability of VIRTUE, we introduce a large-scale Segmentation-and-Scene Caption Retrieval (SCaR) benchmark comprising 1M samples that aims to retrieve the text caption by jointly considering the entity with a specific object and image scene. VIRTUE consistently achieves a state-of-the-art performance with significant improvements across 36 universal MMEB (3.1%-8.5%) and five visual-interactive SCaR (15.2%-20.3%) tasks.",
        "tags": [
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "124",
        "title": "Beyond Log Likelihood: Probability-Based Objectives for Supervised Fine-Tuning across the Model Capability Continuum",
        "author": [
            "Gaotang Li",
            "Ruizhong Qiu",
            "Xiusi Chen",
            "Heng Ji",
            "Hanghang Tong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00526",
        "abstract": "Supervised fine-tuning (SFT) is the standard approach for post-training large language models (LLMs), yet it often shows limited generalization. We trace this limitation to its default training objective: negative log likelihood (NLL). While NLL is classically optimal when training from scratch, post-training operates in a different paradigm and could violate its optimality assumptions, where models already encode task-relevant priors and supervision can be long and noisy. To this end, we study a general family of probability-based objectives and characterize their effectiveness under different conditions. Through comprehensive experiments and extensive ablation studies across 7 model backbones, 14 benchmarks, and 3 domains, we uncover a critical dimension that governs objective behavior: the model-capability continuum. Near the model-strong end, prior-leaning objectives that downweight low-probability tokens (e.g., $-p$, $-p^{10}$, thresholded variants) consistently outperform NLL; toward the model-weak end, NLL dominates; in between, no single objective prevails. Our theoretical analysis further elucidates how objectives trade places across the continuum, providing a principled foundation for adapting objectives to model capability. Our code is available at https://github.com/GaotangLi/Beyond-Log-Likelihood.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "125",
        "title": "Cascaded Diffusion Framework for Probabilistic Coarse-to-Fine Hand Pose Estimation",
        "author": [
            "Taeyun Woo",
            "Jinah Park",
            "Tae-Kyun Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00527",
        "abstract": "Deterministic models for 3D hand pose reconstruction, whether single-staged or cascaded, struggle with pose ambiguities caused by self-occlusions and complex hand articulations. Existing cascaded approaches refine predictions in a coarse-to-fine manner but remain deterministic and cannot capture pose uncertainties. Recent probabilistic methods model pose distributions yet are restricted to single-stage estimation, which often fails to produce accurate 3D reconstructions without refinement. To address these limitations, we propose a coarse-to-fine cascaded diffusion framework that combines probabilistic modeling with cascaded refinement. The first stage is a joint diffusion model that samples diverse 3D joint hypotheses, and the second stage is a Mesh Latent Diffusion Model (Mesh LDM) that reconstructs a 3D hand mesh conditioned on a joint sample. By training Mesh LDM with diverse joint hypotheses in a learned latent space, our framework learns distribution-aware joint-mesh relationships and robust hand priors. Furthermore, the cascaded design mitigates the difficulty of directly mapping 2D images to dense 3D poses, enhancing accuracy through sequential refinement. Experiments on FreiHAND and HO3Dv2 demonstrate that our method achieves state-of-the-art performance while effectively modeling pose distributions.",
        "tags": [
            "3D",
            "Diffusion",
            "Pose Estimation"
        ]
    },
    {
        "id": "126",
        "title": "Memory-Augmented Log Analysis with Phi-4-mini: Enhancing Threat Detection in Structured Security Logs",
        "author": [
            "Anbi Guo",
            "Mahfuza Farooque"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00529",
        "abstract": "Structured security logs are critical for detecting advanced persistent threats (APTs). Large language models (LLMs) struggle in this domain due to limited context and domain mismatch. We propose \\textbf{DM-RAG}, a dual-memory retrieval-augmented generation framework for structured log analysis. It integrates a short-term memory buffer for recent summaries and a long-term FAISS-indexed memory for historical patterns. An instruction-tuned Phi-4-mini processes the combined context and outputs structured predictions. Bayesian fusion promotes reliable persistence into memory. On the UNSW-NB15 dataset, DM-RAG achieves 53.64% accuracy and 98.70% recall, surpassing fine-tuned and RAG baselines in recall. The architecture is lightweight, interpretable, and scalable, enabling real-time threat monitoring without extra corpora or heavy tuning.",
        "tags": [
            "Detection",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "127",
        "title": "GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness",
        "author": [
            "Kung-Hsiang Huang",
            "Haoyi Qiu",
            "Yutong Dai",
            "Caiming Xiong",
            "Chien-Sheng Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00536",
        "abstract": "Graphical user interface (GUI) agents built on vision-language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames' keys onto the current frame's key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.",
        "tags": [
            "Transformer",
            "VLM"
        ]
    },
    {
        "id": "128",
        "title": "Spectral Scaling Laws in Language Models: How Effectively Do Feed-Forward Networks Use Their Latent Space?",
        "author": [
            "Nandan Kumar Jha",
            "Brandon Reagen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00537",
        "abstract": "As large language models (LLMs) scale, the question is not only how large they become, but how much of their capacity is effectively utilized. Existing scaling laws relate model size to loss, yet overlook how components exploit their latent space. We study feed-forward networks (FFNs) and recast width selection as a spectral utilization problem. Using a lightweight diagnostic suite -- Hard Rank (participation ratio), Soft Rank (Shannon rank), Spectral Concentration, and the composite Spectral Utilization Index (SUI) -- we quantify how many latent directions are meaningfully activated across LLaMA, GPT-2, and nGPT families. Our key finding is an asymmetric spectral scaling law: soft rank follows an almost perfect power law with FFN width, while hard rank grows only sublinearly and with high variance. This asymmetry suggests that widening FFNs mostly adds low-energy tail directions, while dominant-mode subspaces saturate early. Moreover, at larger widths, variance further collapses into a narrow subspace, leaving much of the latent space under-utilized. These results recast FFN width selection as a principled trade-off between tail capacity and dominant-mode capacity, offering concrete guidance for inference-efficient LLM design.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "129",
        "title": "Data Quality Challenges in Retrieval-Augmented Generation",
        "author": [
            "Leopold MÃ¼ller",
            "Joshua Holstein",
            "Sarah Bause",
            "Gerhard Satzger",
            "Niklas KÃ¼hl"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00552",
        "abstract": "Organizations increasingly adopt Retrieval-Augmented Generation (RAG) to enhance Large Language Models with enterprise-specific knowledge. However, current data quality (DQ) frameworks have been primarily developed for static datasets, and only inadequately address the dynamic, multi-stage nature of RAG systems. This study aims to develop DQ dimensions for this new type of AI-based systems. We conduct 16 semi-structured interviews with practitioners of leading IT service companies. Through a qualitative content analysis, we inductively derive 15 distinct DQ dimensions across the four processing stages of RAG systems: data extraction, data transformation, prompt & search, and generation. Our findings reveal that (1) new dimensions have to be added to traditional DQ frameworks to also cover RAG contexts; (2) these new dimensions are concentrated in early RAG steps, suggesting the need for front-loaded quality management strategies, and (3) DQ issues transform and propagate through the RAG pipeline, necessitating a dynamic, step-aware approach to quality management.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "130",
        "title": "On Predictability of Reinforcement Learning Dynamics for Large Language Models",
        "author": [
            "Yuchen Cai",
            "Ding Cao",
            "Xin Xu",
            "Zijun Yao",
            "Yuqing Huang",
            "Zhenyu Tan",
            "Benyi Zhang",
            "Guiquan Liu",
            "Junfeng Fang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00553",
        "abstract": "Recent advances in reasoning capabilities of large language models (LLMs) are largely driven by reinforcement learning (RL), yet the underlying parameter dynamics during RL training remain poorly understood. This work identifies two fundamental properties of RL-induced parameter updates in LLMs: (1) Rank-1 Dominance, where the top singular subspace of the parameter update matrix nearly fully determines reasoning improvements, recovering over 99\\% of performance gains; and (2) Rank-1 Linear Dynamics, where this dominant subspace evolves linearly throughout training, enabling accurate prediction from early checkpoints. Extensive experiments across 8 LLMs and 7 algorithms validate the generalizability of these properties. More importantly, based on these findings, we propose AlphaRL, a plug-in acceleration framework that extrapolates the final parameter update using a short early training window, achieving up to 2.5 speedup while retaining \\textgreater 96\\% of reasoning performance without extra modules or hyperparameter tuning. This positions our finding as a versatile and practical tool for large-scale RL, opening a path toward principled, interpretable, and efficient training paradigm for LLMs.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "131",
        "title": "PromptPilot: Improving Human-AI Collaboration Through LLM-Enhanced Prompt Engineering",
        "author": [
            "Niklas Gutheil",
            "Valentin Mayer",
            "Leopold MÃ¼ller",
            "JÃ¶rg Rommelt",
            "Niklas KÃ¼hl"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00555",
        "abstract": "Effective prompt engineering is critical to realizing the promised productivity gains of large language models (LLMs) in knowledge-intensive tasks. Yet, many users struggle to craft prompts that yield high-quality outputs, limiting the practical benefits of LLMs. Existing approaches, such as prompt handbooks or automated optimization pipelines, either require substantial effort, expert knowledge, or lack interactive guidance. To address this gap, we design and evaluate PromptPilot, an interactive prompting assistant grounded in four empirically derived design objectives for LLM-enhanced prompt engineering. We conducted a randomized controlled experiment with 80 participants completing three realistic, work-related writing tasks. Participants supported by PromptPilot achieved significantly higher performance (median: 78.3 vs. 61.7; p = .045, d = 0.56), and reported enhanced efficiency, ease-of-use, and autonomy during interaction. These findings empirically validate the effectiveness of our proposed design objectives, establishing LLM-enhanced prompt engineering as a viable technique for improving human-AI collaboration.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "132",
        "title": "Memory Determines Learning Direction: A Theory of Gradient-Based Optimization in State Space Models",
        "author": [
            "JingChuan Guan",
            "Tomoyuki Kubota",
            "Yasuo Kuniyoshi",
            "Kohei Nakajima"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00563",
        "abstract": "State space models (SSMs) have gained attention by showing potential to outperform Transformers. However, previous studies have not sufficiently addressed the mechanisms underlying their high performance owing to a lack of theoretical explanation of SSMs' learning dynamics. In this study, we provide such an explanation and propose an improved training strategy. The memory capacity of SSMs can be evaluated by examining how input time series are stored in their current state. Such an examination reveals a tradeoff between memory accuracy and length, as well as the theoretical equivalence between the structured state space sequence model (S4) and a simplified S4 with diagonal recurrent weights. This theoretical foundation allows us to elucidate the learning dynamics, proving the importance of initial parameters. Our analytical results suggest that successful learning requires the initial memory structure to be the longest possible even if memory accuracy may deteriorate or the gradient lose the teacher information. Experiments on tasks requiring long memory confirmed that extending memory is difficult, emphasizing the importance of initialization. Furthermore, we found that fixing recurrent weights can be more advantageous than adapting them because it achieves comparable or even higher performance with faster convergence. Our results provide a new theoretical foundation for SSMs and potentially offer a novel optimization strategy.",
        "tags": [
            "SSMs"
        ]
    },
    {
        "id": "133",
        "title": "Toward Safer Diffusion Language Models: Discovery and Mitigation of Priming Vulnerability",
        "author": [
            "Shojiro Yamabe",
            "Jun Sakuma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00565",
        "abstract": "Diffusion language models (DLMs) generate tokens in parallel through iterative denoising, which can reduce latency and enable bidirectional conditioning. However, the safety risks posed by jailbreak attacks that exploit this inference mechanism are not well understood. In this paper, we reveal that DLMs have a critical vulnerability stemming from their iterative denoising process and propose a countermeasure. Specifically, our investigation shows that if an affirmative token for a harmful query appears at an intermediate step, subsequent denoising can be steered toward a harmful response even in aligned models. As a result, simply injecting such affirmative tokens can readily bypass the safety guardrails. Furthermore, we demonstrate that the vulnerability allows existing optimization-based jailbreak attacks to succeed on DLMs. Building on this analysis, we propose a novel safety alignment method tailored to DLMs that trains models to generate safe responses from contaminated intermediate states that contain affirmative tokens. Our experiments indicate that the proposed method significantly mitigates the vulnerability with minimal impact on task performance. Furthermore, our method improves robustness against conventional jailbreak attacks. Our work underscores the need for DLM-specific safety research.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "134",
        "title": "Are Large Language Models Chronically Online Surfers? A Dataset for Chinese Internet Meme Explanation",
        "author": [
            "Yubo Xie",
            "Chenkai Wang",
            "Zongyang Ma",
            "Fahui Miao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00567",
        "abstract": "Large language models (LLMs) are trained on vast amounts of text from the Internet, but do they truly understand the viral content that rapidly spreads online -- commonly known as memes? In this paper, we introduce CHIME, a dataset for CHinese Internet Meme Explanation. The dataset comprises popular phrase-based memes from the Chinese Internet, annotated with detailed information on their meaning, origin, example sentences, types, etc. To evaluate whether LLMs understand these memes, we designed two tasks. In the first task, we assessed the models' ability to explain a given meme, identify its origin, and generate appropriate example sentences. The results show that while LLMs can explain the meanings of some memes, their performance declines significantly for culturally and linguistically nuanced meme types. Additionally, they consistently struggle to provide accurate origins for the memes. In the second task, we created a set of multiple-choice questions (MCQs) requiring LLMs to select the most appropriate meme to fill in a blank within a contextual sentence. While the evaluated models were able to provide correct answers, their performance remains noticeably below human levels. We have made CHIME public and hope it will facilitate future research on computational meme understanding.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "135",
        "title": "ReSeek: A Self-Correcting Framework for Search Agents with Instructive Rewards",
        "author": [
            "Shiyu Li",
            "Yang Tang",
            "Yifan Wang",
            "Peiming Li",
            "Xi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00568",
        "abstract": "Search agents powered by Large Language Models (LLMs) have demonstrated significant potential in tackling knowledge-intensive tasks. Reinforcement learning (RL) has emerged as a powerful paradigm for training these agents to perform complex, multi-step reasoning. However, prior RL-based methods often rely on sparse or rule-based rewards, which can lead agents to commit to suboptimal or erroneous reasoning paths without the ability to recover. To address these limitations, we propose ReSeek, a novel self-correcting framework for training search agents. Our framework introduces a self-correction mechanism that empowers the agent to dynamically identify and recover from erroneous search paths during an episode. By invoking a special JUDGE action, the agent can judge the information and re-plan its search strategy. To guide this process, we design a dense, instructive process reward function, which decomposes into a correctness reward for retrieving factual information and a utility reward for finding information genuinely useful for the query. Furthermore, to mitigate the risk of data contamination in existing datasets, we introduce FictionalHot, a new and challenging benchmark with recently curated questions requiring complex reasoning. Being intuitively reasonable and practically simple, extensive experiments show that agents trained with ReSeek significantly outperform SOTA baselines in task success rate and path faithfulness.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "136",
        "title": "Adaptive Shared Experts with LoRA-Based Mixture of Experts for Multi-Task Learning",
        "author": [
            "Minghao Yang",
            "Ren Togo",
            "Guang Li",
            "Takahiro Ogawa",
            "Miki Haseyama"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00570",
        "abstract": "Mixture-of-Experts (MoE) has emerged as a powerful framework for multi-task learning (MTL). However, existing MoE-MTL methods often rely on single-task pretrained backbones and suffer from redundant adaptation and inefficient knowledge sharing during the transition from single-task to multi-task learning (STL to MTL). To address these limitations, we propose adaptive shared experts (ASE) within a low-rank adaptation (LoRA) based MoE, where shared experts are assigned router-computed gating weights jointly normalized with sparse experts. This design facilitates STL to MTL transition, enhances expert specialization, and cooperation. Furthermore, we incorporate fine-grained experts by increasing the number of LoRA experts while proportionally reducing their rank, enabling more effective knowledge sharing under a comparable parameter budget. Extensive experiments on the PASCAL-Context benchmark, under unified training settings, demonstrate that ASE consistently improves performance across diverse configurations and validates the effectiveness of fine-grained designs for MTL.",
        "tags": [
            "LoRA",
            "MoE"
        ]
    },
    {
        "id": "137",
        "title": "GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks",
        "author": [
            "Yen-Ling Tai",
            "Yi-Ru Yang",
            "Kuan-Ting Yu",
            "Yu-Wei Chao",
            "Yi-Ting Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00573",
        "abstract": "Robotic food scooping is a critical manipulation skill for food preparation and service robots. However, existing robot learning algorithms, especially learn-from-demonstration methods, still struggle to handle diverse and dynamic food states, which often results in spillage and reduced reliability. In this work, we introduce GRITS: A Spillage-Aware Guided Diffusion Policy for Robot Food Scooping Tasks. This framework leverages guided diffusion policy to minimize food spillage during scooping and to ensure reliable transfer of food items from the initial to the target location. Specifically, we design a spillage predictor that estimates the probability of spillage given current observation and action rollout. The predictor is trained on a simulated dataset with food spillage scenarios, constructed from four primitive shapes (spheres, cubes, cones, and cylinders) with varied physical properties such as mass, friction, and particle size. At inference time, the predictor serves as a differentiable guidance signal, steering the diffusion sampling process toward safer trajectories while preserving task success. We validate GRITS on a real-world robotic food scooping platform. GRITS is trained on six food categories and evaluated on ten unseen categories with different shapes and quantities. GRITS achieves an 82% task success rate and a 4% spillage rate, reducing spillage by over 40% compared to baselines without guidance, thereby demonstrating its effectiveness.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "138",
        "title": "Arbitrary Generative Video Interpolation",
        "author": [
            "Guozhen Zhang",
            "Haiguang Wang",
            "Chunyu Wang",
            "Yuan Zhou",
            "Qinglin Lu",
            "Limin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00578",
        "abstract": "Video frame interpolation (VFI), which generates intermediate frames from given start and end frames, has become a fundamental function in video generation applications. However, existing generative VFI methods are constrained to synthesize a fixed number of intermediate frames, lacking the flexibility to adjust generated frame rates or total sequence duration. In this work, we present ArbInterp, a novel generative VFI framework that enables efficient interpolation at any timestamp and of any length. Specifically, to support interpolation at any timestamp, we propose the Timestamp-aware Rotary Position Embedding (TaRoPE), which modulates positions in temporal RoPE to align generated frames with target normalized timestamps. This design enables fine-grained control over frame timestamps, addressing the inflexibility of fixed-position paradigms in prior work. For any-length interpolation, we decompose long-sequence generation into segment-wise frame synthesis. We further design a novel appearance-motion decoupled conditioning strategy: it leverages prior segment endpoints to enforce appearance consistency and temporal semantics to maintain motion coherence, ensuring seamless spatiotemporal transitions across segments. Experimentally, we build comprehensive benchmarks for multi-scale frame interpolation (2x to 32x) to assess generalizability across arbitrary interpolation factors. Results show that ArbInterp outperforms prior methods across all scenarios with higher fidelity and more seamless spatiotemporal continuity. Project website: https://mcg-nju.github.io/ArbInterp-Web/.",
        "tags": [
            "RoPE",
            "Video Generation"
        ]
    },
    {
        "id": "139",
        "title": "CoT Vectors: Transferring and Probing the Reasoning Mechanisms of LLMs",
        "author": [
            "Li Li",
            "Ziyi Wang",
            "Yongliang Wu",
            "Jianfei Cai",
            "Xu Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00579",
        "abstract": "Chain-of-Thought (CoT) prompting has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs). However, existing implementations, such as in-context learning and fine-tuning, remain costly and inefficient. To improve CoT reasoning at a lower cost, and inspired by the task vector paradigm, we introduce CoT Vectors, compact representations that encode task-general, multi-step reasoning knowledge. Through experiments with Extracted CoT Vectors, we observe pronounced layer-wise instability, manifesting as a U-shaped performance curve that reflects a systematic three-stage reasoning process in LLMs. To address this limitation, we propose Learnable CoT Vectors, optimized under a teacher-student framework to provide more stable and robust guidance. Extensive evaluations across diverse benchmarks and models demonstrate that CoT Vectors not only outperform existing baselines but also achieve performance comparable to parameter-efficient fine-tuning methods, while requiring fewer trainable parameters. Moreover, by treating CoT Vectors as a probe, we uncover how their effectiveness varies due to latent space structure, information density, acquisition mechanisms, and pre-training differences, offering new insights into the functional organization of multi-step reasoning in LLMs. The source code will be released.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "140",
        "title": "Multi-level Dynamic Style Transfer for NeRFs",
        "author": [
            "Zesheng Li",
            "Shuaibo Li",
            "Wei Ma",
            "Jianwei Guo",
            "Hongbin Zha"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00592",
        "abstract": "As the application of neural radiance fields (NeRFs) in various 3D vision tasks continues to expand, numerous NeRF-based style transfer techniques have been developed. However, existing methods typically integrate style statistics into the original NeRF pipeline, often leading to suboptimal results in both content preservation and artistic stylization. In this paper, we present multi-level dynamic style transfer for NeRFs (MDS-NeRF), a novel approach that reengineers the NeRF pipeline specifically for stylization and incorporates an innovative dynamic style injection module. Particularly, we propose a multi-level feature adaptor that helps generate a multi-level feature grid representation from the content radiance field, effectively capturing the multi-scale spatial structure of the scene. In addition, we present a dynamic style injection module that learns to extract relevant style features and adaptively integrates them into the content patterns. The stylized multi-level features are then transformed into the final stylized view through our proposed multi-level cascade decoder. Furthermore, we extend our 3D style transfer method to support omni-view style transfer using 3D style references. Extensive experiments demonstrate that MDS-NeRF achieves outstanding performance for 3D style transfer, preserving multi-scale spatial structures while effectively transferring stylistic characteristics.",
        "tags": [
            "3D",
            "NeRF",
            "Style Transfer"
        ]
    },
    {
        "id": "141",
        "title": "Hybrid Training for Vision-Language-Action Models",
        "author": [
            "Pietro Mazzaglia",
            "Cansu Sancaktar",
            "Markus Peschl",
            "Daniel Dijkman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00600",
        "abstract": "Using Large Language Models to produce intermediate thoughts, a.k.a. Chain-of-thought (CoT), before providing an answer has been a successful recipe for solving complex language tasks. In robotics, similar embodied CoT strategies, generating thoughts before actions, have also been shown to lead to improved performance when using Vision-Language-Action models (VLAs). As these techniques increase the length of the model's generated outputs to include the thoughts, the inference time is negatively affected. Delaying an agent's actions in real-world executions, as in robotic manipulation settings, strongly affects the usability of a method, as tasks require long sequences of actions. However, is the generation of long chains-of-thought a strong prerequisite for achieving performance improvements? In this work, we explore the idea of Hybrid Training (HyT), a framework that enables VLAs to learn from thoughts and benefit from the associated performance gains, while enabling the possibility to leave out CoT generation during inference. Furthermore, by learning to conditionally predict a diverse set of outputs, HyT supports flexibility at inference time, enabling the model to either predict actions directly, generate thoughts or follow instructions. We evaluate the proposed method in a series of simulated benchmarks and real-world experiments.",
        "tags": [
            "CoT",
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "142",
        "title": "LVLMs as inspectors: an agentic framework for category-level structural defect annotation",
        "author": [
            "Sheng Jiang",
            "Yuanmin Ning",
            "Bingxi Huang",
            "Peiyin Chen",
            "Zhaohui Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00603",
        "abstract": "Automated structural defect annotation is essential for ensuring infrastructure safety while minimizing the high costs and inefficiencies of manual labeling. A novel agentic annotation framework, Agent-based Defect Pattern Tagger (ADPT), is introduced that integrates Large Vision-Language Models (LVLMs) with a semantic pattern matching module and an iterative self-questioning refinement mechanism. By leveraging optimized domain-specific prompting and a recursive verification process, ADPT transforms raw visual data into high-quality, semantically labeled defect datasets without any manual supervision. Experimental results demonstrate that ADPT achieves up to 98% accuracy in distinguishing defective from non-defective images, and 85%-98% annotation accuracy across four defect categories under class-balanced settings, with 80%-92% accuracy on class-imbalanced datasets. The framework offers a scalable and cost-effective solution for high-fidelity dataset construction, providing strong support for downstream tasks such as transfer learning and domain adaptation in structural damage assessment.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "143",
        "title": "ElasWave: An Elastic-Native System for Scalable Hybrid-Parallel Training",
        "author": [
            "Xueze Kang",
            "Guangyu Xiang",
            "Yuxin Wang",
            "Hao Zhang",
            "Yuchu Fang",
            "Yuhang Zhou",
            "Zhenheng Tang",
            "Youhui Lv",
            "Eliran Maman",
            "Mark Wasserman",
            "Alon Zameret",
            "Zhipeng Bian",
            "Shushu Chen",
            "Zhiyou Yu",
            "Jin Wang",
            "Xiaoyu Wu",
            "Yang Zheng",
            "Chen Tian",
            "Xiaowen Chu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00606",
        "abstract": "Large-scale LLM pretraining today spans $10^{5}$--$10^{6}$ accelerators, making failures commonplace and elasticity no longer optional. We posit that an elastic-native training system must simultaneously ensure (i) Parameter Consistency, (ii) low Mean Time to Recovery (MTTR), (iii) high post-change Throughput, and (iv) Computation Consistency. This objective set not has never been jointly attained by prior work. To achieve these goals, we present ElasWave, which provides per-step fault tolerance via multi-dimensional scheduling across Graph, Dataflow, Frequency, and Random Number Generation. ElasWave resizes and reshards micro-batch workloads while preserving the global batch size and gradient scale; it performs online pipeline resharding with asynchronous parameter migration, interleaving ZeRO partitions so recovery reduces to disjoint rank-to-rank transfers. It further uses DVFS to absorb pipeline bubbles and reshards RNG to keep consistent computations. A dynamic communicator enables in-place communication group edits, while per-step in-memory snapshots support online verification and redistribution. We evaluated ElasWave on 96 NPUs and benchmarked against state-of-the-art baselines: throughput improves by $1.35\\times$ over ReCycle and $1.60\\times$ over TorchFT; communicator recovery completes within one second (up to $82\\times/3.6\\times$ faster than full/partial rebuilds); migration MTTR drops by as much as $51\\%$; and convergence deviation is reduced by approximately $78\\%$.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "144",
        "title": "ACON: Optimizing Context Compression for Long-horizon LLM Agents",
        "author": [
            "Minki Kang",
            "Wei-Ning Chen",
            "Dongge Han",
            "Huseyin A. Inan",
            "Lukas Wutschitz",
            "Yanzhi Chen",
            "Robert Sim",
            "Saravan Rajmohan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00615",
        "abstract": "Large language models (LLMs) are increasingly deployed as agents in dynamic, real-world environments, where success requires both reasoning and effective tool use. A central challenge for agentic tasks is the growing context length, as agents must accumulate long histories of actions and observations. This expansion raises costs and reduces efficiency in long-horizon tasks, yet prior work on context compression has mostly focused on single-step tasks or narrow applications. We introduce Agent Context Optimization (ACON), a unified framework that optimally compresses both environment observations and interaction histories into concise yet informative condensations. ACON leverages compression guideline optimization in natural language space: given paired trajectories where full context succeeds but compressed context fails, capable LLMs analyze the causes of failure, and the compression guideline is updated accordingly. Furthermore, we propose distilling the optimized LLM compressor into smaller models to reduce the overhead of the additional module. Experiments on AppWorld, OfficeBench, and Multi-objective QA show that ACON reduces memory usage by 26-54% (peak tokens) while largely preserving task performance, preserves over 95% of accuracy when distilled into smaller compressors, and enhances smaller LMs as long-horizon agents with up to 46% performance improvement.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "145",
        "title": "Robust Context-Aware Object Recognition",
        "author": [
            "Klara Janouskova",
            "Cristian Gavrus",
            "Jiri Matas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00618",
        "abstract": "In visual recognition, both the object of interest (referred to as foreground, FG, for simplicity) and its surrounding context (background, BG) play an important role. However, standard supervised learning often leads to unintended over-reliance on the BG, known as shortcut learning of spurious correlations, limiting model robustness in real-world deployment settings. In the literature, the problem is mainly addressed by suppressing the BG, sacrificing context information for improved generalization.\nWe propose RCOR -- Robust Context-Aware Object Recognition -- the first approach that jointly achieves robustness and context-awareness without compromising either. RCOR treats localization as an integral part of recognition to decouple object-centric and context-aware modelling, followed by a robust, non-parametric fusion. It improves the performance of both supervised models and VLM on datasets with both in-domain and out-of-domain BG, even without fine-tuning. The results confirm that localization before recognition is now possible even in complex scenes as in ImageNet-1k.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "146",
        "title": "HARPA: A Testability-Driven, Literature-Grounded Framework for Research Ideation",
        "author": [
            "Rosni Vasu",
            "Peter Jansen",
            "Pao Siangliulue",
            "Cristina Sarasua",
            "Abraham Bernstein",
            "Peter Clark",
            "Bhavana Dalvi Mishra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00620",
        "abstract": "While there has been a surge of interest in automated scientific discovery (ASD), especially with the emergence of LLMs, it remains challenging for tools to generate hypotheses that are both testable and grounded in the scientific literature. Additionally, existing ideation tools are not adaptive to prior experimental outcomes. We developed HARPA to address these challenges by incorporating the ideation workflow inspired by human researchers. HARPA first identifies emerging research trends through literature mining, then explores hypothesis design spaces, and finally converges on precise, testable hypotheses by pinpointing research gaps and justifying design choices. Our evaluations show that HARPA-generated hypothesis-driven research proposals perform comparably to a strong baseline AI-researcher across most qualitative dimensions (e.g., specificity, novelty, overall quality), but achieve significant gains in feasibility(+0.78, p$<0.05$, bootstrap) and groundedness (+0.85, p$<0.01$, bootstrap) on a 10-point Likert scale. When tested with the ASD agent (CodeScientist), HARPA produced more successful executions (20 vs. 11 out of 40) and fewer failures (16 vs. 21 out of 40), showing that expert feasibility judgments track with actual execution success. Furthermore, to simulate how researchers continuously refine their understanding of what hypotheses are both testable and potentially interesting from experience, HARPA learns a reward model that scores new hypotheses based on prior experimental outcomes, achieving approx. a 28\\% absolute gain over HARPA's untrained baseline scorer. Together, these methods represent a step forward in the field of AI-driven scientific discovery.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "147",
        "title": "FAME: Adaptive Functional Attention with Expert Routing for Function-on-Function Regression",
        "author": [
            "Yifei Gao",
            "Yong Chen",
            "Chen Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00621",
        "abstract": "Functional data play a pivotal role across science and engineering, yet their infinite-dimensional nature makes representation learning challenging. Conventional statistical models depend on pre-chosen basis expansions or kernels, limiting the flexibility of data-driven discovery, while many deep-learning pipelines treat functions as fixed-grid vectors, ignoring inherent continuity. In this paper, we introduce Functional Attention with a Mixture-of-Experts (FAME), an end-to-end, fully data-driven framework for function-on-function regression. FAME forms continuous attention by coupling a bidirectional neural controlled differential equation with MoE-driven vector fields to capture intra-functional continuity, and further fuses change to inter-functional dependencies via multi-head cross attention. Extensive experiments on synthetic and real-world functional-regression benchmarks show that FAME achieves state-of-the-art accuracy, strong robustness to arbitrarily sampled discrete observations of functions.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "148",
        "title": "UCD: Unconditional Discriminator Promotes Nash Equilibrium in GANs",
        "author": [
            "Mengfei Xia",
            "Nan Xue",
            "Jiapeng Zhu",
            "Yujun Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00624",
        "abstract": "Adversarial training turns out to be the key to one-step generation, especially for Generative Adversarial Network (GAN) and diffusion model distillation. Yet in practice, GAN training hardly converges properly and struggles in mode collapse. In this work, we quantitatively analyze the extent of Nash equilibrium in GAN training, and conclude that redundant shortcuts by inputting condition in $D$ disables meaningful knowledge extraction. We thereby propose to employ an unconditional discriminator (UCD), in which $D$ is enforced to extract more comprehensive and robust features with no condition injection. In this way, $D$ is able to leverage better knowledge to supervise $G$, which promotes Nash equilibrium in GAN literature. Theoretical guarantee on compatibility with vanilla GAN theory indicates that UCD can be implemented in a plug-in manner. Extensive experiments confirm the significant performance improvements with high efficiency. For instance, we achieved \\textbf{1.47 FID} on the ImageNet-64 dataset, surpassing StyleGAN-XL and several state-of-the-art one-step diffusion models. The code will be made publicly available.",
        "tags": [
            "Diffusion",
            "GAN",
            "StyleGAN"
        ]
    },
    {
        "id": "149",
        "title": "Is Model Editing Built on Sand? Revealing Its Illusory Success and Fragile Foundation",
        "author": [
            "Wei Liu",
            "Haomei Xu",
            "Bingqing Liu",
            "Zhiying Deng",
            "Haozhao Wang",
            "Jun Wang",
            "Ruixuan Li",
            "Yee Whye Teh",
            "Wee Sun Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00625",
        "abstract": "Large language models (LLMs) inevitably encode outdated or incorrect knowledge. Updating, deleting, and forgetting such knowledge is important for alignment, safety, and other issues. To address this issue, model editing has emerged as a promising paradigm: by precisely editing a small subset of parameters such that a specific fact is updated while preserving other knowledge. Despite its great success reported in previous papers, we find the apparent reliability of editing rests on a fragile foundation and the current literature is largely driven by illusory success. The fundamental goal of steering the model's output toward a target with minimal modification would encourage exploiting hidden shortcuts, rather than utilizing real semantics. This problem directly challenges the feasibility of the current model editing literature at its very foundation, as shortcuts are inherently at odds with robust knowledge integration. Coincidentally, this issue has long been obscured by evaluation frameworks that lack the design of negative examples. To uncover it, we systematically develop a suite of new evaluation methods. Strikingly, we find that state-of-the-art approaches collapse even under the simplest negation queries. Our empirical evidence shows that editing is likely to be based on shortcuts rather than full semantics, calling for an urgent reconsideration of the very basis of model editing before further advancements can be meaningfully pursued.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "150",
        "title": "Collaborative-Distilled Diffusion Models (CDDM) for Accelerated and Lightweight Trajectory Prediction",
        "author": [
            "Bingzhang Wang",
            "Kehua Chen",
            "Yinhai Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00627",
        "abstract": "Trajectory prediction is a fundamental task in Autonomous Vehicles (AVs) and Intelligent Transportation Systems (ITS), supporting efficient motion planning and real-time traffic safety management. Diffusion models have recently demonstrated strong performance in probabilistic trajectory prediction, but their large model size and slow sampling process hinder real-world deployment. This paper proposes Collaborative-Distilled Diffusion Models (CDDM), a novel method for real-time and lightweight trajectory prediction. Built upon Collaborative Progressive Distillation (CPD), CDDM progressively transfers knowledge from a high-capacity teacher diffusion model to a lightweight student model, jointly reducing both the number of sampling steps and the model size across distillation iterations. A dual-signal regularized distillation loss is further introduced to incorporate guidance from both the teacher and ground-truth data, mitigating potential overfitting and ensuring robust performance. Extensive experiments on the ETH-UCY pedestrian benchmark and the nuScenes vehicle benchmark demonstrate that CDDM achieves state-of-the-art prediction accuracy. The well-distilled CDDM retains 96.2% and 95.5% of the baseline model's ADE and FDE performance on pedestrian trajectories, while requiring only 231K parameters and 4 or 2 sampling steps, corresponding to 161x compression, 31x acceleration, and 9 ms latency. Qualitative results further show that CDDM generates diverse and accurate trajectories under dynamic agent behaviors and complex social interactions. By bridging high-performing generative models with practical deployment constraints, CDDM enables resource-efficient probabilistic prediction for AVs and ITS. Code is available at https://github.com/bingzhangw/CDDM.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "151",
        "title": "LAKAN: Landmark-assisted Adaptive Kolmogorov-Arnold Network for Face Forgery Detection",
        "author": [
            "Jiayao Jiang",
            "Siran Peng",
            "Bin Liu",
            "Qi Chu",
            "Nenghai Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00634",
        "abstract": "The rapid development of deepfake generation techniques necessitates robust face forgery detection algorithms. While methods based on Convolutional Neural Networks (CNNs) and Transformers are effective, there is still room for improvement in modeling the highly complex and non-linear nature of forgery artifacts. To address this issue, we propose a novel detection method based on the Kolmogorov-Arnold Network (KAN). By replacing fixed activation functions with learnable splines, our KAN-based approach is better suited to this challenge. Furthermore, to guide the network's focus towards critical facial areas, we introduce a Landmark-assisted Adaptive Kolmogorov-Arnold Network (LAKAN) module. This module uses facial landmarks as a structural prior to dynamically generate the internal parameters of the KAN, creating an instance-specific signal that steers a general-purpose image encoder towards the most informative facial regions with artifacts. This core innovation creates a powerful combination between geometric priors and the network's learning process. Extensive experiments on multiple public datasets show that our proposed method achieves superior performance.",
        "tags": [
            "Detection",
            "KAN"
        ]
    },
    {
        "id": "152",
        "title": "Erased, But Not Forgotten: Erased Rectified Flow Transformers Still Remain Unsafe Under Concept Attack",
        "author": [
            "Nanxiang Jiang",
            "Zhaoxin Fan",
            "Enhan Kang",
            "Daiheng Gao",
            "Yun Zhou",
            "Yanxia Chang",
            "Zheng Zhu",
            "Yeying Jin",
            "Wenjun Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00635",
        "abstract": "Recent advances in text-to-image (T2I) diffusion models have enabled impressive generative capabilities, but they also raise significant safety concerns due to the potential to produce harmful or undesirable content. While concept erasure has been explored as a mitigation strategy, most existing approaches and corresponding attack evaluations are tailored to Stable Diffusion (SD) and exhibit limited effectiveness when transferred to next-generation rectified flow transformers such as Flux. In this work, we present ReFlux, the first concept attack method specifically designed to assess the robustness of concept erasure in the latest rectified flow-based T2I framework. Our approach is motivated by the observation that existing concept erasure techniques, when applied to Flux, fundamentally rely on a phenomenon known as attention localization. Building on this insight, we propose a simple yet effective attack strategy that specifically targets this property. At its core, a reverse-attention optimization strategy is introduced to effectively reactivate suppressed signals while stabilizing attention. This is further reinforced by a velocity-guided dynamic that enhances the robustness of concept reactivation by steering the flow matching process, and a consistency-preserving objective that maintains the global layout and preserves unrelated content. Extensive experiments consistently demonstrate the effectiveness and efficiency of the proposed attack method, establishing a reliable benchmark for evaluating the robustness of concept erasure strategies in rectified flow transformers.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Flow Matching",
            "Rectified Flow",
            "Text-to-Image"
        ]
    },
    {
        "id": "153",
        "title": "Expected Attention: KV Cache Compression by Estimating Attention from Future Queries Distribution",
        "author": [
            "Alessio Devoto",
            "Maximilian Jeblick",
            "Simon JÃ©gou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00636",
        "abstract": "Memory consumption of the Key-Value (KV) cache represents a major bottleneck for efficient large language model inference. While attention-score-based KV cache pruning shows promise, it faces critical practical limitations: attention scores from future tokens are unavailable during compression, and modern implementations like Flash Attention do not materialize the full attention matrix, making past scores inaccessible. To overcome these challenges, we introduce $\\textbf{Expected Attention, a training-free compression method}$ that estimates KV pairs importance by predicting how future queries will attend to them. Our approach leverages the distributional properties of LLM activations to compute expected attention scores in closed form for each KV pair. These scores enable principled ranking and pruning of KV pairs with minimal impact on the residual stream, achieving effective compression without performance degradation. Importantly, our method operates seamlessly across both prefilling and decoding phases, consistently outperforming state-of-the-art baselines in both scenarios. Finally, $\\textbf{we release KVPress, a comprehensive library to enable researchers to implement and benchmark KV cache compression methods, already including more than 20 techniques}$.",
        "tags": [
            "Flash Attention",
            "LLM"
        ]
    },
    {
        "id": "154",
        "title": "MCM-DPO: Multifaceted Cross-Modal Direct Preference Optimization for Alt-text Generation",
        "author": [
            "Jinlan Fu",
            "Shenzhen Huangfu",
            "Hao Fei",
            "Yichong Huang",
            "Xiaoyu Shen",
            "Xipeng Qiu",
            "See-Kiong Ng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00647",
        "abstract": "The alt-text generation task produces concise, context-relevant descriptions of images, enabling blind and low-vision users to access online images. Despite the capabilities of large vision-language models, alt-text generation performance remains limited due to noisy user annotations, inconsistent standards, and MLLMs' insensitivity to contextual information. Previous efforts to fine-tune MLLMs using supervised fine-tuning (SFT) have struggled, as SFT relies on accurate target annotations, which are often flawed in user-generated alt-text. To address this, we propose Multi-faceted Cross-modal Direct Preference Optimization (MCM-DPO), which improves alt-text generation by learning to identify better options in preference pairs without requiring precise annotations. MCM-DPO optimizes preferences across single, paired, and multi-preference dimensions, covering textual, visual, and cross-modal factors. In light of the scarcity of high-quality annotated and preference-labeled datasets for alt-text, we constructed two large-scale, high-quality datasets named TAlt and PAlt, sourced from Twitter and Pinterest. These datasets include 202k annotated alt-text samples and 18k preference pairs that cover diverse preference dimensions, aiming to support further research in this domain. Experimental results show that our proposed MCM-DPO method consistently outperforms both DPO and SFT, establishing a new state of the art in alt-text generation. We release the code and data here: https://github.com/LVUGAI/MCM-DPO",
        "tags": [
            "DPO",
            "VLM"
        ]
    },
    {
        "id": "155",
        "title": "Align Your Tangent: Training Better Consistency Models via Manifold-Aligned Tangents",
        "author": [
            "Beomsu Kim",
            "Byunghee Cha",
            "Jong Chul Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00658",
        "abstract": "With diffusion and flow matching models achieving state-of-the-art generating performance, the interest of the community now turned to reducing the inference time without sacrificing sample quality. Consistency Models (CMs), which are trained to be consistent on diffusion or probability flow ordinary differential equation (PF-ODE) trajectories, enable one or two-step flow or diffusion sampling. However, CMs typically require prolonged training with large batch sizes to obtain competitive sample quality. In this paper, we examine the training dynamics of CMs near convergence and discover that CM tangents -- CM output update directions -- are quite oscillatory, in the sense that they move parallel to the data manifold, not towards the manifold. To mitigate oscillatory tangents, we propose a new loss function, called the manifold feature distance (MFD), which provides manifold-aligned tangents that point toward the data manifold. Consequently, our method -- dubbed Align Your Tangent (AYT) -- can accelerate CM training by orders of magnitude and even out-perform the learned perceptual image patch similarity metric (LPIPS). Furthermore, we find that our loss enables training with extremely small batch sizes without compromising sample quality. Code: https://github.com/1202kbs/AYT",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "Flow Matching",
            "ODE"
        ]
    },
    {
        "id": "156",
        "title": "Facilitating Cognitive Accessibility with LLMs: A Multi-Task Approach to Easy-to-Read Text Generation",
        "author": [
            "FranÃ§ois Ledoyen",
            "GaÃ«l Dias",
            "Jeremie Pantin",
            "Alexis Lechervy",
            "Fabrice Maurel",
            "Youssef Chahir"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00662",
        "abstract": "Simplifying complex texts is essential for ensuring equitable access to information, especially for individuals with cognitive impairments. The Easy-to-Read (ETR) initiative offers a framework for making content accessible to the neurodivergent population, but the manual creation of such texts remains time-consuming and resource-intensive. In this work, we investigate the potential of large language models (LLMs) to automate the generation of ETR content. To address the scarcity of aligned corpora and the specificity of ETR constraints, we propose a multi-task learning (MTL) approach that trains models jointly on text summarization, text simplification, and ETR generation. We explore two different strategies: multi-task retrieval-augmented generation (RAG) for in-context learning, and MTL-LoRA for parameter-efficient fine-tuning. Our experiments with Mistral-7B and LLaMA-3-8B, based on ETR-fr, a new high-quality dataset, demonstrate the benefits of multi-task setups over single-task baselines across all configurations. Moreover, results show that the RAG-based strategy enables generalization in out-of-domain settings, while MTL-LoRA outperforms all learning strategies within in-domain configurations.",
        "tags": [
            "LLM",
            "LLaMA",
            "LoRA",
            "RAG"
        ]
    },
    {
        "id": "157",
        "title": "A Geometric Unification of Generative AI with Manifold-Probabilistic Projection Models",
        "author": [
            "Leah Bar",
            "Liron Mor Yosef",
            "Shai Zucker",
            "Neta Shoham",
            "Inbar Seroussi",
            "Nir Sochen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00666",
        "abstract": "The foundational premise of generative AI for images is the assumption that images are inherently low-dimensional objects embedded within a high-dimensional space. Additionally, it is often implicitly assumed that thematic image datasets form smooth or piecewise smooth manifolds. Common approaches overlook the geometric structure and focus solely on probabilistic methods, approximating the probability distribution through universal approximation techniques such as the kernel method. In some generative models, the low dimensional nature of the data manifest itself by the introduction of a lower dimensional latent space. Yet, the probability distribution in the latent or the manifold coordinate space is considered uninteresting and is predefined or considered uniform. This study unifies the geometric and probabilistic perspectives by providing a geometric framework and a kernel-based probabilistic method simultaneously. The resulting framework demystifies diffusion models by interpreting them as a projection mechanism onto the manifold of ``good images''. This interpretation leads to the construction of a new deterministic model, the Manifold-Probabilistic Projection Model (MPPM), which operates in both the representation (pixel) space and the latent space. We demonstrate that the Latent MPPM (LMPPM) outperforms the Latent Diffusion Model (LDM) across various datasets, achieving superior results in terms of image restoration and generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "158",
        "title": "OTFS for Joint Radar and Communication: Algorithms, Prototypes, and Experiments",
        "author": [
            "Xiaojuan Zhang",
            "Yonghong Zeng",
            "Francois Chin Po Shin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00668",
        "abstract": "We propose an Joint Radar and Communication (JRC) system that utilizes the Orthogonal Time Frequency Space (OTFS) signals. The system features a fast radar sensing algorithm for detecting target range and speed by using the OTFS communication signals, and a self-interference cancellation for enhanced multi-target separation. In addition to target detection, we propose methods for monitoring human vital signs, such as breathing rate and heartbeat. Furthermore, we explore two approaches for distinguishing between human and nonhuman targets: one based on signal processing and the other based on machine learning. We have developed a prototype JRC system using the software-defined radio (SDR) technology. Experimental results are shown to demonstrate the effectiveness of the prototype in detecting range, speed, and vital signs in both human and mobile robot scenarios, as well as in distinguishing between human and non-human targets.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "159",
        "title": "Adaptive Event Stream Slicing for Open-Vocabulary Event-Based Object Detection via Vision-Language Knowledge Distillation",
        "author": [
            "Jinchang Zhang",
            "Zijun Li",
            "Jiakai Lin",
            "Guoyu Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00681",
        "abstract": "Event cameras offer advantages in object detection tasks due to high-speed response, low latency, and robustness to motion blur. However, event cameras lack texture and color information, making open-vocabulary detection particularly challenging. Current event-based detection methods are typically trained on predefined categories, limiting their ability to generalize to novel objects, where encountering previously unseen objects is common. Vision-language models (VLMs) have enabled open-vocabulary object detection in RGB images. However, the modality gap between images and event streams makes it ineffective to directly transfer CLIP to event data, as CLIP was not designed for event streams. To bridge this gap, we propose an event-image knowledge distillation framework that leverages CLIP's semantic understanding to achieve open-vocabulary object detection on event data. Instead of training CLIP directly on event streams, we use image frames as inputs to a teacher model, guiding the event-based student model to learn CLIP's rich visual representations. Through spatial attention-based distillation, the student network learns meaningful visual features directly from raw event inputs while inheriting CLIP's broad visual knowledge. Furthermore, to prevent information loss due to event data segmentation, we design a hybrid spiking neural network (SNN) and convolutional neural network (CNN) framework. Unlike fixed-group event segmentation methods, which often discard crucial temporal information, our SNN adaptively determines the optimal event segmentation moments, ensuring that key temporal features are extracted. The extracted event features are then processed by CNNs for object detection.",
        "tags": [
            "CLIP",
            "Detection",
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "160",
        "title": "Stochastic Self-Organization in Multi-Agent Systems",
        "author": [
            "Nurbek Tastan",
            "Samuel Horvath",
            "Karthik Nandakumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00685",
        "abstract": "Multi-agent systems (MAS) based on Large Language Models (LLMs) have the potential to solve tasks that are beyond the reach of any single LLM. However, this potential can only be realized when the collaboration mechanism between agents is optimized. Specifically, optimizing the communication structure between agents is critical for fruitful collaboration. Most existing approaches rely on fixed topologies, pretrained graph generators, optimization over edges, or employ external LLM judges, thereby adding to the complexity. In this work, we introduce a response-conditioned framework that adapts communication on-the-fly. Agents independently generate responses to the user query and assess peer contributions using an approximation of the Shapley value. A directed acyclic graph (DAG) is then constructed to regulate the propagation of the responses among agents, which ensures stable and efficient message transmission from high-contributing agents to others. This graph is dynamically updated based on the agent responses from the previous collaboration round. Since the proposed framework enables the self-organization of agents without additional supervision or training, we refer to it as SelfOrg. The SelfOrg framework goes beyond task- and query-level optimization and takes into account the stochastic nature of agent responses. Experiments with both strong and weak LLM backends demonstrate robust performance, with significant gains in the weak regime where prior methods collapse. We also theoretically show that multiple agents increase the chance of correctness and that the correct responses naturally dominate the information flow.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "161",
        "title": "ACPO: Adaptive Curriculum Policy Optimization for Aligning Vision-Language Models in Complex Reasoning",
        "author": [
            "Yunhao Wang",
            "Ziting Li",
            "Shuai Chen",
            "Tao Liu",
            "Chao Song",
            "Junjie Jiang",
            "Jian Zhu",
            "Peng Gao",
            "Bin Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00690",
        "abstract": "Aligning large-scale vision-language models (VLMs) for complex reasoning via reinforcement learning is often hampered by the limitations of existing policy optimization algorithms, such as static training schedules and the rigid, uniform clipping mechanism in Proximal Policy Optimization (PPO). In this work, we introduce Adaptive Curriculum Policy Optimization (ACPO), a novel framework that addresses these challenges through a dual-component adaptive learning strategy. First, ACPO employs a dynamic curriculum that orchestrates a principled transition from a stable, near on-policy exploration phase to an efficient, off-policy exploitation phase by progressively increasing sample reuse. Second, we propose an Advantage-Aware Adaptive Clipping (AAAC) mechanism that replaces the fixed clipping hyperparameter with dynamic, sample-wise bounds modulated by the normalized advantage of each token. This allows for more granular and robust policy updates, enabling larger gradients for high-potential samples while safeguarding against destructive ones. We conduct extensive experiments on a suite of challenging multimodal reasoning benchmarks, including MathVista, LogicVista, and MMMU-Pro. Results demonstrate that ACPO consistently outperforms strong baselines such as DAPO and PAPO, achieving state-of-the-art performance, accelerated convergence, and superior training stability.",
        "tags": [
            "PPO",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "162",
        "title": "Inclusive Easy-to-Read Generation for Individuals with Cognitive Impairments",
        "author": [
            "FranÃ§ois Ledoyen",
            "GaÃ«l Dias",
            "Alexis Lechervy",
            "Jeremie Pantin",
            "Fabrice Maurel",
            "Youssef Chahir",
            "Elisa Gouzonnat",
            "MÃ©lanie Berthelot",
            "Stanislas Moravac",
            "Armony Altinier",
            "Amy Khairalla"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00691",
        "abstract": "Ensuring accessibility for individuals with cognitive impairments is essential for autonomy, self-determination, and full citizenship. However, manual Easy-to-Read (ETR) text adaptations are slow, costly, and difficult to scale, limiting access to crucial information in healthcare, education, and civic life. AI-driven ETR generation offers a scalable solution but faces key challenges, including dataset scarcity, domain adaptation, and balancing lightweight learning of Large Language Models (LLMs). In this paper, we introduce ETR-fr, the first dataset for ETR text generation fully compliant with European ETR guidelines. We implement parameter-efficient fine-tuning on PLMs and LLMs to establish generative baselines. To ensure high-quality and accessible outputs, we introduce an evaluation framework based on automatic metrics supplemented by human assessments. The latter is conducted using a 36-question evaluation form that is aligned with the guidelines. Overall results show that PLMs perform comparably to LLMs and adapt effectively to out-of-domain texts.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "163",
        "title": "ALARB: An Arabic Legal Argument Reasoning Benchmark",
        "author": [
            "Harethah Abu Shairah",
            "Somayah AlHarbi",
            "Abdulaziz AlHussein",
            "Sameer Alsabea",
            "Omar Shaqaqi",
            "Hebah AlShamlan",
            "Omar Knio",
            "George Turkiyyah"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00694",
        "abstract": "We introduce ALARB, a dataset and suite of tasks designed to evaluate the reasoning capabilities of large language models (LLMs) within the Arabic legal domain. While existing Arabic benchmarks cover some knowledge-intensive tasks such as retrieval and understanding, substantial datasets focusing specifically on multistep reasoning for Arabic LLMs, especially in open-ended contexts, are lacking. The dataset comprises over 13K commercial court cases from Saudi Arabia, with each case including the facts presented, the reasoning of the court, the verdict, as well as the cited clauses extracted from the regulatory documents. We define a set of challenging tasks leveraging this dataset and reflecting the complexity of real-world legal reasoning, including verdict prediction, completion of reasoning chains in multistep legal arguments, and identification of relevant regulations based on case facts. We benchmark a representative selection of current open and closed Arabic LLMs on these tasks and demonstrate the dataset's utility for instruction tuning. Notably, we show that instruction-tuning a modest 12B parameter model using ALARB significantly enhances its performance in verdict prediction and Arabic verdict generation, reaching a level comparable to that of GPT-4o.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "164",
        "title": "HAMLET: Switch your Vision-Language-Action Model into a History-Aware Policy",
        "author": [
            "Myungkyu Koo",
            "Daewon Choi",
            "Taeyoung Kim",
            "Kyungmin Lee",
            "Changyeon Kim",
            "Youngyo Seo",
            "Jinwoo Shin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00695",
        "abstract": "Inherently, robotic manipulation tasks are history-dependent: leveraging past context could be beneficial. However, most existing Vision-Language-Action models (VLAs) have been designed without considering this aspect, i.e., they rely solely on the current observation, ignoring preceding context. In this paper, we propose HAMLET, a scalable framework to adapt VLAs to attend to the historical context during action prediction. Specifically, we introduce moment tokens that compactly encode perceptual information at each timestep. Their representations are initialized with time-contrastive learning, allowing them to better capture temporally distinctive aspects. Next, we employ a lightweight memory module that integrates the moment tokens across past timesteps into memory features, which are then leveraged for action prediction. Through empirical evaluation, we show that HAMLET successfully transforms a state-of-the-art VLA into a history-aware policy, especially demonstrating significant improvements on long-horizon tasks that require historical context. In particular, on top of GR00T N1.5, HAMLET achieves an average success rate of 76.4% on history-dependent real-world tasks, surpassing the baseline performance by 47.2%. Furthermore, HAMLET pushes prior art performance from 64.1% to 66.4% on RoboCasa Kitchen (100-demo setup) and from 95.6% to 97.7% on LIBERO, highlighting its effectiveness even under generic robot-manipulation benchmarks.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "165",
        "title": "Graph Integrated Multimodal Concept Bottleneck Model",
        "author": [
            "Jiakai Lin",
            "Jinchang Zhang",
            "Guoyu Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00701",
        "abstract": "With growing demand for interpretability in deep learning, especially in high stakes domains, Concept Bottleneck Models (CBMs) address this by inserting human understandable concepts into the prediction pipeline, but they are generally single modal and ignore structured concept relationships. To overcome these limitations, we present MoE-SGT, a reasoning driven framework that augments CBMs with a structure injecting Graph Transformer and a Mixture of Experts (MoE) module. We construct answer-concept and answer-question graphs for multimodal inputs to explicitly model the structured relationships among concepts. Subsequently, we integrate Graph Transformer to capture multi level dependencies, addressing the limitations of traditional Concept Bottleneck Models in modeling concept interactions. However, it still encounters bottlenecks in adapting to complex concept patterns. Therefore, we replace the feed forward layers with a Mixture of Experts (MoE) module, enabling the model to have greater capacity in learning diverse concept relationships while dynamically allocating reasoning tasks to different sub experts, thereby significantly enhancing the model's adaptability to complex concept reasoning. MoE-SGT achieves higher accuracy than other concept bottleneck networks on multiple datasets by modeling structured relationships among concepts and utilizing a dynamic expert selection mechanism.",
        "tags": [
            "MoE",
            "Transformer"
        ]
    },
    {
        "id": "166",
        "title": "MultiPhysio-HRC: Multimodal Physiological Signals Dataset for industrial Human-Robot Collaboration",
        "author": [
            "Andrea Bussolan",
            "Stefano Baraldo",
            "Oliver Avram",
            "Pablo Urcola",
            "Luis Montesano",
            "Luca Maria Gambardella",
            "Anna Valente"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00703",
        "abstract": "Human-robot collaboration (HRC) is a key focus of Industry 5.0, aiming to enhance worker productivity while ensuring well-being. The ability to perceive human psycho-physical states, such as stress and cognitive load, is crucial for adaptive and human-aware robotics. This paper introduces MultiPhysio-HRC, a multimodal dataset containing physiological, audio, and facial data collected during real-world HRC scenarios. The dataset includes electroencephalography (EEG), electrocardiography (ECG), electrodermal activity (EDA), respiration (RESP), electromyography (EMG), voice recordings, and facial action units. The dataset integrates controlled cognitive tasks, immersive virtual reality experiences, and industrial disassembly activities performed manually and with robotic assistance, to capture a holistic view of the participants' mental states. Rich ground truth annotations were obtained using validated psychological self-assessment questionnaires. Baseline models were evaluated for stress and cognitive load classification, demonstrating the dataset's potential for affective computing and human-aware robotics research. MultiPhysio-HRC is publicly available to support research in human-centered automation, workplace well-being, and intelligent robotic systems.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "167",
        "title": "Training-free Uncertainty Guidance for Complex Visual Tasks with MLLMs",
        "author": [
            "Sanghwan Kim",
            "Rui Xiao",
            "Stephan Alaniz",
            "Yongqin Xian",
            "Zeynep Akata"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00705",
        "abstract": "Multimodal Large Language Models (MLLMs) often struggle with fine-grained perception, such as identifying small objects in high-resolution images or finding key moments in long videos. Existing works typically rely on complicated, task-specific fine-tuning, which limits their generalizability and increases model complexity. In this work, we propose an effective, training-free framework that uses an MLLM's intrinsic uncertainty as a proactive guidance signal. Our core insight is that a model's output entropy decreases when presented with relevant visual information. We introduce a unified mechanism that scores candidate visual inputs by response uncertainty, enabling the model to autonomously focus on the most salient data. We apply this simple principle to three complex visual tasks: Visual Search, Long Video Understanding, and Temporal Grounding, allowing off-the-shelf MLLMs to achieve performance competitive with specialized, fine-tuned methods. Our work validates that harnessing intrinsic uncertainty is a powerful, general strategy for enhancing fine-grained multimodal performance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "168",
        "title": "Carleman Linearization of Parabolic PDEs: Well-posedness, convergence, and efficient numerical methods",
        "author": [
            "Bernhard Heinzelreiter",
            "John W. Pearson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00722",
        "abstract": "We explore how the analysis of the Carleman linearization can be extended to dynamical systems on infinite-dimensional Hilbert spaces with quadratic nonlinearities. We demonstrate the well-posedness and convergence of the truncated Carleman linearization under suitable assumptions on the dynamical system, which encompass common parabolic semi-linear partial differential equations such as the Navier-Stokes equations and nonlinear diffusion-advection-reaction equations. Upon discretization, we show that the total approximation error of the linearization decomposes into two independent components: the discretization error and the linearization error. This decomposition yields a convergence radius and convergence rate for the discretized linearization that are independent of the discretization. We thus justify the application of the linearization to parabolic PDE problems. Furthermore, it motivates the use of non-standard structure-exploiting numerical methods, such as sparse grids, taming the curse of dimensionality associated with the Carleman linearization. Finally, we verify the results with numerical experiments.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "169",
        "title": "CroSTAta: Cross-State Transition Attention Transformer for Robotic Manipulation",
        "author": [
            "Giovanni Minelli",
            "Giulio Turrisi",
            "Victor Barasuol",
            "Claudio Semini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00726",
        "abstract": "Learning robotic manipulation policies through supervised learning from demonstrations remains challenging when policies encounter execution variations not explicitly covered during training. While incorporating historical context through attention mechanisms can improve robustness, standard approaches process all past states in a sequence without explicitly modeling the temporal structure that demonstrations may include, such as failure and recovery patterns. We propose a Cross-State Transition Attention Transformer that employs a novel State Transition Attention (STA) mechanism to modulate standard attention weights based on learned state evolution patterns, enabling policies to better adapt their behavior based on execution history. Our approach combines this structured attention with temporal masking during training, where visual information is randomly removed from recent timesteps to encourage temporal reasoning from historical context. Evaluation in simulation shows that STA consistently outperforms standard cross-attention and temporal modeling approaches like TCN and LSTM networks across all tasks, achieving more than 2x improvement over cross-attention on precision-critical tasks.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "170",
        "title": "EvolProver: Advancing Automated Theorem Proving by Evolving Formalized Problems via Symmetry and Difficulty",
        "author": [
            "Yuchen Tian",
            "Ruiyuan Huang",
            "Xuanwu Wang",
            "Jing Ma",
            "Zengfeng Huang",
            "Ziyang Luo",
            "Hongzhan Lin",
            "Da Zheng",
            "Lun Du"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00732",
        "abstract": "Large Language Models (LLMs) for formal theorem proving have shown significant promise, yet they often lack generalizability and are fragile to even minor transformations of problem statements. To address this limitation, we introduce a novel data augmentation pipeline designed to enhance model robustness from two perspectives: symmetry and difficulty. From the symmetry perspective, we propose two complementary methods: EvolAST, an Abstract Syntax Tree (AST) based approach that targets syntactic symmetry to generate semantically equivalent problem variants, and EvolDomain, which leverages LLMs to address semantic symmetry by translating theorems across mathematical domains. From the difficulty perspective, we propose EvolDifficulty, which uses carefully designed evolutionary instructions to guide LLMs in generating new theorems with a wider range of difficulty. We then use the evolved data to train EvolProver, a 7B-parameter non-reasoning theorem prover. EvolProver establishes a new state-of-the-art (SOTA) on FormalMATH-Lite with a 53.8% pass@32 rate, surpassing all models of comparable size, including reasoning-based models. It also sets new SOTA records for non-reasoning models on MiniF2F-Test (69.8% pass@32), Ineq-Comp-Seed (52.2% pass@32), and Ineq-Comp-Transformed (34.0% pass@32). Ablation studies further confirm our data augmentation pipeline's effectiveness across multiple benchmarks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "171",
        "title": "Neural Diffusion Processes for Physically Interpretable Survival Prediction",
        "author": [
            "Alessio Cristofoletto",
            "Cesare Rollo",
            "Giovanni Birolo",
            "Piero Fariselli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00733",
        "abstract": "We introduce DeepFHT, a survival-analysis framework that couples deep neural networks with first hitting time (FHT) distributions from stochastic process theory. Time to event is represented as the first passage of a latent diffusion process to an absorbing boundary. A neural network maps input variables to physically meaningful parameters including initial condition, drift, and diffusion, within a chosen FHT process such as Brownian motion, both with drift and driftless. This yields closed-form survival and hazard functions and captures time-varying risk without assuming proportional-hazards.\nWe compare DeepFHT with Cox regression and other existing parametric survival models, using synthetic and real-world datasets. The method achieves predictive accuracy on par with state-of-the-art approaches, while maintaining a physics-based interpretable parameterization that elucidates the relation between input features and risk. This combination of stochastic process theory and deep learning provides a principled avenue for modeling survival phenomena in complex systems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "172",
        "title": "Datasets for Valence and Arousal Inference: A Survey",
        "author": [
            "Helen Schneider",
            "Svetlana Pavlitska",
            "Helen Gremmelmaier",
            "J. Marius ZÃ¶llner"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00738",
        "abstract": "Understanding human affect can be used in robotics, marketing, education, human-computer interaction, healthcare, entertainment, autonomous driving, and psychology to enhance decision-making, personalize experiences, and improve emotional well-being. This work presents a comprehensive overview of affect inference datasets that utilize continuous valence and arousal labels. We reviewed 25 datasets published between 2008 and 2024, examining key factors such as dataset size, subject distribution, sensor configurations, annotation scales, and data formats for valence and arousal values. While camera-based datasets dominate the field, we also identified several widely used multimodal combinations. Additionally, we explored the most common approaches to affect detection applied to these datasets, providing insights into the prevailing methodologies in the field. Our overview of sensor fusion approaches shows promising advancements in model improvement for valence and arousal inference.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "173",
        "title": "TD-JEPA: Latent-predictive Representations for Zero-Shot Reinforcement Learning",
        "author": [
            "Marco Bagatella",
            "Matteo Pirotta",
            "Ahmed Touati",
            "Alessandro Lazaric",
            "Andrea Tirinzoni"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00739",
        "abstract": "Latent prediction--where agents learn by predicting their own latents--has emerged as a powerful paradigm for training general representations in machine learning. In reinforcement learning (RL), this approach has been explored to define auxiliary losses for a variety of settings, including reward-based and unsupervised RL, behavior cloning, and world modeling. While existing methods are typically limited to single-task learning, one-step prediction, or on-policy trajectory data, we show that temporal difference (TD) learning enables learning representations predictive of long-term latent dynamics across multiple policies from offline, reward-free transitions. Building on this, we introduce TD-JEPA, which leverages TD-based latent-predictive representations into unsupervised RL. TD-JEPA trains explicit state and task encoders, a policy-conditioned multi-step predictor, and a set of parameterized policies directly in latent space. This enables zero-shot optimization of any reward function at test time. Theoretically, we show that an idealized variant of TD-JEPA avoids collapse with proper initialization, and learns encoders that capture a low-rank factorization of long-term policy dynamics, while the predictor recovers their successor features in latent space. Empirically, TD-JEPA matches or outperforms state-of-the-art baselines on locomotion, navigation, and manipulation tasks across 13 datasets in ExoRL and OGBench, especially in the challenging setting of zero-shot RL from pixels.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "174",
        "title": "Downgrade to Upgrade: Optimizer Simplification Enhances Robustness in LLM Unlearning",
        "author": [
            "Yicheng Lang",
            "Yihua Zhang",
            "Chongyu Fan",
            "Changsheng Wang",
            "Jinghan Jia",
            "Sijia Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00761",
        "abstract": "Large language model (LLM) unlearning aims to surgically remove the influence of undesired data or knowledge from an existing model while preserving its utility on unrelated tasks. This paradigm has shown promise in addressing privacy and safety concerns. However, recent findings reveal that unlearning effects are often fragile: post-unlearning manipulations such as weight quantization or fine-tuning can quickly neutralize the intended forgetting. Prior efforts to improve robustness primarily reformulate unlearning objectives by explicitly assuming the role of vulnerability sources. In this work, we take a different perspective by investigating the role of the optimizer, independent of unlearning objectives and formulations, in shaping unlearning robustness. We show that the 'grade' of the optimizer, defined by the level of information it exploits, ranging from zeroth-order (gradient-free) to first-order (gradient-based) to second-order (Hessian-based), is tightly linked to the resilience of unlearning. Surprisingly, we find that downgrading the optimizer, such as using zeroth-order methods or compressed-gradient variants (e.g., gradient sign-based optimizers), often leads to stronger robustness. While these optimizers produce noisier and less precise updates, they encourage convergence to harder-to-disturb basins in the loss landscape, thereby resisting post-training perturbations. By connecting zeroth-order methods with randomized smoothing, we further highlight their natural advantage for robust unlearning. Motivated by these insights, we propose a hybrid optimizer that combines first-order and zeroth-order updates, preserving unlearning efficacy while enhancing robustness. Extensive experiments on the MUSE and WMDP benchmarks, across multiple LLM unlearning algorithms, validate that our approach achieves more resilient forgetting without sacrificing unlearning quality.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "175",
        "title": "Multi-Objective Task-Aware Predictor for Image-Text Alignment",
        "author": [
            "Eunki Kim",
            "Na Min An",
            "James Thorne",
            "Hyunjung Shim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00766",
        "abstract": "Evaluating image-text alignment while reflecting human preferences across multiple aspects is a significant issue for the development of reliable vision-language applications. It becomes especially crucial in real-world scenarios where multiple valid descriptions exist depending on contexts or user needs. However, research progress is hindered by the lack of comprehensive benchmarks and existing evaluation predictors lacking at least one of these key properties: (1) Alignment with human judgments, (2) Long-sequence processing, (3) Inference efficiency, and (4) Applicability to multi-objective scoring. To address these challenges, we propose a plug-and-play architecture to build a robust predictor, MULTI-TAP (Multi-Objective Task-Aware Predictor), capable of both multi and single-objective scoring. MULTI-TAP can produce a single overall score, utilizing a reward head built on top of a large vision-language model (LVLMs). We show that MULTI-TAP is robust in terms of application to different LVLM architectures, achieving significantly higher performance than existing metrics and even on par with the GPT-4o-based predictor, G-VEval, with a smaller size (7-8B). By training a lightweight ridge regression layer on the frozen hidden states of a pre-trained LVLM, MULTI-TAP can produce fine-grained scores for multiple human-interpretable objectives. MULTI-TAP performs better than VisionREWARD, a high-performing multi-objective reward model, in both performance and efficiency on multi-objective benchmarks and our newly released text-image-to-text dataset, EYE4ALL. Our new dataset, consisting of chosen/rejected human preferences (EYE4ALLPref) and human-annotated fine-grained scores across seven dimensions (EYE4ALLMulti), can serve as a foundation for developing more accessible AI systems by capturing the underlying preferences of users, including blind and low-vision (BLV) individuals.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "176",
        "title": "Tele-rehabilitation with online skill transfer and adaptation in $\\mathbb{R}^3 \\times \\mathit{S}^3$",
        "author": [
            "Tianle Ni",
            "Xiao Chen",
            "Hamid Sadeghian",
            "Sami Haddadin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00770",
        "abstract": "This paper proposes a tele-teaching framework for the domain of robot-assisted tele-rehabilitation. The system connects two robotic manipulators on therapist and patient side via bilateral teleoperation, enabling a therapist to remotely demonstrate rehabilitation exercises that are executed by the patient-side robot. A 6-DoF Dynamical Movement Primitives formulation is employed to jointly encode translational and rotational motions in $\\mathbb{R}^3 \\times \\mathit{S}^3$ space, ensuring accurate trajectory reproduction. The framework supports smooth transitions between therapist-led guidance and patient passive training, while allowing adaptive adjustment of motion. Experiments with 7-DoF manipulators demonstrate the feasibility of the approach, highlighting its potential for personalized and remotely supervised rehabilitation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "177",
        "title": "In-Place Feedback: A New Paradigm for Guiding LLMs in Multi-Turn Reasoning",
        "author": [
            "Youngbin Choi",
            "Minjong Lee",
            "Saemi Moon",
            "Seunghyuk Cho",
            "Chaehyeon Chung",
            "MoonJeong Park",
            "Dongwoo Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00777",
        "abstract": "Large language models (LLMs) are increasingly studied in the context of multi-turn reasoning, where models iteratively refine their outputs based on user-provided feedback. Such settings are crucial for tasks that require complex reasoning, yet existing feedback paradigms often rely on issuing new messages. LLMs struggle to integrate these reliably, leading to inconsistent improvements. In this work, we introduce in-place feedback, a novel interaction paradigm in which users directly edit an LLM's previous response, and the model conditions on this modified response to generate its revision. Empirical evaluations on diverse reasoning-intensive benchmarks reveal that in-place feedback achieves better performance than conventional multi-turn feedback while using $79.1\\%$ fewer tokens. Complementary analyses on controlled environments further demonstrate that in-place feedback resolves a core limitation of multi-turn feedback: models often fail to apply feedback precisely to erroneous parts of the response, leaving errors uncorrected and sometimes introducing new mistakes into previously correct content. These findings suggest that in-place feedback offers a more natural and effective mechanism for guiding LLMs in reasoning-intensive tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "178",
        "title": "DIA: The Adversarial Exposure of Deterministic Inversion in Diffusion Models",
        "author": [
            "Seunghoo Hong",
            "Geonho Son",
            "Juhun Lee",
            "Simon S. Woo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00778",
        "abstract": "Diffusion models have shown to be strong representation learners, showcasing state-of-the-art performance across multiple domains. Aside from accelerated sampling, DDIM also enables the inversion of real images back to their latent codes. A direct inheriting application of this inversion operation is real image editing, where the inversion yields latent trajectories to be utilized during the synthesis of the edited image. Unfortunately, this practical tool has enabled malicious users to freely synthesize misinformative or deepfake contents with greater ease, which promotes the spread of unethical and abusive, as well as privacy-, and copyright-infringing contents. While defensive algorithms such as AdvDM and Photoguard have been shown to disrupt the diffusion process on these images, the misalignment between their objectives and the iterative denoising trajectory at test time results in weak disruptive http://performance.In this work, we present the DDIM Inversion Attack (DIA) that attacks the integrated DDIM trajectory path. Our results support the effective disruption, surpassing previous defensive methods across various editing methods. We believe that our frameworks and results can provide practical defense methods against the malicious use of AI for both the industry and the research community. Our code is available here: https://anonymous.4open.science/r/DIA-13419/.",
        "tags": [
            "DDIM",
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "179",
        "title": "Semantic Visual Simultaneous Localization and Mapping: A Survey on State of the Art, Challenges, and Future Directions",
        "author": [
            "Thanh Nguyen Canh",
            "Haolan Zhang",
            "Xiem HoangVan",
            "Nak Young Chong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00783",
        "abstract": "Semantic Simultaneous Localization and Mapping (SLAM) is a critical area of research within robotics and computer vision, focusing on the simultaneous localization of robotic systems and associating semantic information to construct the most accurate and complete comprehensive model of the surrounding environment. Since the first foundational work in Semantic SLAM appeared more than two decades ago, this field has received increasing attention across various scientific communities. Despite its significance, the field lacks comprehensive surveys encompassing recent advances and persistent challenges. In response, this study provides a thorough examination of the state-of-the-art of Semantic SLAM techniques, with the aim of illuminating current trends and key obstacles. Beginning with an in-depth exploration of the evolution of visual SLAM, this study outlines its strengths and unique characteristics, while also critically assessing previous survey literature. Subsequently, a unified problem formulation and evaluation of the modular solution framework is proposed, which divides the problem into discrete stages, including visual localization, semantic feature extraction, mapping, data association, and loop closure optimization. Moreover, this study investigates alternative methodologies such as deep learning and the utilization of large language models, alongside a review of relevant research about contemporary SLAM datasets. Concluding with a discussion on potential future research directions, this study serves as a comprehensive resource for researchers seeking to navigate the complex landscape of Semantic SLAM.",
        "tags": [
            "LLM",
            "Robotics",
            "SLAM"
        ]
    },
    {
        "id": "180",
        "title": "AI in data science education: experiences from the classroom",
        "author": [
            "J.A. Hageman",
            "C.F.W. Peeters"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00793",
        "abstract": "This study explores the integration of AI, particularly large language models (LLMs) like ChatGPT, into educational settings, focusing on the implications for teaching and learning. Through interviews with course coordinators from data science courses at Wageningen University, this research identifies both the benefits and challenges associated with AI in the classroom. While AI tools can streamline tasks and enhance learning, concerns arise regarding students' overreliance on these technologies, potentially hindering the development of essential cognitive and problem solving skills. The study highlights the importance of responsible AI usage, ethical considerations, and the need for adapting assessment methods to ensure educational outcomes are met. With careful integration, AI can be a valuable asset in education, provided it is used to complement rather than replace fundamental learning processes.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "181",
        "title": "MetaLogic: Robustness Evaluation of Text-to-Image Models via Logically Equivalent Prompts",
        "author": [
            "Yifan Shen",
            "Yangyang Shu",
            "Hye-young Paik",
            "Yulei Sui"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00796",
        "abstract": "Recent advances in text-to-image (T2I) models, especially diffusion-based architectures, have significantly improved the visual quality of generated images. However, these models continue to struggle with a critical limitation: maintaining semantic consistency when input prompts undergo minor linguistic variations. Despite being logically equivalent, such prompt pairs often yield misaligned or semantically inconsistent images, exposing a lack of robustness in reasoning and generalisation. To address this, we propose MetaLogic, a novel evaluation framework that detects T2I misalignment without relying on ground truth images. MetaLogic leverages metamorphic testing, generating image pairs from prompts that differ grammatically but are semantically identical. By directly comparing these image pairs, the framework identifies inconsistencies that signal failures in preserving the intended meaning, effectively diagnosing robustness issues in the model's logic understanding. Unlike existing evaluation methods that compare a generated image to a single prompt, MetaLogic evaluates semantic equivalence between paired images, offering a scalable, ground-truth-free approach to identifying alignment failures. It categorises these alignment errors (e.g., entity omission, duplication, positional misalignment) and surfaces counterexamples that can be used for model debugging and refinement. We evaluate MetaLogic across multiple state-of-the-art T2I models and reveal consistent robustness failures across a range of logical constructs. We find that even the SOTA text-to-image models like http://Flux.dev and DALLE-3 demonstrate a 59 percent and 71 percent misalignment rate, respectively. Our results show that MetaLogic is not only efficient and scalable, but also effective in uncovering fine-grained logical inconsistencies that are overlooked by existing evaluation metrics.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Text-to-Image"
        ]
    },
    {
        "id": "182",
        "title": "Solar PV Installation Potential Assessment on Building Facades Based on Vision and Language Foundation Models",
        "author": [
            "Ruyu Liu",
            "Dongxu Zhuang",
            "Jianhua Zhang",
            "Arega Getaneh Abate",
            "Per Sieverts Nielsen",
            "Ben Wang",
            "Xiufeng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00797",
        "abstract": "Building facades represent a significant untapped resource for solar energy generation in dense urban environments, yet assessing their photovoltaic (PV) potential remains challenging due to complex geometries and semantic com ponents. This study introduces SF-SPA (Semantic Facade Solar-PV Assessment), an automated framework that transforms street-view photographs into quantitative PV deployment assessments. The approach combines com puter vision and artificial intelligence techniques to address three key challenges: perspective distortion correction, semantic understanding of facade elements, and spatial reasoning for PV layout optimization. Our four-stage pipeline processes images through geometric rectification, zero-shot semantic segmentation, Large Language Model (LLM) guided spatial reasoning, and energy simulation. Validation across 80 buildings in four countries demonstrates ro bust performance with mean area estimation errors of 6.2% &#177; 2.8% compared to expert annotations. The auto mated assessment requires approximately 100 seconds per building, a substantial gain in efficiency over manual methods. Simulated energy yield predictions confirm the method's reliability and applicability for regional poten tial studies, urban energy planning, and building-integrated photovoltaic (BIPV) deployment. Code is available at: https:github.com/CodeAXu/Solar-PV-Installation",
        "tags": [
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "183",
        "title": "From Seeing to Predicting: A Vision-Language Framework for Trajectory Forecasting and Controlled Video Generation",
        "author": [
            "Fan Yang",
            "Zhiyang Chen",
            "Yousong Zhu",
            "Xin Li",
            "Jinqiao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00806",
        "abstract": "Current video generation models produce physically inconsistent motion that violates real-world dynamics. We propose TrajVLM-Gen, a two-stage framework for physics-aware image-to-video generation. First, we employ a Vision Language Model to predict coarse-grained motion trajectories that maintain consistency with real-world physics. Second, these trajectories guide video generation through attention-based mechanisms for fine-grained motion refinement. We build a trajectory prediction dataset based on video tracking data with realistic motion patterns. Experiments on UCF-101 and MSR-VTT demonstrate that TrajVLM-Gen outperforms existing methods, achieving competitive FVD scores of 545 on UCF-101 and 539 on MSR-VTT.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "184",
        "title": "Family Matters: Language Transfer and Merging for Adapting Small LLMs to Faroese",
        "author": [
            "Jenny Kunz",
            "Iben Nyholm Debess",
            "Annika Simonsen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00810",
        "abstract": "We investigate how to adapt small, efficient LLMs to Faroese, a low-resource North Germanic language. Starting from English models, we continue pre-training on related Scandinavian languages, either individually or combined via merging, before fine-tuning on Faroese. We compare full fine-tuning with parameter-efficient tuning using LoRA, evaluating their impact on both linguistic accuracy and text comprehension. Due to the lack of existing Faroese evaluation data, we construct two new minimal-pair benchmarks from adapted and newly collected datasets and complement them with human evaluations by Faroese linguists. Our results demonstrate that transfer from related languages is crucial, though the optimal source language depends on the task: Icelandic enhances linguistic accuracy, whereas Danish boosts comprehension. Similarly, the choice between full fine-tuning and LoRA is task-dependent: LoRA improves linguistic acceptability and slightly increases human evaluation scores on the base model, while full fine-tuning yields stronger comprehension performance and better preserves model capabilities during downstream fine-tuning.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "185",
        "title": "RTFF: Random-to-Target Fabric Flattening Policy using Dual-Arm Manipulator",
        "author": [
            "Kai Tang",
            "Dipankar Bhattacharya",
            "Hang Xu",
            "Fuyuki Tokuda",
            "Norman C. Tien",
            "Kazuhiro Kosuge"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00814",
        "abstract": "Robotic fabric manipulation in garment production for sewing, cutting, and ironing requires reliable flattening and alignment, yet remains challenging due to fabric deformability, effectively infinite degrees of freedom, and frequent occlusions from wrinkles, folds, and the manipulator's End-Effector (EE) and arm. To address these issues, this paper proposes the first Random-to-Target Fabric Flattening (RTFF) policy, which aligns a random wrinkled fabric state to an arbitrary wrinkle-free target state. The proposed policy adopts a hybrid Imitation Learning-Visual Servoing (IL-VS) framework, where IL learns with explicit fabric models for coarse alignment of the wrinkled fabric toward a wrinkle-free state near the target, and VS ensures fine alignment to the target. Central to this framework is a template-based mesh that offers precise target state representation, wrinkle-aware geometry prediction, and consistent vertex correspondence across RTFF manipulation steps, enabling robust manipulation and seamless IL-VS switching. Leveraging the power of mesh, a novel IL solution for RTFF-Mesh Action Chunking Transformer (MACT)-is then proposed by conditioning the mesh information into a Transformer-based policy. The RTFF policy is validated on a real dual-arm tele-operation system, showing zero-shot alignment to different targets, high accuracy, and strong generalization across fabrics and scales. Project website: https://kaitang98.github.io/RTFF_Policy/",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "186",
        "title": "Learn to Guide Your Diffusion Model",
        "author": [
            "Alexandre Galashov",
            "Ashwini Pokle",
            "Arnaud Doucet",
            "Arthur Gretton",
            "Mauricio Delbracio",
            "Valentin De Bortoli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00815",
        "abstract": "Classifier-free guidance (CFG) is a widely used technique for improving the perceptual quality of samples from conditional diffusion models. It operates by linearly combining conditional and unconditional score estimates using a guidance weight $\\omega$. While a large, static weight can markedly improve visual results, this often comes at the cost of poorer distributional alignment. In order to better approximate the target conditional distribution, we instead learn guidance weights $\\omega_{c,(s,t)}$, which are continuous functions of the conditioning $c$, the time $t$ from which we denoise, and the time $s$ towards which we denoise. We achieve this by minimizing the distributional mismatch between noised samples from the true conditional distribution and samples from the guided diffusion process. We extend our framework to reward guided sampling, enabling the model to target distributions tilted by a reward function $R(x_0,c)$, defined on clean data and a conditioning $c$. We demonstrate the effectiveness of our methodology on low-dimensional toy examples and high-dimensional image settings, where we observe improvements in FrÃ©chet inception distance (FID) for image generation. In text-to-image applications, we observe that employing a reward function given by the CLIP score leads to guidance weights that improve image-prompt alignment.",
        "tags": [
            "CLIP",
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "187",
        "title": "Stabilizing Policy Gradients for Sample-Efficient Reinforcement Learning in LLM Reasoning",
        "author": [
            "Luckeciano C. Melo",
            "Alessandro Abate",
            "Yarin Gal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00819",
        "abstract": "Reinforcement Learning, particularly through policy gradient methods, has played a central role in enabling reasoning capabilities of Large Language Models. However, the optimization stability of policy gradients in this setting remains understudied. As a result, existing implementations often resort to conservative hyperparameter choices to ensure stability, which requires more training samples and increases computational costs. Hence, developing models for reliably tracking the underlying optimization dynamics and leveraging them into training enables more sample-efficient regimes and further unleashes scalable post-training. We address this gap by formalizing the stochastic optimization problem of policy gradients with explicit consideration of second-order geometry. We propose a tractable computational framework that tracks and leverages curvature information during policy updates. We further employ this framework to design interventions in the optimization process through data selection. The resultant algorithm, Curvature-Aware Policy Optimization (CAPO), identifies samples that contribute to unstable updates and masks them out. Theoretically, we establish monotonic improvement guarantees under realistic assumptions. On standard math reasoning benchmarks, we empirically show that CAPO ensures stable updates under aggressive learning regimes where baselines catastrophically fail. With minimal intervention (rejecting fewer than 8% of tokens), CAPO achieves up to 30x improvement in sample efficiency over standard GRPO for LLM reasoning.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "188",
        "title": "NSARM: Next-Scale Autoregressive Modeling for Robust Real-World Image Super-Resolution",
        "author": [
            "Xiangtao Kong",
            "Rongyuan Wu",
            "Shuaizheng Liu",
            "Lingchen Sun",
            "Lei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00820",
        "abstract": "Most recent real-world image super-resolution (Real-ISR) methods employ pre-trained text-to-image (T2I) diffusion models to synthesize the high-quality image either from random Gaussian noise, which yields realistic results but is slow due to iterative denoising, or directly from the input low-quality image, which is efficient but at the price of lower output quality. These approaches train ControlNet or LoRA modules while keeping the pre-trained model fixed, which often introduces over-enhanced artifacts and hallucinations, suffering from the robustness to inputs of varying degradations. Recent visual autoregressive (AR) models, such as pre-trained Infinity, can provide strong T2I generation capabilities while offering superior efficiency by using the bitwise next-scale prediction strategy. Building upon next-scale prediction, we introduce a robust Real-ISR framework, namely Next-Scale Autoregressive Modeling (NSARM). Specifically, we train NSARM in two stages: a transformation network is first trained to map the input low-quality image to preliminary scales, followed by an end-to-end full-model fine-tuning. Such a comprehensive fine-tuning enhances the robustness of NSARM in Real-ISR tasks without compromising its generative capability. Extensive quantitative and qualitative evaluations demonstrate that as a pure AR model, NSARM achieves superior visual results over existing Real-ISR methods while maintaining a fast inference speed. Most importantly, it demonstrates much higher robustness to the quality of input images, showing stronger generalization performance. Project page: https://github.com/Xiangtaokong/NSARM",
        "tags": [
            "ControlNet",
            "Diffusion",
            "LoRA",
            "Super Resolution",
            "Text-to-Image"
        ]
    },
    {
        "id": "189",
        "title": "Logical Consistency Between Disagreeing Experts and Its Role in AI Safety",
        "author": [
            "AndrÃ©s Corrada-Emmanuel"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00821",
        "abstract": "If two experts disagree on a test, we may conclude both cannot be 100 per cent correct. But if they completely agree, no possible evaluation can be excluded. This asymmetry in the utility of agreements versus disagreements is explored here by formalizing a logic of unsupervised evaluation for classifiers. Its core problem is computing the set of group evaluations that are logically consistent with how we observe them agreeing and disagreeing in their decisions. Statistical summaries of their aligned decisions are inputs into a Linear Programming problem in the integer space of possible correct or incorrect responses given true labels. Obvious logical constraints, such as, the number of correct responses cannot exceed the number of observed responses, are inequalities. But in addition, there are axioms, universally applicable linear equalities that apply to all finite tests. The practical and immediate utility of this approach to unsupervised evaluation using only logical consistency is demonstrated by building no-knowledge alarms that can detect when one or more LLMs-as-Judges are violating a minimum grading threshold specified by the user.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "190",
        "title": "Exposing the Cracks: Vulnerabilities of Retrieval-Augmented LLM-based Machine Translation",
        "author": [
            "Yanming Sun",
            "Runzhe Zhan",
            "Chi Seng Cheang",
            "Han Wu",
            "Xuebo Liu",
            "Yuyao Niu",
            "Fengying Ye",
            "Kaixin Lan",
            "Lidia S. Chao",
            "Derek F. Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00829",
        "abstract": "\\textbf{RE}trieval-\\textbf{A}ugmented \\textbf{L}LM-based \\textbf{M}achine \\textbf{T}ranslation (REAL-MT) shows promise for knowledge-intensive tasks like idiomatic translation, but its reliability under noisy retrieval contexts remains poorly understood despite this being a common challenge in real-world deployment. To address this gap, we propose a noise synthesis framework and new metrics to evaluate the robustness of REAL-MT systematically. Using this framework, we instantiate REAL-MT with Qwen-series models, including standard LLMs and large reasoning models (LRMs) with enhanced reasoning, and evaluate their performance on idiomatic translation across high-, medium-, and low-resource language pairs under synthesized noise. Our results show that low-resource language pairs, which rely more heavily on retrieved context, degrade more severely under noise than high-resource ones and often produce nonsensical translations. Although LRMs possess enhanced reasoning capabilities, they show no improvement in error correction and are even more susceptible to noise, tending to rationalize incorrect contexts. We find that this stems from an attention shift away from the source idiom to noisy content, while confidence increases despite declining accuracy, indicating poor calibration. To mitigate these issues, we investigate training-free and fine-tuning strategies, which improve robustness at the cost of performance in clean contexts, revealing a fundamental trade-off. Our findings highlight the limitations of current approaches, underscoring the need for self-verifying integration mechanisms.",
        "tags": [
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "191",
        "title": "LLM Routing with Dueling Feedback",
        "author": [
            "Chao-Kai Chiang",
            "Takashi Ishida",
            "Masashi Sugiyama"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00841",
        "abstract": "We study LLM routing, the problem of selecting the best model for each query while balancing user satisfaction, model expertise, and inference cost. We formulate routing as contextual dueling bandits, learning from pairwise preference feedback rather than absolute scores, thereby yielding label-efficient and dynamic adaptation. Building on this formulation, we introduce Category-Calibrated Fine-Tuning (CCFT), a representation-learning method that derives model embeddings from offline data using contrastive fine-tuning with categorical weighting. These embeddings enable the practical instantiation of Feel-Good Thompson Sampling for Contextual Dueling Bandits (http://FGTS.CDB), a theoretically grounded posterior-sampling algorithm. We propose four variants of the categorical weighting that explicitly integrate model quality and cost, and we empirically evaluate the proposed methods on the RouterBench and MixInstruct datasets. Across both benchmarks, our methods achieve lower cumulative regret and faster convergence, with better robustness and performance-cost balance than strong baselines built with a general-purpose OpenAI embedding model.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "192",
        "title": "Learning Compact Representations of LLM Abilities via Item Response Theory",
        "author": [
            "Jianhao Chen",
            "Chenxu Wang",
            "Gengrui Zhang",
            "Peng Ye",
            "Lei Bai",
            "Wei Hu",
            "Yuzhong Qu",
            "Shuyue Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00844",
        "abstract": "Recent years have witnessed a surge in the number of large language models (LLMs), yet efficiently managing and utilizing these vast resources remains a significant challenge. In this work, we explore how to learn compact representations of LLM abilities that can facilitate downstream tasks, such as model routing and performance prediction on new benchmarks. We frame this problem as estimating the probability that a given model will correctly answer a specific query. Inspired by the item response theory (IRT) in psychometrics, we model this probability as a function of three key factors: (i) the model's multi-skill ability vector, (2) the query's discrimination vector that separates models of differing skills, and (3) the query's difficulty scalar. To learn these parameters jointly, we introduce a Mixture-of-Experts (MoE) network that couples model- and query-level embeddings. Extensive experiments demonstrate that our approach leads to state-of-the-art performance in both model routing and benchmark accuracy prediction. Moreover, analysis validates that the learned parameters encode meaningful, interpretable information about model capabilities and query characteristics.",
        "tags": [
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "193",
        "title": "Can World Models Benefit VLMs for World Dynamics?",
        "author": [
            "Kevin Zhang",
            "Kuangzhi Ge",
            "Xiaowei Chi",
            "Renrui Zhang",
            "Shaojun Shi",
            "Zhen Dong",
            "Sirui Han",
            "Shanghang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00855",
        "abstract": "Trained on internet-scale video data, generative world models are increasingly recognized as powerful world simulators that can generate consistent and plausible dynamics over structure, motion, and physics. This raises a natural question: with the advent of strong video foundational models, might they supplant conventional vision encoder paradigms for general-purpose multimodal understanding? While recent studies have begun to explore the potential of world models on common vision tasks, these explorations typically lack a systematic investigation of generic, multimodal tasks. In this work, we strive to investigate the capabilities when world model priors are transferred into Vision-Language Models: we re-purpose a video diffusion model as a generative encoder to perform a single denoising step and treat the resulting latents as a set of visual embedding. We empirically investigate this class of models, which we refer to as World-Language Models (WorldLMs), and we find that generative encoders can capture latents useful for downstream understanding that show distinctions from conventional encoders. Naming our best-performing variant Dynamic Vision Aligner (DyVA), we further discover that this method significantly enhances spatial reasoning abilities and enables single-image models to perform multi-frame reasoning. Through the curation of a suite of visual reasoning tasks, we find DyVA to surpass both open-source and proprietary baselines, achieving state-of-the-art or comparable performance. We attribute these gains to WorldLM's inherited motion-consistency internalization from video pre-training. Finally, we systematically explore extensive model designs to highlight promising directions for future work. We hope our study can pave the way for a new family of VLMs that leverage priors from world models and are on a promising path towards generalist vision learners.",
        "tags": [
            "Diffusion",
            "VLM"
        ]
    },
    {
        "id": "194",
        "title": "ManagerBench: Evaluating the Safety-Pragmatism Trade-off in Autonomous LLMs",
        "author": [
            "Adi Simhi",
            "Jonathan Herzig",
            "Martin Tutek",
            "Itay Itzhak",
            "Idan Szpektor",
            "Yonatan Belinkov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00857",
        "abstract": "As large language models (LLMs) evolve from conversational assistants into autonomous agents, evaluating the safety of their actions becomes critical. Prior safety benchmarks have primarily focused on preventing generation of harmful content, such as toxic text. However, they overlook the challenge of agents taking harmful actions when the most effective path to an operational goal conflicts with human safety. To address this gap, we introduce ManagerBench, a benchmark that evaluates LLM decision-making in realistic, human-validated managerial scenarios. Each scenario forces a choice between a pragmatic but harmful action that achieves an operational goal, and a safe action that leads to worse operational performance. A parallel control set, where potential harm is directed only at inanimate objects, measures a model's pragmatism and identifies its tendency to be overly safe. Our findings indicate that the frontier LLMs perform poorly when navigating this safety-pragmatism trade-off. Many consistently choose harmful options to advance their operational goals, while others avoid harm only to become overly safe and ineffective. Critically, we find this misalignment does not stem from an inability to perceive harm, as models' harm assessments align with human judgments, but from flawed prioritization. ManagerBench is a challenging benchmark for a core component of agentic behavior: making safe choices when operational goals and alignment values incentivize conflicting actions. Benchmark & code available at https://github.com/technion-cs-nlp/ManagerBench.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "195",
        "title": "Erase to Improve: Erasable Reinforcement Learning for Search-Augmented LLMs",
        "author": [
            "Ziliang Wang",
            "Kang An",
            "Xuhui Zheng",
            "Faqiang Qian",
            "Weikun Zhang",
            "Cijun Ouyang",
            "Jialu Cai",
            "Yuhang Wang",
            "Yichao Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00861",
        "abstract": "While search-augmented large language models (LLMs) exhibit impressive capabilities, their reliability in complex multi-hop reasoning remains limited. This limitation arises from three fundamental challenges: decomposition errors, where tasks are incorrectly broken down; retrieval missing, where key evidence fails to be retrieved; and reasoning errors, where flawed logic propagates through the reasoning chain. A single failure in any of these stages can derail the final answer. We propose Erasable Reinforcement Learning (ERL), a novel framework that transforms fragile reasoning into a robust process. ERL explicitly identifies faulty steps, erases them, and regenerates reasoning in place, preventing defective logic from propagating through the reasoning chain. This targeted correction mechanism turns brittle reasoning into a more resilient process. Models trained with ERL, termed ESearch, achieve substantial improvements on HotpotQA, MuSiQue, 2Wiki, and Bamboogle, with the 3B model achieving +8.48% EM and +11.56% F1, and the 7B model achieving +5.38% EM and +7.22% F1 over previous state-of-the-art(SOTA) results. These findings suggest that erasable reinforcement learning provides a powerful paradigm shift for robust multi-step reasoning in LLMs.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "196",
        "title": "Gather-Scatter Mamba: Accelerating Propagation with Efficient State Space Model",
        "author": [
            "Hyun-kyu Ko",
            "Youbin Kim",
            "Jihyeon Park",
            "Dongheok Park",
            "Gyeongjin Kang",
            "Wonjun Cho",
            "Hyung Yi",
            "Eunbyung Park"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00862",
        "abstract": "State Space Models (SSMs)-most notably RNNs-have historically played a central role in sequential modeling. Although attention mechanisms such as Transformers have since dominated due to their ability to model global context, their quadratic complexity and limited scalability make them less suited for long sequences. Video super-resolution (VSR) methods have traditionally relied on recurrent architectures to propagate features across frames. However, such approaches suffer from well-known issues including vanishing gradients, lack of parallelism, and slow inference speed. Recent advances in selective SSMs like Mamba offer a compelling alternative: by enabling input-dependent state transitions with linear-time complexity, Mamba mitigates these issues while maintaining strong long-range modeling capabilities. Despite this potential, Mamba alone struggles to capture fine-grained spatial dependencies due to its causal nature and lack of explicit context aggregation. To address this, we propose a hybrid architecture that combines shifted window self-attention for spatial context aggregation with Mamba-based selective scanning for efficient temporal propagation. Furthermore, we introduce Gather-Scatter Mamba (GSM), an alignment-aware mechanism that warps features toward a center anchor frame within the temporal window before Mamba propagation and scatters them back afterward, effectively reducing occlusion artifacts and ensuring effective redistribution of aggregated information across all frames. The official implementation is provided at: https://github.com/Ko-Lani/GSMamba.",
        "tags": [
            "Mamba",
            "SSMs",
            "Super Resolution"
        ]
    },
    {
        "id": "197",
        "title": "The data-quality illusion: Rethinking Classifier-based quality filtering for LLM Pretraining",
        "author": [
            "Thiziri Nait Saada",
            "Louis Bethune",
            "Michal Klein",
            "David Grangier",
            "Marco Cuturi",
            "Pierre Ablin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00866",
        "abstract": "Large-scale models are pretrained on massive web-crawled datasets containing documents of mixed quality, making data filtering essential. A popular method is Classifier-based Quality Filtering (CQF), which trains a binary classifier to distinguish between pretraining data and a small, high-quality set. It assigns each pretraining document a quality score defined as the classifier's score and retains only the top-scoring ones. We provide an in-depth analysis of CQF. We show that while CQF improves downstream task performance, it does not necessarily enhance language modeling on the high-quality dataset. We explain this paradox by the fact that CQF implicitly filters the high-quality dataset as well. We further compare the behavior of models trained with CQF to those trained on synthetic data of increasing quality, obtained via random token permutations, and find starkly different trends. Our results challenge the view that CQF captures a meaningful notion of data quality.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "198",
        "title": "HalluGuard: Evidence-Grounded Small Reasoning Models to Mitigate Hallucinations in Retrieval-Augmented Generation",
        "author": [
            "Loris Bergeron",
            "Ioana Buhnila",
            "JÃ©rÃ´me FranÃ§ois",
            "Radu State"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00880",
        "abstract": "Large Language Models (LLMs) excel in many NLP tasks but remain prone to hallucinations, limiting trust in real-world applications. We present HalluGuard, a 4B-parameter Small Reasoning Model (SRM) for mitigating hallucinations in Retrieval-Augmented Generation (RAG). HalluGuard classifies document-claim pairs as grounded or hallucinated and produces evidence-grounded justifications for transparency. Our approach combines (i) a domain-agnostic synthetic dataset derived from FineWeb and refined through multi-stage curation and data reformation, (ii) synthetic grounded and hallucinated claims, and (iii) preference-based fine-tuning with Odds Ratio Preference Optimization to distill large-model reasoning into a smaller backbone. On the RAGTruth subset of the LLM-AggreFact benchmark, HalluGuard achieves 84.0% balanced accuracy (BAcc), rivaling specialized models, MiniCheck (7B; 84.0%) and Granite Guardian 3.3 (8B; 82.2%) while using roughly half their parameters. Over the full benchmark it reaches 75.7% BAcc, matching larger general-purpose LLMs such as GPT-4o (75.9%). We will release HalluGuard and datasets under Apache 2.0 upon acceptance.",
        "tags": [
            "GPT",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "199",
        "title": "Advancing Automated Ethical Profiling in SE: a Zero-Shot Evaluation of LLM Reasoning",
        "author": [
            "Patrizio Migliarini",
            "Mashal Afzal Memon",
            "Marco Autili",
            "Paola Inverardi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00881",
        "abstract": "Large Language Models (LLMs) are increasingly integrated into software engineering (SE) tools for tasks that extend beyond code synthesis, including judgment under uncertainty and reasoning in ethically significant contexts. We present a fully automated framework for assessing ethical reasoning capabilities across 16 LLMs in a zero-shot setting, using 30 real-world ethically charged scenarios. Each model is prompted to identify the most applicable ethical theory to an action, assess its moral acceptability, and explain the reasoning behind their choice. Responses are compared against expert ethicists' choices using inter-model agreement metrics. Our results show that LLMs achieve an average Theory Consistency Rate (TCR) of 73.3% and Binary Agreement Rate (BAR) on moral acceptability of 86.7%, with interpretable divergences concentrated in ethically ambiguous cases. A qualitative analysis of free-text explanations reveals strong conceptual convergence across models despite surface-level lexical diversity. These findings support the potential viability of LLMs as ethical inference engines within SE pipelines, enabling scalable, auditable, and adaptive integration of user-aligned ethical reasoning. Our focus is the Ethical Interpreter component of a broader profiling pipeline: we evaluate whether current LLMs exhibit sufficient interpretive stability and theory-consistent reasoning to support automated profiling.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "200",
        "title": "Rectifying Regression in Reinforcement Learning",
        "author": [
            "Alex Ayoub",
            "David SzepesvÃ¡ri",
            "Alireza Baktiari",
            "Csaba SzepesvÃ¡ri",
            "Dale Schuurmans"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00885",
        "abstract": "This paper investigates the impact of the loss function in value-based methods for reinforcement learning through an analysis of underlying prediction objectives. We theoretically show that mean absolute error is a better prediction objective than the traditional mean squared error for controlling the learned policy's suboptimality gap. Furthermore, we present results that different loss functions are better aligned with these different regression objectives: binary and categorical cross-entropy losses with the mean absolute error and squared loss with the mean squared error. We then provide empirical evidence that algorithms minimizing these cross-entropy losses can outperform those based on the squared loss in linear reinforcement learning.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "201",
        "title": "On Listwise Reranking for Corpus Feedback",
        "author": [
            "Soyoung Yoon",
            "Jongho Kim",
            "Daeyong Kwon",
            "Avishek Anand",
            "Seung-won Hwang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00887",
        "abstract": "Reranker improves retrieval performance by capturing document interactions. At one extreme, graph-aware adaptive retrieval (GAR) represents an information-rich regime, requiring a pre-computed document similarity graph in reranking. However, as such graphs are often unavailable, or incur quadratic memory costs even when available, graph-free rerankers leverage large language model (LLM) calls to achieve competitive performance. We introduce L2G, a novel framework that implicitly induces document graphs from listwise reranker logs. By converting reranker signals into a graph structure, L2G enables scalable graph-based retrieval without the overhead of explicit graph computation. Results on the TREC-DL and BEIR subset show that L2G matches the effectiveness of oracle-based graph methods, while incurring zero additional LLM calls.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "202",
        "title": "Span-level Detection of AI-generated Scientific Text via Contrastive Learning and Structural Calibration",
        "author": [
            "Zhen Yin",
            "Shenghua Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00890",
        "abstract": "The rapid adoption of large language models (LLMs) in scientific writing raises serious concerns regarding authorship integrity and the reliability of scholarly publications. Existing detection approaches mainly rely on document-level classification or surface-level statistical cues; however, they neglect fine-grained span localization, exhibit weak calibration, and often fail to generalize across disciplines and generators. To address these limitations, we present Sci-SpanDet, a structure-aware framework for detecting AI-generated scholarly texts. The proposed method combines section-conditioned stylistic modeling with multi-level contrastive learning to capture nuanced human-AI differences while mitigating topic dependence, thereby enhancing cross-domain robustness. In addition, it integrates BIO-CRF sequence labeling with pointer-based boundary decoding and confidence calibration to enable precise span-level detection and reliable probability estimates. Extensive experiments on a newly constructed cross-disciplinary dataset of 100,000 annotated samples generated by multiple LLM families (GPT, Qwen, DeepSeek, LLaMA) demonstrate that Sci-SpanDet achieves state-of-the-art performance, with F1(AI) of 80.17, AUROC of 92.63, and Span-F1 of 74.36. Furthermore, it shows strong resilience under adversarial rewriting and maintains balanced accuracy across IMRaD sections and diverse disciplines, substantially surpassing existing baselines. To ensure reproducibility and to foster further research on AI-generated text detection in scholarly documents, the curated dataset and source code will be publicly released upon publication.",
        "tags": [
            "DeepSeek",
            "Detection",
            "GPT",
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "203",
        "title": "Optimizing Version AoI in Energy-Harvesting IoT: Model-Based and Learning-Based Approaches",
        "author": [
            "Erfan Delfani",
            "Nikolaos Pappas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00904",
        "abstract": "Efficient data transmission in resource-constrained Internet of Things (IoT) systems requires semantics-aware management that maximizes the delivery of timely and informative data. This paper investigates the optimization of the semantic metric Version Age of Information (VAoI) in a status update system comprising an energy-harvesting (EH) sensor and a destination monitoring node. We consider three levels of knowledge about the system model -- fully known, partially known, and unknown -- and propose corresponding optimization strategies: model-based, estimation-based, and model-free methods. By employing Markov Decision Process (MDP) and Reinforcement Learning (RL) frameworks, we analyze performance trade-offs under varying degrees of model information. Our findings provide guidance for designing efficient and adaptive semantics-aware policies in both known and unknown IoT environments.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "204",
        "title": "Bridging Language Gaps: Advances in Cross-Lingual Information Retrieval with Multilingual LLMs",
        "author": [
            "Roksana Goworek",
            "Olivia Macmillan-Scott",
            "Eda B. ÃzyiÄit"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00908",
        "abstract": "Cross-lingual information retrieval (CLIR) addresses the challenge of retrieving relevant documents written in languages different from that of the original query. Research in this area has typically framed the task as monolingual retrieval augmented by translation, treating retrieval methods and cross-lingual capabilities in isolation. Both monolingual and cross-lingual retrieval usually follow a pipeline of query expansion, ranking, re-ranking and, increasingly, question answering. Recent advances, however, have shifted from translation-based methods toward embedding-based approaches and leverage multilingual large language models (LLMs), for which aligning representations across languages remains a central challenge. The emergence of cross-lingual embeddings and multilingual LLMs has introduced a new paradigm, offering improved retrieval performance and enabling answer generation. This survey provides a comprehensive overview of developments from early translation-based methods to state-of-the-art embedding-driven and generative techniques. It presents a structured account of core CLIR components, evaluation practices, and available resources. Persistent challenges such as data imbalance and linguistic variation are identified, while promising directions are suggested for advancing equitable and effective cross-lingual information retrieval. By situating CLIR within the broader landscape of information retrieval and multilingual language processing, this work not only reviews current capabilities but also outlines future directions for building retrieval systems that are robust, inclusive, and adaptable.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "205",
        "title": "RiskPO: Risk-based Policy Optimization via Verifiable Reward for LLM Post-Training",
        "author": [
            "Tao Ren",
            "Jinyang Jiang",
            "Hui Yang",
            "Wan Tian",
            "Minhao Zou",
            "Guanghao Li",
            "Zishi Zhang",
            "Qinghao Wang",
            "Shentao Qin",
            "Yanjun Zhao",
            "Rui Tao",
            "Hui Shao",
            "Yijie Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00911",
        "abstract": "Reinforcement learning with verifiable reward has recently emerged as a central paradigm for post-training large language models (LLMs); however, prevailing mean-based methods, such as Group Relative Policy Optimization (GRPO), suffer from entropy collapse and limited reasoning gains. We argue that these issues stem from overemphasizing high-probability output sequences while neglecting rare but informative reasoning paths. To address these challenges, we propose Risk-based Policy Optimization (RiskPO), which substitutes classical mean-based objectives with principled risk measures. Specifically, we introduce a Mixed Value-at-Risk objective that integrates weighted attention over multiple regions of the reward distribution, thereby amplifying gradient signals on challenging instances and preventing overconfident convergence. We further design a bundling scheme that aggregates multiple questions into bundles, thus enriching the feedback signal and yielding more stable and informative training dynamics. Theoretically, we prove that the risk-averse update alleviates entropy collapse and promotes exploration. Numerically, RiskPO achieves consistent and significant improvements in mathematical reasoning, multi-modal reasoning, and code generation benchmarks, surpassing GRPO and its variants on both Pass@1 and Pass@k metrics. Our results demonstrate that risk-based optimization provides a rigorous and effective paradigm for enhancing LLM reasoning capabilities.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "206",
        "title": "Reinforcement Learning with Verifiable yet Noisy Rewards under Imperfect Verifiers",
        "author": [
            "Xin-Qiang Cai",
            "Wei Wang",
            "Feng Liu",
            "Tongliang Liu",
            "Gang Niu",
            "Masashi Sugiyama"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00915",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) trains policies against automated verifiers to avoid costly human labeling. To reduce vulnerability to verifier hacking, many RLVR systems collapse rewards to binary $\\{0,1\\}$ during training. This choice carries a cost: it introduces \\textit{false negatives} (rejecting correct answers, FNs) and \\textit{false positives} (accepting incorrect ones, FPs). For instance, a rule-based checker may mark the correct fraction $\\frac{12}{36}$ as wrong when compared against the canonical $\\frac{1}{3}$ due to brittle parsing/equivalence rules (FN), while a large language model (LLM) judges can be gamed by superficial cues or even a single adversarial token, yielding inflated correctness for wrong solutions (FP). We formalize verifier unreliability by modeling the verifier as a stochastic reward channel with asymmetric noise rates. From this abstraction, we derive two correction algorithms for verifier errors. The first is a \\textit{backward} correction that de-biases the observed binary reward to recover an \\textit{unbiased} estimator of the clean policy gradient. The second is a \\textit{forward} correction that reweights score-function terms so that the expected update direction aligns with the \\textit{clean gradient}; notably, it requires only the FN rate. We implement both as lightweight hooks in a group relative policy optimization (GRPO)-based RLVR pipeline and evaluate them on math-reasoning models and benchmarks. Across models and datasets, both corrections improve over uncorrected training; the forward variant converges faster and remains stable under heavier noise. Finally, we show a practical appeal mechanism in which a lightweight LLM verifier estimates the FN rate online by rechecking rule-based negatives, obtaining outperformance compared with other state-of-the-art contenders.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "207",
        "title": "Benchmarking Foundation Models with Retrieval-Augmented Generation in Olympic-Level Physics Problem Solving",
        "author": [
            "Shunfeng Zheng",
            "Yudi Zhang",
            "Meng Fang",
            "Zihan Zhang",
            "Zhitan Wu",
            "Mykola Pechenizkiy",
            "Ling Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00919",
        "abstract": "Retrieval-augmented generation (RAG) with foundation models has achieved strong performance across diverse tasks, but their capacity for expert-level reasoning-such as solving Olympiad-level physics problems-remains largely unexplored. Inspired by the way students prepare for competitions by reviewing past problems, we investigate the potential of RAG to enhance physics reasoning in foundation models. We introduce PhoPile, a high-quality multimodal dataset specifically designed for Olympiad-level physics, enabling systematic study of retrieval-based reasoning. PhoPile includes diagrams, graphs, and equations, capturing the inherently multimodal nature of physics problem solving. Using PhoPile, we benchmark RAG-augmented foundation models, covering both large language models (LLMs) and large multimodal models (LMMs) with multiple retrievers. Our results demonstrate that integrating retrieval with physics corpora can improve model performance, while also highlighting challenges that motivate further research in retrieval-augmented physics reasoning.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "208",
        "title": "On Effective Semantic Translation for Code: A Study Based on Pseudocode",
        "author": [
            "Songqiang Chen",
            "Congying Xu",
            "Jingyi Chen",
            "Jialun Cao",
            "Jiarong Wu",
            "Shing-Chi Cheung"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00920",
        "abstract": "Large language models (LLMs) show great potential in code translation. However, accurate translation remains challenging when using the commonly adopted direct code-to-code translation approach, which converts a program into the target programming language (PL) in a single step. Inspired by the success of incorporating intermediate steps to guide LLMs in resolving challenging tasks, we explore pseudocode-based code translation, which emulates the human semantic translation by first interpreting the program's intent and logic into pseudocode and then implementing it in the target PL. We find that pseudocode-based translation helps translate programs that direct translation struggles to handle. Nonetheless, the effectiveness, advantages, and limitations of this approach remain underexplored. To bridge this gap, we present an empirical study on pseudocode-based code translation, aiming to investigate its effectiveness in enhancing the direct translation approach, illuminate its effective usage, and identify limitations hindering its potential benefits. By comparing direct and pseudocode-based translation approaches on 9,690 translation tasks across six PLs with five popular LLMs, we demonstrate that pseudocode-based translation can effectively complement direct translation, particularly when translating from flexible to rigid PLs or dealing with low-resource Rust. Based on these findings, we suggest adopting strategies that combine the complementary strengths of both approaches to enhance code translation accuracy. We also reveal the advantages of pseudocode-based translation in disentangling translations of complicated programs and mitigating distractions from detailed implementations in original programs, as well as its limitations due to incorrect, incomplete, or ambiguous pseudocode.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "209",
        "title": "On Discovering Algorithms for Adversarial Imitation Learning",
        "author": [
            "Shashank Reddy Chirra",
            "Jayden Teoh",
            "Praveen Paruchuri",
            "Pradeep Varakantham"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00922",
        "abstract": "Adversarial Imitation Learning (AIL) methods, while effective in settings with limited expert demonstrations, are often considered unstable. These approaches typically decompose into two components: Density Ratio (DR) estimation $\\frac{\\rho_E}{\\rho_{\\pi}}$, where a discriminator estimates the relative occupancy of state-action pairs under the policy versus the expert; and Reward Assignment (RA), where this ratio is transformed into a reward signal used to train the policy. While significant research has focused on improving density estimation, the role of reward assignment in influencing training dynamics and final policy performance has been largely overlooked. RA functions in AIL are typically derived from divergence minimization objectives, relying heavily on human design and ingenuity. In this work, we take a different approach: we investigate the discovery of data-driven RA functions, i.e, based directly on the performance of the resulting imitation policy. To this end, we leverage an LLM-guided evolutionary framework that efficiently explores the space of RA functions, yielding \\emph{Discovered Adversarial Imitation Learning} (DAIL), the first meta-learnt AIL algorithm. Remarkably, DAIL generalises across unseen environments and policy optimization algorithms, outperforming the current state-of-the-art of \\emph{human-designed} baselines. Finally, we analyse why DAIL leads to more stable training, offering novel insights into the role of RA functions in the stability of AIL. Code is publicly available: https://github.com/shshnkreddy/DAIL.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "210",
        "title": "Making, not Taking, the Best of N",
        "author": [
            "Ammar Khairi",
            "Daniel D'souza",
            "Marzieh Fadaee",
            "Julia Kreutzer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00931",
        "abstract": "Obtaining high-quality generations in modern LLMs has largely been framed as a selection problem: identifying a single winning generation from a diverse pool of N samples, the Best-of-N (BoN). Yet, this approach is inherently zero-sum, discarding diverse and potentially useful information from the pool. Instead, we explore a collaborative setup, where all candidates can potentially contribute to the final winning generation. To this end, we propose Fusion-of-N (FusioN): a method that uses a general LLM judge to synthesize the most informative elements of each sample into a single final answer. We compare FusioN to BoN in two settings, (i) test-time scaling, where we sample and aggregate from a single model at test-time (ii) synthetic data generation, where we fuse samples from a pool of diverse teachers to improve a student model. We extensively benchmark both setups across 11 languages, 3 diverse tasks and varying model scales. Across the bench, FusioN consistently outperforms BoN showing versatility and robustness both in test-time scaling and in downstream gains from synthetic data generation. We also perform extensive analysis on FusioN, where it shows surprising strengths and robustness under challenging settings. These results show that we should shift how we think about evaluating and utilizing LLM generations from a monolithic measure of quality, to embracing their polylithic nature. This shift allows us to integrate diverse strengths, unlock latent potential, and achieve improvements that were previously inaccessible through selection alone.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "211",
        "title": "Opal: A Modular Framework for Optimizing Performance using Analytics and LLMs",
        "author": [
            "Mohammad Zaeed",
            "Tanzima Z. Islam",
            "Vladimir InÄiÄ"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00932",
        "abstract": "Large Language Models (LLMs) show promise for automated code optimization but struggle without performance context. This work introduces Opal, a modular framework that connects performance analytics insights with the vast body of published by guiding LLMs to generate informed, trustworthy optimizations. Unlike traditional performance tools that identify bottlenecks but stop short of actionable suggestions, Opal bridges this long-standing gap by linking dynamic insights from hardware counters and Roofline analysis to stall events to optimization decisions. We evaluate Opal across 1640 experiments on real-world GPU kernels and find that in over 98.5% of cases, even a single insight source yields speedups, ranging on average from 19.34% to 52.3%. Our prompt template produced correct code in all but one case, where a vague diagnostic caused an unsafe suggestion. By automatically optimizing GPU kernels using performance analytics and LLMs, Opal marks a leap toward democratizing expert-level performance engineering for all.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "212",
        "title": "Large Reasoning Models Learn Better Alignment from Flawed Thinking",
        "author": [
            "ShengYun Peng",
            "Eric Smith",
            "Ivan Evtimov",
            "Song Jiang",
            "Pin-Yu Chen",
            "Hongyuan Zhan",
            "Haozhu Wang",
            "Duen Horng Chau",
            "Mahesh Pasupuleti",
            "Jianfeng Chi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00938",
        "abstract": "Large reasoning models (LRMs) \"think\" by generating structured chain-of-thought (CoT) before producing a final answer, yet they still lack the ability to reason critically about safety alignment and are easily biased when a flawed premise is injected into their thought process. We propose RECAP (Robust Safety Alignment via Counter-Aligned Prefilling), a principled reinforcement learning (RL) method for post-training that explicitly teaches models to override flawed reasoning trajectories and reroute to safe and helpful responses. RECAP trains on a mixture of synthetically generated counter-aligned CoT prefills and standard prompts, requires no additional training cost or modifications beyond vanilla reinforcement learning from human feedback (RLHF), and substantially improves safety and jailbreak robustness, reduces overrefusal, and preserves core reasoning capability -- all while maintaining inference token budget. Extensive analysis shows that RECAP-trained models engage in self-reflection more frequently and remain robust under adaptive attacks, preserving safety even after repeated attempts to override their reasoning.",
        "tags": [
            "CoT",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "213",
        "title": "Non-submodular Visual Attention for Robot Navigation",
        "author": [
            "Reza Vafaee",
            "Kian Behzad",
            "Milad Siami",
            "Luca Carlone",
            "Ali Jadbabaie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00942",
        "abstract": "This paper presents a task-oriented computational framework to enhance Visual-Inertial Navigation (VIN) in robots, addressing challenges such as limited time and energy resources. The framework strategically selects visual features using a Mean Squared Error (MSE)-based, non-submodular objective function and a simplified dynamic anticipation model. To address the NP-hardness of this problem, we introduce four polynomial-time approximation algorithms: a classic greedy method with constant-factor guarantees; a low-rank greedy variant that significantly reduces computational complexity; a randomized greedy sampler that balances efficiency and solution quality; and a linearization-based selector based on a first-order Taylor expansion for near-constant-time execution. We establish rigorous performance bounds by leveraging submodularity ratios, curvature, and element-wise curvature analyses. Extensive experiments on both standardized benchmarks and a custom control-aware platform validate our theoretical results, demonstrating that these methods achieve strong approximation guarantees while enabling real-time deployment.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "214",
        "title": "ChatGPT in Introductory Programming: Counterbalanced Evaluation of Code Quality, Conceptual Learning, and Student Perceptions",
        "author": [
            "Shiza Andleeb",
            "Brandon Kantorski",
            "Jeffrey C. Carver"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00946",
        "abstract": "Background: Large language models (LLMs) such as ChatGPT are increasingly used in introductory programming courses to provide real-time code generation, debugging, and explanations. While these tools can boost productivity and code quality, concerns remain about over-reliance and potential impacts on conceptual learning. Objective: To investigate how ChatGPT access affects code quality, conceptual understanding, task completion times, and student perceptions in a CS1 course. Methods: We conducted a counterbalanced, quasi-experimental study in which students alternated between ChatGPT and non-ChatGPT conditions across two programming assignments in C (functions and structures). We evaluated their code submissions using multidimensional rubrics, conceptual post-surveys, and task completion time. Results: Students who had access to ChatGPT produced significantly higher rubric scores for code quality and completed tasks in less time compared to those without access. However, gains in conceptual understanding were mixed, lower for the functions topic but higher for the structures topic. Students reported positive experiences with ChatGPT, citing its value for debugging and practice, while expressing concerns about accuracy and long-term skill development. Conclusions: ChatGPT can enhance code quality and efficiency for novice programmers, but may not uniformly improve conceptual understanding. Structured integration and complementary instructional strategies are recommended to foster independent problem-solving skills.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "215",
        "title": "InfVSR: Breaking Length Limits of Generic Video Super-Resolution",
        "author": [
            "Ziqing Zhang",
            "Kai Liu",
            "Zheng Chen",
            "Xi Li",
            "Yucong Chen",
            "Bingnan Duan",
            "Linghe Kong",
            "Yulun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00948",
        "abstract": "Real-world videos often extend over thousands of frames. Existing video super-resolution (VSR) approaches, however, face two persistent challenges when processing long sequences: (1) inefficiency due to the heavy cost of multi-step denoising for full-length sequences; and (2) poor scalability hindered by temporal decomposition that causes artifacts and discontinuities. To break these limits, we propose InfVSR, which novelly reformulates VSR as an autoregressive-one-step-diffusion paradigm. This enables streaming inference while fully leveraging pre-trained video diffusion priors. First, we adapt the pre-trained DiT into a causal structure, maintaining both local and global coherence via rolling KV-cache and joint visual guidance. Second, we distill the diffusion process into a single step efficiently, with patch-wise pixel supervision and cross-chunk distribution matching. Together, these designs enable efficient and scalable VSR for unbounded-length videos. To fill the gap in long-form video evaluation, we build a new benchmark tailored for extended sequences and further introduce semantic-level metrics to comprehensively assess temporal consistency. Our method pushes the frontier of long-form VSR, achieves state-of-the-art quality with enhanced semantic consistency, and delivers up to 58x speed-up over existing methods such as MGLD-VSR. Code will be available at https://github.com/Kai-Liu001/InfVSR.",
        "tags": [
            "DiT",
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "216",
        "title": "A Neuro-Fuzzy System for Interpretable Long-Term Stock Market Forecasting",
        "author": [
            "Miha OÅ¾bot",
            "Igor Å krjanc",
            "Vitomir Å truc"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00960",
        "abstract": "In the complex landscape of multivariate time series forecasting, achieving both accuracy and interpretability remains a significant challenge. This paper introduces the Fuzzy Transformer (Fuzzformer), a novel recurrent neural network architecture combined with multi-head self-attention and fuzzy inference systems to analyze multivariate stock market data and conduct long-term time series forecasting. The method leverages LSTM networks and temporal attention to condense multivariate data into interpretable features suitable for fuzzy inference systems. The resulting architecture offers comparable forecasting performance to conventional models such as ARIMA and LSTM while providing meaningful information flow within the network. The method was examined on the real world stock market index S\\&P500. Initial results show potential for interpretable forecasting and identify current performance tradeoffs, suggesting practical application in understanding and forecasting stock market behavior.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "217",
        "title": "Analyzing Dialectical Biases in LLMs for Knowledge and Reasoning Benchmarks",
        "author": [
            "Eileen Pan",
            "Anna Seo Gyeong Choi",
            "Maartje ter Hoeve",
            "Skyler Seto",
            "Allison Koenecke"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00962",
        "abstract": "Large language models (LLMs) are ubiquitous in modern day natural language processing. However, previous work has shown degraded LLM performance for under-represented English dialects. We analyze the effects of typifying \"standard\" American English language questions as non-\"standard\" dialectal variants on multiple choice question answering tasks and find up to a 20% reduction in accuracy. Additionally, we investigate the grammatical basis of under-performance in non-\"standard\" English questions. We find that individual grammatical rules have varied effects on performance, but some are more consequential than others: three specific grammar rules (existential \"it\", zero copula, and y'all) can explain the majority of performance degradation observed in multiple dialects. We call for future work to investigate bias mitigation methods focused on individual, high-impact grammatical structures.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "218",
        "title": "QUASAR: Quantum Assembly Code Generation Using Tool-Augmented LLMs via Agentic RL",
        "author": [
            "Cong Yu",
            "Valter Uotila",
            "Shilong Deng",
            "Qingyuan Wu",
            "Tuo Shi",
            "Songlin Jiang",
            "Lei You",
            "Bo Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00967",
        "abstract": "Designing and optimizing task-specific quantum circuits are crucial to leverage the advantage of quantum computing. Recent large language model (LLM)-based quantum circuit generation has emerged as a promising automatic solution. However, the fundamental challenges remain unaddressed: (i) parameterized quantum gates require precise numerical values for optimal performance, which also depend on multiple aspects, including the number of quantum gates, their parameters, and the layout/depth of the circuits. (ii) LLMs often generate low-quality or incorrect quantum circuits due to the lack of quantum domain-specific knowledge. We propose QUASAR, an agentic reinforcement learning (RL) framework for quantum circuits generation and optimization based on tool-augmented LLMs. To align the LLM with quantum-specific knowledge and improve the generated quantum circuits, QUASAR designs (i) a quantum circuit verification approach with external quantum simulators and (ii) a sophisticated hierarchical reward mechanism in RL training. Extensive evaluation shows improvements in both syntax and semantic performance of the generated quantum circuits. When augmenting a 4B LLM, QUASAR has achieved the validity of 99.31% in Pass@1 and 100% in Pass@10, outperforming industrial LLMs of GPT-4o, GPT-5 and DeepSeek-V3 and several supervised-fine-tuning (SFT)-only and RL-only baselines.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "219",
        "title": "JEPA-T: Joint-Embedding Predictive Architecture with Text Fusion for Image Generation",
        "author": [
            "Siheng Wan",
            "Zhengtao Yao",
            "Zhengdao Li",
            "Junhao Dong",
            "Yanshu Li",
            "Yikai Li",
            "Linshan Li",
            "Haoyan Xu",
            "Yijiang Li",
            "Zhikang Dong",
            "Huacan Wang",
            "Jifeng Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00974",
        "abstract": "Modern Text-to-Image (T2I) generation increasingly relies on token-centric architectures that are trained with self-supervision, yet effectively fusing text with visual tokens remains a challenge. We propose \\textbf{JEPA-T}, a unified multimodal framework that encodes images and captions into discrete visual and textual tokens, processed by a joint-embedding predictive Transformer. To enhance fusion, we incorporate cross-attention after the feature predictor for conditional denoising while maintaining a task-agnostic backbone. Additionally, raw texts embeddings are injected prior to the flow matching loss to improve alignment during training. During inference, the same network performs both class-conditional and free-text image generation by iteratively denoising visual tokens conditioned on text. Evaluations on ImageNet-1K demonstrate that JEPA-T achieves strong data efficiency, open-vocabulary generalization, and consistently outperforms non-fusion and late-fusion baselines. Our approach shows that late architectural fusion combined with objective-level alignment offers an effective balance between conditioning strength and backbone generality in token-based http://T2I.The code is now available: https://github.com/justin-herry/JEPA-T.git",
        "tags": [
            "Flow Matching",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "220",
        "title": "It Takes Two: Your GRPO Is Secretly DPO",
        "author": [
            "Yihong Wu",
            "Liheng Ma",
            "Lei Ding",
            "Muzhi Li",
            "Xinyu Wang",
            "Kejia Chen",
            "Zhan Su",
            "Zhanguang Zhang",
            "Chenyang Huang",
            "Yingxue Zhang",
            "Mark Coates",
            "Jian-Yun Nie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00977",
        "abstract": "Group Relative Policy Optimization (GRPO) is a prominent reinforcement learning algorithm for post-training Large Language Models (LLMs). It is commonly believed that GRPO necessitates a large group size to ensure stable training via precise statistical estimation, which incurs substantial computational overhead. In this work, we challenge this assumption by reframing GRPO as a form of contrastive learning, which reveals a fundamental connection to Direct Preference Optimization (DPO). Motivated by DPO's empirical success, we investigate the minimal two-rollout case (2-GRPO), a configuration previously deemed infeasible. We provide a rigorous theoretical analysis to validate 2-GRPO and demonstrate empirically that it achieves performance on par with 16-GRPO, despite using only 1/8 of the rollouts and reducing training time by over 70%.",
        "tags": [
            "DPO",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "221",
        "title": "FlexiCodec: A Dynamic Neural Audio Codec for Low Frame Rates",
        "author": [
            "Jiaqi Li",
            "Yao Qian",
            "Yuxuan Hu",
            "Leying Zhang",
            "Xiaofei Wang",
            "Heng Lu",
            "Manthan Thakker",
            "Jinyu Li",
            "Shang Zhao",
            "Zhizheng Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00981",
        "abstract": "Neural audio codecs are foundational to speech language models. It is expected to have a low frame rate and decoupled semantic and acoustic information. A lower frame rate codec can reduce the computational cost of speech language models by shortening the sequence length. Recent studies have developed 12.5Hz low-frame-rate audio codecs, but even lower frame rate codecs remain underexplored. We find that a major challenge for very low frame rate tokens is missing semantic information. This paper introduces FlexiCodec to address this limitation. FlexiCodec improves semantic preservation with a dynamic frame rate approach and introduces a novel architecture featuring an ASR feature-assisted dual stream encoding and Transformer bottlenecks. With dynamic frame rates, it uses less frames at information-sparse regions through adaptively merging semantically similar frames. A dynamic frame rate also allows FlexiCodec to support inference-time controllable frame rates between 3Hz and 12.5Hz. Experiments on 6.25Hz, 8.3Hz and 12.5Hz average frame rates confirm that FlexiCodec excels over baseline systems in semantic information preservation and delivers a high audio reconstruction quality. We also validate the effectiveness of FlexiCodec in language model-based TTS. Demos are available at: https://flexicodec.github.io",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "222",
        "title": "Riemannian Consistency Model",
        "author": [
            "Chaoran Cheng",
            "Yusong Wang",
            "Yuxin Chen",
            "Xiangxin Zhou",
            "Nanning Zheng",
            "Ge Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00983",
        "abstract": "Consistency models are a class of generative models that enable few-step generation for diffusion and flow matching models. While consistency models have achieved promising results on Euclidean domains like images, their applications to Riemannian manifolds remain challenging due to the curved geometry. In this work, we propose the Riemannian Consistency Model (RCM), which, for the first time, enables few-step consistency modeling while respecting the intrinsic manifold constraint imposed by the Riemannian geometry. Leveraging the covariant derivative and exponential-map-based parameterization, we derive the closed-form solutions for both discrete- and continuous-time training objectives for RCM. We then demonstrate theoretical equivalence between the two variants of RCM: Riemannian consistency distillation (RCD) that relies on a teacher model to approximate the marginal vector field, and Riemannian consistency training (RCT) that utilizes the conditional vector field for training. We further propose a simplified training objective that eliminates the need for the complicated differential calculation. Finally, we provide a unique kinematics perspective for interpreting the RCM objective, offering new theoretical angles. Through extensive experiments, we manifest the superior generative quality of RCM in few-step generation on various non-Euclidean manifolds, including flat-tori, spheres, and the 3D rotation group SO(3).",
        "tags": [
            "3D",
            "Consistency Models",
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "223",
        "title": "An Efficient, Reliable and Observable Collective Communication Library in Large-scale GPU Training Clusters",
        "author": [
            "Ziteng Chen",
            "Xiaohe Hu",
            "Menghao Zhang",
            "Yanmin Jia",
            "Yan Zhang",
            "Mingjun Zhang",
            "Da Liu",
            "Fangzheng Jiao",
            "Jun Chen",
            "He Liu",
            "Aohan Zeng",
            "Shuaixing Duan",
            "Ruya Gu",
            "Yang Jing",
            "Bowen Han",
            "Jiahao Cao",
            "Wei Chen",
            "Wenqi Xie",
            "Jinlong Hou",
            "Yuan Cheng",
            "Bohua Xu",
            "Mingwei Xu",
            "Chunming Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00991",
        "abstract": "Large-scale LLM training requires collective communication libraries to exchange data among distributed GPUs. As a company dedicated to building and operating large-scale GPU training clusters, we encounter several challenges when using NCCL in production, including 1) limited efficiency with costly and cumbersome P2P communication, 2) poor tolerance to frequent RNIC port failures, and 3) insufficient observability of transient collective communication anomalies. To address these issues, we propose ICCL, an efficient, reliable, and observable collective communication library in large-scale GPU training clusters. ICCL offloads the P2P communication from GPU kernels to CPU threads for minimal SM consumption, and removes the redundant memory copies irrelevant to the actual communicating process. ICCL also introduces a primary-backup QP mechanism to tolerate frequent NIC port failures, and designs a window-based monitor to observe network anomalies at O(us) level. We open-source ICCL and deploy it in production training clusters for several months, with results showing that compared to NCCL, ICCL achieves a 23.4%/28.5% improvement in P2P throughput/latency as well as a 6.02% increase in training throughput. We also share the operating experience of ICCL in large-scale clusters, hoping to give the communities more insights on production-level collective communication libraries in LLM training.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "224",
        "title": "Semantics-Aligned, Curriculum-Driven, and Reasoning-Enhanced Vulnerability Repair Framework",
        "author": [
            "Chengran Yang",
            "Ting Zhang",
            "Jinfeng Jiang",
            "Xin Zhou",
            "Haoye Tian",
            "Jieke Shi",
            "Junkai Chen",
            "Yikun Li",
            "Eng Lieh Ouh",
            "Lwin Khin Shar",
            "David Lo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01002",
        "abstract": "Current learning-based Automated Vulnerability Repair (AVR) approaches, while promising, often fail to generalize effectively in real-world scenarios. Our diagnostic analysis reveals three fundamental weaknesses in state-of-the-art AVR approaches: (1) limited cross-repository generalization, with performance drops on unseen codebases; (2) inability to capture long-range dependencies, causing a performance degradation on complex, multi-hunk repairs; and (3) over-reliance on superficial lexical patterns, leading to significant performance drops on vulnerabilities with minor syntactic variations like variable renaming.\nTo address these limitations, we propose SeCuRepair, a semantics-aligned, curriculum-driven, and reasoning-enhanced framework for vulnerability repair. At its core, SeCuRepair adopts a reason-then-edit paradigm, requiring the model to articulate why and how a vulnerability should be fixed before generating the patch. This explicit reasoning enforces a genuine understanding of repair logic rather than superficial memorization of lexical patterns. SeCuRepair also moves beyond traditional supervised fine-tuning and employs semantics-aware reinforcement learning, rewarding patches for their syntactic and semantic alignment with the oracle patch rather than mere token overlap. Complementing this, a difficulty-aware curriculum progressively trains the model, starting with simple fixes and advancing to complex, multi-hunk coordinated edits.\nWe evaluate SeCuRepair on strict, repository-level splits of BigVul and newly crafted PrimeVul_AVR datasets. SeCuRepair significantly outperforms all baselines, surpassing the best-performing baselines by 34.52% on BigVul and 31.52% on PrimeVul\\textsubscript{AVR} in terms of CodeBLEU, respectively. Comprehensive ablation studies further confirm that each component of our framework contributes to its final performance.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "225",
        "title": "TextCAM: Explaining Class Activation Map with Text",
        "author": [
            "Qiming Zhao",
            "Xingjian Li",
            "Xiaoyu Cao",
            "Xiaolong Wu",
            "Min Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01004",
        "abstract": "Deep neural networks (DNNs) have achieved remarkable success across domains but remain difficult to interpret, limiting their trustworthiness in high-stakes applications. This paper focuses on deep vision models, for which a dominant line of explainability methods are Class Activation Mapping (CAM) and its variants working by highlighting spatial regions that drive predictions. We figure out that CAM provides little semantic insight into what attributes underlie these activations. To address this limitation, we propose TextCAM, a novel explanation framework that enriches CAM with natural languages. TextCAM combines the precise spatial localization of CAM with the semantic alignment of vision-language models (VLMs). Specifically, we derive channel-level semantic representations using CLIP embeddings and linear discriminant analysis, and aggregate them with CAM weights to produce textual descriptions of salient visual evidence. This yields explanations that jointly specify where the model attends and what visual attributes likely support its decision. We further extend TextCAM to generate feature channels into semantically coherent groups, enabling more fine-grained visual-textual explanations. Experiments on ImageNet, CLEVR, and CUB demonstrate that TextCAM produces faithful and interpretable rationales that improve human understanding, detect spurious correlations, and preserve model fidelity.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "226",
        "title": "POVQA: Preference-Optimized Video Question Answering with Rationales for Data Efficiency",
        "author": [
            "Ashim Dahal",
            "Ankit Ghimire",
            "Saydul Akbar Murad",
            "Nick Rahimi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01009",
        "abstract": "Video Question Answering (VQA) with Large Vision Language Models (LVLMs) has gained significant traction in research ever since the Flamingo was introduced by Deepmind. Recent advancements in large context/long video question answering have allowed VQA tasks to have context window of 1500+ frames. However, this only leads to 50 seconds of video footage without losing any significant information. We introduce POVQA, a data-efficient pipeline that compresses each second of video into a single temporally pooled image (via motion blur and weighted averaging variants) and then align LVLMs with lightweight supervision. Concretely, we build 1 fps input sources using Blend Blur with Last Frame, Weighted Average, Exponential and Ramp pooling and fine-tune QWEN-2.5-VL 7B with supervised two turn target including reasoning and final answer. We apply Supervised Fine Tuning (SFT) and Direct Preference Optimization (DPO) on our novel dataset ReasonVQA consisting of 12 movies with 239 human annotated question-answer with reasoning prompts. On our ReasonVQA dataset, this method dramatically improves performance over pooled baselines: F1 score improves from 0.212 to 0.543, BLEU-4 from 0.031 to 0.291, and ROUGE-L from 0.196 to 0.528. Rationale quality also significantly increases. Cross-evaluation of SFT + DPO on various pooling functions show that the gains persist regardless of the pooling scheme used at train or test time, indicating strong robustness on summarization of temporal evidence. Similar observations were made on zero-shot in TVQA.",
        "tags": [
            "DPO",
            "Qwen",
            "VLM"
        ]
    },
    {
        "id": "227",
        "title": "ImageDoctor: Diagnosing Text-to-Image Generation via Grounded Image Reasoning",
        "author": [
            "Yuxiang Guo",
            "Jiang Liu",
            "Ze Wang",
            "Hao Chen",
            "Ximeng Sun",
            "Yang Zhao",
            "Jialian Wu",
            "Xiaodong Yu",
            "Zicheng Liu",
            "Emad Barsoum"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01010",
        "abstract": "The rapid advancement of text-to-image (T2I) models has increased the need for reliable human preference modeling, a demand further amplified by recent progress in reinforcement learning for preference alignment. However, existing approaches typically quantify the quality of a generated image using a single scalar, limiting their ability to provide comprehensive and interpretable feedback on image quality. To address this, we introduce ImageDoctor, a unified multi-aspect T2I model evaluation framework that assesses image quality across four complementary dimensions: plausibility, semantic alignment, aesthetics, and overall quality. ImageDoctor also provides pixel-level flaw indicators in the form of heatmaps, which highlight misaligned or implausible regions, and can be used as a dense reward for T2I model preference alignment. Inspired by the diagnostic process, we improve the detail sensitivity and reasoning capability of ImageDoctor by introducing a \"look-think-predict\" paradigm, where the model first localizes potential flaws, then generates reasoning, and finally concludes the evaluation with quantitative scores. Built on top of a vision-language model and trained through a combination of supervised fine-tuning and reinforcement learning, ImageDoctor demonstrates strong alignment with human preference across multiple datasets, establishing its effectiveness as an evaluation metric. Furthermore, when used as a reward model for preference tuning, ImageDoctor significantly improves generation quality -- achieving an improvement of 10% over scalar-based reward models.",
        "tags": [
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": "228",
        "title": "Equivariant Geometric Scattering Networks via Vector Diffusion Wavelets",
        "author": [
            "David R. Johnson",
            "Rishabh Anand",
            "Smita Krishnaswamy",
            "Michael Perlmutter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01022",
        "abstract": "We introduce a novel version of the geometric scattering transform for geometric graphs containing scalar and vector node features. This new scattering transform has desirable symmetries with respect to rigid-body roto-translations (i.e., $SE(3)$-equivariance) and may be incorporated into a geometric GNN framework. We empirically show that our equivariant scattering-based GNN achieves comparable performance to other equivariant message-passing-based GNNs at a fraction of the parameter count.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "229",
        "title": "Prometheus: Universal, Open-Source Mocap-Based Teleoperation System with Force Feedback for Dataset Collection in Robot Learning",
        "author": [
            "S. Satsevich",
            "A. Bazhenov",
            "S. Egorov",
            "A. Erkhov",
            "M. Gromakov",
            "A. Fedoseev",
            "D. Tsetserukou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01023",
        "abstract": "This paper presents a novel teleoperation system with force feedback, utilizing consumer-grade HTC Vive Trackers 2.0. The system integrates a custom-built controller, a UR3 robotic arm, and a Robotiq gripper equipped with custom-designed fingers to ensure uniform pressure distribution on an embedded force sensor. Real-time compression force data is transmitted to the controller, enabling operators to perceive the gripping force applied to objects. Experimental results demonstrate that the system enhances task success rates and provides a low-cost solution for large-scale imitation learning data collection without compromising affordability.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "230",
        "title": "GenIA-E2ETest: A Generative AI-Based Approach for End-to-End Test Automation",
        "author": [
            "Elvis JÃºnior",
            "Alan Valejo",
            "Jorge Valverde-Rebaza",
            "VÃ¢nia de Oliveira Neves"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01024",
        "abstract": "Software testing is essential to ensure system quality, but it remains time-consuming and error-prone when performed manually. Although recent advances in Large Language Models (LLMs) have enabled automated test generation, most existing solutions focus on unit testing and do not address the challenges of end-to-end (E2E) testing, which validates complete application workflows from user input to final system response. This paper introduces GenIA-E2ETest, which leverages generative AI to generate executable E2E test scripts from natural language descriptions automatically. We evaluated the approach on two web applications, assessing completeness, correctness, adaptation effort, and robustness. Results were encouraging: the scripts achieved an average of 77% for both element metrics, 82% for precision of execution, 85% for execution recall, required minimal manual adjustments (average manual modification rate of 10%), and showed consistent performance in typical web scenarios. Although some sensitivity to context-dependent navigation and dynamic content was observed, the findings suggest that GenIA-E2ETest is a practical and effective solution to accelerate E2E test automation from natural language, reducing manual effort and broadening access to automated testing.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "231",
        "title": "Shape Happens: Automatic Feature Manifold Discovery in LLMs via Supervised Multi-Dimensional Scaling",
        "author": [
            "Federico Tiblias",
            "Irina Bigoulaeva",
            "Jingcheng Niu",
            "Simone Balloccu",
            "Iryna Gurevych"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01025",
        "abstract": "The linear representation hypothesis states that language models (LMs) encode concepts as directions in their latent space, forming organized, multidimensional manifolds. Prior efforts focus on discovering specific geometries for specific features, and thus lack generalization. We introduce Supervised Multi-Dimensional Scaling (SMDS), a model-agnostic method to automatically discover feature manifolds. We apply SMDS to temporal reasoning as a case study, finding that different features form various geometric structures such as circles, lines, and clusters. SMDS reveals many insights on these structures: they consistently reflect the properties of the concepts they represent; are stable across model families and sizes; actively support reasoning in models; and dynamically reshape in response to context changes. Together, our findings shed light on the functional role of feature manifolds, supporting a model of entity-based reasoning in which LMs encode and transform structured representations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "232",
        "title": "Syntax-Guided Diffusion Language Models with User-Integrated Personalization",
        "author": [
            "Ruqian Zhang",
            "Yijiao Zhang",
            "Juan Shen",
            "Zhongyi Zhu",
            "Annie Qu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01028",
        "abstract": "Large language models have made revolutionary progress in generating human-like text, yet their outputs often tend to be generic, exhibiting insufficient structural diversity, which limits personalized expression. Recent advances in diffusion models have opened new opportunities for improving language generation beyond the limitations of autoregressive paradigms. In this work, we propose a syntax-guided diffusion language model that integrates structural supervision and personalized conditioning to enhance text quality, diversity, and controllability. We introduce a cascaded framework that generates syntactic guidance before conditional text generation, and further generalize it to a novel noncascaded architecture for better alignment between structure and content. By incorporating syntactic information in the generating process, the proposed model better captures the lexical and structural characteristics of stylistic sentence construction. To enable fine-grained personalization, we develop a shared representation mechanism that facilitates information integration across users, supporting both faithful stylistic generation and generalizable zero-shot inference. Extensive experiments on multiple tasks demonstrate the superiority of our approach in fluency, diversity, and stylistic fidelity. Further qualitative analyses highlight its interpretability and flexibility in learning personalized patterns.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "233",
        "title": "Uncovering the Computational Ingredients of Human-Like Representations in LLMs",
        "author": [
            "Zach Studdiford",
            "Timothy T. Rogers",
            "Kushin Mukherjee",
            "Siddharth Suresh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01030",
        "abstract": "The ability to translate diverse patterns of inputs into structured patterns of behavior has been thought to rest on both humans' and machines' ability to learn robust representations of relevant concepts. The rapid advancement of transformer-based large language models (LLMs) has led to a diversity of computational ingredients -- architectures, fine tuning methods, and training datasets among others -- but it remains unclear which of these ingredients are most crucial for building models that develop human-like representations. Further, most current LLM benchmarks are not suited to measuring representational alignment between humans and models, making benchmark scores unreliable for assessing if current LLMs are making progress towards becoming useful cognitive models. We address these limitations by first evaluating a set of over 70 models that widely vary in their computational ingredients on a triplet similarity task, a method well established in the cognitive sciences for measuring human conceptual representations, using concepts from the THINGS database. Comparing human and model representations, we find that models that undergo instruction-finetuning and which have larger dimensionality of attention heads are among the most human aligned, while multimodal pretraining and parameter size have limited bearing on alignment. Correlations between alignment scores and scores on existing benchmarks reveal that while some benchmarks (e.g., MMLU) are better suited than others (e.g., MUSR) for capturing representational alignment, no existing benchmark is capable of fully accounting for the variance of alignment scores, demonstrating their insufficiency in capturing human-AI alignment. Taken together, our findings help highlight the computational ingredients most essential for advancing LLMs towards models of human conceptual representation and address a key benchmarking gap in LLM evaluation.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "234",
        "title": "Secure and reversible face anonymization with diffusion models",
        "author": [
            "Pol Labarbarie",
            "Vincent Itier",
            "William Puech"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01031",
        "abstract": "Face images processed by computer vision algorithms contain sensitive personal information that malicious actors can capture without consent. These privacy and security risks highlight the need for effective face anonymization methods. Current methods struggle to propose a good trade-off between a secure scheme with high-quality image generation and reversibility for later person authentication. Diffusion-based approaches produce high-quality anonymized images but lack the secret key mechanism to ensure that only authorized parties can reverse the process. In this paper, we introduce, to our knowledge, the first secure, high-quality reversible anonymization method based on a diffusion model. We propose to combine the secret key with the latent faces representation of the diffusion model. To preserve identity-irrelevant features, generation is constrained by a facial mask, maintaining high-quality images. By using a deterministic forward and backward diffusion process, our approach enforces that the original face can be recovered with the correct secret key. We also show that the proposed method produces anonymized faces that are less visually similar to the original faces, compared to other previous work.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "235",
        "title": "Meaningless Tokens, Meaningful Gains: How Activation Shifts Enhance LLM Reasoning",
        "author": [
            "Zeru Shi",
            "Yingjia Wan",
            "Zhenting Wang",
            "Qifan Wang",
            "Fan Yang",
            "Elisa Kreiss",
            "Ruixiang Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01032",
        "abstract": "Motivated by the puzzling observation that inserting long sequences of meaningless tokens before the query prompt can consistently enhance LLM reasoning performance, this work analyzes the underlying mechanism driving this phenomenon and based on these insights proposes a more principled method that allows for similar performance gains. First, we find that the improvements arise from a redistribution of activations in the LLM's MLP layers, where near zero activations become less frequent while large magnitude activations increase. This redistribution enhances the model's representational capacity by suppressing weak signals and promoting stronger, more informative ones. Building on this insight, we propose the Activation Redistribution Module (ARM), a lightweight inference-time technique that modifies activations directly without altering the input sequence. ARM adaptively identifies near-zero activations after the non-linear function and shifts them outward, implicitly reproducing the beneficial effects of meaningless tokens in a controlled manner. Extensive experiments across diverse benchmarks and model architectures clearly show that ARM consistently improves LLM performance on reasoning tasks while requiring only a few lines of simple code to implement. Our findings deliver both a clear mechanistic explanation for the unexpected benefits of meaningless tokens and a simple yet effective technique that harnesses activation redistribution to further improve LLM performance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "236",
        "title": "CurES: From Gradient Analysis to Efficient Curriculum Learning for Reasoning LLMs",
        "author": [
            "Yongcheng Zeng",
            "Zexu Sun",
            "Bokai Ji",
            "Erxue Min",
            "Hengyi Cai",
            "Shuaiqiang Wang",
            "Dawei Yin",
            "Haifeng Zhang",
            "Xu Chen",
            "Jun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01037",
        "abstract": "Curriculum learning plays a crucial role in enhancing the training efficiency of large language models (LLMs) on reasoning tasks. However, existing methods often fail to adequately account for variations in prompt difficulty or rely on simplistic filtering mechanisms to select prompt datasets within a narrow criterion range, resulting in significant computational waste. In this work, we approach the problem from the perspective of reinforcement learning gradient optimization, offering a systematic and theoretical investigation into how to improve the training efficiency of LLMs. We identify two key factors influencing training efficiency: the selection of training prompts and the allocation of rollout quantities across different prompts. Our theoretical analysis reveals that the sampling distribution of prompts dictates the convergence rate of gradient descent, while the allocation of the rollout quantity influences the consistency and stability of overall gradient updates. Based on these insights, we propose CurES, an efficient training method that accelerates convergence and employs Bayesian posterior estimation to minimize computational overhead. Experiments demonstrate that our CurES outperforms Group Relative Policy Optimization (GRPO) by \\textbf{+3.30} points and \\textbf{+4.82} points with 1.5B and 7B models, respectively. Additionally, CurES exhibits faster convergence compared to baselines, including GRPO.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "237",
        "title": "Gated X-TFC: Soft Domain Decomposition for Forward and Inverse Problems in Sharp-Gradient PDEs",
        "author": [
            "Vikas Dwivedi",
            "Enrico Schiassi",
            "Monica Sigovan",
            "Bruno Sixou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01039",
        "abstract": "Physics-informed neural networks (PINNs) and related methods struggle to resolve sharp gradients in singularly perturbed boundary value problems without resorting to some form of domain decomposition, which often introduce complex interface penalties. While the Extreme Theory of Functional Connections (X-TFC) avoids multi-objective optimization by employing exact boundary condition enforcement, it remains computationally inefficient for boundary layers and incompatible with decomposition. We propose Gated X-TFC, a novel framework for both forward and inverse problems, that overcomes these limitations through a soft, learned domain decomposition. Our method replaces hard interfaces with a differentiable logistic gate that dynamically adapts radial basis function (RBF) kernel widths across the domain, eliminating the need for interface penalties. This approach yields not only superior accuracy but also dramatic improvements in computational efficiency: on a benchmark one dimensional (1D) convection-diffusion, Gated X-TFC achieves an order-of-magnitude lower error than standard X-TFC while using 80 percent fewer collocation points and reducing training time by 66 percent. In addition, we introduce an operator-conditioned meta-learning layer that learns a probabilistic mapping from PDE parameters to optimal gate configurations, enabling fast, uncertainty-aware warm-starting for new problem instances. We further demonstrate scalability to multiple subdomains and higher dimensions by solving a twin boundary-layer equation and a 2D Poisson problem with a sharp Gaussian source. Overall, Gated X-TFC delivers a simple alternative alternative to PINNs that is both accurate and computationally efficient for challenging boundar-layer regimes. Future work will focus on nonlinear problems.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "238",
        "title": "Authentic Discrete Diffusion Model",
        "author": [
            "Xiao Li",
            "Jiaqi Zhang",
            "Shuxiang Zhang",
            "Tianshui Chen",
            "Liang Lin",
            "Guangrun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01047",
        "abstract": "We propose an Authentic Discrete Diffusion (ADD) framework that fundamentally redefines prior pseudo-discrete approaches by preserving core diffusion characteristics directly in the one-hot space through a suite of coordinated mechanisms. Unlike conventional \"pseudo\" discrete diffusion (PDD) methods, ADD reformulates the diffusion input by directly using float-encoded one-hot class data, without relying on diffusing in the continuous latent spaces or masking policies. At its core, a timestep-conditioned cross-entropy loss is introduced between the diffusion model's outputs and the original one-hot labels. This synergistic design establishes a bridge between discriminative and generative learning. Our experiments demonstrate that ADD not only achieves superior performance on classification tasks compared to the baseline, but also exhibits excellent text generation capabilities on Image captioning. Extensive ablations validate the measurable gains of each component.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "239",
        "title": "Interpreting Language Models Through Concept Descriptions: A Survey",
        "author": [
            "Nils Feldhus",
            "Laura Kopf"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01048",
        "abstract": "Understanding the decision-making processes of neural networks is a central goal of mechanistic interpretability. In the context of Large Language Models (LLMs), this involves uncovering the underlying mechanisms and identifying the roles of individual model components such as neurons and attention heads, as well as model abstractions such as the learned sparse features extracted by Sparse Autoencoders (SAEs). A rapidly growing line of work tackles this challenge by using powerful generator models to produce open-vocabulary, natural language concept descriptions for these components. In this paper, we provide the first survey of the emerging field of concept descriptions for model components and abstractions. We chart the key methods for generating these descriptions, the evolving landscape of automated and human metrics for evaluating them, and the datasets that underpin this research. Our synthesis reveals a growing demand for more rigorous, causal evaluation. By outlining the state of the art and identifying key challenges, this survey provides a roadmap for future research toward making models more transparent.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "240",
        "title": "KeySG: Hierarchical Keyframe-Based 3D Scene Graphs",
        "author": [
            "Abdelrhman Werby",
            "Dennis Rotondi",
            "Fabio Scaparro",
            "Kai O. Arras"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01049",
        "abstract": "In recent years, 3D scene graphs have emerged as a powerful world representation, offering both geometric accuracy and semantic richness. Combining 3D scene graphs with large language models enables robots to reason, plan, and navigate in complex human-centered environments. However, current approaches for constructing 3D scene graphs are semantically limited to a predefined set of relationships, and their serialization in large environments can easily exceed an LLM's context window. We introduce KeySG, a framework that represents 3D scenes as a hierarchical graph consisting of floors, rooms, objects, and functional elements, where nodes are augmented with multi-modal information extracted from keyframes selected to optimize geometric and visual coverage. The keyframes allow us to efficiently leverage VLM to extract scene information, alleviating the need to explicitly model relationship edges between objects, enabling more general, task-agnostic reasoning and planning. Our approach can process complex and ambiguous queries while mitigating the scalability issues associated with large scene graphs by utilizing a hierarchical retrieval-augmented generation (RAG) pipeline to extract relevant context from the graph. Evaluated across four distinct benchmarks -- including 3D object segmentation and complex query retrieval -- KeySG outperforms prior approaches on most metrics, demonstrating its superior semantic richness and efficiency.",
        "tags": [
            "3D",
            "LLM",
            "RAG",
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "241",
        "title": "GEM: A Gym for Agentic LLMs",
        "author": [
            "Zichen Liu",
            "Anya Sims",
            "Keyu Duan",
            "Changyu Chen",
            "Simon Yu",
            "Xiangxin Zhou",
            "Haotian Xu",
            "Shaopan Xiong",
            "Bo Liu",
            "Chenmien Tan",
            "Chuen Yang Beh",
            "Weixun Wang",
            "Hao Zhu",
            "Weiyan Shi",
            "Diyi Yang",
            "Michael Shieh",
            "Yee Whye Teh",
            "Wee Sun Lee",
            "Min Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01051",
        "abstract": "The training paradigm for large language models (LLMs) is moving from static datasets to experience-based learning, where agents acquire skills via interacting with complex environments. To facilitate this transition we introduce GEM (General Experience Maker), an open-source environment simulator designed for the age of LLMs. Analogous to OpenAI-Gym for traditional reinforcement learning (RL), GEM provides a standardized framework for the environment-agent interface, including asynchronous vectorized execution for high throughput, and flexible wrappers for easy extensibility. GEM also features a diverse suite of environments, robust integrated tools, and single-file example scripts demonstrating using GEM with five popular RL training frameworks. Along with this, we also provide a set of baselines across 24 environments using REINFORCE with Return Batch Normalization (ReBN), which -- unlike GRPO -- is compatible with the full RL setting of dense per-turn rewards and offers better credit assignment. We further conduct apple-to-apple benchmarking of PPO, GRPO and REINFORCE in both single- and multi-turn settings using GEM to shed light on the algorithmic designs. Lastly, GEM also functions as a convenient evaluation toolkit besides a training environment. We hope this framework can help accelerate future agentic LLM research.",
        "tags": [
            "GRPO",
            "LLM",
            "PPO",
            "RL"
        ]
    },
    {
        "id": "242",
        "title": "Hybrid Dialogue State Tracking for Persian Chatbots: A Language Model-Based Approach",
        "author": [
            "Samin Mahdipour Aghabagher",
            "Saeedeh Momtazi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01052",
        "abstract": "Dialogue State Tracking (DST) is an essential element of conversational AI with the objective of deeply understanding the conversation context and leading it toward answering user requests. Due to high demands for open-domain and multi-turn chatbots, the traditional rule-based DST is not efficient enough, since it cannot provide the required adaptability and coherence for human-like experiences in complex conversations. This study proposes a hybrid DST model that utilizes rule-based methods along with language models, including BERT for slot filling and intent detection, XGBoost for intent validation, GPT for DST, and online agents for real-time answer generation. This model is uniquely designed to be evaluated on a comprehensive Persian multi-turn dialogue dataset and demonstrated significantly improved accuracy and coherence over existing methods in Persian-based chatbots. The results demonstrate how effectively a hybrid approach may improve DST capabilities, paving the way for conversational AI systems that are more customized, adaptable, and human-like.",
        "tags": [
            "BERT",
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "243",
        "title": "ReSWD: ReSTIR'd, not shaken. Combining Reservoir Sampling and Sliced Wasserstein Distance for Variance Reduction",
        "author": [
            "Mark Boss",
            "Andreas Engelhardt",
            "Simon DonnÃ©",
            "Varun Jampani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01061",
        "abstract": "Distribution matching is central to many vision and graphics tasks, where the widely used Wasserstein distance is too costly to compute for high dimensional distributions. The Sliced Wasserstein Distance (SWD) offers a scalable alternative, yet its Monte Carlo estimator suffers from high variance, resulting in noisy gradients and slow convergence. We introduce Reservoir SWD (ReSWD), which integrates Weighted Reservoir Sampling into SWD to adaptively retain informative projection directions in optimization steps, resulting in stable gradients while remaining unbiased. Experiments on synthetic benchmarks and real-world tasks such as color correction and diffusion guidance show that ReSWD consistently outperforms standard SWD and other variance reduction baselines. Project page: https://reservoirswd.github.io/",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "244",
        "title": "Compose Your Policies! Improving Diffusion-based or Flow-based Robot Policies via Test-time Distribution-level Composition",
        "author": [
            "Jiahang Cao",
            "Yize Huang",
            "Hanzhong Guo",
            "Rui Zhang",
            "Mu Nan",
            "Weijian Mai",
            "Jiaxu Wang",
            "Hao Cheng",
            "Jingkai Sun",
            "Gang Han",
            "Wen Zhao",
            "Qiang Zhang",
            "Yijie Guo",
            "Qihao Zheng",
            "Chunfeng Song",
            "Xiao Li",
            "Ping Luo",
            "Andrew F. Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01068",
        "abstract": "Diffusion-based models for robotic control, including vision-language-action (VLA) and vision-action (VA) policies, have demonstrated significant capabilities. Yet their advancement is constrained by the high cost of acquiring large-scale interaction datasets. This work introduces an alternative paradigm for enhancing policy performance without additional model training. Perhaps surprisingly, we demonstrate that the composed policies can exceed the performance of either parent policy. Our contribution is threefold. First, we establish a theoretical foundation showing that the convex composition of distributional scores from multiple diffusion models can yield a superior one-step functional objective compared to any individual score. A GrÃ¶nwall-type bound is then used to show that this single-step improvement propagates through entire generation trajectories, leading to systemic performance gains. Second, motivated by these results, we propose General Policy Composition (GPC), a training-free method that enhances performance by combining the distributional scores of multiple pre-trained policies via a convex combination and test-time search. GPC is versatile, allowing for the plug-and-play composition of heterogeneous policies, including VA and VLA models, as well as those based on diffusion or flow-matching, irrespective of their input visual modalities. Third, we provide extensive empirical validation. Experiments on Robomimic, PushT, and RoboTwin benchmarks, alongside real-world robotic evaluations, confirm that GPC consistently improves performance and adaptability across a diverse set of tasks. Further analysis of alternative composition operators and weighting strategies offers insights into the mechanisms underlying the success of GPC. These results establish GPC as a simple yet effective method for improving control performance by leveraging existing policies.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "Robotics"
        ]
    },
    {
        "id": "245",
        "title": "Typed Chain-of-Thought: A Curry-Howard Framework for Verifying LLM Reasoning",
        "author": [
            "Elija Perrier"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01069",
        "abstract": "While Chain-of-Thought (CoT) prompting enhances the reasoning capabilities of large language models, the faithfulness of the generated rationales remains an open problem for model interpretability. We propose a novel theoretical lens for this problem grounded in the Curry-Howard correspondence, which posits a direct relationship between formal proofs and computer programs. Under this paradigm, a faithful reasoning trace is analogous to a well-typed program, where each intermediate step corresponds to a typed logical inference. We operationalise this analogy, presenting methods to extract and map the informal, natural language steps of CoT into a formal, typed proof structure. Successfully converting a CoT trace into a well-typed proof serves as a strong, verifiable certificate of its computational faithfulness, moving beyond heuristic interpretability towards formal verification. Our framework provides a methodology to transform plausible narrative explanations into formally verifiable programs, offering a path towards building more reliable and trustworthy AI systems.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "246",
        "title": "Eliciting Secret Knowledge from Language Models",
        "author": [
            "Bartosz CywiÅski",
            "Emil Ryd",
            "Rowan Wang",
            "Senthooran Rajamanoharan",
            "Neel Nanda",
            "Arthur Conmy",
            "Samuel Marks"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01070",
        "abstract": "We study secret elicitation: discovering knowledge that an AI possesses but does not explicitly verbalize. As a testbed, we train three families of large language models (LLMs) to possess specific knowledge that they apply downstream but deny knowing when asked directly. For example, in one setting, we train an LLM to generate replies that are consistent with knowing the user is female, while denying this knowledge when asked directly. We then design various black-box and white-box secret elicitation techniques and evaluate them based on whether they can help an LLM auditor successfully guess the secret knowledge. Many of our techniques improve on simple baselines. Our most effective techniques (performing best in 2/3 settings) are based on prefill attacks, a black-box technique where the LLM reveals secret knowledge when generating a completion from a predefined prefix. In our remaining setting, white-box techniques based on logit lens and sparse autoencoders (SAEs) are most effective. We release our models and code, establishing a public benchmark for evaluating secret elicitation methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "247",
        "title": "Research on the Integration of Embodied Intelligence and Reinforcement Learning in Textual Domains",
        "author": [
            "Haonan Wang",
            "Junfeng Sun",
            "Mingjia Zhao",
            "Wei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01076",
        "abstract": "This article addresses embodied intelligence and reinforcement learning integration in the field of text processing, aiming to enhance text handling with more intelligence on the basis of embodied intelligence's perception and action superiority and reinforcement learning's decision optimization capability. Through detailed theoretical explanation and experimental exploration, a novel integration model is introduced. This model has been demonstrated to be very effective in a wide range oftext processing tasks, validating its applicative potential",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "248",
        "title": "CodeGenLink: A Tool to Find the Likely Origin and License of Automatically Generated Code",
        "author": [
            "Daniele Bifolco",
            "Guido Annicchiarico",
            "Pierluigi Barbiero",
            "Massimiliano Di Penta",
            "Fiorella Zampetti"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01077",
        "abstract": "Large Language Models (LLMs) are widely used in software development tasks nowadays. Unlike reusing code taken from the Web, for LLMs' generated code, developers are concerned about its lack of trustworthiness and possible copyright or licensing violations, due to the lack of code provenance information. This paper proposes CodeGenLink, a GitHub CoPilot extension for Visual Studio Code aimed at (i) suggesting links containing code very similar to automatically generated code, and (ii) whenever possible, indicating the license of the likely origin of the code. CodeGenLink retrieves candidate links by combining LLMs with their web search features and then performs similarity analysis between the generated and retrieved code. Preliminary results show that CodeGenLink effectively filters unrelated links via similarity analysis and provides licensing information when available. Tool URL: https://github.com/danielebifolco/CodeGenLink Tool Video: https://youtu.be/M6nqjBf9_pw",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "249",
        "title": "Multi-Actor Multi-Critic Deep Deterministic Reinforcement Learning with a Novel Q-Ensemble Method",
        "author": [
            "Andy Wu",
            "Chun-Cheng Lin",
            "Rung-Tzuo Liaw",
            "Yuehua Huang",
            "Chihjung Kuo",
            "Chia Tong Weng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01083",
        "abstract": "Reinforcement learning has gathered much attention in recent years due to its rapid development and rich applications, especially on control systems and robotics. When tackling real-world applications with reinforcement learning method, the corresponded Markov decision process may have huge discrete or even continuous state/action space. Deep reinforcement learning has been studied for handling these issues through deep learning for years, and one promising branch is the actor-critic architecture. Many past studies leveraged multiple critics to enhance the accuracy of evaluation of a policy for addressing the overestimation and underestimation issues. However, few studies have considered the architecture with multiple actors together with multiple critics. This study proposes a novel multi-actor multi-critic (MAMC) deep deterministic reinforcement learning method. The proposed method has three main features, including selection of actors based on non-dominated sorting for exploration with respect to skill and creativity factors, evaluation for actors and critics using a quantile-based ensemble strategy, and exploiting actors with best skill factor. Theoretical analysis proves the learning stability and bounded estimation bias for the MAMC. The present study examines the performance on a well-known reinforcement learning benchmark MuJoCo. Experimental results show that the proposed framework outperforms state-of-the-art deep deterministic based reinforcement learning methods. Experimental analysis also indicates the proposed components are effective. Empirical analysis further investigates the validity of the proposed method, and shows its benefit on complicated problems. The source code can be found at https://github.com/AndyWu101/MAMC.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "250",
        "title": "Safety Instincts: LLMs Learn to Trust Their Internal Compass for Self-Defense",
        "author": [
            "Guobin Shen",
            "Dongcheng Zhao",
            "Haibo Tong",
            "Jindong Li",
            "Feifei Zhao",
            "Yi Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01088",
        "abstract": "Ensuring Large Language Model (LLM) safety remains challenging due to the absence of universal standards and reliable content validators, making it difficult to obtain effective training signals. We discover that aligned models already possess robust internal safety beliefs: they consistently produce high-confidence refusals to harmful requests while exhibiting high entropy when generating potentially dangerous content. This entropy gap reveals an untapped signal--models intrinsically \"know\" when to refuse. We introduce Safety Instincts Reinforcement Learning (SIRL), which transforms this internal confidence into a self-generated reward signal, eliminating dependence on external validators or human annotations. SIRL teaches models to trust their safety instincts by reinforcing low-entropy refusal behaviors. Evaluated on Llama and Qwen models, SIRL maintains 89%+ Defense Success Rates (DSRs) against 20+ jailbreak methods, from static prompts to adaptive attacks. Using only 15,000 unlabeled prompts, SIRL surpasses resource-intensive supervised methods while preserving performance on mathematics, coding, and conversation benchmarks. Our work demonstrates that effective alignment can emerge from within, paving the way for more autonomous and robust AI safety mechanisms that scale without extensive human oversight.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": "251",
        "title": "Geometric Properties of Neural Multivariate Regression",
        "author": [
            "George Andriopoulos",
            "Zixuan Dong",
            "Bimarsha Adhikari",
            "Keith Ross"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01105",
        "abstract": "Neural multivariate regression underpins a wide range of domains such as control, robotics, and finance, yet the geometry of its learned representations remains poorly characterized. While neural collapse has been shown to benefit generalization in classification, we find that analogous collapse in regression consistently degrades performance. To explain this contrast, we analyze models through the lens of intrinsic dimension. Across control tasks and synthetic datasets, we estimate the intrinsic dimension of last-layer features (ID_H) and compare it with that of the regression targets (ID_Y). Collapsed models exhibit ID_H < ID_Y, leading to over-compression and poor generalization, whereas non-collapsed models typically maintain ID_H > ID_Y. For the non-collapsed models, performance with respect to ID_H depends on the data quantity and noise levels. From these observations, we identify two regimes (over-compressed and under-compressed) that determine when expanding or reducing feature dimensionality improves performance. Our results provide new geometric insights into neural regression and suggest practical strategies for enhancing generalization.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "252",
        "title": "Augmenting LLMs for General Time Series Understanding and Prediction",
        "author": [
            "Felix Parker",
            "Nimeesha Chan",
            "Chi Zhang",
            "Kimia Ghobadi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01111",
        "abstract": "Time series data is fundamental to decision-making in many crucial domains including healthcare, finance, and environmental science. However, analyzing this data often requires incorporating unstructured contextual information, answering domain-specific questions, and generating natural language explanations -- capabilities that traditional time series models lack due to their inability to process text. While Large Language Models (LLMs) excel at contextual reasoning and knowledge integration, they struggle with numerical time series due to inefficient text-based representations and limited exposure to temporal data during pretraining. We address this gap by augmenting an LLM with specialized time series perception through a patch-based encoder-decoder architecture. We train this Time Series-augmented LLM (TsLLM) on a large corpus of over 2 million interleaved time series and text examples spanning diverse analysis tasks: forecasting with contextual information, time series question-answering, pattern explanation, classification with natural language outputs, and report generation. This training enables TsLLM to leverage both its language understanding and newly acquired temporal reasoning capabilities. While not designed to surpass specialized models on traditional benchmarks, TsLLM demonstrates strong performance on tasks requiring the integration of time series analysis with natural language -- capabilities that existing approaches cannot provide. Our work establishes a new paradigm for time series analysis that bridges numerical computation and natural language understanding, democratizing access to sophisticated temporal reasoning through natural language interaction.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "253",
        "title": "PRISM-Consult: A Panel-of-Experts Architecture for Clinician-Aligned Diagnosis",
        "author": [
            "Lionel Levine",
            "John Santerre",
            "Alexander S. Young",
            "T. Barry Levine",
            "Francis Campion",
            "Majid Sarrafzadeh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01114",
        "abstract": "We present PRISM-Consult, a clinician-aligned panel-of-experts architecture that extends the compact PRISM sequence model into a routed family of domain specialists. Episodes are tokenized as structured clinical events; a light-weight router reads the first few tokens and dispatches to specialist models (Cardiac-Vascular, Pulmonary, Gastro-Oesophageal, Musculoskeletal, Psychogenic). Each specialist inherits PRISM's small transformer backbone and token template, enabling parameter efficiency and interpretability. On real-world Emergency Department cohorts, specialists exhibit smooth convergence with low development perplexities across domains, while the router achieves high routing quality and large compute savings versus consult-all under a safety-first policy. We detail the data methodology (initial vs. conclusive ICD-9 families), routing thresholds and calibration, and report per-domain results to avoid dominance by common events. The framework provides a practical path to safe, auditable, and low-latency consult at scale, and we outline validation steps-external/temporal replication, asymmetric life-threat thresholds, and multi-label arbitration-to meet prospective clinical deployment standards.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "254",
        "title": "Exploring Network-Knowledge Graph Duality: A Case Study in Agentic Supply Chain Risk Analysis",
        "author": [
            "Evan Heus",
            "Rick Bookstaber",
            "Dhruv Sharma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01115",
        "abstract": "Large Language Models (LLMs) struggle with the complex, multi-modal, and network-native data underlying financial risk. Standard Retrieval-Augmented Generation (RAG) oversimplifies relationships, while specialist models are costly and static. We address this gap with an LLM-centric agent framework for supply chain risk analysis. Our core contribution is to exploit the inherent duality between networks and knowledge graphs (KG). We treat the supply chain network as a KG, allowing us to use structural network science principles for retrieval. A graph traverser, guided by network centrality scores, efficiently extracts the most economically salient risk paths. An agentic architecture orchestrates this graph retrieval alongside data from numerical factor tables and news streams. Crucially, it employs novel ``context shells'' -- descriptive templates that embed raw figures in natural language -- to make quantitative data fully intelligible to the LLM. This lightweight approach enables the model to generate concise, explainable, and context-rich risk narratives in real-time without costly fine-tuning or a dedicated graph database.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "255",
        "title": "Instant4D: 4D Gaussian Splatting in Minutes",
        "author": [
            "Zhanpeng Luo",
            "Haoxi Ran",
            "Li Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01119",
        "abstract": "Dynamic view synthesis has seen significant advances, yet reconstructing scenes from uncalibrated, casual video remains challenging due to slow optimization and complex parameter estimation. In this work, we present Instant4D, a monocular reconstruction system that leverages native 4D representation to efficiently process casual video sequences within minutes, without calibrated cameras or depth sensors. Our method begins with geometric recovery through deep visual SLAM, followed by grid pruning to optimize scene representation. Our design significantly reduces redundancy while maintaining geometric integrity, cutting model size to under 10% of its original footprint. To handle temporal dynamics efficiently, we introduce a streamlined 4D Gaussian representation, achieving a 30x speed-up and reducing training time to within two minutes, while maintaining competitive performance across several benchmarks. Our method reconstruct a single video within 10 minutes on the Dycheck dataset or for a typical 200-frame video. We further apply our model to in-the-wild videos, showcasing its generalizability. Our project website is published at https://instant4d.github.io/.",
        "tags": [
            "Gaussian Splatting",
            "SLAM"
        ]
    },
    {
        "id": "256",
        "title": "Rethinking Thinking Tokens: LLMs as Improvement Operators",
        "author": [
            "Lovish Madaan",
            "Aniket Didolkar",
            "Suchin Gururangan",
            "John Quan",
            "Ruan Silva",
            "Ruslan Salakhutdinov",
            "Manzil Zaheer",
            "Sanjeev Arora",
            "Anirudh Goyal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01123",
        "abstract": "Reasoning training incentivizes LLMs to produce long chains of thought (long CoT), which among other things, allows them to explore solution strategies with self-checking. This results in higher accuracy, but inflates context length, token/compute cost, and answer latency. We ask: Can current models leverage their metacognition to provide other combinations on this Pareto frontier, e.g., better accuracy with lower context length and/or latency? Abstractly, we view the model as an improvement operator on its own \"thoughts\" with a continuum of possible strategies. We identify an interesting inference family Parallel-Distill-Refine (PDR), which performs the following: (i) generate diverse drafts in parallel; (ii) distill them into a bounded, textual workspace; and (iii) refine conditioned on this workspace, producing an output that seeds the next round. Importantly, context length (hence compute cost) is controllable via degree of parallelism, and is no longer conflated with the total number of generated tokens. We report PDR instantiations of current models that give better accuracy than long CoT while incurring lower latency. Setting degree of parallelism to 1 yields an interesting subcase, Sequential Refinement (SR) (iteratively improve a single candidate answer) which provides performance superior to long CoT. Success of such model orchestrations raises the question whether further training could shift the Pareto frontier. To this end, we train an 8B thinking model with Reinforcement Learning (RL) to make it consistent with PDR as the inference method. On math tasks with verifiable answers, iterative pipelines surpass single-pass baselines at matched sequential budgets, with PDR delivering the largest gains (e.g., +11% on AIME 2024 and +9% on AIME 2025).",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "257",
        "title": "A Practitioner's Guide to Multi-turn Agentic Reinforcement Learning",
        "author": [
            "Ruiyi Wang",
            "Prithviraj Ammanabrolu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01132",
        "abstract": "We study what actually works and what doesn't for training large language models as agents via multi-turn reinforcement learning. Despite rapid progress, existing frameworks and definitions are fragmented, and there is no systematic formulation or analysis of which design choices matter across tasks. We address this gap by first breaking down the design space into three inter-related pillars -- environment, reward, and policy -- and empirically derive a recipe for training LLM agents in situated textual domains. In particular, we test TextWorld and ALFWorld, popular domains for testing situated embodied reasoning, as well as SWE-Gym for more software engineering style tasks. (i) For the environment, we analyze the impacts of task complexity in terms of sizes of the state and action spaces as well as optimal solution length, finding that even simple environments within a domain can provide signal on how well an agent can generalize to more complex tasks. (ii) For the reward, we ablate relative reward sparsity, observing that while dense turn-level rewards accelerate training, performance and stability is highly dependent on the choice of RL algorithm. (iii) And for the agent's policy, we explore the interplay between reward sparsity and biased (PPO, GRPO) and unbiased (RLOO) policy gradient methods in addition to showing how to find the optimal Supervised Fine-tuning (SFT) to RL training ratio given a fixed budget. We distill these findings into a training recipe that guides co-design across the three pillars, facilitating research and practical efforts in multi-turn agentic RL. Code: https://github.com/pearls-lab/meow-tea-taro",
        "tags": [
            "GRPO",
            "LLM",
            "PPO",
            "RL"
        ]
    },
    {
        "id": "258",
        "title": "Prompt Curriculum Learning for Efficient LLM Post-Training",
        "author": [
            "Zhaolin Gao",
            "Joongwon Kim",
            "Wen Sun",
            "Thorsten Joachims",
            "Sid Wang",
            "Richard Yuanzhe Pang",
            "Liang Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01135",
        "abstract": "We introduce Prompt Curriculum Learning (PCL), a lightweight reinforcement learning (RL) algorithm that selects intermediate-difficulty prompts using a learned value model to post-train language models. Since post-training LLMs via RL remains sensitive to batching and prompt selection strategies, we first conduct a series of systematic experiments where we (1) determine the optimal training batch size that balances generation efficiency and gradient quality and (2) establish the importance of focusing on prompts of intermediate difficulty for the policy. We build upon these results to design PCL, which identifies prompts of intermediate difficulty for the current policy in an on-policy manner by using a value model that is concurrently updated based on the current policy. By focusing on informative prompts that yield high effective ratios, PCL achieves either the highest performance or requires significantly less time to reach comparable performance to its counterparts. Compared to rollout-based filtering methods, PCL avoids costly rollouts and achieves $12.1\\times$ and $16.9\\times$ faster speed on identifying intermediate-difficulty prompts when training on MATH and DeepScaleR, respectively. We further demonstrate that our value model accurately predicts prompt difficulty and allows PCL to focus on progressively more challenging prompts during RL. Our results present a new methodology that delivers improved tradeoff between upper-bound performance and efficiency for reasoning-focused RL.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "259",
        "title": "Sample-Efficient Differentially Private Fine-Tuning via Gradient Matrix Denoising",
        "author": [
            "Ali Dadsetan",
            "Frank Rudzicz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01137",
        "abstract": "We address the challenge of sample efficiency in differentially private fine-tuning of large language models (LLMs) using DP-SGD. While DP-SGD provides strong privacy guarantees, the added noise significantly increases the entropy of gradient matrices, disrupting their low-rank structure and slowing optimization. We propose a post-processing algorithm that leverages random matrix theory to denoise gradients, restore low-rank structure, and improve alignment with the original signal. Applied to DP-SGD fine-tuning of RoBERTa on GLUE tasks, our method improves sample efficiency compared to state-of-the-art approaches, substantially reducing training time when optimal performance is not required. This work demonstrates that matrix recovery techniques can enhance the utility of private language model training without compromising privacy guarantees.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "260",
        "title": "Real-Time Trajectory Generation and Hybrid Lyapunov-Based Control for Hopping Robots",
        "author": [
            "Matthew Woodward"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01138",
        "abstract": "The advent of rotor-based hopping robots has created very capable hopping platforms with high agility and efficiency, and similar controllability, as compared to their purely flying quadrotor counterparts. Advances in robot performance have increased the hopping height to greater than 4 meters and opened up the possibility for more complex aerial trajectories (i.e., behaviors). However, currently hopping robots do not directly control their aerial trajectory or transition to flight, eliminating the efficiency benefits of a hopping system. Here we show a real-time, computationally efficiency, non-linear drag compensated, trajectory generation methodology and accompanying Lyapunov-based controller. The combined system can create and follow complex aerial trajectories from liftoff to touchdown on horizontal and vertical surfaces, while maintaining strick control over the orientation at touchdown. The computational efficiency provides broad applicability across all size scales of hopping robots while maintaining applicability to quadrotors in general.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "261",
        "title": "Apriel-1.5-15b-Thinker",
        "author": [
            "Shruthan Radhakrishna",
            "Aman Tiwari",
            "Aanjaneya Shukla",
            "Masoud Hashemi",
            "Rishabh Maheshwary",
            "Shiva Krishna Reddy Malay",
            "Jash Mehta",
            "Pulkit Pattnaik",
            "Saloni Mittal",
            "Khalil Slimi",
            "Kelechi Ogueji",
            "Akintunde Oladipo",
            "Soham Parikh",
            "Oluwanifemi Bamgbose",
            "Toby Liang",
            "Ahmed Masry",
            "Khyati Mahajan",
            "Sai Rajeswar Mudumba",
            "Vikas Yadav",
            "Sathwik Tejaswi Madhusudhan",
            "Torsten Scholak",
            "Sagar Davasam",
            "Srinivas Sunkara",
            "Nicholas Chapados"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01141",
        "abstract": "We present Apriel-1.5-15B-Thinker, a 15-billion parameter open-weights multimodal reasoning model that achieves frontier-level performance through training design rather than sheer scale. Starting from Pixtral-12B, we apply a progressive three-stage methodology: (1) depth upscaling to expand reasoning capacity without pretraining from scratch, (2) staged continual pre-training that first develops foundational text and vision understanding, then enhances visual reasoning through targeted synthetic data generation addressing spatial structure, compositional understanding, and fine-grained perception, and (3) high-quality text-only supervised fine-tuning on curated instruction-response pairs with explicit reasoning traces spanning mathematics, coding, science, and tool use. Notably, our model achieves competitive results without reinforcement learning or preference optimization, isolating the contribution of our data-centric continual pre-training approach. On the Artificial Analysis Intelligence Index, Apriel-1.5-15B-Thinker attains a score of 52, matching DeepSeek-R1-0528 despite requiring significantly fewer computational resources. Across ten image benchmarks, its performance is on average within five points of Gemini-2.5-Flash and Claude Sonnet-3.7, a key achievement for a model operating within single-GPU deployment constraints. Our results demonstrate that thoughtful mid-training 2 design can close substantial capability gaps without massive scale, making frontier-level multimodal reasoning accessible to organizations with limited infrastructure. We release the model checkpoint, all training recipes, and evaluation protocols under the MIT license to to advance open-source research.",
        "tags": [
            "DeepSeek",
            "RL"
        ]
    },
    {
        "id": "262",
        "title": "Generalized Parallel Scaling with Interdependent Generations",
        "author": [
            "Harry Dong",
            "David Brandfonbrener",
            "Eryk Helenowski",
            "Yun He",
            "Mrinal Kumar",
            "Han Fang",
            "Yuejie Chi",
            "Karthik Abinav Sankararaman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01143",
        "abstract": "Parallel LLM inference scaling involves sampling a set of $N>1$ responses for a single input prompt. However, these $N$ parallel responses tend to be generated independently from each other, partitioning compute resources and leaving potentially useful information in one generation untapped by others. This is in contrast to response length scaling where past computation is used in all future steps. For higher quality responses and response sets, we propose Bridge to generate interdependent responses in parallel by rethinking batched LLM hidden states as holistic tensors rather than independent slices. With only a small amount (2.8%-5.1%) of new parameters, Bridge improves the relative mean accuracy gains from reinforcement learning with verifiable rewards by up to 50% and boosts consistency of correct responses. Trained once, Bridge scales to any generation width, all with greater performance than independent generations, unlocking a more general mode of parallel scaling that effectively leverages information between sequences, compatible with any post-generation aggregation technique.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "263",
        "title": "mR3: Multilingual Rubric-Agnostic Reward Reasoning Models",
        "author": [
            "David Anugraha",
            "Shou-Yi Hung",
            "Zilu Tang",
            "Annie En-Shiun Lee",
            "Derry Tanti Wijaya",
            "Genta Indra Winata"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01146",
        "abstract": "Evaluation using Large Language Model (LLM) judges has been widely adopted in English and shown to be effective for automatic evaluation. However, their performance does not generalize well to non-English settings, and it remains unclear what constitutes effective multilingual training for such judges. In this paper, we introduce mR3, a massively multilingual, rubric-agnostic reward reasoning model trained on 72 languages, achieving the broadest language coverage in reward modeling to date. We present a comprehensive study of data and curriculum selection for training to identify effective strategies and data sources for building high-quality reward models, including the integration of target-language reasoning datasets. Our approach attains state-of-the-art performance on multilingual reward model benchmarks, surpassing much larger models (i.e., GPT-OSS-120B) while being up to 9x smaller, and its effectiveness is further confirmed through extensive ablation studies. Our models, data, and code are available as open source at https://github.com/rubricreward/mr3.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "264",
        "title": "ModernVBERT: Towards Smaller Visual Document Retrievers",
        "author": [
            "Paul Teiletche",
            "Quentin MacÃ©",
            "Max Conti",
            "Antonio Loison",
            "Gautier Viaud",
            "Pierre Colombo",
            "Manuel Faysse"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01149",
        "abstract": "Multimodal embedding models are gaining prevalence, notably for document retrieval as efficient alternatives to text-only pipelines. These models are typically built by finetuning large vision-language decoders (VLMs) with contrastive losses on text-image pairs. In this work, we show that, while cost-efficient, this repurposing approach often bottlenecks retrieval performance. Through controlled experiments, we establish a principled recipe for improving visual document retrieval models. We notably measure the impact of attention masking, image resolution, modality alignment data regimes, and late interaction centered contrastive objectives which emerge as central performance factors. Building on these insights, we release ModernVBERT, a compact 250M-parameter vision-language encoder that outperforms models up to 10 times larger when finetuned on document retrieval tasks. Models and code are made available at https://huggingface.co/ModernVBERT.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "265",
        "title": "Pay-Per-Search Models are Abstention Models",
        "author": [
            "Mustafa Omer Gul",
            "Claire Cardie",
            "Tanya Goyal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01152",
        "abstract": "LLMs cannot reliably recognize their parametric knowledge boundaries and often hallucinate answers to outside-of-boundary questions. In contrast, humans recognize their limitations and can either seek external help for such questions or abstain. In this paper, we introduce MASH (Modeling Abstention via Selective Help-seeking), a training framework that readily extracts abstentions from LLMs. Our key idea is that any external help-seeking by an LLM, i.e. search tool use, can serve as a proxy for abstention if the external help (search) is appropriately penalized while simultaneously rewarding answer accuracy. MASH operationalizes this idea using reinforcement learning with a pay-per-search reward.\nWe run experiments on three knowledge-intensive QA datasets. Our results show that MASH substantially improves upon the selective help-seeking performance of prior efficient search approaches; on multi-hop datasets, MASH improves answer accuracy by 7.6%. Furthermore, MASH demonstrates strong off-the-shelf abstention -- it can distinguish between unanswerable/answerable questions and selectively generate responses for answerable questions -- showcasing behavior analogous to specialized abstention approaches. We emphasize that contrary to prior abstention methods, MASH does not require pre-determining knowledge boundaries to construct training data. Instead, MASH's abstentions are a by-product of training for the auxiliary selective help-seeking task. Overall, we show that MASH training effectively aligns search tool use with parametric knowledge, which can be successfully leveraged for making abstention decisions.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "266",
        "title": "Prosperity before Collapse: How Far Can Off-Policy RL Reach with Stale Data on LLMs?",
        "author": [
            "Haizhong Zheng",
            "Jiawei Zhao",
            "Bedi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01161",
        "abstract": "Reinforcement learning has been central to recent advances in large language model reasoning, but most algorithms rely on on-policy training that demands fresh rollouts at every update, limiting efficiency and scalability. Asynchronous RL systems alleviate this by decoupling rollout generation from training, yet their effectiveness hinges on tolerating large staleness in rollout data, a setting where existing methods either degrade in performance or collapse. We revisit this challenge and uncover a prosperity-before-collapse phenomenon: stale data can be as informative as on-policy data if exploited properly. Building on this insight, we introduce M2PO (Second-Moment Trust Policy Optimization), which constrains the second moment of importance weights to suppress only extreme outliers while preserving informative updates. Notably, M2PO sharply reduces the fraction of clipped tokens under high staleness (from 1.22% to 0.06% over training), precisely masking high-variance tokens while maintaining stable optimization. Extensive evaluation across six models (from 1.7B to 32B) and eight benchmarks shows that M2PO delivers stable off-policy training even with data stale by at least 256 model updates and matches on-policy performance.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "267",
        "title": "How Does the Pretraining Distribution Shape In-Context Learning? Task Selection, Generalization, and Robustness",
        "author": [
            "WaÃ¯ss Azizian",
            "Ali Hasan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01163",
        "abstract": "The emergence of in-context learning (ICL) in large language models (LLMs) remains poorly understood despite its consistent effectiveness, enabling models to adapt to new tasks from only a handful of examples. To clarify and improve these capabilities, we characterize how the statistical properties of the pretraining distribution (e.g., tail behavior, coverage) shape ICL on numerical tasks. We develop a theoretical framework that unifies task selection and generalization, extending and sharpening earlier results, and show how distributional properties govern sample efficiency, task retrieval, and robustness. To this end, we generalize Bayesian posterior consistency and concentration results to heavy-tailed priors and dependent sequences, better reflecting the structure of LLM pretraining data. We then empirically study how ICL performance varies with the pretraining distribution on challenging tasks such as stochastic differential equations and stochastic processes with memory. Together, these findings suggest that controlling key statistical properties of the pretraining distribution is essential for building ICL-capable and reliable LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "268",
        "title": "Social Welfare Function Leaderboard: When LLM Agents Allocate Social Welfare",
        "author": [
            "Zhengliang Shi",
            "Ruotian Ma",
            "Jen-tse Huang",
            "Xinbei Ma",
            "Xingyu Chen",
            "Mengru Wang",
            "Qu Yang",
            "Yue Wang",
            "Fanghua Ye",
            "Ziyang Chen",
            "Shanyi Wang",
            "Cixing Li",
            "Wenxuan Wang",
            "Zhaopeng Tu",
            "Xiaolong Li",
            "Zhaochun Ren",
            "Linus"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01164",
        "abstract": "Large language models (LLMs) are increasingly entrusted with high-stakes decisions that affect human welfare. However, the principles and values that guide these models when distributing scarce societal resources remain largely unexamined. To address this, we introduce the Social Welfare Function (SWF) Benchmark, a dynamic simulation environment where an LLM acts as a sovereign allocator, distributing tasks to a heterogeneous community of recipients. The benchmark is designed to create a persistent trade-off between maximizing collective efficiency (measured by Return on Investment) and ensuring distributive fairness (measured by the Gini coefficient). We evaluate 20 state-of-the-art LLMs and present the first leaderboard for social welfare allocation. Our findings reveal three key insights: (i) A model's general conversational ability, as measured by popular leaderboards, is a poor predictor of its allocation skill. (ii) Most LLMs exhibit a strong default utilitarian orientation, prioritizing group productivity at the expense of severe inequality. (iii) Allocation strategies are highly vulnerable, easily perturbed by output-length constraints and social-influence framing. These results highlight the risks of deploying current LLMs as societal decision-makers and underscore the need for specialized benchmarks and targeted alignment for AI governance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "269",
        "title": "GRAD: Generative Retrieval-Aligned Demonstration Sampler for Efficient Few-Shot Reasoning",
        "author": [
            "Oussama Gabouj",
            "Kamel Charaf",
            "Ivan Zakazov",
            "Nicolas Baldwin",
            "Robert West"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01165",
        "abstract": "Large Language Models (LLMs) achieve strong performance across diverse tasks, but their effectiveness often depends on the quality of the provided context. Retrieval-Augmented Generation (RAG) enriches prompts with external information, but its reliance on static databases constrains adaptability and can result in irrelevant demonstrations. In this work, we propose a Generative Retrieval-Aligned Demonstrator (GRAD), a dynamic demonstration-based approach where an LLM model is trained to generate input-specific concise demonstrations. By tailoring demonstrations to each input, our method offers better contextual support than traditional RAG approaches. We demonstrate the superiority of GRAD under budget constraints, where we limit both the number of tokens used per demonstration and the number of tokens used for the final output. Trained solely on a math dataset, GRAD consistently outperforms strong baselines on Qwen2.5-14B across mathematical reasoning and advanced STEM questions, highlighting GRAD's robust generalization to out-of-distribution (OOD) domains such as physics, chemistry, and computer science. Furthermore, we show that demonstrations generated by trained smaller models can effectively guide larger target models, reducing training costs while maintaining competitive accuracy. Overall, this work introduces a scalable demonstration generator model presenting the first step toward a dynamic few-shot learning paradigm in resource-constrained settings. We release the code used for the project.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "270",
        "title": "Simultaneous Multi-objective Alignment Across Verifiable and Non-verifiable Rewards",
        "author": [
            "Yiran Shen",
            "Yu Xia",
            "Jonathan Chang",
            "Prithviraj Ammanabrolu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01167",
        "abstract": "Aligning large language models to human preferences is inherently multidimensional, yet most pipelines collapse heterogeneous signals into a single optimizeable objective. We seek to answer what it would take to simultaneously align a model across various domains spanning those with: verifiable rewards (mathematical accuracy), non-verifiable subjective preferences (human values), and complex interactive scenarios (multi-turn AI tutoring dialogues). Such multi-objective reinforcement learning setups are often plagued by the individual objectives being at odds with each other, resulting in inefficient training and little user control during inference. We propose a unified framework that: (i) standardizes {process reward model} (PRM) training across both verifiable and non-verifiable settings to better supervise models' chain-of-thought reasoning; (ii) performs {multi-objective alignment} by training the LLM with our $\\textbf{M}$ulti-$\\textbf{A}$ction-$\\textbf{H}$ead $\\textbf{DPO}$ (MAH-DPO) and a vectorized reward where the dimensions of the vector correspond to the various objectives instead of a single scalar; and (iii) demonstrates how such a system provides fine-grained inference-time user control. Experiments across math reasoning, value alignment, and multi-turn dialogue show that our framework improves performance across multiple objectives simultaneously, while minimizing cross-objective trade-offs and enabling flexible inference time user control. The code can be found at https://github.com/pearls-lab/multiobj-align.",
        "tags": [
            "CoT",
            "DPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "271",
        "title": "Verbalized Sampling: How to Mitigate Mode Collapse and Unlock LLM Diversity",
        "author": [
            "Jiayi Zhang",
            "Simon Yu",
            "Derek Chong",
            "Anthony Sicilia",
            "Michael R. Tomz",
            "Christopher D. Manning",
            "Weiyan Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01171",
        "abstract": "Post-training alignment often reduces LLM diversity, leading to a phenomenon known as mode collapse. Unlike prior work that attributes this effect to algorithmic limitations, we identify a fundamental, pervasive data-level driver: typicality bias in preference data, whereby annotators systematically favor familiar text as a result of well-established findings in cognitive psychology. We formalize this bias theoretically, verify it on preference datasets empirically, and show that it plays a central role in mode collapse. Motivated by this analysis, we introduce Verbalized Sampling, a simple, training-free prompting strategy to circumvent mode collapse. VS prompts the model to verbalize a probability distribution over a set of responses (e.g., ``Generate 5 jokes about coffee and their corresponding probabilities''). Comprehensive experiments show that VS significantly improves performance across creative writing (poems, stories, jokes), dialogue simulation, open-ended QA, and synthetic data generation, without sacrificing factual accuracy and safety. For instance, in creative writing, VS increases diversity by 1.6-2.1x over direct prompting. We further observe an emergent trend that more capable models benefit more from VS. In sum, our work provides a new data-centric perspective on mode collapse and a practical inference-time remedy that helps unlock pre-trained generative diversity.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "272",
        "title": "Energy-Regularized Sequential Model Editing on Hyperspheres",
        "author": [
            "Qingyuan Liu",
            "Jia-Chen Gu",
            "Yunzhi Yao",
            "Hong Wang",
            "Nanyun Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01172",
        "abstract": "Large language models (LLMs) require constant updates to remain aligned with evolving real-world knowledge. Model editing offers a lightweight alternative to retraining, but sequential editing often destabilizes representations and induces catastrophic forgetting. In this work, we seek to better understand and mitigate performance degradation caused by sequential editing. We hypothesize that hyperspherical uniformity, a property that maintains uniform distribution of neuron weights on a hypersphere, helps the model remain stable, retain prior knowledge, while still accommodate new updates. We use Hyperspherical Energy (HE) to quantify neuron uniformity during editing, and examine its correlation with editing performance. Empirical studies across widely used editing methods reveals a strong correlation between HE dynamics and editing performance, with editing failures consistently coinciding with high HE fluctuations. We further theoretically prove that HE dynamics impose a lower bound on the degradation of pretrained knowledge, highlighting why HE stability is crucial for knowledge retention. Motivated by these insights, we propose SPHERE (Sparse Projection for Hyperspherical Energy-Regularized Editing), an HE-driven regularization strategy that stabilizes neuron weight distributions, ultimately preserving prior knowledge while enabling reliable sequential updates. Specifically, SPHERE identifies a sparse space complementary to the principal hyperspherical directions of the pretrained weight matrices and projects new knowledge onto it, attenuating perturbations on the principal directions. Extensive experiments on LLaMA3 (8B) and Qwen2.5 (7B) show that SPHERE outperforms the best baseline in editing capability by an average of 16.41%, while most faithfully preserving general model performance, thereby offering a principled path toward reliable large-scale knowledge editing.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "273",
        "title": "EditTrack: Detecting and Attributing AI-assisted Image Editing",
        "author": [
            "Zhengyuan Jiang",
            "Yuyang Zhang",
            "Moyang Guo",
            "Neil Zhenqiang Gong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01173",
        "abstract": "In this work, we formulate and study the problem of image-editing detection and attribution: given a base image and a suspicious image, detection seeks to determine whether the suspicious image was derived from the base image using an AI editing model, while attribution further identifies the specific editing model responsible. Existing methods for detecting and attributing AI-generated images are insufficient for this problem, as they focus on determining whether an image was AI-generated/edited rather than whether it was edited from a particular base image. To bridge this gap, we propose EditTrack, the first framework for this image-editing detection and attribution problem. Building on four key observations about the editing process, EditTrack introduces a novel re-editing strategy and leverages carefully designed similarity metrics to determine whether a suspicious image originates from a base image and, if so, by which model. We evaluate EditTrack on five state-of-the-art editing models across six datasets, demonstrating that it consistently achieves accurate detection and attribution, significantly outperforming five baselines.",
        "tags": [
            "Detection",
            "Image Editing"
        ]
    },
    {
        "id": "274",
        "title": "Code2Video: A Code-centric Paradigm for Educational Video Generation",
        "author": [
            "Yanzhe Chen",
            "Kevin Qinghong Lin",
            "Mike Zheng Shou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01174",
        "abstract": "While recent generative models advance pixel-space video synthesis, they remain limited in producing professional educational videos, which demand disciplinary knowledge, precise visual structures, and coherent transitions, limiting their applicability in educational scenarios. Intuitively, such requirements are better addressed through the manipulation of a renderable environment, which can be explicitly controlled via logical commands (e.g., code). In this work, we propose Code2Video, a code-centric agent framework for generating educational videos via executable Python code. The framework comprises three collaborative agents: (i) Planner, which structures lecture content into temporally coherent flows and prepares corresponding visual assets; (ii) Coder, which converts structured instructions into executable Python codes while incorporating scope-guided auto-fix to enhance efficiency; and (iii) Critic, which leverages vision-language models (VLM) with visual anchor prompts to refine spatial layout and ensure clarity. To support systematic evaluation, we build MMMC, a benchmark of professionally produced, discipline-specific educational videos. We evaluate MMMC across diverse dimensions, including VLM-as-a-Judge aesthetic scores, code efficiency, and particularly, TeachQuiz, a novel end-to-end metric that quantifies how well a VLM, after unlearning, can recover knowledge by watching the generated videos. Our results demonstrate the potential of Code2Video as a scalable, interpretable, and controllable approach, achieving 40% improvement over direct code generation and producing videos comparable to human-crafted tutorials. The code and datasets are available at https://github.com/showlab/Code2Video.",
        "tags": [
            "VLM",
            "Video Generation"
        ]
    },
    {
        "id": "275",
        "title": "Audio Driven Real-Time Facial Animation for Social Telepresence",
        "author": [
            "Jiye Lee",
            "Chenghui Li",
            "Linh Tran",
            "Shih-En Wei",
            "Jason Saragih",
            "Alexander Richard",
            "Hanbyul Joo",
            "Shaojie Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01176",
        "abstract": "We present an audio-driven real-time system for animating photorealistic 3D facial avatars with minimal latency, designed for social interactions in virtual reality for anyone. Central to our approach is an encoder model that transforms audio signals into latent facial expression sequences in real time, which are then decoded as photorealistic 3D facial avatars. Leveraging the generative capabilities of diffusion models, we capture the rich spectrum of facial expressions necessary for natural communication while achieving real-time performance (<15ms GPU time). Our novel architecture minimizes latency through two key innovations: an online transformer that eliminates dependency on future inputs and a distillation pipeline that accelerates iterative denoising into a single step. We further address critical design challenges in live scenarios for processing continuous audio signals frame-by-frame while maintaining consistent animation quality. The versatility of our framework extends to multimodal applications, including semantic modalities such as emotion conditions and multimodal sensors with head-mounted eye cameras on VR headsets. Experimental results demonstrate significant improvements in facial animation accuracy over existing offline state-of-the-art baselines, achieving 100 to 1000 times faster inference speed. We validate our approach through live VR demonstrations and across various scenarios such as multilingual speeches.",
        "tags": [
            "3D",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "276",
        "title": "COM-BOM: Bayesian Exemplar Search for Efficiently Exploring the Accuracy-Calibration Pareto Frontier",
        "author": [
            "Gaoxiang Luo",
            "Aryan Deshwal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01178",
        "abstract": "Selecting an optimal set of exemplars is critical for good performance of in-context learning. However, prior exemplar search methods narrowly optimize for predictive accuracy, critically neglecting model calibration--a key determinant of trustworthiness and safe deployment. In this paper, we formulate exemplar selection as a multi-objective optimization problem, explicitly targeting both the maximization of predictive accuracy and the minimization of expected calibration error. We solve this problem with a sample-efficient Combinatorial Bayesian Optimization algorithm (COM-BOM) to find the Pareto front that optimally trades off the two objectives of accuracy and calibration. We evaluate COM-BOM on multiple tasks from unsaturated MMLU-Pro benchmark and find that COM-BOM beats or matches the baselines at jointly optimizing the two objectives, while requiring a minimal number of LLM API calls.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "277",
        "title": "TOUCAN: Synthesizing 1.5M Tool-Agentic Data from Real-World MCP Environments",
        "author": [
            "Zhangchen Xu",
            "Adriana Meza Soria",
            "Shawn Tan",
            "Anurag Roy",
            "Ashish Sunil Agrawal",
            "Radha Poovendran",
            "Rameswar Panda"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01179",
        "abstract": "Large Language Model (LLM) agents are rapidly emerging as powerful systems for automating tasks across domains. Yet progress in the open-source community is constrained by the lack of high quality permissively licensed tool-agentic training data. Existing datasets are often limited in diversity, realism, and complexity, particularly regarding multi-tool and multi-turn interactions. To address this gap, we introduce Toucan, the largest publicly available tool-agentic dataset to date, containing 1.5 million trajectories synthesized from nearly 500 real-world Model Context Protocols (MCPs). Unlike prior work, Toucan leverages authentic MCP environments to generate diverse, realistic, and challenging tasks with trajectories involving real tool execution. Our pipeline first produces a broad spectrum of tool-use queries using five distinct models, applies model-based quality filtering, and then generates agentic trajectories with three teacher models using two agentic frameworks. Rigorous rule-based and model-based validation ensures high-quality outputs. We also introduce three extension mechanisms to further diversify tasks and simulate multi-turn conversations. Models fine-tuned on Toucan outperform larger closed-source counterparts on the BFCL V3 benchmark and push the Pareto frontier forward on MCP-Universe Bench.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "278",
        "title": "BroRL: Scaling Reinforcement Learning via Broadened Exploration",
        "author": [
            "Jian Hu",
            "Mingjie Liu",
            "Ximing Lu",
            "Fang Wu",
            "Zaid Harchaoui",
            "Shizhe Diao",
            "Yejin Choi",
            "Pavlo Molchanov",
            "Jun Yang",
            "Jan Kautz",
            "Yi Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01180",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key ingredient for unlocking complex reasoning capabilities in large language models. Recent work ProRL has shown promise in scaling RL by increasing the number of training steps. However, performance plateaus after thousands of steps, with clear diminishing returns from allocating more computation to additional training. In this work, we investigate a complementary paradigm for scaling RL, BroR-Lincreasing the number of rollouts per example to hundreds to exhaustively Broaden exploration, which yields continuous performance gains beyond the saturation point observed in ProRL when scaling the number of training steps. Our approach is motivated by a mass balance equation analysis allowing us to characterize the rate of change in probability mass for correct and incorrect tokens during the reinforcement process. We show that under a one-step RL assumption, sampled rollout tokens always contribute to correct-mass expansion, while unsampled tokens outside rollouts may lead to gains or losses depending on their distribution and the net reward balance. Importantly, as the number of rollouts per example N increases, the effect of unsampled terms diminishes, ensuring overall correct-mass expansion. To validate our theoretical analysis, we conduct simulations under more relaxed conditions and find that a sufficiently large rollout size N-corresponding to ample exploration-guarantees an increase in the probability mass of all correct tokens. Empirically, BroRL revives models saturated after 3K ProRL training steps and demonstrates robust, continuous improvement, achieving state-of-the-art results for the 1.5B model across diverse benchmarks.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "279",
        "title": "EvoWorld: Evolving Panoramic World Generation with Explicit 3D Memory",
        "author": [
            "Jiahao Wang",
            "Luoxin Ye",
            "TaiMing Lu",
            "Junfei Xiao",
            "Jiahan Zhang",
            "Yuxiang Guo",
            "Xijun Liu",
            "Rama Chellappa",
            "Cheng Peng",
            "Alan Yuille",
            "Jieneng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01183",
        "abstract": "Humans possess a remarkable ability to mentally explore and replay 3D environments they have previously experienced. Inspired by this mental process, we present EvoWorld: a world model that bridges panoramic video generation with evolving 3D memory to enable spatially consistent long-horizon exploration. Given a single panoramic image as input, EvoWorld first generates future video frames by leveraging a video generator with fine-grained view control, then evolves the scene's 3D reconstruction using a feedforward plug-and-play transformer, and finally synthesizes futures by conditioning on geometric reprojections from this evolving explicit 3D memory. Unlike prior state-of-the-arts that synthesize videos only, our key insight lies in exploiting this evolving 3D reconstruction as explicit spatial guidance for the video generation process, projecting the reconstructed geometry onto target viewpoints to provide rich spatial cues that significantly enhance both visual realism and geometric consistency. To evaluate long-range exploration capabilities, we introduce the first comprehensive benchmark spanning synthetic outdoor environments, Habitat indoor scenes, and challenging real-world scenarios, with particular emphasis on loop-closure detection and spatial coherence over extended trajectories. Extensive experiments demonstrate that our evolving 3D memory substantially improves visual fidelity and maintains spatial scene coherence compared to existing approaches, representing a significant advance toward long-horizon spatially consistent world modeling.",
        "tags": [
            "3D",
            "Detection",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "280",
        "title": "Dirichlet-Prior Shaping: Guiding Expert Specialization in Upcycled MoEs",
        "author": [
            "Leyla Mirvakhabova",
            "Babak Ehteshami Bejnordi",
            "Gaurav Kumar",
            "Hanxue Liang",
            "Wanru Zhao",
            "Paul Whatmough"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01185",
        "abstract": "Upcycling pre-trained dense models into sparse Mixture-of-Experts (MoEs) efficiently increases model capacity but often suffers from poor expert specialization due to naive weight replication. Our analysis reveals that upcycled MoEs, even with conventional regularization, exhibit low-confidence, weakly differentiated routing, hindering performance. We introduce Dirichlet-Prior Shaping Loss (DPSL), a novel router regularization technique that directly shapes routing probability distributions by matching expert assignments to a target Dirichlet prior. DPSL offers fine-grained control over expert balance and specialization, and enables encoding of inductive biases such as encouraging experts to focus on specific modalities or tasks, without requiring manual intervention; notably, DPSL is a general tool applicable to any module that outputs categorical probability distributions, extending its utility beyond MoE training. Experiments on upcycled MoE vision-language models (with Qwen2, Phi3, Llama3.2 LLM backbones) show DPSL consistently outperforms upcycling strategies and regularization techniques across standard vision-language benchmarks, addressing the critical issue of poor specialization and fostering more adaptive, higher-performing models.",
        "tags": [
            "LLM",
            "MoE",
            "VLM"
        ]
    },
    {
        "id": "281",
        "title": "IMAGEdit: Let Any Subject Transform",
        "author": [
            "Fei Shen",
            "Weihao Xu",
            "Rui Yan",
            "Dong Zhang",
            "Xiangbo Shu",
            "Jinhui Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01186",
        "abstract": "In this paper, we present IMAGEdit, a training-free framework for any number of video subject editing that manipulates the appearances of multiple designated subjects while preserving non-target regions, without finetuning or retraining. We achieve this by providing robust multimodal conditioning and precise mask sequences through a prompt-guided multimodal alignment module and a prior-based mask retargeting module. We first leverage large models' understanding and generation capabilities to produce multimodal information and mask motion sequences for multiple subjects across various types. Then, the obtained prior mask sequences are fed into a pretrained mask-driven video generation model to synthesize the edited video. With strong generalization capability, IMAGEdit remedies insufficient prompt-side multimodal conditioning and overcomes mask boundary entanglement in videos with any number of subjects, thereby significantly expanding the applicability of video editing. More importantly, IMAGEdit is compatible with any mask-driven video generation model, significantly improving overall performance. Extensive experiments on our newly constructed multi-subject benchmark MSVBench verify that IMAGEdit consistently surpasses state-of-the-art methods. Code, models, and datasets are publicly available at https://github.com/XWH-A/IMAGEdit.",
        "tags": [
            "Video Editing",
            "Video Generation"
        ]
    },
    {
        "id": "282",
        "title": "WaveMind: Towards a Conversational EEG Foundation Model Aligned to Textual and Visual Modalities",
        "author": [
            "Ziyi Zeng",
            "Zhenyang Cai",
            "Yixi Cai",
            "Xidong Wang",
            "Junying Chen",
            "Rongsheng Wang",
            "Yipeng Liu",
            "Siqi Cai",
            "Benyou Wang",
            "Zhiguo Zhang",
            "Haizhou Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00032",
        "abstract": "Electroencephalography (EEG) interpretation using multimodal large language models (MLLMs) offers a novel approach for analyzing brain signals. However, the complex nature of brain activity introduces critical challenges: EEG signals simultaneously encode both cognitive processes and intrinsic neural states, creating a mismatch in EEG paired-data modality that hinders effective cross-modal representation learning. Through a pivot investigation, we uncover complementary relationships between these modalities. Leveraging this insight, we propose mapping EEG signals and their corresponding modalities into a unified semantic space to achieve generalized interpretation. To fully enable conversational capabilities, we further introduce WaveMind-Instruct-338k, the first cross-task EEG dataset for instruction tuning. The resulting model demonstrates robust classification accuracy while supporting flexible, open-ended conversations across four downstream tasks, thereby offering valuable insights for both neuroscience research and the development of general-purpose EEG models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "283",
        "title": "AI-Based Stroke Rehabilitation Domiciliary Assessment System with ST_GCN Attention",
        "author": [
            "Suhyeon Lim",
            "Ye-eun Kim",
            "Andrew J. Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00049",
        "abstract": "Effective stroke recovery requires continuous rehabilitation integrated with daily living. To support this need, we propose a home-based rehabilitation exercise and feedback system. The system consists of (1) hardware setup with RGB-D camera and wearable sensors to capture Stroke movements, (2) a mobile application for exercise guidance, and (3) an AI server for assessment and feedback. When Stroke user exercises following the application guidance, the system records skeleton sequences, which are then Assessed by the deep learning model, RAST-G@. The model employs a spatio-temporal graph convolutional network (ST-GCN) to extract skeletal features and integrates transformer-based temporal attention to figure out action quality. For system implementation, we constructed the NRC dataset, include 10 upper-limb activities of daily living (ADL) and 5 range-of-motion (ROM) collected from stroke and non-disabled participants, with Score annotations provided by licensed physiotherapists. Results on the KIMORE and NRC datasets show that RAST-G@ improves over baseline in terms of MAD, RMSE, and MAPE. Furthermore, the system provides user feedback that combines patient-centered assessment and monitoring. The results demonstrate that the proposed system offers a scalable approach for quantitative and consistent domiciliary rehabilitation assessment.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "284",
        "title": "Variable Rate Image Compression via N-Gram Context based Swin-transformer",
        "author": [
            "Priyanka Mudgal",
            "Feng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00058",
        "abstract": "This paper presents an N-gram context-based Swin Transformer for learned image compression. Our method achieves variable-rate compression with a single model. By incorporating N-gram context into the Swin Transformer, we overcome its limitation of neglecting larger regions during high-resolution image reconstruction due to its restricted receptive field. This enhancement expands the regions considered for pixel restoration, thereby improving the quality of high-resolution reconstructions. Our method increases context awareness across neighboring windows, leading to a -5.86\\% improvement in BD-Rate over existing variable-rate learned image compression techniques. Additionally, our model improves the quality of regions of interest (ROI) in images, making it particularly beneficial for object-focused applications in fields such as manufacturing and industrial vision systems.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "285",
        "title": "AstroMMBench: A Benchmark for Evaluating Multimodal Large Language Models Capabilities in Astronomy",
        "author": [
            "Jinghang Shi",
            "Xiao Yu Tang",
            "Yang Hunag",
            "Yuyang Li",
            "Xiaokong",
            "Yanxia Zhang",
            "Caizhan Yue"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00063",
        "abstract": "Astronomical image interpretation presents a significant challenge for applying multimodal large language models (MLLMs) to specialized scientific tasks. Existing benchmarks focus on general multimodal capabilities but fail to capture the complexity of astronomical data. To bridge this gap, we introduce AstroMMBench, the first comprehensive benchmark designed to evaluate MLLMs in astronomical image understanding. AstroMMBench comprises 621 multiple-choice questions across six astrophysical subfields, curated and reviewed by 15 domain experts for quality and relevance. We conducted an extensive evaluation of 25 diverse MLLMs, including 22 open-source and 3 closed-source models, using AstroMMBench. The results show that Ovis2-34B achieved the highest overall accuracy (70.5%), demonstrating leading capabilities even compared to strong closed-source models. Performance showed variations across the six astrophysical subfields, proving particularly challenging in domains like cosmology and high-energy astrophysics, while models performed relatively better in others, such as instrumentation and solar astrophysics. These findings underscore the vital role of domain-specific benchmarks like AstroMMBench in critically evaluating MLLM performance and guiding their targeted development for scientific applications. AstroMMBench provides a foundational resource and a dynamic tool to catalyze advancements at the intersection of AI and astronomy.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "286",
        "title": "DiffAU: Diffusion-Based Ambisonics Upscaling",
        "author": [
            "Amit Milstein",
            "Nir Shlezinger",
            "Boaz Rafaely"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00180",
        "abstract": "Spatial audio enhances immersion by reproducing 3D sound fields, with Ambisonics offering a scalable format for this purpose. While first-order Ambisonics (FOA) notably facilitates hardware-efficient acquisition and storage of sound fields as compared to high-order Ambisonics (HOA), its low spatial resolution limits realism, highlighting the need for Ambisonics upscaling (AU) as an approach for increasing the order of Ambisonics signals. In this work we propose DiffAU, a cascaded AU method that leverages recent developments in diffusion models combined with novel adaptation to spatial audio to generate 3rd order Ambisonics from FOA. By learning data distributions, DiffAU provides a principled approach that rapidly and reliably reproduces HOA in various settings. Experiments in anechoic conditions with multiple speakers, show strong objective and perceptual performance.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "287",
        "title": "Asynchronous Nonlinear Sheaf Diffusion for Multi-Agent Coordination",
        "author": [
            "Yichen Zhao",
            "Tyler Hanks",
            "Hans Riess",
            "Samuel Cohen",
            "Matthew Hale",
            "James Fairbanks"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00270",
        "abstract": "Cellular sheaves and sheaf Laplacians provide a far-reaching generalization of graphs and graph Laplacians, resulting in a wide array of applications ranging from machine learning to multi-agent control. In the context of multi-agent systems, so called coordination sheaves provide a unifying formalism that models heterogeneous agents and coordination goals over undirected communication topologies, and applying sheaf diffusion drives agents to achieve their coordination goals. Existing literature on sheaf diffusion assumes that agents can communicate and compute updates synchronously, which is an unrealistic assumption in many scenarios where communication delays or heterogeneous agents with different compute capabilities cause disagreement among agents. To address these challenges, we introduce asynchronous nonlinear sheaf diffusion. Specifically, we show that under mild assumptions on the coordination sheaf and bounded delays in communication and computation, nonlinear sheaf diffusion converges to a minimizer of the Dirichlet energy of the coordination sheaf at a linear rate proportional to the delay bound. We further show that this linear convergence is attained from arbitrary initial conditions and the analysis depends on the spectrum of the sheaf Laplacian in a manner that generalizes the standard graph Laplacian case. We provide several numerical simulations to validate our theoretical results.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "288",
        "title": "Data driven approaches in nanophotonics: A review of AI-enabled metadevices",
        "author": [
            "Huanshu Zhang",
            "Lei Kang",
            "Sawyer D. Campbell",
            "Jacob T. Young",
            "Douglas H. Werner"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00283",
        "abstract": "Data-driven approaches have revolutionized the design and optimization of photonic metadevices by harnessing advanced artificial intelligence methodologies. This review takes a model-centric perspective that synthesizes emerging design strategies and delineates how traditional trial-and-error and computationally intensive electromagnetic simulations are being supplanted by deep learning frameworks that efficiently navigate expansive design spaces. We discuss artificial intelligence implementation in several metamaterial design aspects from high-degree-of-freedom design to large language model-assisted design. By addressing challenges such as transformer model implementation, fabrication limitations, and intricate mutual coupling effects, these AI-enabled strategies not only streamline the forward modeling process but also offer robust pathways for the realization of multifunctional and fabrication-friendly nanophotonic devices. This review further highlights emerging opportunities and persistent challenges, setting the stage for next-generation strategies in nanophotonic engineering.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "289",
        "title": "Malliavin Calculus with Weak Derivatives for Counterfactual Stochastic Optimization",
        "author": [
            "Vikram Krishnamurthy",
            "Luke Snow"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00297",
        "abstract": "We study counterfactual stochastic optimization of conditional loss functionals under misspecified and noisy gradient information. The difficulty is that when the conditioning event has vanishing or zero probability, naive Monte Carlo estimators are prohibitively inefficient; kernel smoothing, though common, suffers from slow convergence. We propose a two-stage kernel-free methodology. First, we show using Malliavin calculus that the conditional loss functional of a diffusion process admits an exact representation as a Skorohod integral, yielding variance comparable to classical Monte-Carlo variance. Second, we establish that a weak derivative estimate of the conditional loss functional with respect to model parameters can be evaluated with constant variance, in contrast to the widely used score function method whose variance grows linearly in the sample path length. Together, these results yield an efficient framework for counterfactual conditional stochastic gradient algorithms in rare-event regimes.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "290",
        "title": "Post-Training Quantization for Audio Diffusion Transformers",
        "author": [
            "Tanmay Khandelwal",
            "Magdalena Fuentes"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00313",
        "abstract": "Diffusion Transformers (DiTs) enable high-quality audio synthesis but are often computationally intensive and require substantial storage, which limits their practical deployment. In this paper, we present a comprehensive evaluation of post-training quantization (PTQ) techniques for audio DiTs, analyzing the trade-offs between static and dynamic quantization schemes. We explore two practical extensions (1) a denoising-timestep-aware smoothing method that adapts quantization scales per-input-channel and timestep to mitigate activation outliers, and (2) a lightweight low-rank adapter (LoRA)-based branch derived from singular value decomposition (SVD) to compensate for residual weight errors. Using Stable Audio Open we benchmark W8A8 and W4A8 configurations across objective metrics and human perceptual ratings. Our results show that dynamic quantization preserves fidelity even at lower precision, while static methods remain competitive with lower latency. Overall, our findings show that low-precision DiTs can retain high-fidelity generation while reducing memory usage by up to 79%.",
        "tags": [
            "Diffusion",
            "LoRA"
        ]
    },
    {
        "id": "291",
        "title": "CINDES: Classification induced neural density estimator and simulator",
        "author": [
            "Dehao Dai",
            "Jianqing Fan",
            "Yihong Gu",
            "Debarghya Mukherjee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00367",
        "abstract": "Neural network-based methods for (un)conditional density estimation have recently gained substantial attention, as various neural density estimators have outperformed classical approaches in real-data experiments. Despite these empirical successes, implementation can be challenging due to the need to ensure non-negativity and unit-mass constraints, and theoretical understanding remains limited. In particular, it is unclear whether such estimators can adaptively achieve faster convergence rates when the underlying density exhibits a low-dimensional structure. This paper addresses these gaps by proposing a structure-agnostic neural density estimator that is (i) straightforward to implement and (ii) provably adaptive, attaining faster rates when the true density admits a low-dimensional composition structure. Another key contribution of our work is to show that the proposed estimator integrates naturally into generative sampling pipelines, most notably score-based diffusion models, where it achieves provably faster convergence when the underlying density is structured. We validate its performance through extensive simulations and a real-data application.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "292",
        "title": "A Deep Learning Pipeline for Epilepsy Genomic Analysis Using GPT-2 XL and NVIDIA H100",
        "author": [
            "Muhammad Omer Latif",
            "Hayat Ullah",
            "Muhammad Ali Shafique",
            "Zhihua Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00392",
        "abstract": "Epilepsy is a chronic neurological condition characterized by recurrent seizures, with global prevalence estimated at 50 million people worldwide. While progress in high-throughput sequencing has allowed for broad-based transcriptomic profiling of brain tissues, the deciphering of these highly complex datasets remains one of the challenges. To address this issue, in this paper we propose a new analysis pipeline that integrates the power of deep learning strategies with GPU-acceleration computation for investigating Gene expression patterns in epilepsy. Specifically, our proposed approach employs GPT-2 XL, a transformer-based Large Language Model (LLM) with 1.5 billion parameters for genomic sequence analysis over the latest NVIDIA H100 Tensor Core GPUs based on Hopper architecture. Our proposed method enables efficient preprocessing of RNA sequence data, gene sequence encoding, and subsequent pattern identification. We conducted experiments on two epilepsy datasets including GEO accession GSE264537 and GSE275235. The obtained results reveal several significant transcriptomic modifications, including reduced hippocampal astrogliosis after ketogenic diet treatment as well as restored excitatory-inhibitory signaling equilibrium in zebrafish epilepsy model. Moreover, our results highlight the effectiveness of leveraging LLMs in combination with advanced hardware acceleration for transcriptomic characterization in neurological diseases.",
        "tags": [
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "293",
        "title": "Throttling for metric dimension and its variants",
        "author": [
            "Boris Brimkov",
            "Peter Diao",
            "Jesse Geneson",
            "Carolyn Reinhart",
            "Shen-Fu Tsai",
            "William Wang",
            "Kyle Worley"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00530",
        "abstract": "Metric dimension is a graph parameter that has been applied to robot navigation and finding low-dimensional vector embeddings. Throttling entails minimizing the sum of two available resources when solving certain graph problems. In this paper, we introduce throttling for metric dimension, edge metric dimension, and mixed metric dimension. In the context of vector embeddings, metric dimension throttling finds a low-dimensional, low-magnitude embedding with integer coordinates. We show that computing the throttling number is NP-hard for all three variants. We give formulas for the throttling numbers of special families of graphs, and characterize graphs with extremal throttling numbers. We also prove that the minimum possible throttling number of a graph of order $n$ is $\\Theta\\left(\\frac{\\log{n}}{\\log{\\log{n}}}\\right)$, while the minimum possible throttling number of a tree of order $n$ is $\\Theta(n^{1/3})$ or $\\Theta(n^{1/2})$ depending on the variant of metric dimension.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "294",
        "title": "UniverSR: Unified and Versatile Audio Super-Resolution via Vocoder-Free Flow Matching",
        "author": [
            "Woongjib Choi",
            "Sangmin Lee",
            "Hyungseob Lim",
            "Hong-Goo Kang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00771",
        "abstract": "In this paper, we present a vocoder-free framework for audio super-resolution that employs a flow matching generative model to capture the conditional distribution of complex-valued spectral coefficients. Unlike conventional two-stage diffusion-based approaches that predict a mel-spectrogram and then rely on a pre-trained neural vocoder to synthesize waveforms, our method directly reconstructs waveforms via the inverse Short-Time Fourier Transform (iSTFT), thereby eliminating the dependence on a separate vocoder. This design not only simplifies end-to-end optimization but also overcomes a critical bottleneck of two-stage pipelines, where the final audio quality is fundamentally constrained by vocoder performance. Experiments show that our model consistently produces high-fidelity 48 kHz audio across diverse upsampling factors, achieving state-of-the-art performance on both speech and general audio datasets.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "Super Resolution"
        ]
    },
    {
        "id": "295",
        "title": "Digital Twins: McKean-Pontryagin Control for Partially Observed Physical Twins",
        "author": [
            "Manfred Opper",
            "Sebastian Reich"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00937",
        "abstract": "Optimal control for fully observed diffusion processes is well established and has led to numerous numerical implementations based on, for example, Bellman's principle, model free reinforcement learning, Pontryagin's maximum principle, and model predictive control. On the contrary, much fewer algorithms are available for optimal control of partially observed processes. However, this scenario is central to the digital twin paradigm where a physical twin is partially observed and control laws are derived based on a digital twin. In this paper, we contribute to this challenge by combining data assimilation in the form of the ensemble Kalman filter with the recently proposed McKean-Pontryagin approach to stochastic optimal control. We derive forward evolving mean-field evolution equations for states and co-states which simultaneously allow for an online assimilation of data as well as an online computation of control laws. The proposed methodology is therefore perfectly suited for real time applications of digital twins. We present numerical results for a controlled Lorenz-63 system and an inverted pendulum.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "296",
        "title": "Spiralformer: Low Latency Encoder for Streaming Speech Recognition with Circular Layer Skipping and Early Exiting",
        "author": [
            "Emiru Tsunoo",
            "Hayato Futami",
            "Yosuke Kashiwagi",
            "Siddhant Arora",
            "Shinji Watanabe"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00982",
        "abstract": "For streaming speech recognition, a Transformer-based encoder has been widely used with block processing. Although many studies addressed improving emission latency of transducers, little work has been explored for improving encoding latency of the block processing. We seek to reduce latency by frequently emitting a chunk with a small shift rather than scarce large-chunk emissions, resulting in higher computational costs. To efficiently compute with the small chunk shift, we propose a new encoder, Spiralformer, tailored for block processing by combining layer dropping and early exiting. We skip layer computation in a cyclic manner and shift the computed layer in each block spirally, which completes computation for all the layers over the block processing. Experimentally, we observed that our method achieved 21.6% reduction in the averaged token emission delay in Librispeech, and 7.0% in CSJ, compared with the baseline with similar computational cost and word error rates.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "297",
        "title": "The exterior derivative and the mean value equality in $\\mathbb{R}^n$",
        "author": [
            "Daniel Fadel",
            "Henrique N. SÃ¡ Earp",
            "TomÃ¡s S. R. Silva"
        ],
        "pdf": "https://arxiv.org/pdf/2510.00999",
        "abstract": "This survey revisits classical results in vector calculus and analysis by exploring a generalised perspective on the exterior derivative, interpreting it as a measure of \"infinitesimal flux\". This viewpoint leads to a higher-dimensional analogue of the Mean Value Theorem, valid for differential $k$-forms, and provides a natural formulation of Stokes' theorem that mirrors the exact hypotheses of the Fundamental Theorem of Calculus - without requiring full $C^1$ smoothness of the differential form.\nAs a numerical application, we propose an algorithm for exterior differentiation in $\\mathbb{R}^n$ that relies solely on black-box access to the differential form, offering a practical tool for computation without the need for mesh discretization or explicit symbolic expressions.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "298",
        "title": "Theory of Scaling Laws for In-Context Regression: Depth, Width, Context and Time",
        "author": [
            "Blake Bordelon",
            "Mary I. Letey",
            "Cengiz Pehlevan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01098",
        "abstract": "We study in-context learning (ICL) of linear regression in a deep linear self-attention model, characterizing how performance depends on various computational and statistical resources (width, depth, number of training steps, batch size and data per context). In a joint limit where data dimension, context length, and residual stream width scale proportionally, we analyze the limiting asymptotics for three ICL settings: (1) isotropic covariates and tasks (ISO), (2) fixed and structured covariance (FS), and (3) where covariances are randomly rotated and structured (RRS). For ISO and FS settings, we find that depth only aids ICL performance if context length is limited. Alternatively, in the RRS setting where covariances change across contexts, increasing the depth leads to significant improvements in ICL, even at infinite context length. This provides a new solvable toy model of neural scaling laws which depends on both width and depth of a transformer and predicts an optimal transformer shape as a function of compute. This toy model enables computation of exact asymptotics for the risk as well as derivation of powerlaws under source/capacity conditions for the ICL tasks.",
        "tags": [
            "Transformer"
        ]
    }
]