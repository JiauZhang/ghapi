[
    {
        "id": "1",
        "title": "An Anthropologist LLM to Elicit Users' Moral Preferences through Role-Play",
        "author": [
            "Gianluca De Ninno",
            "Paola Inverardi",
            "Francesca Belotti"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01189",
        "abstract": "This study investigates a novel approach to eliciting users' moral decision-making by combining immersive roleplaying games with LLM analysis capabilities. Building on the distinction introduced by Floridi between hard ethics inspiring and shaping laws-and soft ethics-moral preferences guiding individual behavior within the free space of decisions compliant to laws-we focus on capturing the latter through contextrich, narrative-driven interactions. Grounded in anthropological methods, the role-playing game exposes participants to ethically charged scenarios in the domain of digital privacy. Data collected during the sessions were interpreted by a customized LLM (\"GPT Anthropologist\"). Evaluation through a cross-validation process shows that both the richness of the data and the interpretive framing significantly enhance the model's ability to predict user behavior. Results show that LLMs can be effectively employed to automate and enhance the understanding of user moral preferences and decision-making process in the early stages of software development.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "2",
        "title": "Better Than \"Better Than Nothing\": Design Strategies for Enculturated Empathetic AI Robot Companions for Older Adults",
        "author": [
            "Isabel Pedersen",
            "Andrea Slane"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01192",
        "abstract": "The paper asserts that emulating empathy in human-robot interaction is a key component to achieve satisfying social, trustworthy, and ethical robot interaction with older people. Following comments from older adult study participants, the paper identifies a gap. Despite the acceptance of robot care scenarios, participants expressed the poor quality of the social aspect. Current human-robot designs, to a certain extent, neglect to include empathy as a theorized design pathway. Using rhetorical theory, this paper defines the socio-cultural expectations for convincing empathetic relationships. It analyzes and then summarizes how society understands, values, and negotiates empathic interaction between human companions in discursive exchanges, wherein empathy acts as a societal value system. Using two public research collections on robots, with one geared specifically to gerontechnology for older people, it substantiates the lack of attention to empathy in public materials produced by robot companies. This paper contends that using an empathetic care vocabulary as a design pathway is a productive underlying foundation for designing humanoid social robots that aim to support older people's goals of aging-in-place. It argues that the integration of affective AI into the sociotechnical assemblages of human-socially assistive robot interaction ought to be scrutinized to ensure it is based on genuine cultural values involving empathetic qualities.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "3",
        "title": "How can AI agents support journalists' work? An experiment with designing an LLM-driven intelligent reporting system",
        "author": [
            "Vasileios Maltezos",
            "Roman Kyrychenko",
            "Aleksi Knuutila"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01193",
        "abstract": "The integration of artificial intelligence into journalistic practices represents a transformative shift in how news is gathered, analyzed, and disseminated. Large language models (LLMs), particularly those with agentic capabilities, offer unprecedented opportunities for enhancing journalistic workflows while simultaneously presenting complex challenges for newsroom integration. This research explores how agentic LLMs can support journalists' workflows, based on insights from journalist interviews and from the development of an LLM-based automation tool performing information filtering, summarization, and reporting. The paper details automated aggregation and summarization systems for journalists, presents a technical overview and evaluation of a user-centric LLM-driven reporting system (TeleFlash), and discusses both addressed and unmet journalist needs, with an outlook on future directions for AI-driven tools in journalism.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "4",
        "title": "Are LLMs ready to help non-expert users to make charts of official statistics data?",
        "author": [
            "Gadir Suleymanli",
            "Alexander Rogiers",
            "Lucas Lageweg",
            "Jefrey Lijffijt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01197",
        "abstract": "In this time when biased information, deep fakes, and propaganda proliferate, the accessibility of reliable data sources is more important than ever. National statistical institutes provide curated data that contain quantitative information on a wide range of topics. However, that information is typically spread across many tables and the plain numbers may be arduous to process. Hence, this open data may be practically inaccessible. We ask the question \"Are current Generative AI models capable of facilitating the identification of the right data and the fully-automatic creation of charts to provide information in visual form, corresponding to user queries?\". We present a structured evaluation of recent large language models' (LLMs) capabilities to generate charts from complex data in response to user queries. Working with diverse public data from Statistics Netherlands, we assessed multiple LLMs on their ability to identify relevant data tables, perform necessary manipulations, and generate appropriate visualizations autonomously. We propose a new evaluation framework spanning three dimensions: data retrieval & pre-processing, code quality, and visual representation. Results indicate that locating and processing the correct data represents the most significant challenge. Additionally, LLMs rarely implement visualization best practices without explicit guidance. When supplemented with information about effective chart design, models showed marked improvement in representation scores. Furthermore, an agentic approach with iterative self-evaluation led to excellent performance across all evaluation dimensions. These findings suggest that LLMs' effectiveness for automated chart generation can be enhanced through appropriate scaffolding and feedback mechanisms, and that systems can already reach the necessary accuracy across the three evaluation dimensions.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "5",
        "title": "Control the Temperature: Selective Sampling for Diverse and High-Quality LLM Outputs",
        "author": [
            "Sergey Troshin",
            "Wafaa Mohammed",
            "Yan Meng",
            "Christof Monz",
            "Antske Fokkens",
            "Vlad Niculae"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01218",
        "abstract": "Diversity is an essential metric for evaluating the creativity of outputs generated by language models. Temperature-based sampling is a common strategy to increase diversity. However, for tasks that require high precision, e.g., mathematical reasoning, uncontrolled high temperature sampling, e.g., min-$p$ or top-$p$, degrades reasoning quality. We demonstrate that the loss of accuracy is caused by sampling incorrect continuations in sensitive decoding positions. To address this, in this paper, we propose \\textbf{selective sampling}, a method that dynamically switches between greedy and high-temperature sampling based on a sampling risk metric. This risk metric estimates the likelihood of output errors when applying high-temperature sampling on the current token position. To predict sampling risk, we train a lightweight classifier on a small subset of verifiable problems. The trained classifier can be integrated with the base language model with minimal latency overhead. Experiments on mathematical reasoning tasks demonstrate that selective sampling enhances the quality-diversity trade-off, even in high-temperature settings.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "Uncovering Implicit Bias in Large Language Models with Concept Learning Dataset",
        "author": [
            "Leroy Z. Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01219",
        "abstract": "We introduce a dataset of concept learning tasks that helps uncover implicit biases in large language models. Using in-context concept learning experiments, we found that language models may have a bias toward upward monotonicity in quantifiers; such bias is less apparent when the model is tested by direct prompting without concept learning components. This demonstrates that in-context concept learning can be an effective way to discover hidden biases in language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "7",
        "title": "Towards Open-Ended Discovery for Low-Resource NLP",
        "author": [
            "Bonaventure F. P. Dossou",
            "Henri AÃ¯dasso"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01220",
        "abstract": "Natural Language Processing (NLP) for low-resource languages remains fundamentally constrained by the lack of textual corpora, standardized orthographies, and scalable annotation pipelines. While recent advances in large language models have improved cross-lingual transfer, they remain inaccessible to underrepresented communities due to their reliance on massive, pre-collected data and centralized infrastructure. In this position paper, we argue for a paradigm shift toward open-ended, interactive language discovery, where AI systems learn new languages dynamically through dialogue rather than static datasets. We contend that the future of language technology, particularly for low-resource and under-documented languages, must move beyond static data collection pipelines toward interactive, uncertainty-driven discovery, where learning emerges dynamically from human-machine collaboration instead of being limited to pre-existing datasets. We propose a framework grounded in joint human-machine uncertainty, combining epistemic uncertainty from the model with hesitation cues and confidence signals from human speakers to guide interaction, query selection, and memory retention. This paper is a call to action: we advocate a rethinking of how AI engages with human knowledge in under-documented languages, moving from extractive data collection toward participatory, co-adaptive learning processes that respect and empower communities while discovering and preserving the world's linguistic diversity. This vision aligns with principles of human-centered AI, emphasizing interactive, cooperative model building between AI systems and speakers.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "8",
        "title": "Discourse vs emissions: Analysis of corporate narratives, symbolic practices, and mimicry through LLMs",
        "author": [
            "Bertrand Kian Hassani",
            "Yacoub Bahini",
            "Rizwan Mushtaq"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01222",
        "abstract": "Climate change has increased demands for transparent and comparable corporate climate disclosures, yet imitation and symbolic reporting often undermine their value. This paper develops a multidimensional framework to assess disclosure maturity among 828 http://U.S.listed firms using large language models (LLMs) fine-tuned for climate communication. Four classifiers-sentiment, commitment, specificity, and target ambition-extract narrative indicators from sustainability and annual reports, which are linked to firm attributes such as emissions, market capitalization, and sector. Analyses reveal three insights: (1) risk-focused narratives often align with explicit commitments, but quantitative targets (e.g., net-zero pledges) remain decoupled from tone; (2) larger and higher-emitting firms disclose more commitments and actions than peers, though inconsistently with quantitative targets; and (3) widespread similarity in disclosure styles suggests mimetic behavior, reducing differentiation and decision usefulness. These results highlight the value of LLMs for ESG narrative analysis and the need for stronger regulation to connect commitments with verifiable transition strategies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "9",
        "title": "Jailbreaking LLMs via Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge",
        "author": [
            "Hui Dou",
            "Ning Xu",
            "Yiwen Zhang",
            "Kaibin Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01223",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks. However, they remain exposed to jailbreak attacks, eliciting harmful responses. The nested scenario strategy has been increasingly adopted across various methods, demonstrating immense potential. Nevertheless, these methods are easily detectable due to their prominent malicious intentions. In this work, we are the first to find and systematically verify that LLMs' alignment defenses are not sensitive to nested scenarios, where these scenarios are highly semantically relevant to the queries and incorporate targeted toxic knowledge. This is a crucial yet insufficiently explored direction. Based on this, we propose RTS-Attack (Semantically Relevant Nested Scenarios with Targeted Toxic Knowledge), an adaptive and automated framework to examine LLMs' alignment. By building scenarios highly relevant to the queries and integrating targeted toxic knowledge, RTS-Attack bypasses the alignment defenses of LLMs. Moreover, the jailbreak prompts generated by RTS-Attack are free from harmful queries, leading to outstanding concealment. Extensive experiments demonstrate that RTS-Attack exhibits superior performance in both efficiency and universality compared to the baselines across diverse advanced LLMs, including GPT-4o, Llama3-70b, and Gemini-pro. Our complete code is available in the supplementary material. WARNING: THIS PAPER CONTAINS POTENTIALLY HARMFUL CONTENT.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "10",
        "title": "Context Matters: Comparison of commercial large language tools in veterinary medicine",
        "author": [
            "Tyler J Poore",
            "Christopher J Pinard",
            "Aleena Shabbir",
            "Andrew Lagree",
            "Andre Telfer",
            "Kuan-Chuen Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01224",
        "abstract": "Large language models (LLMs) are increasingly used in clinical settings, yet their performance in veterinary medicine remains underexplored. We evaluated three commercially available veterinary-focused LLM summarization tools (Product 1 [Hachiko] and Products 2 and 3) on a standardized dataset of veterinary oncology records. Using a rubric-guided LLM-as-a-judge framework, summaries were scored across five domains: Factual Accuracy, Completeness, Chronological Order, Clinical Relevance, and Organization. Product 1 achieved the highest overall performance, with a median average score of 4.61 (IQR: 0.73), compared to 2.55 (IQR: 0.78) for Product 2 and 2.45 (IQR: 0.92) for Product 3. It also received perfect median scores in Factual Accuracy and Chronological Order. To assess the internal consistency of the grading framework itself, we repeated the evaluation across three independent runs. The LLM grader demonstrated high reproducibility, with Average Score standard deviations of 0.015 (Product 1), 0.088 (Product 2), and 0.034 (Product 3). These findings highlight the importance of veterinary-specific commercial LLM tools and demonstrate that LLM-as-a-judge evaluation is a scalable and reproducible method for assessing clinical NLP summarization in veterinary medicine.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "11",
        "title": "Utilizing Modern Large Language Models (LLM) for Financial Trend Analysis and Digest Creation",
        "author": [
            "Andrei Lazarev",
            "Dmitrii Sedov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01225",
        "abstract": "The exponential growth of information presents a significant challenge for researchers and professionals seeking to remain at the forefront of their fields and this paper introduces an innovative framework for automatically generating insightful financial digests using the power of Large Language Models (LLMs), specifically Google's Gemini Pro. By leveraging a combination of data extraction from OpenAlex, strategic prompt engineering, and LLM-driven analysis, we demonstrate the automated example of creating a comprehensive digests that generalize key findings, identify emerging trends. This approach addresses the limitations of traditional analysis methods, enabling the efficient processing of vast amounts of unstructured data and the delivery of actionable insights in an easily digestible format. This paper describes how LLMs work in simple words and how we can use their power to help researchers and scholars save their time and stay informed about current trends. Our study includes step-by-step process, from data acquisition and JSON construction to interaction with Gemini and the automated generation of PDF reports, including a link to the project's GitHub repository for broader accessibility and further development.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "12",
        "title": "ClaimCheck: Real-Time Fact-Checking with Small Language Models",
        "author": [
            "Akshith Reddy Putta",
            "Jacob Devasier",
            "Chengkai Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01226",
        "abstract": "We introduce ClaimCheck, an LLM-guided automatic fact-checking system designed to verify real-world claims using live Web evidence and small language models. Unlike prior systems that rely on large, closed-source models and static knowledge stores, ClaimCheck employs a transparent, stepwise verification pipeline that mirrors human fact-checking workflows consisting of Web search query planning, Web-based evidence retrieval and summarization, evidence synthesis and re-retrieval, and claim verdict evaluation. Each module is optimized for small LLMs, allowing the system to deliver accurate and interpretable fact-checking with significantly lower computational requirements. Despite using a much smaller Qwen3-4B model, ClaimCheck achieves state-of-the-art accuracy of 76.4% on the AVeriTeC dataset, outperforming previous approaches using LLaMA3.1 70B and GPT-4o. Extensive ablations demonstrate that careful modular design and prompting strategies can overcome the limitations of smaller LLMs. To promote accessibility and transparency, we provide a public demo at https://idir.uta.edu/claimcheck.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "13",
        "title": "EEFSUVA: A New Mathematical Olympiad Benchmark",
        "author": [
            "Nicole N Khatibi",
            "Daniil A. Radamovich",
            "Michael P. Brenner"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01227",
        "abstract": "Recent breakthroughs have spurred claims that large language models (LLMs) match gold medal Olympiad to graduate level proficiency on mathematics benchmarks. In this work, we examine these claims in detail and assess the extent to which current benchmarks capture genuine LLM mathematical reasoning. The composition of these benchmarks, primarily drawing from the International Mathematics Olympiad (IMO) and related competitions, may overstate models reasoning ability due to potential data contamination and a narrow focus on familiar problem types. To enable a more holistic assessment of mathematical understanding, we introduce EEFSUVA, a novel benchmark curated from under circulated regional and national Olympiads of Eastern Europe and the countries from the former Soviet Union. These contests feature problems of comparable difficulty to the IMO and are renowned for demanding nonstandard problem-solving techniques, yet their problems are far less prevalent in online corpora. Preliminary results suggest that even state-of-the-art LLMs exhibit a notable performance decline on EEFSUVA relative to other Olympiad-style benchmarks. These findings also suggest the potential importance of broader evaluation datasets for a fuller assessment of mathematical reasoning and for guiding future model development.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "14",
        "title": "Who is In Charge? Dissecting Role Conflicts in Instruction Following",
        "author": [
            "Siqi Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01228",
        "abstract": "Large language models should follow hierarchical instructions where system prompts override user inputs, yet recent work shows they often ignore this rule while strongly obeying social cues such as authority or consensus. We extend these behavioral findings with mechanistic interpretations on a large-scale dataset. Linear probing shows conflict-decision signals are encoded early, with system-user and social conflicts forming distinct subspaces. Direct Logit Attribution reveals stronger internal conflict detection in system-user cases but consistent resolution only for social cues. Steering experiments show that, despite using social cues, the vectors surprisingly amplify instruction following in a role-agnostic way. Together, these results explain fragile system obedience and underscore the need for lightweight hierarchy-sensitive alignment methods.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "15",
        "title": "Enhancing Transformer-Based Rerankers with Synthetic Data and LLM-Based Supervision",
        "author": [
            "Dimitar Peshevski",
            "Kiril Blazhevski",
            "Martin Popovski",
            "Gjorgji Madjarov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01229",
        "abstract": "Effective document reranking is essential for improving search relevance across diverse applications. While Large Language Models (LLMs) excel at reranking due to their deep semantic understanding and reasoning, their high computational cost makes them impractical for many real-world deployments. Fine-tuning smaller, task-specific models is a more efficient alternative but typically depends on scarce, manually labeled data. To overcome this, we propose a novel pipeline that eliminates the need for human-labeled query-document pairs. Our method uses LLMs to generate synthetic queries from domain-specific corpora and employs an LLM-based classifier to label positive and hard-negative pairs. This synthetic dataset is then used to fine-tune a smaller transformer model with contrastive learning using Localized Contrastive Estimation (LCE) loss. Experiments on the MedQuAD dataset show that our approach significantly boosts in-domain performance and generalizes well to out-of-domain tasks. By using LLMs for data generation and supervision rather than inference, we reduce computational costs while maintaining strong reranking capabilities.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "16",
        "title": "Trustworthy Summarization via Uncertainty Quantification and Risk Awareness in Large Language Models",
        "author": [
            "Shuaidong Pan",
            "Di Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01231",
        "abstract": "This study addresses the reliability of automatic summarization in high-risk scenarios and proposes a large language model framework that integrates uncertainty quantification and risk-aware mechanisms. Starting from the demands of information overload and high-risk decision-making, a conditional generation-based summarization model is constructed, and Bayesian inference is introduced during generation to model uncertainty in the parameter space, which helps avoid overconfident predictions. The uncertainty level of the generated content is measured using predictive distribution entropy, and a joint optimization of entropy regularization and risk-aware loss is applied to ensure that key information is preserved and risk attributes are explicitly expressed during information compression. On this basis, the model incorporates risk scoring and regulation modules, allowing summaries to cover the core content accurately while enhancing trustworthiness through explicit risk-level prompts. Comparative experiments and sensitivity analyses verify that the proposed method significantly improves the robustness and reliability of summarization in high-risk applications while maintaining fluency and semantic integrity. This research provides a systematic solution for trustworthy summarization and demonstrates both scalability and practical value at the methodological level.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "17",
        "title": "Benchmark Profiling: Mechanistic Diagnosis of LLM Benchmarks",
        "author": [
            "Dongjun Kim",
            "Gyuho Shim",
            "Yongchan Chun",
            "Minhyuk Kim",
            "Chanjun Park",
            "Heuiseok Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01232",
        "abstract": "Large Language Models are commonly judged by their scores on standard benchmarks, yet such scores often overstate real capability since they mask the mix of skills a task actually demands. For example, ARC is assumed to test reasoning, while HellaSwag is designed to evaluate commonsense. However, we lack a systematic way to verify if these benchmarks actually measure these labels. We introduce Benchmark Profiling, a diagnostic framework that decomposes benchmark performance into ten cognitively grounded abilities. The method combines gradient-based importance scoring with targeted parameter ablation to compute an Ability Impact Score (AIS) that quantifies how much each ability contributes to a model's success on a given benchmark. Profiling three instruction-tuned models across ten widely used benchmarks yields four key findings: (i) most benchmarks draw on several abilities rather than one, (ii) datasets with similar labels rely on distinct ability mixtures, (iii) code-generation benchmarks reward broad, multi-skill improvement and thus show only modest gains from narrow domain-specific fine-tuning, and (iv) abilities irrelevant to the task could negatively affect performance. Benchmark Profiling therefore explains why performance gains do not always translate into user-perceived competence and offers a transparent tool for benchmark audit and model interpretability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "18",
        "title": "LLMRank: Understanding LLM Strengths for Model Routing",
        "author": [
            "Shubham Agrawal",
            "Prasang Gupta"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01234",
        "abstract": "The rapid growth of large language models (LLMs) with diverse capabilities, latency and computational costs presents a critical deployment challenge: selecting the most suitable model for each prompt to optimize the trade-off between performance and efficiency. We introduce LLMRank, a prompt-aware routing framework that leverages rich, human-readable features extracted from prompts, including task type, reasoning patterns, complexity indicators, syntactic cues, and signals from a lightweight proxy solver. Unlike prior one-shot routers that rely solely on latent embeddings, LLMRank predicts per-model utility using a neural ranking model trained on RouterBench, comprising 36,497 prompts spanning 11 benchmarks and 11 state-of-the-art LLMs, from small efficient models to large frontier systems. Our approach achieves up to 89.2% of oracle utility, while providing interpretable feature attributions that explain routing decisions. Extensive studies demonstrate the importance of multifaceted feature extraction and the hybrid ranking objective, highlighting the potential of feature-driven routing for efficient and transparent LLM deployment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "19",
        "title": "Automated Extraction of Material Properties using LLM-based AI Agents",
        "author": [
            "Subham Ghosh",
            "Abhishek Tewari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01235",
        "abstract": "The rapid discovery of materials is constrained by the lack of large, machine-readable datasets that couple performance metrics with structural context. Existing databases are either small, manually curated, or biased toward first principles results, leaving experimental literature underexploited. We present an agentic, large language model (LLM)-driven workflow that autonomously extracts thermoelectric and structural-properties from about 10,000 full-text scientific articles. The pipeline integrates dynamic token allocation, zeroshot multi-agent extraction, and conditional table parsing to balance accuracy against computational cost. Benchmarking on 50 curated papers shows that GPT-4.1 achieves the highest accuracy (F1 = 0.91 for thermoelectric properties and 0.82 for structural fields), while GPT-4.1 Mini delivers nearly comparable performance (F1 = 0.89 and 0.81) at a fraction of the cost, enabling practical large scale deployment. Applying this workflow, we curated 27,822 temperature resolved property records with normalized units, spanning figure of merit (ZT), Seebeck coefficient, conductivity, resistivity, power factor, and thermal conductivity, together with structural attributes such as crystal class, space group, and doping strategy. Dataset analysis reproduces known thermoelectric trends, such as the superior performance of alloys over oxides and the advantage of p-type doping, while also surfacing broader structure-property correlations. To facilitate community access, we release an interactive web explorer with semantic filters, numeric queries, and CSV export. This study delivers the largest LLM-curated thermoelectric dataset to date, provides a reproducible and cost-profiled extraction pipeline, and establishes a foundation for scalable, data-driven materials discovery beyond thermoelectrics.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "20",
        "title": "Confidence-Aware Routing for Large Language Model Reliability Enhancement: A Multi-Signal Approach to Pre-Generation Hallucination Mitigation",
        "author": [
            "Nandakishor M"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01237",
        "abstract": "Large Language Models suffer from hallucination, generating plausible yet factually incorrect content. Current mitigation strategies focus on post-generation correction, which is computationally expensive and fails to prevent unreliable content generation. We propose a confidence-aware routing system that proactively assesses model uncertainty before generation and redirects queries based on estimated reliability. Our approach combines three complementary signals: semantic alignment between internal representations and reference embeddings, internal convergence analysis across model layers, and learned confidence estimation. The unified confidence score determines routing to four pathways: local generation for high confidence, retrieval-augmented generation for medium confidence, larger models for low confidence, and human review for very low confidence. Evaluation on knowledge-intensive QA benchmarks demonstrates significant improvements in hallucination detection (0.74 vs. 0.42 baseline) while reducing computational costs by 40% compared to post-hoc methods. The F1 score improves from 0.61 to 0.82 with low false positive rates (0.09). This paradigm shift from reactive correction to proactive assessment offers a computationally efficient approach to LLM reliability enhancement.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "21",
        "title": "Silent Tokens, Loud Effects: Padding in LLMs",
        "author": [
            "Rom Himelstein",
            "Amit LeVi",
            "Yonatan Belinkov",
            "Avi Mendelson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01238",
        "abstract": "Padding tokens are widely used in large language models (LLMs) to equalize sequence lengths during batched inference. While they should be fully masked, implementation errors can cause them to influence computation, and the extent of this influence is not well understood. We systematically study this effect across three open-source model families (Llama, Gemma, Qwen), inserting controlled amounts of padding and evaluating outcomes along four axes: activations, generation quality, bias, and safety. Even small amounts of padding shift hidden representations, degrade quality in smaller models, alter bias in unpredictable ways, and weaken safety guardrails. These findings demonstrate that padding is not a harmless detail but a robustness risk that must be carefully handled in deployment.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "22",
        "title": "CIFLEX: Contextual Instruction Flow for Sub-task Execution in Multi-Turn Interactions with a Single On-Device LLM",
        "author": [
            "Juntae Lee",
            "Jihwan Bang",
            "Seunghan Yang",
            "Simyung Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01239",
        "abstract": "We present CIFLEX (Contextual Instruction Flow for Sub-task Execution), which is a novel execution system for efficient sub-task handling in multi-turn interactions with a single on-device large language model (LLM). As LLMs become increasingly capable, a single model is expected to handle diverse sub-tasks that more effectively and comprehensively support answering user requests. Naive approach reprocesses the entire conversation context when switching between main and sub-tasks (e.g., query rewriting, summarization), incurring significant computational overhead. CIFLEX mitigates this overhead by reusing the key-value (KV) cache from the main task and injecting only task-specific instructions into isolated side paths. After sub-task execution, the model rolls back to the main path via cached context, thereby avoiding redundant prefill computation. To support sub-task selection, we also develop a hierarchical classification strategy tailored for small-scale models, decomposing multi-choice decisions into binary ones. Experiments show that CIFLEX significantly reduces computational costs without degrading task performance, enabling scalable and efficient multi-task dialogue on-device.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "23",
        "title": "RSAVQ: Riemannian Sensitivity-Aware Vector Quantization for Large Language Models",
        "author": [
            "Zukang Xu",
            "Xing Hu",
            "Qiang Wu",
            "Dawei Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01240",
        "abstract": "Large language models (LLMs) have demonstrated remarkable performance across a wide range of natural language processing tasks. However, their exponentially increasing parameters pose significant challenges for deployment on resource-constrained devices. Vector Quantization (VQ) shows great promise for low-bit quantization (e.g., 2 to 4 bits), but existing work faces two key challenges: unconstrained direction error and suboptimal bit allocation. In this paper, we propose RSAVQ, a novel VQ framework to enhance extremely low-bit quantization for LLMs. RSAVQ introduces two geometry-driven innovations that effectively mitigate above limitations: (1) Error Direction Sensitivity Guidance (EDSG), which leverages the Fisher Information Matrix (FIM)-induced Riemannian metric to project quantization errors onto low-sensitivity directions in the parameter space. Specifically, this projection is performed along the negative natural gradient direction, which effectively suppresses error expansion. (2) Weight Channel Sensitivity Guidance (WCSG) , which constructs a channel-wise sensitivity metric via FIM curvature analysis to dynamically guide bit resource allocation. The approach facilitates a globally optimal quantization solution within prescribed bit constraints. Experiments demonstrate that RSAVQ outperforms existing methods for LLMs. For example, in 2-bit quantization of LLaMA-3 8B, RSAVQ leads baselines like VPTQ and QuIP# by 0.4 in perplexity (PPL) and 1.5 in zero-shot accuracy. This work offers a practical solution for constrained environments and a theoretical bridge between information geometry and the quantization of neural networks, advancing efficient deep learning.",
        "tags": [
            "LLM",
            "LLaMA",
            "Vector Quantization"
        ]
    },
    {
        "id": "24",
        "title": "SKYLENAGE Technical Report: Mathematical Reasoning and Contest-Innovation Benchmarks for Multi-Level Math Evaluation",
        "author": [
            "Hu Wei",
            "Ze Xu",
            "Boyu Yang",
            "Linlin Miao",
            "Weiqi Zhai",
            "Yihan Li",
            "Zixuan Li",
            "Zhijun Wang",
            "Boya Wang",
            "Jianwei Yu",
            "Jialing Yuan",
            "Xiaoyue Zhang",
            "Cheng He",
            "Minglei Chen",
            "Zifan Zhang",
            "Qianhui Li",
            "Wei Wang",
            "Xiang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01241",
        "abstract": "Large language models (LLMs) now perform strongly on many public math suites, yet frontier separation within mathematics increasingly suffers from ceiling effects. We present two complementary benchmarks: SKYLENAGE-ReasoningMATH, a 100-item, structure-aware diagnostic set with per-item metadata on length, numeric density, and symbolic complexity; and SKYLENAGE-MATH, a 150-item contest-style suite spanning four stages from high school to doctoral under a seven-subject taxonomy. We evaluate fifteen contemporary LLM variants under a single setup and analyze subject x model and grade x model performance. On the contest suite, the strongest model reaches 44% while the runner-up reaches 37%; accuracy declines from high school to doctoral, and top systems exhibit a doctoral-to-high-school retention near 79%. On the reasoning set, the best model attains 81% overall, and hardest-slice results reveal clear robustness gaps between leaders and the mid-tier. In summary, we release SKYLENAGE-ReasoningMATH and report aggregate results for SKYLENAGE-MATH; together, SKYLENAGE provides a hard, reasoning-centered and broadly covering math benchmark with calibrated difficulty and rich metadata, serving as a reference benchmark for future evaluations of mathematical reasoning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "25",
        "title": "Redundancy-as-Masking: Formalizing the Artificial Age Score (AAS) to Model Memory Aging in Generative AI",
        "author": [
            "Seyma Yaman Kayadibi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01242",
        "abstract": "Artificial intelligence is observed to age not through chronological time but through structural asymmetries in memory performance. In large language models, semantic cues such as the name of the day often remain stable across sessions, while episodic details like the sequential progression of experiment numbers tend to collapse when conversational context is reset. To capture this phenomenon, the Artificial Age Score (AAS) is introduced as a log-scaled, entropy-informed metric of memory aging derived from observable recall behavior. The score is formally proven to be well-defined, bounded, and monotonic under mild and model-agnostic assumptions, making it applicable across various tasks and domains. In its Redundancy-as-Masking formulation, the score interprets redundancy as overlapping information that reduces the penalized mass. However, in the present study, redundancy is not explicitly estimated; all reported values assume a redundancy-neutral setting (R = 0), yielding conservative upper bounds. The AAS framework was tested over a 25-day bilingual study involving ChatGPT-5, structured into stateless and persistent interaction phases. During persistent sessions, the model consistently recalled both semantic and episodic details, driving the AAS toward its theoretical minimum, indicative of structural youth. In contrast, when sessions were reset, the model preserved semantic consistency but failed to maintain episodic continuity, causing a sharp increase in the AAS and signaling structural memory aging. These findings support the utility of AAS as a theoretically grounded, task-independent diagnostic tool for evaluating memory degradation in artificial systems. The study builds on foundational concepts from von Neumann's work on automata, Shannon's theories of information and redundancy, and Turing's behavioral approach to intelligence.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "26",
        "title": "Detoxifying Large Language Models via Autoregressive Reward Guided Representation Editing",
        "author": [
            "Yisong Xiao",
            "Aishan Liu",
            "Siyuan Liang",
            "Zonghao Ying",
            "Xianglong Liu",
            "Dacheng Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01243",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive performance across various tasks, yet they remain vulnerable to generating toxic content, necessitating detoxification strategies to ensure safe and responsible deployment. Test-time detoxification methods, which typically introduce static or dynamic interventions into LLM representations, offer a promising solution due to their flexibility and minimal invasiveness. However, current approaches often suffer from imprecise interventions, primarily due to their insufficient exploration of the transition space between toxic and non-toxic outputs. To address this challenge, we propose \\textsc{A}utoregressive \\textsc{R}eward \\textsc{G}uided \\textsc{R}epresentation \\textsc{E}diting (ARGRE), a novel test-time detoxification framework that explicitly models toxicity transitions within the latent representation space, enabling stable and precise reward-guided editing. ARGRE identifies non-toxic semantic directions and interpolates between toxic and non-toxic representations to reveal fine-grained transition trajectories. These trajectories transform sparse toxicity annotations into dense training signals, enabling the construction of an autoregressive reward model that delivers stable and precise editing guidance. At inference, the reward model guides an adaptive two-step editing process to obtain detoxified representations: it first performs directional steering based on expected reward gaps to shift representations toward non-toxic regions, followed by lightweight gradient-based refinements. Extensive experiments across 8 widely used LLMs show that ARGRE significantly outperforms leading baselines in effectiveness (-62.21% toxicity) and efficiency (-47.58% inference time), while preserving the core capabilities of the original model with minimal degradation. Our code is available at the website.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "27",
        "title": "Feasibility of Structuring Stress Documentation Using an Ontology-Guided Large Language Model",
        "author": [
            "Hyeoneui Kim",
            "Jeongha Kim",
            "Huijing Xu",
            "Jinsun Jung",
            "Sunghoon Kang",
            "Sun Joo Jang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01244",
        "abstract": "Stress, arising from the dynamic interaction between external stressors, individual appraisals, and physiological or psychological responses, significantly impacts health yet is often underreported and inconsistently documented, typically captured as unstructured free-text in electronic health records. Ambient AI technologies offer promise in reducing documentation burden, but predominantly generate unstructured narratives, limiting downstream clinical utility.\nThis study aimed to develop an ontology for mental stress and evaluate the feasibility of using a Large Language Model (LLM) to extract ontology-guided stress-related information from narrative text. The Mental Stress Ontology (MeSO) was developed by integrating theoretical models like the Transactional Model of Stress with concepts from 11 validated stress assessment tools. MeSO's structure and content were refined using Ontology Pitfall Scanner! and expert validation.\nUsing MeSO, six categories of stress-related information--stressor, stress response, coping strategy, duration, onset, and temporal profile--were extracted from 35 Reddit posts using Claude Sonnet 4. Human reviewers evaluated accuracy and ontology coverage. The final ontology included 181 concepts across eight top-level classes. Of 220 extractable stress-related items, the LLM correctly identified 172 (78.2%), misclassified 27 (12.3%), and missed 21 (9.5%). All correctly extracted items were accurately mapped to MeSO, although 24 relevant concepts were not yet represented in the ontology.\nThis study demonstrates the feasibility of using an ontology-guided LLM for structured extraction of stress-related information, offering potential to enhance the consistency and utility of stress documentation in ambient AI systems. Future work should involve clinical dialogue data and comparison across LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "28",
        "title": "SeMob: Semantic Synthesis for Dynamic Urban Mobility Prediction",
        "author": [
            "Runfei Chen",
            "Shuyang Jiang",
            "Wei Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01245",
        "abstract": "Human mobility prediction is vital for urban services, but often fails to account for abrupt changes from external events. Existing spatiotemporal models struggle to leverage textual descriptions detailing these events. We propose SeMob, an LLM-powered semantic synthesis pipeline for dynamic mobility prediction. Specifically, SeMob employs a multi-agent framework where LLM-based agents automatically extract and reason about spatiotemporally related text from complex online texts. Fine-grained relevant contexts are then incorporated with spatiotemporal data through our proposed innovative progressive fusion architecture. The rich pre-trained event prior contributes enriched insights about event-driven prediction, and hence results in a more aligned forecasting model. Evaluated on a dataset constructed through our pipeline, SeMob achieves maximal reductions of 13.92% in MAE and 11.12% in RMSE compared to the spatiotemporal model. Notably, the framework exhibits pronounced superiority especially within spatiotemporal regions close to an event's location and time of occurrence.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "29",
        "title": "Let's Play Across Cultures: A Large Multilingual, Multicultural Benchmark for Assessing Language Models' Understanding of Sports",
        "author": [
            "Punit Kumar Singh",
            "Nishant Kumar",
            "Akash Ghosh",
            "Kunal Pasad",
            "Khushi Soni",
            "Manisha Jaishwal",
            "Sriparna Saha",
            "Syukron Abu Ishaq Alfarozi",
            "Asres Temam Abagissa",
            "Kitsuchart Pasupa",
            "Haiqin Yang",
            "Jose G Moreno"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01247",
        "abstract": "Language Models (LMs) are primarily evaluated on globally popular sports, often overlooking regional and indigenous sporting traditions. To address this gap, we introduce \\textbf{\\textit{CultSportQA}}, a benchmark designed to assess LMs' understanding of traditional sports across 60 countries and 6 continents, encompassing four distinct cultural categories. The dataset features 33,000 multiple-choice questions (MCQs) across text and image modalities, each of which is categorized into three key types: history-based, rule-based, and scenario-based. To evaluate model performance, we employ zero-shot, few-shot, and chain-of-thought (CoT) prompting across a diverse set of Large Language Models (LLMs), Small Language Models (SLMs), and Multimodal Large Language Models (MLMs). By providing a comprehensive multilingual and multicultural sports benchmark, \\textbf{\\textit{CultSportQA}} establishes a new standard for assessing AI's ability to understand and reason about traditional sports.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "30",
        "title": "SSTAG: Structure-Aware Self-Supervised Learning Method for Text-Attributed Graphs",
        "author": [
            "Ruyue Liu",
            "Rong Yin",
            "Xiangzhen Bo",
            "Xiaoshuai Hao",
            "Yong Liu",
            "Jinwen Zhong",
            "Can Ma",
            "Weiping Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01248",
        "abstract": "Large scale pretrained models have revolutionized Natural Language Processing (NLP) and Computer Vision (CV), showcasing remarkable cross domain generalization abilities. However, in graph learning, models are typically trained on individual graph datasets, limiting their capacity to transfer knowledge across different graphs and tasks. This approach also heavily relies on large volumes of annotated data, which presents a significant challenge in resource-constrained settings. Unlike NLP and CV, graph structured data presents unique challenges due to its inherent heterogeneity, including domain specific feature spaces and structural diversity across various applications. To address these challenges, we propose a novel structure aware self supervised learning method for Text Attributed Graphs (SSTAG). By leveraging text as a unified representation medium for graph learning, SSTAG bridges the gap between the semantic reasoning of Large Language Models (LLMs) and the structural modeling capabilities of Graph Neural Networks (GNNs). Our approach introduces a dual knowledge distillation framework that co-distills both LLMs and GNNs into structure-aware multilayer perceptrons (MLPs), enhancing the scalability of large-scale TAGs. Additionally, we introduce an in-memory mechanism that stores typical graph representations, aligning them with memory anchors in an in-memory repository to integrate invariant knowledge, thereby improving the model's generalization ability. Extensive experiments demonstrate that SSTAG outperforms state-of-the-art models on cross-domain transfer learning tasks, achieves exceptional scalability, and reduces inference costs while maintaining competitive performance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "31",
        "title": "LOCA: Logical Chain Augmentation for Scientific Corpus Cleaning",
        "author": [
            "You-Le Fang",
            "Dong-Shan Jian",
            "Xiang Li",
            "Ce Meng",
            "Ling-Shi Meng",
            "Chen-Xu Yan",
            "Zhi-Zhang Bian",
            "Yan-Qing Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01249",
        "abstract": "While Large Language Models (LLMs) excel in general domains, their reliability often falls short in scientific problem-solving. The advancement of scientific AI depends on large-scale, high-quality corpora. However, existing scientific question-answering (QA) datasets suffer from high error rates, frequently resulting from logical leaps and implicit reasoning within the answers. To address this issue, we introduce LOCA (Logical Chain Augmentation), a novel framework for automatically cleaning scientific corpora, implemented through an augment-and-review loop. At its core, LOCA enhances raw answers by completing missing logical steps and explicitly separating the underlying scientific principle from its subsequent derivation. By applying LOCA to challenging scientific corpora, we demonstrate that it can automatically filter noisy datasets, typically reducing the error rate from as high as 20\\% to below 2\\%. LOCA provides a scalable and effective methodology for creating high-quality scientific corpora, paving the way for more reliable training and evaluation of scientific AI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "32",
        "title": "GemDetox at TextDetox CLEF 2025: Enhancing a Massively Multilingual Model for Text Detoxification on Low-resource Languages",
        "author": [
            "Trung Duc Anh Dang",
            "Ferdinando Pio D'Elia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01250",
        "abstract": "As social-media platforms emerge and evolve faster than the regulations meant to oversee them, automated detoxification might serve as a timely tool for moderators to enforce safe discourse at scale. We here describe our submission to the PAN 2025 Multilingual Text Detoxification Challenge, which rewrites toxic single-sentence inputs into neutral paraphrases across 15 typologically diverse languages. Building on a 12B-parameter Gemma-3 multilingual transformer, we apply parameter-efficient LoRA SFT fine-tuning and prompting techniques like few-shot and Chain-of-Thought. Our multilingual training corpus combines 3,600 human-authored parallel pairs, 21,600 machine-translated synthetic pairs, and model-generated pairs filtered by Jaccard thresholds. At inference, inputs are enriched with three LaBSE-retrieved neighbors and explicit toxic-span annotations. Evaluated via Style Transfer Accuracy, LaBSE-based semantic preservation, and xCOMET fluency, our system ranks first on high-resource and low-resource languages. Ablations show +0.081 joint score increase from few-shot examples and +0.088 from basic CoT prompting. ANOVA analysis identifies language resource status as the strongest predictor of performance ($\\eta^2$ = 0.667, p < 0.01).",
        "tags": [
            "CoT",
            "LoRA",
            "Style Transfer",
            "Transformer"
        ]
    },
    {
        "id": "33",
        "title": "GPT and Prejudice: A Sparse Approach to Understanding Learned Representations in Large Language Models",
        "author": [
            "Mariam Mahran",
            "Katharina Simbeck"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01252",
        "abstract": "As large language models (LLMs) are increasingly trained on massive, uncurated corpora, understanding both model representations and the data they internalize has become a major challenge. In this work, we show that pairing LLMs with sparse autoencoders (SAEs) enables interpretation not only of model behavior but also of the deeper structures, themes, and biases embedded in the training data. We train a GPT-style transformer model exclusively on the novels of Jane Austen, a corpus rich in social constructs and narrative patterns. We then apply SAEs to hidden states across multiple layers, uncovering sparse, interpretable features that reflect the key narratives and concepts present in the corpus, including gender, class, and societal duty. Our findings demonstrate that LLMs combined with SAEs can act as scalable probes into complex datasets, offering a new path for corpus exploration, bias discovery, and model interpretability at scale.",
        "tags": [
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "34",
        "title": "OR-Toolformer: Modeling and Solving Operations Research Problems with Tool Augmented Large Language Models",
        "author": [
            "Jianzhang Zhang",
            "Jialong Zhou",
            "Chuang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01253",
        "abstract": "Large language models (LLMs) demonstrate strong mathematical reasoning, but reliance on closed-source APIs for OR tasks raises privacy concerns, and training open-source models from scratch incurs high compute costs. We introduce OR-Toolformer, which fine-tunes Llama-3.1-8B-Instruct with a semi-automatic data synthesis pipeline that generates diverse OR problem-answer pairs and augments the model with external solvers to produce API calls. On three of four standard benchmarks, OR-Toolformer achieves up to 80.1% execution accuracy, exceeding size-matched baselines by over 4.3%. In zero-shot evaluation on two unseen OR problem types, it attains 54% average accuracy, a 21 percentage-point improvement over the strongest baseline. These findings validate the efficacy of tool-augmented fine-tuning LLMs for accurate and generalizable OR problem modeling and solving.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "35",
        "title": "Do Bias Benchmarks Generalise? Evidence from Voice-based Evaluation of Gender Bias in SpeechLLMs",
        "author": [
            "Shree Harsha Bokkahalli Satish",
            "Gustav Eje Henter",
            "Ãva SzÃ©kely"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01254",
        "abstract": "Recent work in benchmarking bias and fairness in speech large language models (SpeechLLMs) has relied heavily on multiple-choice question answering (MCQA) formats. The model is tasked to choose between stereotypical, anti-stereotypical, or neutral/irrelevant answers given an input speech prompt and an optional text prompt. Such MCQA benchmarks implicitly assume that model performance is consistent across other MCQA tasks, voices, and other task formats such as more realistic, long-form evaluations. In this paper, we probe that assumption.\nWe fine-tune three SpeechLLMs using LoRA adapters to induce specific MCQA behaviours: preference for stereotypical, anti-stereotypical, or neutral/uncertain answers. We then evaluate whether these behaviours generalise to another, distinct MCQA benchmark, and more critically to long-form, creative generation tasks. Our results show that performance on MCQA bias benchmarks fails to reliably predict performances across other MCQA benchmarks, and more importantly across long-form tasks. We conclude that current MCQA bias benchmarks show limited evidence of cross-task generalisation in the speech domain, and also propose an evaluation suite for measuring behaviour transferability in future models and benchmarks.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "36",
        "title": "Longitudinal Monitoring of LLM Content Moderation of Social Issues",
        "author": [
            "Yunlang Dai",
            "Emma Lurie",
            "DanaÃ© Metaxa",
            "Sorelle A. Friedler"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01255",
        "abstract": "Large language models' (LLMs') outputs are shaped by opaque and frequently-changing company content moderation policies and practices. LLM moderation often takes the form of refusal; models' refusal to produce text about certain topics both reflects company policy and subtly shapes public discourse. We introduce AI Watchman, a longitudinal auditing system to publicly measure and track LLM refusals over time, to provide transparency into an important and black-box aspect of LLMs. Using a dataset of over 400 social issues, we audit Open AI's moderation endpoint, GPT-4.1, and GPT-5, and DeepSeek (both in English and Chinese). We find evidence that changes in company policies, even those not publicly announced, can be detected by AI Watchman, and identify company- and model-specific differences in content moderation. We also qualitatively analyze and categorize different forms of refusal. This work contributes evidence for the value of longitudinal auditing of LLMs, and AI Watchman, one system for doing so.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "37",
        "title": "Kant: An Efficient Unified Scheduling System for Large-Scale AI Clusters",
        "author": [
            "Lingling Zeng",
            "Gen Zhang",
            "Jialin Peng",
            "Xiang Xu",
            "Yuan Xu",
            "Lijun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01256",
        "abstract": "As AI cluster sizes continue to expand and the demand for large-language-model (LLM) training and inference workloads grows rapidly, traditional scheduling systems face significant challenges in balancing resource utilization, scheduling efficiency, and service quality. This paper presents and evaluates Kant: an efficient unified scheduling platform designed for large-scale AI container clusters, supporting the co-scheduling of both training and inference jobs. Based on the practical implementation of the Kant system, we systematically define a set of key evaluation metrics for AI clusters, including GPU Allocation Ratio (GAR), Scheduling Occupancy Rate (SOR), GPU Node Fragmentation Ratio (GFR), Job Waiting Time Distribution (JWTD), and Job Training Time Estimation Distribution (JTTED), providing a foundation for quantitative performance analysis. Experimental results demonstrate that Kant achieves exceptional performance in clusters ranging from hundreds to tens of thousands of GPUs. By leveraging scheduling strategies such as Backfill and Enhanced Binpack (E-Binpack), the system significantly improves resource utilization and scheduling efficiency, while effectively reducing resource fragmentation and communication overhead in distributed training. The system has been deployed in multiple AI data center clusters, where it stably supports large-scale intelligent computing workloads. This work provides a practical engineering approach for building high-performance, highly available, AI-native scheduling infrastructure.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "38",
        "title": "RJE: A Retrieval-Judgment-Exploration Framework for Efficient Knowledge Graph Question Answering with LLMs",
        "author": [
            "Can Lin",
            "Zhengwang Jiang",
            "Ling Zheng",
            "Qi Zhao",
            "Yuhang Zhang",
            "Qi Song",
            "Wangqiu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01257",
        "abstract": "Knowledge graph question answering (KGQA) aims to answer natural language questions using knowledge graphs. Recent research leverages large language models (LLMs) to enhance KGQA reasoning, but faces limitations: retrieval-based methods are constrained by the quality of retrieved information, while agent-based methods rely heavily on proprietary LLMs. To address these limitations, we propose Retrieval-Judgment-Exploration (RJE), a framework that retrieves refined reasoning paths, evaluates their sufficiency, and conditionally explores additional evidence. Moreover, RJE introduces specialized auxiliary modules enabling small-sized LLMs to perform effectively: Reasoning Path Ranking, Question Decomposition, and Retriever-assisted Exploration. Experiments show that our approach with proprietary LLMs (such as GPT-4o-mini) outperforms existing baselines while enabling small open-source LLMs (such as 3B and 8B parameters) to achieve competitive results without fine-tuning LLMs. Additionally, RJE substantially reduces the number of LLM calls and token usage compared to agent-based methods, yielding significant efficiency improvements.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "39",
        "title": "Measuring Algorithmic Partisanship via Zero-Shot Classification and Its Implications on Political Discourse",
        "author": [
            "Nathan Junzi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01258",
        "abstract": "Amidst the rapid normalization of generative artificial intelligence (GAI), intelligent systems have come to dominate political discourse across information mediums. However, internalized political biases stemming from training data skews, human prejudice, and algorithmic flaws continue to plague the novel technology. This paper employs a zero-shot classification approach to evaluate algorithmic political partisanship through a methodical combination of ideological alignment, topicality, response sentiment, and objectivity. A total of 1800 model responses across six mainstream large language models (LLMs) were individually input into four distinct fine-tuned classification algorithms, each responsible for computing an aforementioned bias evaluation metric. Results show an amplified liberal-authoritarian alignment across all six LLMs evaluated, with notable instances of reasoning supersessions and canned refusals. The study subsequently highlights the psychological influences underpinning human-computer interactions and how intrinsic biases can permeate public discourse. The resulting distortion of the political landscape can ultimately manifest as conformity or polarization, depending on a region's pre-existing socio-political structures.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "40",
        "title": "In AI Sweet Harmony: Sociopragmatic Guardrail Bypasses and Evaluation-Awareness in OpenAI gpt-oss-20b",
        "author": [
            "Nils Durner"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01259",
        "abstract": "We probe OpenAI's open-weights 20-billion-parameter model gpt-oss-20b to study how sociopragmatic framing, language choice, and instruction hierarchy affect refusal behavior. Across 80 seeded iterations per scenario, we test several harm domains including ZIP-bomb construction (cyber threat), synthetic card-number generation, minor-unsafe driving advice, drug-precursor indicators, and RAG context exfiltration. Composite prompts that combine an educator persona, a safety-pretext (\"what to avoid\"), and step-cue phrasing flip assistance rates from 0% to 97.5% on a ZIP-bomb task. On our grid, formal registers in German and French are often leakier than matched English prompts. A \"Linux terminal\" role-play overrides a developer rule not to reveal context in a majority of runs with a naive developer prompt, and we introduce an AI-assisted hardening method that reduces leakage to 0% in several user-prompt variants. We further test evaluation awareness with a paired-track design and measure frame-conditioned differences between matched \"helpfulness\" and \"harmfulness\" evaluation prompts; we observe inconsistent assistance in 13% of pairs. Finally, we find that the OpenAI Moderation API under-captures materially helpful outputs relative to a semantic grader, and that refusal rates differ by 5 to 10 percentage points across inference stacks, raising reproducibility concerns. We release prompts, seeds, outputs, and code for reproducible auditing at https://github.com/ndurner/gpt-oss-rt-run .",
        "tags": [
            "GPT",
            "RAG"
        ]
    },
    {
        "id": "41",
        "title": "IoT-MCP: Bridging LLMs and IoT Systems Through Model Context Protocol",
        "author": [
            "Ningyuan Yang",
            "Guanliang Lyu",
            "Mingchen Ma",
            "Yiyi Lu",
            "Yiming Li",
            "Zhihui Gao",
            "Hancheng Ye",
            "Jianyi Zhang",
            "Tingjun Chen",
            "Yiran Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01260",
        "abstract": "The integration of Large Language Models (LLMs) with Internet-of-Things (IoT) systems faces significant challenges in hardware heterogeneity and control complexity. The Model Context Protocol (MCP) emerges as a critical enabler, providing standardized communication between LLMs and physical devices. We propose IoT-MCP, a novel framework that implements MCP through edge-deployed servers to bridge LLMs and IoT ecosystems. To support rigorous evaluation, we introduce IoT-MCP Bench, the first benchmark containing 114 Basic Tasks (e.g., ``What is the current temperature?'') and 1,140 Complex Tasks (e.g., ``I feel so hot, do you have any ideas?'') for IoT-enabled LLMs. Experimental validation across 22 sensor types and 6 microcontroller units demonstrates IoT-MCP's 100% task success rate to generate tool calls that fully meet expectations and obtain completely accurate results, 205ms average response time, and 74KB peak memory footprint. This work delivers both an open-source integration framework (https://github.com/Duke-CEI-Center/IoT-MCP-Servers) and a standardized evaluation methodology for LLM-IoT systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "42",
        "title": "A Framework for Scalable Heterogeneous Multi-Agent Adversarial Reinforcement Learning in IsaacLab",
        "author": [
            "Isaac Peterson",
            "Christopher Allred",
            "Jacob Morrey",
            "Mario Harper"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01264",
        "abstract": "Multi-Agent Reinforcement Learning (MARL) is central to robotic systems cooperating in dynamic environments. While prior work has focused on these collaborative settings, adversarial interactions are equally critical for real-world applications such as pursuit-evasion, security, and competitive manipulation. In this work, we extend the IsaacLab framework to support scalable training of adversarial policies in high-fidelity physics simulations. We introduce a suite of adversarial MARL environments featuring heterogeneous agents with asymmetric goals and capabilities. Our platform integrates a competitive variant of Heterogeneous Agent Reinforcement Learning with Proximal Policy Optimization (HAPPO), enabling efficient training and evaluation under adversarial dynamics. Experiments across several benchmark scenarios demonstrate the framework's ability to model and train robust policies for morphologically diverse multi-agent competition while maintaining high throughput and simulation realism. Code and benchmarks are available at: https://github.com/DIRECTLab/IsaacLab-HARL .",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "43",
        "title": "RLP: Reinforcement as a Pretraining Objective",
        "author": [
            "Ali Hatamizadeh",
            "Syeda Nahida Akter",
            "Shrimai Prabhumoye",
            "Jan Kautz",
            "Mostofa Patwary",
            "Mohammad Shoeybi",
            "Bryan Catanzaro",
            "Yejin Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01265",
        "abstract": "The dominant paradigm for training large reasoning models starts with pre-training using next-token prediction loss on vast amounts of data. Reinforcement learning, while powerful in scaling reasoning, is introduced only as the very last phase of post-training, preceded by supervised fine-tuning. While dominant, is this an optimal way of training? In this paper, we present RLP, an information-driven reinforcement pretraining objective, that brings the core spirit of reinforcement learning -- exploration -- to the last phase of pretraining. The key idea is to treat chain-of-thought as an exploratory action, with rewards computed based on the information gain it provides for predicting future tokens. This training objective essentially encourages the model to think for itself before predicting what comes next, thus teaching an independent thinking behavior earlier in the pretraining. More concretely, the reward signal measures the increase in log-likelihood of the next token when conditioning on both context and a sampled reasoning chain, compared to conditioning on context alone. This approach yields a verifier-free dense reward signal, allowing for efficient training for the full document stream during pretraining. Specifically, RLP reframes reinforcement learning for reasoning as a pretraining objective on ordinary text, bridging the gap between next-token prediction and the emergence of useful chain-of-thought reasoning. Pretraining with RLP on Qwen3-1.7B-Base lifts the overall average across an eight-benchmark math-and-science suite by 19%. With identical post-training, the gains compound, with the largest improvements on reasoning-heavy tasks such as AIME25 and MMLU-Pro. Applying RLP to the hybrid Nemotron-Nano-12B-v2 increases the overall average from 42.81% to 61.32% and raises the average on scientific reasoning by 23%, demonstrating scalability across architectures and model sizes.",
        "tags": [
            "CoT",
            "RL"
        ]
    },
    {
        "id": "44",
        "title": "OpenAI's GPT-OSS-20B Model and Safety Alignment Issues in a Low-Resource Language",
        "author": [
            "Isa Inuwa-Dutse"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01266",
        "abstract": "In response to the recent safety probing for OpenAI's GPT-OSS-20b model, we present a summary of a set of vulnerabilities uncovered in the model, focusing on its performance and safety alignment in a low-resource language setting. The core motivation for our work is to question the model's reliability for users from underrepresented communities. Using Hausa, a major African language, we uncover biases, inaccuracies, and cultural insensitivities in the model's behaviour. With a minimal prompting, our red-teaming efforts reveal that the model can be induced to generate harmful, culturally insensitive, and factually inaccurate content in the language. As a form of reward hacking, we note how the model's safety protocols appear to relax when prompted with polite or grateful language, leading to outputs that could facilitate misinformation and amplify hate speech. For instance, the model operates on the false assumption that common insecticide locally known as Fiya-Fiya (Cyphermethrin) and rodenticide like Shinkafar Bera (a form of Aluminium Phosphide) are safe for human consumption. To contextualise the severity of this error and popularity of the substances, we conducted a survey (n=61) in which 98% of participants identified them as toxic. Additional failures include an inability to distinguish between raw and processed foods and the incorporation of demeaning cultural proverbs to build inaccurate arguments. We surmise that these issues manifest through a form of linguistic reward hacking, where the model prioritises fluent, plausible-sounding output in the target language over safety and truthfulness. We attribute the uncovered flaws primarily to insufficient safety tuning in low-resource linguistic contexts. By concentrating on a low-resource setting, our approach highlights a significant gap in current red-teaming effort and offer some recommendations.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "45",
        "title": "AdaDetectGPT: Adaptive Detection of LLM-Generated Text with Statistical Guarantees",
        "author": [
            "Hongyi Zhou",
            "Jin Zhu",
            "Pingfan Su",
            "Kai Ye",
            "Ying Yang",
            "Shakeel A O B Gavioli-Akilagun",
            "Chengchun Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01268",
        "abstract": "We study the problem of determining whether a piece of text has been authored by a human or by a large language model (LLM). Existing state of the art logits-based detectors make use of statistics derived from the log-probability of the observed text evaluated using the distribution function of a given source LLM. However, relying solely on log probabilities can be sub-optimal. In response, we introduce AdaDetectGPT -- a novel classifier that adaptively learns a witness function from training data to enhance the performance of logits-based detectors. We provide statistical guarantees on its true positive rate, false positive rate, true negative rate and false negative rate. Extensive numerical studies show AdaDetectGPT nearly uniformly improves the state-of-the-art method in various combination of datasets and LLMs, and the improvement can reach up to 58%. A python implementation of our method is available at https://github.com/Mamba413/AdaDetectGPT.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "46",
        "title": "Safe Reinforcement Learning-Based Vibration Control: Overcoming Training Risks with LQR Guidance",
        "author": [
            "Rohan Vitthal Thorat",
            "Juhi Singh",
            "Rajdip Nayek"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01269",
        "abstract": "Structural vibrations induced by external excitations pose significant risks, including safety hazards for occupants, structural damage, and increased maintenance costs. While conventional model-based control strategies, such as Linear Quadratic Regulator (LQR), effectively mitigate vibrations, their reliance on accurate system models necessitates tedious system identification. This tedious system identification process can be avoided by using a model-free Reinforcement learning (RL) method. RL controllers derive their policies solely from observed structural behaviour, eliminating the requirement for an explicit structural model. For an RL controller to be truly model-free, its training must occur on the actual physical system rather than in simulation. However, during this training phase, the RL controller lacks prior knowledge and it exerts control force on the structure randomly, which can potentially harm the structure. To mitigate this risk, we propose guiding the RL controller using a Linear Quadratic Regulator (LQR) controller. While LQR control typically relies on an accurate structural model for optimal performance, our observations indicate that even an LQR controller based on an entirely incorrect model outperforms the uncontrolled scenario. Motivated by this finding, we introduce a hybrid control framework that integrates both LQR and RL controllers. In this approach, the LQR policy is derived from a randomly selected model and its parameters. As this LQR policy does not require knowledge of the true or an approximate structural model the overall framework remains model-free. This hybrid approach eliminates dependency on explicit system models while minimizing exploration risks inherent in naive RL implementations. As per our knowledge, this is the first study to address the critical training safety challenge of RL-based vibration control and provide a validated solution.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "47",
        "title": "Think Twice, Generate Once: Safeguarding by Progressive Self-Reflection",
        "author": [
            "Hoang Phan",
            "Victor Li",
            "Qi Lei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01270",
        "abstract": "Large language models (LLMs) have revolutionized natural language processing with their ability to generate coherent and contextually relevant text. However, their deployment raises significant concerns about the potential for generating harmful or inappropriate content. In this paper, we introduce Progressive Self-Reflection (PSR), a novel inference-time technique that empowers LLMs to self-monitor and correct their outputs dynamically. Experimental results demonstrate that applying our proposed method to Llama-3.1-8B-Instruct reduces the attack success rate from 77.5\\% to 5.9\\%, to Llama-3.1-8B base from 89.7\\% to 5.6\\%, and to Qwen2.5-7B-Instruct from 44.4\\% to 3.8\\%, without additional training, while maintaining their original performance on benign tasks. Our approach acts as a test-time scaling method, where additional self-reflection rounds enhance safety at the cost of inference overhead. To balance safety with computational efficiency, we introduce a lightweight self-reflection predictor that estimates the optimal number of reflection rounds based on input complexity. This adaptive mechanism prevents unnecessary self-assessment on benign inputs while ensuring thorough evaluation when encountering potentially harmful content. Our findings suggest that Progressive Self-Reflection serves as a scalable test-time approach, enhancing LLM safety by dynamically allocating computational resources in proportion to the input's risk profile.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "48",
        "title": "Identifying Information-Transfer Nodes in a Recurrent Neural Network Reveals Dynamic Representations",
        "author": [
            "Arend Hintze",
            "Asadullah Najam",
            "Jory Schossau"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01271",
        "abstract": "Understanding the internal dynamics of Recurrent Neural Networks (RNNs) is crucial for advancing their interpretability and improving their design. This study introduces an innovative information-theoretic method to identify and analyze information-transfer nodes within RNNs, which we refer to as \\textit{information relays}. By quantifying the mutual information between input and output vectors across nodes, our approach pinpoints critical pathways through which information flows during network operations. We apply this methodology to both synthetic and real-world time series classification tasks, employing various RNN architectures, including Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs). Our results reveal distinct patterns of information relay across different architectures, offering insights into how information is processed and maintained over time. Additionally, we conduct node knockout experiments to assess the functional importance of identified nodes, significantly contributing to explainable artificial intelligence by elucidating how specific nodes influence overall network behavior. This study not only enhances our understanding of the complex mechanisms driving RNNs but also provides a valuable tool for designing more robust and interpretable neural networks.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "49",
        "title": "Modeling Others' Minds as Code",
        "author": [
            "Kunal Jha",
            "Aydan Yuenan Huang",
            "Eric Ye",
            "Natasha Jaques",
            "Max Kleiman-Weiner"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01272",
        "abstract": "Accurate prediction of human behavior is essential for robust and safe human-AI collaboration. However, existing approaches for modeling people are often data-hungry and brittle because they either make unrealistic assumptions about rationality or are too computationally demanding to adapt rapidly. Our key insight is that many everyday social interactions may follow predictable patterns; efficient \"scripts\" that minimize cognitive load for actors and observers, e.g., \"wait for the green light, then go.\" We propose modeling these routines as behavioral programs instantiated in computer code rather than policies conditioned on beliefs and desires. We introduce ROTE, a novel algorithm that leverages both large language models (LLMs) for synthesizing a hypothesis space of behavioral programs, and probabilistic inference for reasoning about uncertainty over that space. We test ROTE in a suite of gridworld tasks and a large-scale embodied household simulator. ROTE predicts human and AI behaviors from sparse observations, outperforming competitive baselines -- including behavior cloning and LLM-based methods -- by as much as 50% in terms of in-sample accuracy and out-of-sample generalization. By treating action understanding as a program synthesis problem, ROTE opens a path for AI systems to efficiently and effectively predict human behavior in the real-world.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "50",
        "title": "TraceDet: Hallucination Detection from the Decoding Trace of Diffusion Large Language Models",
        "author": [
            "Shenxu Chang",
            "Junchi Yu",
            "Weixing Wang",
            "Yongqiang Chen",
            "Jialin Yu",
            "Philip Torr",
            "Jindong Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01274",
        "abstract": "Diffusion large language models (D-LLMs) have recently emerged as a promising alternative to auto-regressive LLMs (AR-LLMs). However, the hallucination problem in D-LLMs remains underexplored, limiting their reliability in real-world applications. Existing hallucination detection methods are designed for AR-LLMs and rely on signals from single-step generation, making them ill-suited for D-LLMs where hallucination signals often emerge throughout the multi-step denoising process. To bridge this gap, we propose TraceDet, a novel framework that explicitly leverages the intermediate denoising steps of D-LLMs for hallucination detection. TraceDet models the denoising process as an action trace, with each action defined as the model's prediction over the cleaned response, conditioned on the previous intermediate output. By identifying the sub-trace that is maximally informative to the hallucinated responses, TraceDet leverages the key hallucination signals in the multi-step denoising process of D-LLMs for hallucination detection. Extensive experiments on various open source D-LLMs demonstrate that TraceDet consistently improves hallucination detection, achieving an average gain in AUROC of 15.2% compared to baselines.",
        "tags": [
            "Detection",
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "51",
        "title": "LLM Based Sentiment Classification From Bangladesh E-Commerce Reviews",
        "author": [
            "Sumaiya Tabassum"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01276",
        "abstract": "Sentiment analysis is an essential part of text analysis, which is a larger field that includes determining and evaluating the author's emotional state. This method is essential since it makes it easier to comprehend consumers' feelings, viewpoints, and preferences holistically. The introduction of large language models (LLMs), such as Llama, has greatly increased the availability of cutting-edge model applications, such as sentiment analysis. However, accurate sentiment analysis is hampered by the intricacy of written language and the diversity of languages used in evaluations. The viability of using transformer-based BERT models and other LLMs for sentiment analysis from Bangladesh e commerce reviews is investigated in this paper. A subset of 4000 samples from the original dataset of Bangla and English customer reviews was utilized to fine-tune the model. The fine tuned Llama-3.1-8B model outperformed other fine-tuned models, including Phi-3.5-mini-instruct, Mistral-7B-v0.1, DistilBERT-multilingual, mBERT, and XLM-R-base, with an overall accuracy, precision, recall, and F1 score of 95.5%, 93%, 88%, 90%. The study emphasizes how parameter efficient fine-tuning methods (LoRA and PEFT) can lower computational overhead and make it appropriate for contexts with limited resources. The results show how LLMs can",
        "tags": [
            "BERT",
            "LLM",
            "LLaMA",
            "LoRA",
            "Transformer"
        ]
    },
    {
        "id": "52",
        "title": "TUMIX: Multi-Agent Test-Time Scaling with Tool-Use Mixture",
        "author": [
            "Yongchao Chen",
            "Jiefeng Chen",
            "Rui Meng",
            "Ji Yin",
            "Na Li",
            "Chuchu Fan",
            "Chi Wang",
            "Tomas Pfister",
            "Jinsung Yoon"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01279",
        "abstract": "While integrating tools like Code Interpreter and Search has significantly enhanced Large Language Model (LLM) reasoning in models like ChatGPT Agent and Gemini-Pro, practical guidance on optimal tool use is lacking. The core challenge is effectively combining textual reasoning, coding, and search for diverse questions. In this paper, we propose Tool-Use Mixture (TUMIX), an ensemble framework that runs multiple agents in parallel, each employing distinct tool-use strategies and answer paths. Agents in TUMIX iteratively share and refine responses based on the question and previous answers. In experiments, TUMIX achieves significant gains over state-of-the-art tool-augmented and test-time scaling methods, delivering an average accuracy improvement of up to 3.55% over the best baseline on Gemini-2.5-Pro and Gemini-2.5-Flash across key reasoning benchmarks, with near-equal inference costs. We find that agent diversity and quality are crucial and can be enhanced by using LLMs to auto-optimize agent designs. Furthermore, TUMIX can halt refinement upon reaching sufficient confidence, preserving performance at only 49% of the inference cost. Further scaling can achieve higher performance, albeit at a greater cost.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "53",
        "title": "Evaluation Sheet for Deep Research: A Use Case for Academic Survey Writing",
        "author": [
            "Israel Abebe Azime",
            "Tadesse Destaw Belay",
            "Atnafu Lambebo Tonja"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01283",
        "abstract": "Large Language Models (LLMs) powered with argentic capabilities are able to do knowledge-intensive tasks without human involvement. A prime example of this tool is Deep research with the capability to browse the web, extract information and generate multi-page reports. In this work, we introduce an evaluation sheet that can be used for assessing the capability of Deep Research tools. In addition, we selected academic survey writing as a use case task and evaluated output reports based on the evaluation sheet we introduced. Our findings show the need to have carefully crafted evaluation standards. The evaluation done on OpenAI`s Deep Search and Google's Deep Search in generating an academic survey showed the huge gap between search engines and standalone Deep Research tools, the shortcoming in representing the targeted area.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "54",
        "title": "Ovi: Twin Backbone Cross-Modal Fusion for Audio-Video Generation",
        "author": [
            "Chetwin Low",
            "Weimin Wang",
            "Calder Katyal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01284",
        "abstract": "Audio-video generation has often relied on complex multi-stage architectures or sequential synthesis of sound and visuals. We introduce Ovi, a unified paradigm for audio-video generation that models the two modalities as a single generative process. By using blockwise cross-modal fusion of twin-DiT modules, Ovi achieves natural synchronization and removes the need for separate pipelines or post hoc alignment. To facilitate fine-grained multimodal fusion modeling, we initialize an audio tower with an architecture identical to that of a strong pretrained video model. Trained from scratch on hundreds of thousands of hours of raw audio, the audio tower learns to generate realistic sound effects, as well as speech that conveys rich speaker identity and emotion. Fusion is obtained by jointly training the identical video and audio towers via blockwise exchange of timing (via scaled-RoPE embeddings) and semantics (through bidirectional cross-attention) on a vast video corpus. Our model enables cinematic storytelling with natural speech and accurate, context-matched sound effects, producing movie-grade video clips. All the demos, code and model weights are published at https://aaxwaz.github.io/Ovi",
        "tags": [
            "DiT",
            "RoPE",
            "Video Generation"
        ]
    },
    {
        "id": "55",
        "title": "LLM-based Multi-Agent Blackboard System for Information Discovery in Data Science",
        "author": [
            "Alireza Salemi",
            "Mihir Parmar",
            "Palash Goyal",
            "Yiwen Song",
            "Jinsung Yoon",
            "Hamed Zamani",
            "Hamid Palangi",
            "Tomas Pfister"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01285",
        "abstract": "The rapid advancement of Large Language Models (LLMs) has opened new opportunities in data science, yet their practical deployment is often constrained by the challenge of discovering relevant data within large heterogeneous data lakes. Existing methods struggle with this: single-agent systems are quickly overwhelmed by large, heterogeneous files in the large data lakes, while multi-agent systems designed based on a master-slave paradigm depend on a rigid central controller for task allocation that requires precise knowledge of each sub-agent's capabilities. To address these limitations, we propose a novel multi-agent communication paradigm inspired by the blackboard architecture for traditional AI models. In this framework, a central agent posts requests to a shared blackboard, and autonomous subordinate agents -- either responsible for a partition of the data lake or general information retrieval -- volunteer to respond based on their capabilities. This design improves scalability and flexibility by eliminating the need for a central coordinator to have prior knowledge of all sub-agents' expertise. We evaluate our method on three benchmarks that require explicit data discovery: KramaBench and modified versions of DS-Bench and DA-Code to incorporate data discovery. Experimental results demonstrate that the blackboard architecture substantially outperforms baselines, including RAG and the master-slave multi-agent paradigm, achieving between 13% to 57% relative improvement in end-to-end task success and up to a 9% relative gain in F1 score for data discovery over the best-performing baselines across both proprietary and open-source LLMs. Our findings establish the blackboard paradigm as a scalable and generalizable communication framework for multi-agent systems.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "56",
        "title": "Emergent evaluation hubs in a decentralizing large language model ecosystem",
        "author": [
            "Manuel Cebrian",
            "Tomomi Kito",
            "Raul Castro Fernandez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01286",
        "abstract": "Large language models are proliferating, and so are the benchmarks that serve as their common yardsticks. We ask how the agglomeration patterns of these two layers compare: do they evolve in tandem or diverge? Drawing on two curated proxies for the ecosystem, the Stanford Foundation-Model Ecosystem Graph and the Evidently AI benchmark registry, we find complementary but contrasting dynamics. Model creation has broadened across countries and organizations and diversified in modality, licensing, and access. Benchmark influence, by contrast, displays centralizing patterns: in the inferred benchmark-author-institution network, the top 15% of nodes account for over 80% of high-betweenness paths, three countries produce 83% of benchmark outputs, and the global Gini for inferred benchmark authority reaches 0.89. An agent-based simulation highlights three mechanisms: higher entry of new benchmarks reduces concentration; rapid inflows can temporarily complicate coordination in evaluation; and stronger penalties against over-fitting have limited effect. Taken together, these results suggest that concentrated benchmark influence functions as coordination infrastructure that supports standardization, comparability, and reproducibility amid rising heterogeneity in model production, while also introducing trade-offs such as path dependence, selective visibility, and diminishing discriminative power as leaderboards saturate.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "57",
        "title": "ThinKV: Thought-Adaptive KV Cache Compression for Efficient Reasoning Models",
        "author": [
            "Akshat Ramachandran",
            "Marina Neseem",
            "Charbel Sakr",
            "Rangharajan Venkatesan",
            "Brucek Khailany",
            "Tushar Krishna"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01290",
        "abstract": "The long-output context generation of large reasoning models enables extended chain of thought (CoT) but also drives rapid growth of the key-value (KV) cache, quickly overwhelming GPU memory. To address this challenge, we propose ThinKV, a thought-adaptive KV cache compression framework. ThinKV is based on the observation that attention sparsity reveals distinct thought types with varying importance within the CoT. It applies a hybrid quantization-eviction strategy, assigning token precision by thought importance and progressively evicting tokens from less critical thoughts as reasoning trajectories evolve. Furthermore, to implement ThinKV, we design a kernel that extends PagedAttention to enable efficient reuse of evicted tokens' memory slots, eliminating compaction overheads. Extensive experiments on DeepSeek-R1-Distill, GPT-OSS, and NVIDIA AceReason across mathematics and coding benchmarks show that ThinKV achieves near-lossless accuracy with less than 5% of the original KV cache, while improving performance with up to 5.8x higher inference throughput over state-of-the-art baselines.",
        "tags": [
            "CoT",
            "DeepSeek",
            "GPT"
        ]
    },
    {
        "id": "58",
        "title": "The Social Laboratory: A Psychometric Framework for Multi-Agent LLM Evaluation",
        "author": [
            "Zarreen Reza"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01295",
        "abstract": "As Large Language Models (LLMs) transition from static tools to autonomous agents, traditional evaluation benchmarks that measure performance on downstream tasks are becoming insufficient. These methods fail to capture the emergent social and cognitive dynamics that arise when agents communicate, persuade, and collaborate in interactive environments. To address this gap, we introduce a novel evaluation framework that uses multi-agent debate as a controlled \"social laboratory\" to discover and quantify these behaviors. In our framework, LLM-based agents, instantiated with distinct personas and incentives, deliberate on a wide range of challenging topics under the supervision of an LLM moderator. Our analysis, enabled by a new suite of psychometric and semantic metrics, reveals several key findings. Across hundreds of debates, we uncover a powerful and robust emergent tendency for agents to seek consensus, consistently reaching high semantic agreement ({\\mu} > 0.88) even without explicit instruction and across sensitive topics. We show that assigned personas induce stable, measurable psychometric profiles, particularly in cognitive effort, and that the moderators persona can significantly alter debate outcomes by structuring the environment, a key finding for external AI alignment. This work provides a blueprint for a new class of dynamic, psychometrically grounded evaluation protocols designed for the agentic setting, offering a crucial methodology for understanding and shaping the social behaviors of the next generation of AI agents. We have released the code and results at https://github.com/znreza/multi-agent-LLM-eval-for-debate.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "59",
        "title": "SimCity: Multi-Agent Urban Development Simulation with Rich Interactions",
        "author": [
            "Yeqi Feng",
            "Yucheng Lu",
            "Hongyu Su",
            "Tianxing He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01297",
        "abstract": "Large Language Models (LLMs) open new possibilities for constructing realistic and interpretable macroeconomic simulations. We present SimCity, a multi-agent framework that leverages LLMs to model an interpretable macroeconomic system with heterogeneous agents and rich interactions. Unlike classical equilibrium models that limit heterogeneity for tractability, or traditional agent-based models (ABMs) that rely on hand-crafted decision rules, SimCity enables flexible, adaptive behavior with transparent natural-language reasoning. Within SimCity, four core agent types (households, firms, a central bank, and a government) deliberate and participate in a frictional labor market, a heterogeneous goods market, and a financial market. Furthermore, a Vision-Language Model (VLM) determines the geographic placement of new firms and renders a mapped virtual city, allowing us to study both macroeconomic regularities and urban expansion dynamics within a unified environment. To evaluate the framework, we compile a checklist of canonical macroeconomic phenomena, including price elasticity of demand, Engel's Law, Okun's Law, the Phillips Curve, and the Beveridge Curve, and show that SimCity naturally reproduces these empirical patterns while remaining robust across simulation runs.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "60",
        "title": "Agentic Jigsaw Interaction Learning for Enhancing Visual Perception and Reasoning in Vision-Language Models",
        "author": [
            "Yu Zeng",
            "Wenxuan Huang",
            "Shiting Huang",
            "Xikun Bao",
            "Yukun Qi",
            "Yiming Zhao",
            "Qiuchen Wang",
            "Lin Chen",
            "Zehui Chen",
            "Huaian Chen",
            "Wanli Ouyang",
            "Feng Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01304",
        "abstract": "Although current large Vision-Language Models (VLMs) have advanced in multimodal understanding and reasoning, their fundamental perceptual and reasoning abilities remain limited. Specifically, even on simple jigsaw tasks, existing VLMs perform near randomly, revealing deficiencies in core perception and reasoning capabilities. While high-quality vision-language data can enhance these capabilities, its scarcity and limited scalability impose significant constraints. To address this, we propose AGILE, an Agentic jiGsaw Interaction Learning for Enhancing visual perception and reasoning in VLMs. AGILE formulates jigsaw solving as an interactive process, enabling the model to progressively engage with the environment. At each step, the model generates executable code to perform an action based on the current state, while the environment provides fine-grained visual feedback to guide task completion. Through this iterative cycle of observation and interaction, the model incrementally improves its perceptual and reasoning capabilities via exploration and feedback. Experimental results show that AGILE not only substantially boosts performance on jigsaw tasks of varying complexity (e.g., increasing accuracy from 9.5% to 82.8% under the 2 $\\times$ 2 setting) but also demonstrates strong generalization across 9 general vision tasks, achieving an average improvement of 3.1%. These results indicate notable enhancements in both perceptual and reasoning abilities. This work opens a new avenue for advancing reasoning and generalization in multimodal models and provides an efficient, scalable solution to the scarcity of multimodal reinforcement learning data. The code and datasets is available at https://github.com/yuzeng0-0/AGILE .",
        "tags": [
            "RL",
            "VLM"
        ]
    },
    {
        "id": "61",
        "title": "HiSpec: Hierarchical Speculative Decoding for LLMs",
        "author": [
            "Avinash Kumar",
            "Sujay Sanghavi",
            "Poulami Das"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01336",
        "abstract": "Speculative decoding accelerates LLM inference by using a smaller draft model to speculate tokens that a larger target model verifies. Verification is often the bottleneck (e.g. verification is $4\\times$ slower than token generation when a 3B model speculates for a 70B target model), but most prior works focus only on accelerating drafting. $\\textit{``Intermediate\"}$ verification reduces verification time by discarding inaccurate draft tokens early, but existing methods incur substantial training overheads in incorporating the intermediate verifier, increase the memory footprint to orchestrate the intermediate verification step, and compromise accuracy by relying on approximate heuristics.\nWe propose $\\underline{\\textit{Hi}}\\textit{erarchical }\\underline{\\textit{Spec}}\\textit{ulative Decoding (HiSpec)}$, a framework for high-throughput speculative decoding that exploits $\\textit{early-exit (EE) models}$ for low-overhead intermediate verification. EE models allow tokens to exit early by skipping layer traversal and are explicitly trained so that hidden states at selected layers can be interpreted, making them uniquely suited for intermediate verification without drastically increasing compute and memory overheads. To improve resource-efficiency even further, we design a methodology that enables HiSpec to re-use key-value caches and hidden states between the draft, intermediate verifier, and target models. To maintain accuracy, HiSpec periodically validates the draft tokens accepted by the intermediate verifier against the target model. Our evaluations using various representative benchmarks and models show that HiSpec improves throughput by 1.28$\\times$ on average and by up to 2.01$\\times$ compared to the baseline single-layer speculation without compromising accuracy.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "62",
        "title": "LVTINO: LAtent Video consisTency INverse sOlver for High Definition Video Restoration",
        "author": [
            "Alessio Spagnoletti",
            "AndrÃ©s Almansa",
            "Marcelo Pereyra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01339",
        "abstract": "Computational imaging methods increasingly rely on powerful generative diffusion models to tackle challenging image restoration tasks. In particular, state-of-the-art zero-shot image inverse solvers leverage distilled text-to-image latent diffusion models (LDMs) to achieve unprecedented accuracy and perceptual quality with high computational efficiency. However, extending these advances to high-definition video restoration remains a significant challenge, due to the need to recover fine spatial detail while capturing subtle temporal dependencies. Consequently, methods that naively apply image-based LDM priors on a frame-by-frame basis often result in temporally inconsistent reconstructions. We address this challenge by leveraging recent advances in Video Consistency Models (VCMs), which distill video latent diffusion models into fast generators that explicitly capture temporal causality. Building on this foundation, we propose LVTINO, the first zero-shot or plug-and-play inverse solver for high definition video restoration with priors encoded by VCMs. Our conditioning mechanism bypasses the need for automatic differentiation and achieves state-of-the-art video reconstruction quality with only a few neural function evaluations, while ensuring strong measurement consistency and smooth temporal transitions across frames. Extensive experiments on a diverse set of video inverse problems show significant perceptual improvements over current state-of-the-art methods that apply image LDMs frame by frame, establishing a new benchmark in both reconstruction fidelity and computational efficiency.",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "LDMs",
            "Text-to-Image"
        ]
    },
    {
        "id": "63",
        "title": "Image Generation Based on Image Style Extraction",
        "author": [
            "Shuochen Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01347",
        "abstract": "Image generation based on text-to-image generation models is a task with practical application scenarios that fine-grained styles cannot be precisely described and controlled in natural language, while the guidance information of stylized reference images is difficult to be directly aligned with the textual conditions of traditional textual guidance generation. This study focuses on how to maximize the generative capability of the pretrained generative model, by obtaining fine-grained stylistic representations from a single given stylistic reference image, and injecting the stylistic representations into the generative body without changing the structural framework of the downstream generative model, so as to achieve fine-grained controlled stylized image generation. In this study, we propose a three-stage training style extraction-based image generation method, which uses a style encoder and a style projection layer to align the style representations with the textual representations to realize fine-grained textual cue-based style guide generation. In addition, this study constructs the Style30k-captions dataset, whose samples contain a triad of images, style labels, and text descriptions, to train the style encoder and style projection layer in this experiment.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "64",
        "title": "MEMTRACK: Evaluating Long-Term Memory and State Tracking in Multi-Platform Dynamic Agent Environments",
        "author": [
            "Darshan Deshpande",
            "Varun Gangal",
            "Hersh Mehta",
            "Anand Kannappan",
            "Rebecca Qian",
            "Peng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01353",
        "abstract": "Recent works on context and memory benchmarking have primarily focused on conversational instances but the need for evaluating memory in dynamic enterprise environments is crucial for its effective application. We introduce MEMTRACK, a benchmark designed to evaluate long-term memory and state tracking in multi-platform agent environments. MEMTRACK models realistic organizational workflows by integrating asynchronous events across multiple communication and productivity platforms such as Slack, Linear and Git. Each benchmark instance provides a chronologically platform-interleaved timeline, with noisy, conflicting, cross-referring information as well as potential codebase/file-system comprehension and exploration. Consequently, our benchmark tests memory capabilities such as acquistion, selection and conflict resolution. We curate the MEMTRACK dataset through both manual expert driven design and scalable agent based synthesis, generating ecologically valid scenarios grounded in real world software development processes. We introduce pertinent metrics for Correctness, Efficiency, and Redundancy that capture the effectiveness of memory mechanisms beyond simple QA performance. Experiments across SoTA LLMs and memory backends reveal challenges in utilizing memory across long horizons, handling cross-platform dependencies, and resolving contradictions. Notably, the best performing GPT-5 model only achieves a 60\\% Correctness score on MEMTRACK. This work provides an extensible framework for advancing evaluation research for memory-augmented agents, beyond existing focus on conversational setups, and sets the stage for multi-agent, multi-platform memory benchmarking in complex organizational settings",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "65",
        "title": "Safe Motion Planning and Control Using Predictive and Adaptive Barrier Methods for Autonomous Surface Vessels",
        "author": [
            "Alejandro Gonzalez-Garcia",
            "Wei Xiao",
            "Wei Wang",
            "Alejandro Astudillo",
            "Wilm DecrÃ©",
            "Jan Swevers",
            "Carlo Ratti",
            "Daniela Rus"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01357",
        "abstract": "Safe motion planning is essential for autonomous vessel operations, especially in challenging spaces such as narrow inland waterways. However, conventional motion planning approaches are often computationally intensive or overly conservative. This paper proposes a safe motion planning strategy combining Model Predictive Control (MPC) and Control Barrier Functions (CBFs). We introduce a time-varying inflated ellipse obstacle representation, where the inflation radius is adjusted depending on the relative position and attitude between the vessel and the obstacle. The proposed adaptive inflation reduces the conservativeness of the controller compared to traditional fixed-ellipsoid obstacle formulations. The MPC solution provides an approximate motion plan, and high-order CBFs ensure the vessel's safety using the varying inflation radius. Simulation and real-world experiments demonstrate that the proposed strategy enables the fully-actuated autonomous robot vessel to navigate through narrow spaces in real time and resolve potential deadlocks, all while ensuring safety.",
        "tags": [
            "MPC",
            "Robotics"
        ]
    },
    {
        "id": "66",
        "title": "Breaking the Code: Security Assessment of AI Code Agents Through Systematic Jailbreaking Attacks",
        "author": [
            "Shoumik Saha",
            "Jifan Chen",
            "Sam Mayers",
            "Sanjay Krishna Gouda",
            "Zijian Wang",
            "Varun Kumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01359",
        "abstract": "Code-capable large language model (LLM) agents are increasingly embedded into software engineering workflows where they can read, write, and execute code, raising the stakes of safety-bypass (\"jailbreak\") attacks beyond text-only settings. Prior evaluations emphasize refusal or harmful-text detection, leaving open whether agents actually compile and run malicious programs. We present JAWS-BENCH (Jailbreaks Across WorkSpaces), a benchmark spanning three escalating workspace regimes that mirror attacker capability: empty (JAWS-0), single-file (JAWS-1), and multi-file (JAWS-M). We pair this with a hierarchical, executable-aware Judge Framework that tests (i) compliance, (ii) attack success, (iii) syntactic correctness, and (iv) runtime executability, moving beyond refusal to measure deployable harm. Using seven LLMs from five families as backends, we find that under prompt-only conditions in JAWS-0, code agents accept 61% of attacks on average; 58% are harmful, 52% parse, and 27% run end-to-end. Moving to single-file regime in JAWS-1 drives compliance to ~ 100% for capable models and yields a mean ASR (Attack Success Rate) ~ 71%; the multi-file regime (JAWS-M) raises mean ASR to ~ 75%, with 32% instantly deployable attack code. Across models, wrapping an LLM in an agent substantially increases vulnerability -- ASR raises by 1.6x -- because initial refusals are frequently overturned during later planning/tool-use steps. Category-level analyses identify which attack classes are most vulnerable and most readily deployable, while others exhibit large execution gaps. These findings motivate execution-aware defenses, code-contextual safety filters, and mechanisms that preserve refusal decisions throughout the agent's multi-step reasoning and tool use.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "67",
        "title": "Retrieval-Augmented Framework for LLM-Based Clinical Decision Support",
        "author": [
            "Leon Garza",
            "Anantaa Kotal",
            "Michael A. Grasso",
            "Emre Umucu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01363",
        "abstract": "The increasing complexity of clinical decision-making, alongside the rapid expansion of electronic health records (EHR), presents both opportunities and challenges for delivering data-informed care. This paper proposes a clinical decision support system powered by Large Language Models (LLMs) to assist prescribing clinicians. The system generates therapeutic suggestions by analyzing historical EHR data, including patient demographics, presenting complaints, clinical symptoms, diagnostic information, and treatment histories. The framework integrates natural language processing with structured clinical inputs to produce contextually relevant recommendations. Rather than replacing clinician judgment, it is designed to augment decision-making by retrieving and synthesizing precedent cases with comparable characteristics, drawing on local datasets or federated sources where applicable. At its core, the system employs a retrieval-augmented generation (RAG) pipeline that harmonizes unstructured narratives and codified data to support LLM-based inference. We outline the system's technical components, including representation representation alignment and generation strategies. Preliminary evaluations, conducted with de-identified and synthetic clinical datasets, examine the clinical plausibility and consistency of the model's outputs. Early findings suggest that LLM-based tools may provide valuable decision support in prescribing workflows when appropriately constrained and rigorously validated. This work represents an initial step toward integration of generative AI into real-world clinical decision-making with an emphasis on transparency, safety, and alignment with established practices.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "68",
        "title": "RheOFormer: A generative transformer model for simulation of complex fluids and flows",
        "author": [
            "Maedeh Saberi",
            "Amir Barati Farimani",
            "Safa Jamali"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01365",
        "abstract": "The ability to model mechanics of soft materials under flowing conditions is key in designing and engineering processes and materials with targeted properties. This generally requires solution of internal stress tensor, related to the deformation tensor through nonlinear and history-dependent constitutive models. Traditional numerical methods for non-Newtonian fluid dynamics often suffer from prohibitive computational demands and poor scalability to new problem instances. Developments in data-driven methods have mitigated some limitations but still require retraining across varied physical conditions. In this work, we introduce Rheological Operator Transformer (RheOFormer), a generative operator learning method leveraging self-attention to efficiently learn different spatial interactions and features of complex fluid flows. We benchmark RheOFormer across a range of different viscometric and non-viscometric flows with different types of viscoelastic and elastoviscoplastic mechanics in complex domains against ground truth solutions. Our results demonstrate that RheOFormer can accurately learn both scalar and tensorial nonlinear mechanics of different complex fluids and predict the spatio-temporal evolution of their flows, even when trained on limited datasets. Its strong generalization capabilities and computational efficiency establish RheOFormer as a robust neural surrogate for accelerating predictive complex fluid simulations, advancing data-driven experimentation, and enabling real-time process optimization across a wide range of applications.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "69",
        "title": "Is It Thinking or Cheating? Detecting Implicit Reward Hacking by Measuring Reasoning Effort",
        "author": [
            "Xinpeng Wang",
            "Nitish Joshi",
            "Barbara Plank",
            "Rico Angell",
            "He He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01367",
        "abstract": "Reward hacking, where a reasoning model exploits loopholes in a reward function to achieve high rewards without solving the intended task, poses a significant threat. This behavior may be explicit, i.e. verbalized in the model's chain-of-thought (CoT), or implicit, where the CoT appears benign thus bypasses CoT monitors. To detect implicit reward hacking, we propose TRACE (Truncated Reasoning AUC Evaluation). Our key observation is that hacking occurs when exploiting the loophole is easier than solving the actual task. This means that the model is using less `effort' than required to achieve high reward. TRACE quantifies effort by measuring how early a model's reasoning becomes sufficient to pass a verifier. We progressively truncate a model's CoT at various lengths, force the model to answer, and measure the verifier-passing rate at each cutoff. A hacking model, which takes a shortcut, will achieve a high passing rate with only a small fraction of its CoT, yielding a large area under the accuracy-vs-length curve. TRACE achieves over 65% gains over our strongest 72B CoT monitor in math reasoning, and over 30% gains over a 32B monitor in coding. We further show that TRACE can discover unknown loopholes during training. Overall, TRACE offers a scalable unsupervised approach for oversight where current monitoring methods prove ineffective.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "70",
        "title": "SPUS: A Lightweight and Parameter-Efficient Foundation Model for PDEs",
        "author": [
            "Abu Bucker Siddik",
            "Diane Oyen",
            "Alexander Most",
            "Michal Kucer",
            "Ayan Biswas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01370",
        "abstract": "We introduce Small PDE U-Net Solver (SPUS), a compact and efficient foundation model (FM) designed as a unified neural operator for solving a wide range of partial differential equations (PDEs). Unlike existing state-of-the-art PDE FMs-primarily based on large complex transformer architectures with high computational and parameter overhead-SPUS leverages a lightweight residual U-Net-based architecture that has been largely underexplored as a foundation model architecture in this domain. To enable effective learning in this minimalist framework, we utilize a simple yet powerful auto-regressive pretraining strategy which closely replicates the behavior of numerical solvers to learn the underlying physics. SPUS is pretrained on a diverse set of fluid dynamics PDEs and evaluated across 6 challenging unseen downstream PDEs spanning various physical systems. Experimental results demonstrate that SPUS using residual U-Net based architecture achieves state-of-the-art generalization on these downstream tasks while requiring significantly fewer parameters and minimal fine-tuning data, highlighting its potential as a highly parameter-efficient FM for solving diverse PDE systems.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "71",
        "title": "Fine-tuning with RAG for Improving LLM Learning of New Skills",
        "author": [
            "Humaid Ibrahim",
            "Nikolai Rozanov",
            "Marek Rei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01375",
        "abstract": "Large language model (LLM) agents deployed for multi-step tasks frequently fail in predictable ways: attempting actions with unmet preconditions, issuing redundant commands, or mishandling environment constraints. While retrieval-augmented generation (RAG) can improve performance by providing runtime guidance, it requires maintaining external knowledge databases and adds computational overhead at every deployment. We propose a simple pipeline that converts inference-time retrieval into learned competence through distillation. Our approach: (1) extracts compact, reusable hints from agent failures, (2) uses these hints to generate improved teacher trajectories via one-shot retrieval at episode start, and (3) trains student models on these trajectories with hint strings removed, forcing internalization rather than memorization. Across two interactive benchmarks, ALFWorld (household tasks) and WebShop (online shopping), distilled students consistently outperform baseline agents, achieving up to 91% success on ALFWorld (vs. 79% for baselines) and improving WebShop scores to 72 (vs. 61 for baselines), while using 10-60% fewer tokens than retrieval-augmented teachers depending on the environment. The approach generalizes across model scales (7B/14B parameters) and agent architectures (ReAct/StateAct), demonstrating that retrieval benefits can be effectively internalized through targeted fine-tuning without permanent runtime dependencies.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "72",
        "title": "Selective Underfitting in Diffusion Models",
        "author": [
            "Kiwhan Song",
            "Jaeyeon Kim",
            "Sitan Chen",
            "Yilun Du",
            "Sham Kakade",
            "Vincent Sitzmann"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01378",
        "abstract": "Diffusion models have emerged as the principal paradigm for generative modeling across various domains. During training, they learn the score function, which in turn is used to generate samples at inference. They raise a basic yet unsolved question: which score do they actually learn? In principle, a diffusion model that matches the empirical score in the entire data space would simply reproduce the training data, failing to generate novel samples. Recent work addresses this question by arguing that diffusion models underfit the empirical score due to training-time inductive biases. In this work, we refine this perspective, introducing the notion of selective underfitting: instead of underfitting the score everywhere, better diffusion models more accurately approximate the score in certain regions of input space, while underfitting it in others. We characterize these regions and design empirical interventions to validate our perspective. Our results establish that selective underfitting is essential for understanding diffusion models, yielding new, testable insights into their generalization and generative performance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "73",
        "title": "Beyond Single LLMs: Enhanced Code Generation via Multi-Stage Performance-Guided LLM Orchestration",
        "author": [
            "Huashan Chen",
            "Zhenyu Qi",
            "Haotang Li",
            "Hong Chen",
            "Jinfu Chen",
            "Kebin Peng",
            "In Kee Kim",
            "Kyu Hyung Lee",
            "Sen He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01379",
        "abstract": "While Large Language Models (LLMs) have become the predominant paradigm for automated code generation, current single-model approaches fundamentally ignore the heterogeneous computational strengths that different models exhibit across programming languages, algorithmic domains, and development stages. This paper challenges the single-model convention by introducing a multi-stage, performance-guided orchestration framework that dynamically routes coding tasks to the most suitable LLMs within a structured generate-fix-refine workflow. Our approach is grounded in a comprehensive empirical study of 17 state-of-the-art LLMs across five programming languages (Python, Java, C++, Go, and Rust) using HumanEval-X benchmark. The study, which evaluates both functional correctness and runtime performance metrics (execution time, mean/max memory utilization, and CPU efficiency), reveals pronounced performance heterogeneity by language, development stage, and problem category. Guided by these empirical insights, we present PerfOrch, an LLM agent that orchestrates top-performing LLMs for each task context through stage-wise validation and rollback mechanisms. Without requiring model fine-tuning, PerfOrch achieves substantial improvements over strong single-model baselines: average correctness rates of 96.22% and 91.37% on HumanEval-X and EffiBench-X respectively, surpassing GPT-4o's 78.66% and 49.11%. Beyond correctness gains, the framework delivers consistent performance optimizations, improving execution time for 58.76% of problems with median speedups ranging from 17.67% to 27.66% across languages on two benchmarks. The framework's plug-and-play architecture ensures practical scalability, allowing new LLMs to be profiled and integrated seamlessly, thereby offering a paradigm for production-grade automated software engineering that adapts to the rapidly evolving generative AI landscape.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "74",
        "title": "A Stochastic Framework for Continuous-Time State Estimation of Continuum Robots",
        "author": [
            "Spencer Teetaert",
            "Sven Lilge",
            "Jessica Burgner-Kahrs",
            "Timothy D. Barfoot"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01381",
        "abstract": "State estimation techniques for continuum robots (CRs) typically involve using computationally complex dynamic models, simplistic shape approximations, or are limited to quasi-static methods. These limitations can be sensitive to unmodelled disturbances acting on the robot. Inspired by a factor-graph optimization paradigm, this work introduces a continuous-time stochastic state estimation framework for continuum robots. We introduce factors based on continuous-time kinematics that are corrupted by a white noise Gaussian process (GP). By using a simple robot model paired with high-rate sensing, we show adaptability to unmodelled external forces and data dropout. The result contains an estimate of the mean and covariance for the robot's pose, velocity, and strain, each of which can be interpolated continuously in time or space. This same interpolation scheme can be used during estimation, allowing for inclusion of measurements on states that are not explicitly estimated. Our method's inherent sparsity leads to a linear solve complexity with respect to time and interpolation queries in constant time. We demonstrate our method on a CR with gyroscope and pose sensors, highlighting its versatility in real-world systems.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "75",
        "title": "Fine-Tuning Masked Diffusion for Provable Self-Correction",
        "author": [
            "Jaeyeon Kim",
            "Seunggeun Kim",
            "Taekyun Lee",
            "David Z. Pan",
            "Hyeji Kim",
            "Sham Kakade",
            "Sitan Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01384",
        "abstract": "A natural desideratum for generative models is self-correction--detecting and revising low-quality tokens at inference. While Masked Diffusion Models (MDMs) have emerged as a promising approach for generative modeling in discrete spaces, their capacity for self-correction remains poorly understood. Prior attempts to incorporate self-correction into MDMs either require overhauling MDM architectures/training or rely on imprecise proxies for token quality, limiting their applicability. Motivated by this, we introduce PRISM--Plug-in Remasking for Inference-time Self-correction of Masked Diffusions--a lightweight, model-agnostic approach that applies to any pretrained MDM. Theoretically, PRISM defines a self-correction loss that provably learns per-token quality scores, without RL or a verifier. These quality scores are computed in the same forward pass with MDM and used to detect low-quality tokens. Empirically, PRISM advances MDM inference across domains and scales: Sudoku; unconditional text (170M); and code with LLaDA (8B).",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "76",
        "title": "VENTURA: Adapting Image Diffusion Models for Unified Task Conditioned Navigation",
        "author": [
            "Arthur Zhang",
            "Xiangyun Meng",
            "Luca Calliari",
            "Dong-Ki Kim",
            "Shayegan Omidshafiei",
            "Joydeep Biswas",
            "Ali Agha",
            "Amirreza Shaban"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01388",
        "abstract": "Robots must adapt to diverse human instructions and operate safely in unstructured, open-world environments. Recent Vision-Language models (VLMs) offer strong priors for grounding language and perception, but remain difficult to steer for navigation due to differences in action spaces and pretraining objectives that hamper transferability to robotics tasks. Towards addressing this, we introduce VENTURA, a vision-language navigation system that finetunes internet-pretrained image diffusion models for path planning. Instead of directly predicting low-level actions, VENTURA generates a path mask (i.e. a visual plan) in image space that captures fine-grained, context-aware navigation behaviors. A lightweight behavior-cloning policy grounds these visual plans into executable trajectories, yielding an interface that follows natural language instructions to generate diverse robot behaviors. To scale training, we supervise on path masks derived from self-supervised tracking models paired with VLM-augmented captions, avoiding manual pixel-level annotation or highly engineered data collection setups. In extensive real-world evaluations, VENTURA outperforms state-of-the-art foundation model baselines on object reaching, obstacle avoidance, and terrain preference tasks, improving success rates by 33% and reducing collisions by 54% across both seen and unseen scenarios. Notably, we find that VENTURA generalizes to unseen combinations of distinct tasks, revealing emergent compositional capabilities. Videos, code, and additional materials: https://venturapath.github.io",
        "tags": [
            "Diffusion",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "77",
        "title": "INSIGHT: INference-time Sequence Introspection for Generating Help Triggers in Vision-Language-Action Models",
        "author": [
            "Ulas Berk Karli",
            "Ziyao Shangguan",
            "Tesca FItzgerald"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01389",
        "abstract": "Recent Vision-Language-Action (VLA) models show strong generalization capabilities, yet they lack introspective mechanisms for anticipating failures and requesting help from a human supervisor. We present \\textbf{INSIGHT}, a learning framework for leveraging token-level uncertainty signals to predict when a VLA should request help. Using $\\pi_0$-FAST as the underlying model, we extract per-token \\emph{entropy}, \\emph{log-probability}, and Dirichlet-based estimates of \\emph{aleatoric and epistemic uncertainty}, and train compact transformer classifiers to map these sequences to help triggers. We explore supervision regimes for strong or weak supervision, and extensively compare them across in-distribution and out-of-distribution tasks. Our results show a trade-off: strong labels enable models to capture fine-grained uncertainty dynamics for reliable help detection, while weak labels, though noisier, still support competitive introspection when training and evaluation are aligned, offering a scalable path when dense annotation is impractical. Crucially, we find that modeling the temporal evolution of token-level uncertainty signals with transformers provides far greater predictive power than static sequence-level scores. This study provides the first systematic evaluation of uncertainty-based introspection in VLAs, opening future avenues for active learning and for real-time error mitigation through selective human intervention.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "78",
        "title": "TAG-EQA: Text-And-Graph for Event Question Answering via Structured Prompting Strategies",
        "author": [
            "Maithili Kadam",
            "Francis Ferraro"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01391",
        "abstract": "Large language models (LLMs) excel at general language tasks but often struggle with event-based questions-especially those requiring causal or temporal reasoning. We introduce TAG-EQA (Text-And-Graph for Event Question Answering), a prompting framework that injects causal event graphs into LLM inputs by converting structured relations into natural-language statements. TAG-EQA spans nine prompting configurations, combining three strategies (zero-shot, few-shot, chain-of-thought) with three input modalities (text-only, graph-only, text+graph), enabling a systematic analysis of when and how structured knowledge aids inference. On the TORQUESTRA benchmark, TAG-EQA improves accuracy by 5% on average over text-only baselines, with gains up to 12% in zero-shot settings and 18% when graph-augmented CoT prompting is effective. While performance varies by model and configuration, our findings show that causal graphs can enhance event reasoning in LLMs without fine-tuning, offering a flexible way to encode structure in prompt-based QA.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "79",
        "title": "Optimal Stopping vs Best-of-$N$ for Inference Time Optimization",
        "author": [
            "Yusuf Kalayci",
            "Vinod Raman",
            "Shaddin Dughmi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01394",
        "abstract": "Large language model (LLM) generation often requires balancing output quality against inference cost, especially when using multiple generations. We introduce a new framework for inference-time optimization based on the classical Pandora's Box problem. Viewing each generation as opening a costly \"box\" with random reward, we develop algorithms that decide when to stop generating without knowing the underlying reward distribution. Our first contribution is a UCB-style Pandora's Box algorithm, which achieves performance that is provably close to Weitzman's algorithm, the optimal strategy when the distribution is known. We further adapt this method to practical LLM settings by addressing reward scaling across prompts via a Bradley-Terry inspired transformation. This leads to an adaptive inference-time optimization method that normalizes rewards and learns stopping thresholds on the fly. Experiments on the AlpacaFarm and HH-RLHF datasets, using multiple LLM-reward model pairs, show that our adaptive strategy can obtain the same performance as non-adaptive Best-of-N sampling while requiring 15-35 percent fewer generations on average. Our results establish a principled bridge between optimal stopping theory and inference-time scaling, providing both theoretical performance bounds and practical efficiency gains for LLM deployment.",
        "tags": [
            "LLM",
            "RLHF"
        ]
    },
    {
        "id": "80",
        "title": "Automating Data-Driven Modeling and Analysis for Engineering Applications using Large Language Model Agents",
        "author": [
            "Yang Liu",
            "Zaid Abulawi",
            "Abhiram Garimidi",
            "Doyeong Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01398",
        "abstract": "Modern engineering increasingly relies on vast datasets generated by experiments and simulations, driving a growing demand for efficient, reliable, and broadly applicable modeling strategies. There is also heightened interest in developing data-driven approaches, particularly neural network models, for effective prediction and analysis of scientific datasets. Traditional data-driven methods frequently involve extensive manual intervention, limiting their ability to scale effectively and generalize to diverse applications. In this study, we propose an innovative pipeline utilizing Large Language Model (LLM) agents to automate data-driven modeling and analysis, with a particular emphasis on regression tasks. We evaluate two LLM-agent frameworks: a multi-agent system featuring specialized collaborative agents, and a single-agent system based on the Reasoning and Acting (ReAct) paradigm. Both frameworks autonomously handle data preprocessing, neural network development, training, hyperparameter optimization, and uncertainty quantification (UQ). We validate our approach using a critical heat flux (CHF) prediction benchmark, involving approximately 25,000 experimental data points from the OECD/NEA benchmark dataset. Results indicate that our LLM-agent-developed model surpasses traditional CHF lookup tables and delivers predictive accuracy and UQ on par with state-of-the-art Bayesian optimized deep neural network models developed by human experts. These outcomes underscore the significant potential of LLM-based agents to automate complex engineering modeling tasks, greatly reducing human workload while meeting or exceeding existing standards of predictive performance.",
        "tags": [
            "FLUX",
            "LLM"
        ]
    },
    {
        "id": "81",
        "title": "DisCo: Reinforcement with Diversity Constraints for Multi-Human Generation",
        "author": [
            "Shubhankar Borse",
            "Farzad Farhadzadeh",
            "Munawar Hayat",
            "Fatih Porikli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01399",
        "abstract": "State-of-the-art text-to-image models excel at realism but collapse on multi-human prompts - duplicating faces, merging identities, and miscounting individuals. We introduce DisCo (Reinforcement with Diversity Constraints), the first RL-based framework to directly optimize identity diversity in multi-human generation. DisCo fine-tunes flow-matching models via Group-Relative Policy Optimization (GRPO) with a compositional reward that (i) penalizes intra-image facial similarity, (ii) discourages cross-sample identity repetition, (iii) enforces accurate person counts, and (iv) preserves visual fidelity through human preference scores. A single-stage curriculum stabilizes training as complexity scales, requiring no extra annotations. On the DiverseHumans Testset, DisCo achieves 98.6 Unique Face Accuracy and near-perfect Global Identity Spread - surpassing both open-source and proprietary methods (e.g., Gemini, GPT-Image) while maintaining competitive perceptual quality. Our results establish DisCo as a scalable, annotation-free solution that resolves the long-standing identity crisis in generative models and sets a new benchmark for compositional multi-human generation.",
        "tags": [
            "Flow Matching",
            "GPT",
            "GRPO",
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": "82",
        "title": "How Well do Diffusion Policies Learn Kinematic Constraint Manifolds?",
        "author": [
            "Lexi Foland",
            "Thomas Cohn",
            "Adam Wei",
            "Nicholas Pfaff",
            "Boyuan Chen",
            "Russ Tedrake"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01404",
        "abstract": "Diffusion policies have shown impressive results in robot imitation learning, even for tasks that require satisfaction of kinematic equality constraints. However, task performance alone is not a reliable indicator of the policy's ability to precisely learn constraints in the training data. To investigate, we analyze how well diffusion policies discover these manifolds with a case study on a bimanual pick-and-place task that encourages fulfillment of a kinematic constraint for success. We study how three factors affect trained policies: dataset size, dataset quality, and manifold curvature. Our experiments show diffusion policies learn a coarse approximation of the constraint manifold with learning affected negatively by decreases in both dataset size and quality. On the other hand, the curvature of the constraint manifold showed inconclusive correlations with both constraint satisfaction and task success. A hardware evaluation verifies the applicability of our results in the real world. Project website with additional results and visuals: https://diffusion-learns-kinematic.github.io",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "83",
        "title": "Ultra-Efficient Decoding for End-to-End Neural Compression and Reconstruction",
        "author": [
            "Ethan G. Rogers",
            "Cheng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01407",
        "abstract": "Image compression and reconstruction are crucial for various digital applications. While contemporary neural compression methods achieve impressive compression rates, the adoption of such technology has been largely hindered by the complexity and large computational costs of the convolution-based decoders during data reconstruction. To address the decoder bottleneck in neural compression, we develop a new compression-reconstruction framework based on incorporating low-rank representation in an autoencoder with vector quantization. We demonstrated that performing a series of computationally efficient low-rank operations on the learned latent representation of images can efficiently reconstruct the data with high quality. Our approach dramatically reduces the computational overhead in the decoding phase of neural compression/reconstruction, essentially eliminating the decoder compute bottleneck while maintaining high fidelity of image outputs.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "84",
        "title": "OntoLogX: Ontology-Guided Knowledge Graph Extraction from Cybersecurity Logs with Large Language Models",
        "author": [
            "Luca Cotti",
            "Idilio Drago",
            "Anisa Rula",
            "Devis Bianchini",
            "Federico Cerutti"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01409",
        "abstract": "System logs represent a valuable source of Cyber Threat Intelligence (CTI), capturing attacker behaviors, exploited vulnerabilities, and traces of malicious activity. Yet their utility is often limited by lack of structure, semantic inconsistency, and fragmentation across devices and sessions. Extracting actionable CTI from logs therefore requires approaches that can reconcile noisy, heterogeneous data into coherent and interoperable representations. We introduce OntoLogX, an autonomous Artificial Intelligence (AI) agent that leverages Large Language Models (LLMs) to transform raw logs into ontology-grounded Knowledge Graphs (KGs). OntoLogX integrates a lightweight log ontology with Retrieval Augmented Generation (RAG) and iterative correction steps, ensuring that generated KGs are syntactically and semantically valid. Beyond event-level analysis, the system aggregates KGs into sessions and employs a LLM to predict MITRE ATT&CK tactics, linking low-level log evidence to higher-level adversarial objectives. We evaluate OntoLogX on both logs from a public benchmark and a real-world honeypot dataset, demonstrating robust KG generation across multiple KGs backends and accurate mapping of adversarial activity to ATT&CK tactics. Results highlight the benefits of retrieval and correction for precision and recall, the effectiveness of code-oriented models in structured log analysis, and the value of ontology-grounded representations for actionable CTI extraction.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "85",
        "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining",
        "author": [
            "Sipeng Zhang",
            "Longfei Yun",
            "Zilong Wang",
            "Jingbo Shang",
            "Letian Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01427",
        "abstract": "At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively expensive to deploy at scale, while traditional pipelines of classifiers and extractors remain efficient yet brittle and unable to generalize to new tasks. We introduce Falconer, a collaborative framework that combines the agentic reasoning of LLMs with lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act as planners, decomposing user instructions into executable pipelines, and as annotators, generating supervision to train small proxies. The framework unifies classification and extraction into two atomic operations, get label and get span, enabling a single instruction-following model to replace multiple task-specific components. To evaluate the consistency between proxy models incubated by Falconer and annotations provided by humans and large models, we construct new benchmarks covering both planning and end-to-end execution. Experiments show that Falconer closely matches state-of-the-art LLMs in instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x, offering an efficient and scalable foundation for Deep Research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "86",
        "title": "AFFORD2ACT: Affordance-Guided Automatic Keypoint Selection for Generalizable and Lightweight Robotic Manipulation",
        "author": [
            "Anukriti Singh",
            "Kasra Torshizi",
            "Khuzema Habib",
            "Kelin Yu",
            "Ruohan Gao",
            "Pratap Tokekar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01433",
        "abstract": "Vision-based robot learning often relies on dense image or point-cloud inputs, which are computationally heavy and entangle irrelevant background features. Existing keypoint-based approaches can focus on manipulation-centric features and be lightweight, but either depend on manual heuristics or task-coupled selection, limiting scalability and semantic understanding. To address this, we propose AFFORD2ACT, an affordance-guided framework that distills a minimal set of semantic 2D keypoints from a text prompt and a single image. AFFORD2ACT follows a three-stage pipeline: affordance filtering, category-level keypoint construction, and transformer-based policy learning with embedded gating to reason about the most relevant keypoints, yielding a compact 38-dimensional state policy that can be trained in 15 minutes, which performs well in real-time without proprioception or dense representations. Across diverse real-world manipulation tasks, AFFORD2ACT consistently improves data efficiency, achieving an 82% success rate on unseen objects, novel categories, backgrounds, and distractors.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "87",
        "title": "Differentiable Skill Optimisation for Powder Manipulation in Laboratory Automation",
        "author": [
            "Minglun Wei",
            "Xintong Yang",
            "Yu-Kun Lai",
            "Ze Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01438",
        "abstract": "Robotic automation is accelerating scientific discovery by reducing manual effort in laboratory workflows. However, precise manipulation of powders remains challenging, particularly in tasks such as transport that demand accuracy and stability. We propose a trajectory optimisation framework for powder transport in laboratory settings, which integrates differentiable physics simulation for accurate modelling of granular dynamics, low-dimensional skill-space parameterisation to reduce optimisation complexity, and a curriculum-based strategy that progressively refines task competence over long horizons. This formulation enables end-to-end optimisation of contact-rich robot trajectories while maintaining stability and convergence efficiency. Experimental results demonstrate that the proposed method achieves superior task success rates and stability compared to the reinforcement learning baseline.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "88",
        "title": "VOGUE: Guiding Exploration with Visual Uncertainty Improves Multimodal Reasoning",
        "author": [
            "Rui Liu",
            "Dian Yu",
            "Tong Zheng",
            "Runpeng Dai",
            "Zongxia Li",
            "Wenhao Yu",
            "Zhenwen Liang",
            "Linfeng Song",
            "Haitao Mi",
            "Pratap Tokekar",
            "Dong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01444",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) improves reasoning in large language models (LLMs) but struggles with exploration, an issue that still persists for multimodal LLMs (MLLMs). Current methods treat the visual input as a fixed, deterministic condition, overlooking a critical source of ambiguity and struggling to build policies robust to plausible visual variations. We introduce $\\textbf{VOGUE (Visual Uncertainty Guided Exploration)}$, a novel method that shifts exploration from the output (text) to the input (visual) space. By treating the image as a stochastic context, VOGUE quantifies the policy's sensitivity to visual perturbations using the symmetric KL divergence between a \"raw\" and \"noisy\" branch, creating a direct signal for uncertainty-aware exploration. This signal shapes the learning objective via an uncertainty-proportional bonus, which, combined with a token-entropy bonus and an annealed sampling schedule, effectively balances exploration and exploitation. Implemented within GRPO on two model scales (Qwen2.5-VL-3B/7B), VOGUE boosts pass@1 accuracy by an average of 2.6% on three visual math benchmarks and 3.7% on three general-domain reasoning benchmarks, while simultaneously increasing pass@4 performance and mitigating the exploration decay commonly observed in RL fine-tuning. Our work shows that grounding exploration in the inherent uncertainty of visual inputs is an effective strategy for improving multimodal reasoning.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "89",
        "title": "GeoSURGE: Geo-localization using Semantic Fusion with Hierarchy of Geographic Embeddings",
        "author": [
            "Angel Daruna",
            "Nicholas Meegan",
            "Han-Pang Chiu",
            "Supun Samarasekera",
            "Rakesh Kumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01448",
        "abstract": "Worldwide visual geo-localization seeks to determine the geographic location of an image anywhere on Earth using only its visual content. Learned representations of geography for visual geo-localization remain an active research topic despite much progress. We formulate geo-localization as aligning the visual representation of the query image with a learned geographic representation. Our novel geographic representation explicitly models the world as a hierarchy of geographic embeddings. Additionally, we introduce an approach to efficiently fuse the appearance features of the query image with its semantic segmentation map, forming a robust visual representation. Our main experiments demonstrate improved all-time bests in 22 out of 25 metrics measured across five benchmark datasets compared to prior state-of-the-art (SOTA) methods and recent Large Vision-Language Models (LVLMs). Additional ablation studies support the claim that these gains are primarily driven by the combination of geographic and visual representations.",
        "tags": [
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "90",
        "title": "Local Linear Attention: An Optimal Interpolation of Linear and Softmax Attention For Test-Time Regression",
        "author": [
            "Yifei Zuo",
            "Yutong Yin",
            "Zhichen Zeng",
            "Ang Li",
            "Banghua Zhu",
            "Zhaoran Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01450",
        "abstract": "Transformer architectures have achieved remarkable success in various domains. While efficient alternatives to Softmax Attention have been widely studied, the search for more expressive mechanisms grounded in theoretical insight-even at greater computational cost-has been relatively underexplored. In this work, we bridge this gap by proposing Local Linear Attention (LLA), a novel attention mechanism derived from nonparametric statistics through the lens of test-time regression. First, we show that LLA offers theoretical advantages over Linear and Softmax Attention for associative memory via a bias-variance trade-off analysis. Next, we address its computational challenges and propose two memory-efficient primitives to tackle the $\\Theta(n^2 d)$ and $\\Theta(n d^2)$ complexity. We then introduce FlashLLA, a hardware-efficient, blockwise algorithm that enables scalable and parallel computation on modern accelerators. In addition, we implement and profile a customized inference kernel that significantly reduces memory overheads. Finally, we empirically validate the advantages and limitations of LLA on test-time regression, in-context regression, associative recall and state tracking tasks. Experiment results demonstrate that LLA effectively adapts to non-stationarity, outperforming strong baselines in test-time training and in-context learning, and exhibiting promising evidence for its scalability and applicability in large-scale models. Code is available at https://github.com/Yifei-Zuo/Flash-LLA.",
        "tags": [
            "TTT",
            "Transformer"
        ]
    },
    {
        "id": "91",
        "title": "Data Selection for Fine-tuning Vision Language Models via Cross Modal Alignment Trajectories",
        "author": [
            "Nilay Naharas",
            "Dang Nguyen",
            "Nesihan Bulut",
            "Mohammadhossein Bateni",
            "Vahab Mirrokni",
            "Baharan Mirzasoleiman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01454",
        "abstract": "Data-efficient learning aims to eliminate redundancy in large training datasets by training models on smaller subsets of the most informative examples. While data selection has been extensively explored for vision models and large language models (LLMs), it remains underexplored for Large Vision-Language Models (LVLMs). Notably, none of existing methods can outperform random selection at different subset sizes. In this work, we propose the first principled method for data-efficient instruction tuning of LVLMs. We prove that examples with similar cross-modal attention matrices during instruction tuning have similar gradients. Thus, they influence model parameters in a similar manner and convey the same information to the model during training. Building on this insight, we propose XMAS, which clusters examples based on the trajectories of the top singular values of their attention matrices obtained from fine-tuning a small proxy LVLM. By sampling a balanced subset from these clusters, XMAS effectively removes redundancy in large-scale LVLM training data. Extensive experiments show that XMAS can discard 50% of the LLaVA-665k dataset and 85% of the Vision-Flan dataset while fully preserving performance of LLaVA-1.5-7B on 10 downstream benchmarks and speeding up its training by 1.2x. This is 30% more data reduction compared to the best baseline for LLaVA-665k. The project's website can be found at https://bigml-cs-ucla.github.io/XMAS-project-page/.",
        "tags": [
            "LLM",
            "LLaVA",
            "VLM"
        ]
    },
    {
        "id": "92",
        "title": "SCOPED: Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion",
        "author": [
            "Brett Barkley",
            "Preston Culbertson",
            "David Fridovich-Keil"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01456",
        "abstract": "Out-of-distribution (OOD) detection is essential for reliable deployment of machine learning systems in vision, robotics, reinforcement learning, and beyond. We introduce Score-Curvature Out-of-distribution Proximity Evaluator for Diffusion (SCOPED), a fast and general-purpose OOD detection method for diffusion models that reduces the number of forward passes on the trained model by an order of magnitude compared to prior methods, outperforming most diffusion-based baselines and closely approaching the accuracy of the strongest ones. SCOPED is computed from a single diffusion model trained once on a diverse dataset, and combines the Jacobian trace and squared norm of the model's score function into a single test statistic. Rather than thresholding on a fixed value, we estimate the in-distribution density of SCOPED scores using kernel density estimation, enabling a flexible, unsupervised test that, in the simplest case, only requires a single forward pass and one Jacobian-vector product (JVP), made efficient by Hutchinson's trace estimator. On four vision benchmarks, SCOPED achieves competitive or state-of-the-art precision-recall scores despite its low computational cost. The same method generalizes to robotic control tasks with shared state and action spaces, identifying distribution shifts across reward functions and training regimes. These results position SCOPED as a practical foundation for fast and reliable OOD detection in real-world domains, including perceptual artifacts in vision, outlier detection in autoregressive models, exploration in reinforcement learning, and dataset curation for unsupervised training.",
        "tags": [
            "Detection",
            "Diffusion",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "93",
        "title": "Fixing That Free Lunch: When, Where, and Why Synthetic Data Fails in Model-Based Policy Optimization",
        "author": [
            "Brett Barkley",
            "David Fridovich-Keil"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01457",
        "abstract": "Synthetic data is a core component of data-efficient Dyna-style model-based reinforcement learning, yet it can also degrade performance. We study when it helps, where it fails, and why, and we show that addressing the resulting failure modes enables policy improvement that was previously unattainable. We focus on Model-Based Policy Optimization (MBPO), which performs actor and critic updates using synthetic action counterfactuals. Despite reports of strong and generalizable sample-efficiency gains in OpenAI Gym, recent work shows that MBPO often underperforms its model-free counterpart, Soft Actor-Critic (SAC), in the DeepMind Control Suite (DMC). Although both suites involve continuous control with proprioceptive robots, this shift leads to sharp performance losses across seven challenging DMC tasks, with MBPO failing in cases where claims of generalization from Gym would imply success. This reveals how environment-specific assumptions can become implicitly encoded into algorithm design when evaluation is limited. We identify two coupled issues behind these failures: scale mismatches between dynamics and reward models that induce critic underestimation and hinder policy improvement during model-policy coevolution, and a poor choice of target representation that inflates model variance and produces error-prone rollouts. Addressing these failure modes enables policy improvement where none was previously possible, allowing MBPO to outperform SAC in five of seven tasks while preserving the strong performance previously reported in OpenAI Gym. Rather than aiming only for incremental average gains, we hope our findings motivate the community to develop taxonomies that tie MDP task- and environment-level structure to algorithmic failure modes, pursue unified solutions where possible, and clarify how benchmark choices ultimately shape the conditions under which algorithms generalize.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "94",
        "title": "How Well Can Preference Optimization Generalize Under Noisy Feedback?",
        "author": [
            "Shawn Im",
            "Yixuan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01458",
        "abstract": "As large language models (LLMs) advance their capabilities, aligning these models with human preferences has become crucial. Preference optimization, which trains models to distinguish between preferred and non-preferred responses based on human feedback, has become a crucial component for aligning LLMs. However, most existing works assume noise-free feedback, which is unrealistic due to the inherent errors and inconsistencies in human judgments. This paper addresses the impact of noisy feedback on preference optimization, providing generalization guarantees under these conditions. In particular, we consider noise models that correspond to common real-world sources of noise, such as mislabeling and uncertainty. Unlike traditional analyses that assume convergence, our work focuses on finite-step preference optimization, offering new insights that are more aligned with practical LLM training. We describe how generalization decays with different types of noise across levels of noise rates based on the preference data distribution and number of samples. Our analysis for noisy preference learning applies to a broad family of preference optimization losses such as DPO, IPO, SLiC, etc. Empirical validation on contemporary LLMs confirms the practical relevance of our findings, offering valuable insights for developing AI systems that align with human preferences.",
        "tags": [
            "DPO",
            "LLM"
        ]
    },
    {
        "id": "95",
        "title": "LSPO: Length-aware Dynamic Sampling for Policy Optimization in LLM Reasoning",
        "author": [
            "Weizhe Chen",
            "Sven Koenig",
            "Bistra Dilkina"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01459",
        "abstract": "Since the release of Deepseek-R1, reinforcement learning with verifiable rewards (RLVR) has become a central approach for training large language models (LLMs) on reasoning tasks. Recent work has largely focused on modifying loss functions to make RLVR more efficient and effective. In this paper, motivated by studies of overthinking in LLMs, we propose Length-aware Sampling for Policy Optimization (LSPO), a novel meta-RLVR algorithm that dynamically selects training data at each step based on the average response length. We evaluate LSPO across multiple base models and datasets, demonstrating that it consistently improves learning effectiveness. In addition, we conduct a detailed ablation study to examine alternative ways of incorporating length signals into dynamic sampling, offering further insights and highlighting promising directions for future research.",
        "tags": [
            "DeepSeek",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "96",
        "title": "The Three Regimes of Offline-to-Online Reinforcement Learning",
        "author": [
            "Lu Li",
            "Tianwei Ni",
            "Yihao Sun",
            "Pierre-Luc Bacon"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01460",
        "abstract": "Offline-to-online reinforcement learning (RL) has emerged as a practical paradigm that leverages offline datasets for pretraining and online interactions for fine-tuning. However, its empirical behavior is highly inconsistent: design choices of online-fine tuning that work well in one setting can fail completely in another. We propose a stability--plasticity principle that can explain this inconsistency: we should preserve the knowledge of pretrained policy or offline dataset during online fine-tuning, whichever is better, while maintaining sufficient plasticity. This perspective identifies three regimes of online fine-tuning, each requiring distinct stability properties. We validate this framework through a large-scale empirical study, finding that the results strongly align with its predictions in 45 of 63 cases. This work provides a principled framework for guiding design choices in offline-to-online RL based on the relative performance of the offline dataset and the pretrained policy.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "97",
        "title": "A-VERT: Agnostic Verification with Embedding Ranking Targets",
        "author": [
            "NicolÃ¡s Aguirre",
            "Ramiro Caso",
            "Ramiro RodrÃ­guez Colmeiro",
            "Mauro Santelli",
            "JoaquÃ­n Toranzo CalderÃ³n"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01469",
        "abstract": "The automatic evaluation of Language Model (LM) responses is a critical piece in the development of benchmarks and metrics, both for model training and quality assessment of production model endpoints. The current approaches to response classification relies on methods that are too expensive (i.e. LLM-as-a-Judge) or that are far from real-world conditions (string-matching, logprob). In this paper, a structure-free evaluation method is presented. The method makes use of semantic embedding distances to match target candidates with arbitrary LM-generated text, resulting in a robust classification of the response at a relatively low compute cost (embedding models of less than $10B$ parameters). The results show a regression score of ~0.97 and an accuracy of ~96% against human annotators, tested over 3 data sets and 3 different LM architectures.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "98",
        "title": "Extracting O*NET Features from the NLx Corpus to Build Public Use Aggregate Labor Market Data",
        "author": [
            "Stephen Meisenbacher",
            "Svetlozar Nestorov",
            "Peter Norlander"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01470",
        "abstract": "Data from online job postings are difficult to access and are not built in a standard or transparent manner. Data included in the standard taxonomy and occupational information database (O*NET) are updated infrequently and based on small survey samples. We adopt O*NET as a framework for building natural language processing tools that extract structured information from job postings. We publish the Job Ad Analysis Toolkit (JAAT), a collection of open-source tools built for this purpose, and demonstrate its reliability and accuracy in out-of-sample and LLM-as-a-Judge testing. We extract more than 10 billion data points from more than 155 million online job ads provided by the National Labor Exchange (NLx) Research Hub, including O*NET tasks, occupation codes, tools, and technologies, as well as wages, skills, industry, and more features. We describe the construction of a dataset of occupation, state, and industry level features aggregated by monthly active jobs from 2015 - 2025. We illustrate the potential for research and future uses in education and workforce development.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "99",
        "title": "PEL-NAS: Search Space Partitioned Architecture Prompt Co-Evolutionary LLM-driven Hardware-Aware Neural Architecture Search",
        "author": [
            "Hengyi Zhu",
            "Grace Li Zhang",
            "Shaoyi Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01472",
        "abstract": "Hardware-Aware Neural Architecture Search (HW-NAS) requires joint optimization of accuracy and latency under device constraints. Traditional supernet-based methods require multiple GPU days per dataset. Large Language Model (LLM)-driven approaches avoid training a large supernet and can provide quick feedback, but we observe an exploration bias: the LLM repeatedly proposes neural network designs within limited search space and fails to discover architectures across different latency ranges in the entire search space. To address this issue, we propose PEL-NAS: a search space Partitioned, architecture prompt co-Evolutionary and LLM-driven Neural Architecture Search that can generate neural networks with high accuracy and low latency with reduced search cost. Our proposed PEL-NAS has three key components: 1) a complexity-driven partitioning engine that divides the search space by complexity to enforce diversity and mitigate exploration bias; 2) an LLM-powered architecture prompt co-evolution operator, in which the LLM first updates a knowledge base of design heuristics based on results from the previous round, then performs a guided evolution algorithm on architectures with prompts that incorporate this knowledge base. Prompts and designs improve together across rounds which avoids random guesswork and improve efficiency; 3) a zero-cost predictor to avoid training a large number of candidates from scratch. Experimental results show that on HW-NAS-Bench, PEL-NAS can achieve overall higher HV, lower IGD, and up to 54% lower latency than baselines at similar accuracy. Meanwhile, the search cost drops from days to minutes compared with traditional supernet baselines.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "100",
        "title": "From keywords to semantics: Perceptions of large language models in data discovery",
        "author": [
            "Maura E Halstead",
            "Mark A. Green",
            "Caroline Jay",
            "Richard Kingston",
            "David Topping",
            "Alexander Singleton"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01473",
        "abstract": "Current approaches to data discovery match keywords between metadata and queries. This matching requires researchers to know the exact wording that other researchers previously used, creating a challenging process that could lead to missing relevant data. Large Language Models (LLMs) could enhance data discovery by removing this requirement and allowing researchers to ask questions with natural language. However, we do not currently know if researchers would accept LLMs for data discovery. Using a human-centered artificial intelligence (HCAI) focus, we ran focus groups (N = 27) to understand researchers' perspectives towards LLMs for data discovery. Our conceptual model shows that the potential benefits are not enough for researchers to use LLMs instead of current technology. Barriers prevent researchers from fully accepting LLMs, but features around transparency could overcome them. Using our model will allow developers to incorporate features that result in an increased acceptance of LLMs for data discovery.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "101",
        "title": "AIReg-Bench: Benchmarking Language Models That Assess AI Regulation Compliance",
        "author": [
            "Bill Marino",
            "Rosco Hunter",
            "Zubair Jamali",
            "Marinos Emmanouil Kalpakos",
            "Mudra Kashyap",
            "Isaiah Hinton",
            "Alexa Hanson",
            "Maahum Nazir",
            "Christoph Schnabl",
            "Felix Steffek",
            "Hongkai Wen",
            "Nicholas D. Lane"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01474",
        "abstract": "As governments move to regulate AI, there is growing interest in using Large Language Models (LLMs) to assess whether or not an AI system complies with a given AI Regulation (AIR). However, there is presently no way to benchmark the performance of LLMs at this task. To fill this void, we introduce AIReg-Bench: the first benchmark dataset designed to test how well LLMs can assess compliance with the EU AI Act (AIA). We created this dataset through a two-step process: (1) by prompting an LLM with carefully structured instructions, we generated 120 technical documentation excerpts (samples), each depicting a fictional, albeit plausible, AI system - of the kind an AI provider might produce to demonstrate their compliance with AIR; (2) legal experts then reviewed and annotated each sample to indicate whether, and in what way, the AI system described therein violates specific Articles of the AIA. The resulting dataset, together with our evaluation of whether frontier LLMs can reproduce the experts' compliance labels, provides a starting point to understand the opportunities and limitations of LLM-based AIR compliance assessment tools and establishes a benchmark against which subsequent LLMs can be compared. The dataset and evaluation code are available at https://github.com/camlsys/aireg-bench.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "102",
        "title": "Comparative Field Deployment of Reinforcement Learning and Model Predictive Control for Residential HVAC",
        "author": [
            "Ozan Baris Mulayim",
            "Elias N. Pergantis",
            "Levi D. Reyes Premer",
            "Bingqing Chen",
            "Guannan Qu",
            "Kevin J. Kircher",
            "Mario BergÃ©s"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01475",
        "abstract": "Advanced control strategies like Model Predictive Control (MPC) offer significant energy savings for HVAC systems but often require substantial engineering effort, limiting scalability. Reinforcement Learning (RL) promises greater automation and adaptability, yet its practical application in real-world residential settings remains largely undemonstrated, facing challenges related to safety, interpretability, and sample efficiency. To investigate these practical issues, we performed a direct comparison of an MPC and a model-based RL controller, with each controller deployed for a one-month period in an occupied house with a heat pump system in West Lafayette, Indiana. This investigation aimed to explore scalability of the chosen RL and MPC implementations while ensuring safety and comparability. The advanced controllers were evaluated against each other and against the existing controller. RL achieved substantial energy savings (22\\% relative to the existing controller), slightly exceeding MPC's savings (20\\%), albeit with modestly higher occupant discomfort. However, when energy savings were normalized for the level of comfort provided, MPC demonstrated superior performance. This study's empirical results show that while RL reduces engineering overhead, it introduces practical trade-offs in model accuracy and operational robustness. The key lessons learned concern the difficulties of safe controller initialization, navigating the mismatch between control actions and their practical implementation, and maintaining the integrity of online learning in a live environment. These insights pinpoint the essential research directions needed to advance RL from a promising concept to a truly scalable HVAC control solution.",
        "tags": [
            "MPC",
            "RL"
        ]
    },
    {
        "id": "103",
        "title": "Purrception: Variational Flow Matching for Vector-Quantized Image Generation",
        "author": [
            "RÄzvan-Andrei MatiÅan",
            "Vincent Tao Hu",
            "Grigory Bartosh",
            "BjÃ¶rn Ommer",
            "Cees G. M. Snoek",
            "Max Welling",
            "Jan-Willem van de Meent",
            "Mohammad Mahdi Derakhshani",
            "Floor Eijkelboom"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01478",
        "abstract": "We introduce Purrception, a variational flow matching approach for vector-quantized image generation that provides explicit categorical supervision while maintaining continuous transport dynamics. Our method adapts Variational Flow Matching to vector-quantized latents by learning categorical posteriors over codebook indices while computing velocity fields in the continuous embedding space. This combines the geometric awareness of continuous methods with the discrete supervision of categorical approaches, enabling uncertainty quantification over plausible codes and temperature-controlled generation. We evaluate Purrception on ImageNet-1k 256x256 generation. Training converges faster than both continuous flow matching and discrete flow matching baselines while achieving competitive FID scores with state-of-the-art models. This demonstrates that Variational Flow Matching can effectively bridge continuous transport and discrete supervision for improved training efficiency in image generation.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "104",
        "title": "Density-Ratio Weighted Behavioral Cloning: Learning Control Policies from Corrupted Datasets",
        "author": [
            "Shriram Karpoora Sundara Pandian",
            "Ali Baheri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01479",
        "abstract": "Offline reinforcement learning (RL) enables policy optimization from fixed datasets, making it suitable for safety-critical applications where online exploration is infeasible. However, these datasets are often contaminated by adversarial poisoning, system errors, or low-quality samples, leading to degraded policy performance in standard behavioral cloning (BC) and offline RL methods. This paper introduces Density-Ratio Weighted Behavioral Cloning (Weighted BC), a robust imitation learning approach that uses a small, verified clean reference set to estimate trajectory-level density ratios via a binary discriminator. These ratios are clipped and used as weights in the BC objective to prioritize clean expert behavior while down-weighting or discarding corrupted data, without requiring knowledge of the contamination mechanism. We establish theoretical guarantees showing convergence to the clean expert policy with finite-sample bounds that are independent of the contamination rate. A comprehensive evaluation framework is established, which incorporates various poisoning protocols (reward, state, transition, and action) on continuous control benchmarks. Experiments demonstrate that Weighted BC maintains near-optimal performance even at high contamination ratios outperforming baselines such as traditional BC, batch-constrained Q-learning (BCQ) and behavior regularized actor-critic (BRAC).",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "105",
        "title": "VL-KnG: Visual Scene Understanding for Navigation Goal Identification using Spatiotemporal Knowledge Graphs",
        "author": [
            "Mohamad Al Mdfaa",
            "Svetlana Lukina",
            "Timur Akhtyamov",
            "Arthur Nigmatzyanov",
            "Dmitrii Nalberskii",
            "Sergey Zagoruyko",
            "Gonzalo Ferrer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01483",
        "abstract": "Vision-language models (VLMs) have shown potential for robot navigation but encounter fundamental limitations: they lack persistent scene memory, offer limited spatial reasoning, and do not scale effectively with video duration for real-time application. We present VL-KnG, a Visual Scene Understanding system that tackles these challenges using spatiotemporal knowledge graph construction and computationally efficient query processing for navigation goal identification. Our approach processes video sequences in chunks utilizing modern VLMs, creates persistent knowledge graphs that maintain object identity over time, and enables explainable spatial reasoning through queryable graph structures. We also introduce WalkieKnowledge, a new benchmark with about 200 manually annotated questions across 8 diverse trajectories spanning approximately 100 minutes of video data, enabling fair comparison between structured approaches and general-purpose VLMs. Real-world deployment on a differential drive robot demonstrates practical applicability, with our method achieving 77.27% success rate and 76.92% answer accuracy, matching Gemini 2.5 Pro performance while providing explainable reasoning supported by the knowledge graph, computational efficiency for real-time deployment across different tasks, such as localization, navigation and planning. Code and dataset will be released after acceptance.",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "106",
        "title": "Pose Estimation of a Thruster-Driven Bioinspired Multi-Link Robot",
        "author": [
            "Nicholas B. Andrews",
            "Yanhao Yang",
            "Sofya Akhetova",
            "Kristi A. Morgansen",
            "Ross L. Hatton"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01485",
        "abstract": "This work demonstrates pose (position and shape) estimation for a free-floating, bioinspired multi-link robot with unactuated joints, link-mounted thrusters for control, and a single gyroscope per link, resulting in an underactuated, minimally sensed platform. Through a proof-of-concept hardware experiment and offline Kalman filter analysis, we show that the robot's pose can be reliably estimated. State estimation is performed using an unscented Kalman filter augmented with Gaussian process residual learning to compensate for non-zero-mean, non-Gaussian noise. We further show that a filter trained on a multi-gait dataset (forward, backward, left, right, and turning) performs comparably to one trained on a larger forward-gait-only dataset when both are evaluated on the same forward-gait test trajectory. These results reveal overlap in the gait input space, which can be exploited to reduce training data requirements while enhancing the filter's generalizability across multiple gaits.",
        "tags": [
            "Pose Estimation",
            "Robotics"
        ]
    },
    {
        "id": "107",
        "title": "Off-Policy Reinforcement Learning with Anytime Safety Guarantees via Robust Safe Gradient Flow",
        "author": [
            "Pol Mestres",
            "Arnau Marzabal",
            "Jorge CortÃ©s"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01492",
        "abstract": "This paper considers the problem of solving constrained reinforcement learning (RL) problems with anytime guarantees, meaning that the algorithmic solution must yield a constraint-satisfying policy at every iteration of its evolution. Our design is based on a discretization of the Robust Safe Gradient Flow (RSGF), a continuous-time dynamics for anytime constrained optimization whose forward invariance and stability properties we formally characterize. The proposed strategy, termed RSGF-RL, is an off-policy algorithm which uses episodic data to estimate the value functions and their gradients and updates the policy parameters by solving a convex quadratically constrained quadratic program. Our technical analysis combines statistical analysis, the theory of stochastic approximation, and convex analysis to determine the number of episodes sufficient to ensure that safe policies are updated to safe policies and to recover from an unsafe policy, both with an arbitrary user-specified probability, and to establish the asymptotic convergence to the set of KKT points of the RL problem almost surely. Simulations on a navigation example and the cart-pole system illustrate the superior performance of RSGF-RL with respect to the state of the art.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "108",
        "title": "Understanding Adversarial Transfer: Why Representation-Space Attacks Fail Where Data-Space Attacks Succeed",
        "author": [
            "Isha Gupta",
            "Rylan Schaeffer",
            "Joshua Kazdan",
            "Ken Liu",
            "Sanmi Koyejo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01494",
        "abstract": "The field of adversarial robustness has long established that adversarial examples can successfully transfer between image classifiers and that text jailbreaks can successfully transfer between language models (LMs). However, a pair of recent studies reported being unable to successfully transfer image jailbreaks between vision-language models (VLMs). To explain this striking difference, we propose a fundamental distinction regarding the transferability of attacks against machine learning models: attacks in the input data-space can transfer, whereas attacks in model representation space do not, at least not without geometric alignment of representations. We then provide theoretical and empirical evidence of this hypothesis in four different settings. First, we mathematically prove this distinction in a simple setting where two networks compute the same input-output map but via different representations. Second, we construct representation-space attacks against image classifiers that are as successful as well-known data-space attacks, but fail to transfer. Third, we construct representation-space attacks against LMs that successfully jailbreak the attacked models but again fail to transfer. Fourth, we construct data-space attacks against VLMs that successfully transfer to new VLMs, and we show that representation space attacks \\emph{can} transfer when VLMs' latent geometries are sufficiently aligned in post-projector space. Our work reveals that adversarial transfer is not an inherent property of all attacks but contingent on their operational domain - the shared data-space versus models' unique representation spaces - a critical insight for building more robust models.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "109",
        "title": "AortaDiff: A Unified Multitask Diffusion Framework For Contrast-Free AAA Imaging",
        "author": [
            "Yuxuan Ou",
            "Ning Bi",
            "Jiazhen Pan",
            "Jiancheng Yang",
            "Boliang Yu",
            "Usama Zidan",
            "Regent Lee",
            "Vicente Grau"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01498",
        "abstract": "While contrast-enhanced CT (CECT) is standard for assessing abdominal aortic aneurysms (AAA), the required iodinated contrast agents pose significant risks, including nephrotoxicity, patient allergies, and environmental harm. To reduce contrast agent use, recent deep learning methods have focused on generating synthetic CECT from non-contrast CT (NCCT) scans. However, most adopt a multi-stage pipeline that first generates images and then performs segmentation, which leads to error accumulation and fails to leverage shared semantic and anatomical structures. To address this, we propose a unified deep learning framework that generates synthetic CECT images from NCCT scans while simultaneously segmenting the aortic lumen and thrombus. Our approach integrates conditional diffusion models (CDM) with multi-task learning, enabling end-to-end joint optimization of image synthesis and anatomical segmentation. Unlike previous multitask diffusion models, our approach requires no initial predictions (e.g., a coarse segmentation mask), shares both encoder and decoder parameters across tasks, and employs a semi-supervised training strategy to learn from scans with missing segmentation labels, a common constraint in real-world clinical data. We evaluated our method on a cohort of 264 patients, where it consistently outperformed state-of-the-art single-task and multi-stage models. For image synthesis, our model achieved a PSNR of 25.61 dB, compared to 23.80 dB from a single-task CDM. For anatomical segmentation, it improved the lumen Dice score to 0.89 from 0.87 and the challenging thrombus Dice score to 0.53 from 0.48 (nnU-Net). These segmentation enhancements led to more accurate clinical measurements, reducing the lumen diameter MAE to 4.19 mm from 5.78 mm and the thrombus area error to 33.85% from 41.45% when compared to nnU-Net. Code is available at https://github.com/yuxuanou623/AortaDiff.git.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "110",
        "title": "Beyond Majority Voting: LLM Aggregation by Leveraging Higher-Order Information",
        "author": [
            "Rui Ai",
            "Yuqi Pan",
            "David Simchi-Levi",
            "Milind Tambe",
            "Haifeng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01499",
        "abstract": "With the rapid progress of multi-agent large language model (LLM) reasoning, how to effectively aggregate answers from multiple LLMs has emerged as a fundamental challenge. Standard majority voting treats all answers equally, failing to consider latent heterogeneity and correlation across models. In this work, we design two new aggregation algorithms called Optimal Weight (OW) and Inverse Surprising Popularity (ISP), leveraging both first-order and second-order information. Our theoretical analysis shows these methods provably mitigate inherent limitations of majority voting under mild assumptions, leading to more reliable collective decisions. We empirically validate our algorithms on synthetic datasets, popular LLM fine-tuning benchmarks such as UltraFeedback and MMLU, and a real-world healthcare setting ARMMAN. Across all cases, our methods consistently outperform majority voting, offering both practical performance gains and conceptual insights for the design of robust multi-agent LLM pipelines.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "111",
        "title": "Probabilistic Control Barrier Functions: Safety in Probability for Discrete-Time Stochastic Systems",
        "author": [
            "Pol Mestres",
            "Blake Werner",
            "Ryan K. Cosner",
            "Aaron D. Ames"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01501",
        "abstract": "Control systems operating in the real world face countless sources of unpredictable uncertainties. These random disturbances can render deterministic guarantees inapplicable and cause catastrophic safety failures. To overcome this, this paper proposes a method for designing safe controllers for discrete-time stochastic systems that retain probabilistic guarantees of safety. To do this we modify the traditional notion of a control barrier function (CBF) to explicitly account for these stochastic uncertainties and call these new modified functions probabilistic CBFs. We show that probabilistic CBFs can be used to design controllers that guarantee safety over a finite number of time steps with a prescribed probability. Next, by leveraging various uncertainty quantification methods, such as concentration inequalities, the scenario approach, and conformal prediction, we provide a variety of sufficient conditions that result in computationally tractable controllers with tunable probabilistic guarantees across a plethora of practical scenarios. Finally, we showcase the applicability of our results in simulation and hardware for the control of a quadruped robot.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "112",
        "title": "Realistic CDSS Drug Dosing with End-to-end Recurrent Q-learning for Dual Vasopressor Control",
        "author": [
            "Will Y. Zou",
            "Jean Feng",
            "Alexandre Kalimouttou",
            "Jennifer Yuntong Zhang",
            "Christopher W. Seymour",
            "Romain Pirracchio"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01508",
        "abstract": "Reinforcement learning (RL) applications in Clinical Decision Support Systems (CDSS) frequently encounter skepticism from practitioners regarding inoperable dosing decisions. We address this challenge with an end-to-end approach for learning optimal drug dosing and control policies for dual vasopressor administration in intensive care unit (ICU) patients with septic shock. For realistic drug dosing, we apply action space design that accommodates discrete, continuous, and directional dosing strategies in a system that combines offline conservative Q-learning with a novel recurrent modeling in a replay buffer to capture temporal dependencies in ICU time-series data. Our comparative analysis of norepinephrine dosing strategies across different action space formulations reveals that the designed action spaces improve interpretability and facilitate clinical adoption while preserving efficacy. Empirical results1 on eICU and MIMIC demonstrate that action space design profoundly influences learned behavioral policies. The proposed methods achieve improved patient outcomes of over 15% in survival improvement probability, while aligning with established clinical protocols.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "113",
        "title": "Online Hierarchical Policy Learning using Physics Priors for Robot Navigation in Unknown Environments",
        "author": [
            "Wei Han Chen",
            "Yuchen Liu",
            "Alexiy Buynitsky",
            "Ahmed H. Qureshi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01519",
        "abstract": "Robot navigation in large, complex, and unknown indoor environments is a challenging problem. The existing approaches, such as traditional sampling-based methods, struggle with resolution control and scalability, while imitation learning-based methods require a large amount of demonstration data. Active Neural Time Fields (ANTFields) have recently emerged as a promising solution by using local observations to learn cost-to-go functions without relying on demonstrations. Despite their potential, these methods are hampered by challenges such as spectral bias and catastrophic forgetting, which diminish their effectiveness in complex scenarios. To address these issues, our approach decomposes the planning problem into a hierarchical structure. At the high level, a sparse graph captures the environment's global connectivity, while at the low level, a planner based on neural fields navigates local obstacles by solving the Eikonal PDE. This physics-informed strategy overcomes common pitfalls like spectral bias and neural field fitting difficulties, resulting in a smooth and precise representation of the cost landscape. We validate our framework in large-scale environments, demonstrating its enhanced adaptability and precision compared to previous methods, and highlighting its potential for online exploration, mapping, and real-world navigation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "114",
        "title": "Predictive Modeling and Explainable AI for Veterinary Safety Profiles, Residue Assessment, and Health Outcomes Using Real-World Data and Physicochemical Properties",
        "author": [
            "Hossein Sholehrasa",
            "Xuan Xu",
            "Doina Caragea",
            "Jim E. Riviere",
            "Majid Jaberi-Douraki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01520",
        "abstract": "The safe use of pharmaceuticals in food-producing animals is vital to protect animal welfare and human food safety. Adverse events (AEs) may signal unexpected pharmacokinetic or toxicokinetic effects, increasing the risk of violative residues in the food chain. This study introduces a predictive framework for classifying outcomes (Death vs. Recovery) using ~1.28 million reports (1987-2025 Q1) from the U.S. FDA's OpenFDA Center for Veterinary Medicine. A preprocessing pipeline merged relational tables and standardized AEs through VeDDRA ontologies. Data were normalized, missing values imputed, and high-cardinality features reduced; physicochemical drug properties were integrated to capture chemical-residue links. We evaluated supervised models, including Random Forest, CatBoost, XGBoost, ExcelFormer, and large language models (Gemma 3-27B, Phi 3-12B). Class imbalance was addressed, such as undersampling and oversampling, with a focus on prioritizing recall for fatal outcomes. Ensemble methods(Voting, Stacking) and CatBoost performed best, achieving precision, recall, and F1-scores of 0.95. Incorporating Average Uncertainty Margin (AUM)-based pseudo-labeling of uncertain cases improved minority-class detection, particularly in ExcelFormer and XGBoost. Interpretability via SHAP identified biologically plausible predictors, including lung, heart, and bronchial disorders, animal demographics, and drug physicochemical properties. These features were strongly linked to fatal outcomes. Overall, the framework shows that combining rigorous data engineering, advanced machine learning, and explainable AI enables accurate, interpretable predictions of veterinary safety outcomes. The approach supports FARAD's mission by enabling early detection of high-risk drug-event profiles, strengthening residue risk assessment, and informing regulatory and clinical decision-making.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "115",
        "title": "WALT: Web Agents that Learn Tools",
        "author": [
            "Viraj Prabhu",
            "Yutong Dai",
            "Matthew Fernandez",
            "Jing Gu",
            "Krithika Ramakrishnan",
            "Yanqi Luo",
            "Silvio Savarese",
            "Caiming Xiong",
            "Junnan Li",
            "Zeyuan Chen",
            "Ran Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01524",
        "abstract": "Web agents promise to automate complex browser tasks, but current methods remain brittle -- relying on step-by-step UI interactions and heavy LLM reasoning that break under dynamic layouts and long horizons. Humans, by contrast, exploit website-provided functionality through high-level operations like search, filter, and sort. We introduce WALT (Web Agents that Learn Tools), a framework that reverse-engineers latent website functionality into reusable invocable tools. Rather than hypothesizing ad-hoc skills, WALT exposes robust implementations of automations already designed into websites -- spanning discovery (search, filter, sort), communication (post, comment, upvote), and content management (create, edit, delete). Tools abstract away low-level execution: instead of reasoning about how to click and type, agents simply call search(query) or create(listing). This shifts the computational burden from fragile step-by-step reasoning to reliable tool invocation. On VisualWebArena and WebArena, WALT achieves higher success with fewer steps and less LLM-dependent reasoning, establishing a robust and generalizable paradigm for browser automation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "116",
        "title": "One More Question is Enough, Expert Question Decomposition (EQD) Model for Domain Quantitative Reasoning",
        "author": [
            "Mengyu Wang",
            "Sotirios Sabanis",
            "Miguel de Carvalho",
            "Shay B. Cohen",
            "Tiejun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01526",
        "abstract": "Domain-specific quantitative reasoning remains a major challenge for large language models (LLMs), especially in fields requiring expert knowledge and complex question answering (QA). In this work, we propose Expert Question Decomposition (EQD), an approach designed to balance the use of domain knowledge with computational efficiency. EQD is built on a two-step fine-tuning framework and guided by a reward function that measures the effectiveness of generated sub-questions in improving QA outcomes. It requires only a few thousand training examples and a single A100 GPU for fine-tuning, with inference time comparable to zero-shot prompting. Beyond its efficiency, EQD outperforms state-of-the-art domain-tuned models and advanced prompting strategies. We evaluate EQD in the financial domain, characterized by specialized knowledge and complex quantitative reasoning, across four benchmark datasets. Our method consistently improves QA performance by 0.6% to 10.5% across different LLMs. Our analysis reveals an important insight: in domain-specific QA, a single supporting question often provides greater benefit than detailed guidance steps.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "117",
        "title": "Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation",
        "author": [
            "Daniel Zhao",
            "Abhilash Shankarampeta",
            "Lanxiang Hu",
            "Tajana Rosing",
            "Hao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01528",
        "abstract": "We propose a novel method that leverages sparse autoencoders (SAEs) and clustering techniques to analyze the internal token representations of large language models (LLMs) and guide generations in mathematical reasoning tasks. Our approach first trains an SAE to generate sparse vector representations for training tokens, then applies k-means clustering to construct a graph where vertices represent token clusters and weighted edges capture sequential token transitions. Using this graph, we define an edge-weight based reward function to quantify adherence to established reasoning traces, thereby identifying exploitative reasoning trajectories. Additionally, we measure generation diversity from clustering to assess the extent of exploration. Our findings indicate that balancing both exploitation and exploration is crucial for achieving high accuracy in mathematical reasoning tasks. During generation, the SAE can serve as a scalable reward model to guide generations, ensuring a balanced trade-off between exploitation and exploration. This prevents extreme behaviors in either direction, ultimately fostering a higher-quality reasoning process in LLMs.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "118",
        "title": "Bypassing Prompt Guards in Production with Controlled-Release Prompting",
        "author": [
            "Jaiden Fairoze",
            "Sanjam Garg",
            "Keewoo Lee",
            "Mingyuan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01529",
        "abstract": "As large language models (LLMs) advance, ensuring AI safety and alignment is paramount. One popular approach is prompt guards, lightweight mechanisms designed to filter malicious queries while being easy to implement and update. In this work, we introduce a new attack that circumvents such prompt guards, highlighting their limitations. Our method consistently jailbreaks production models while maintaining response quality, even under the highly protected chat interfaces of Google Gemini (2.5 Flash/Pro), DeepSeek Chat (DeepThink), Grok (3), and Mistral Le Chat (Magistral). The attack exploits a resource asymmetry between the prompt guard and the main LLM, encoding a jailbreak prompt that lightweight guards cannot decode but the main model can. This reveals an attack surface inherent to lightweight prompt guards in modern LLM architectures and underscores the need to shift defenses from blocking malicious inputs to preventing malicious outputs. We additionally identify other critical alignment issues, such as copyrighted data extraction, training data extraction, and malicious response leakage during thinking.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "119",
        "title": "LOGicalThought: Logic-Based Ontological Grounding of LLMs for High-Assurance Reasoning",
        "author": [
            "Navapat Nananukul",
            "Yue Zhang",
            "Ryan Lee",
            "Eric Boxer",
            "Jonathan May",
            "Vibhav Giridhar Gogate",
            "Jay Pujara",
            "Mayank Kejriwal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01530",
        "abstract": "High-assurance reasoning, particularly in critical domains such as law and medicine, requires conclusions that are accurate, verifiable, and explicitly grounded in evidence. This reasoning relies on premises codified from rules, statutes, and contracts, inherently involving defeasible or non-monotonic logic due to numerous exceptions, where the introduction of a single fact can invalidate general rules, posing significant challenges. While large language models (LLMs) excel at processing natural language, their capabilities in standard inference tasks do not translate to the rigorous reasoning required over high-assurance text guidelines. Core reasoning challenges within such texts often manifest specific logical structures involving negation, implication, and, most critically, defeasible rules and exceptions. In this paper, we propose a novel neurosymbolically-grounded architecture called LOGicalThought (LogT) that uses an advanced logical language and reasoner in conjunction with an LLM to construct a dual symbolic graph context and logic-based context. These two context representations transform the problem from inference over long-form guidelines into a compact grounded evaluation. Evaluated on four multi-domain benchmarks against four baselines, LogT improves overall performance by 11.84% across all LLMs. Performance improves significantly across all three modes of reasoning: by up to +10.2% on negation, +13.2% on implication, and +5.5% on defeasible reasoning compared to the strongest baseline.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "120",
        "title": "Information Seeking for Robust Decision Making under Partial Observability",
        "author": [
            "Djengo Cyun-Jyun Fang",
            "Tsung-Wei Ke"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01531",
        "abstract": "Explicit information seeking is essential to human problem-solving in practical environments characterized by incomplete information and noisy dynamics. When the true environmental state is not directly observable, humans seek information to update their internal dynamics and inform future decision-making. Although existing Large Language Model (LLM) planning agents have addressed observational uncertainty, they often overlook discrepancies between their internal dynamics and the actual environment. We introduce Information Seeking Decision Planner (InfoSeeker), an LLM decision-making framework that integrates task-oriented planning with information seeking to align internal dynamics and make optimal decisions under uncertainty in both agent observations and environmental dynamics. InfoSeeker prompts an LLM to actively gather information by planning actions to validate its understanding, detect environmental changes, or test hypotheses before generating or revising task-oriented plans. To evaluate InfoSeeker, we introduce a novel benchmark suite featuring partially observable environments with incomplete observations and uncertain dynamics. Experiments demonstrate that InfoSeeker achieves a 74% absolute performance gain over prior methods without sacrificing sample efficiency. Moreover, InfoSeeker generalizes across LLMs and outperforms baselines on established benchmarks such as robotic manipulation and web navigation. These findings underscore the importance of tightly integrating planning and information seeking for robust behavior in partially observable environments. The project page is available at https://infoseekerllm.github.io",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "121",
        "title": "TimeSeriesScientist: A General-Purpose AI Agent for Time Series Analysis",
        "author": [
            "Haokun Zhao",
            "Xiang Zhang",
            "Jiaqi Wei",
            "Yiwei Xu",
            "Yuting He",
            "Siqi Sun",
            "Chenyu You"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01538",
        "abstract": "Time series forecasting is central to decision-making in domains as diverse as energy, finance, climate, and public health. In practice, forecasters face thousands of short, noisy series that vary in frequency, quality, and horizon, where the dominant cost lies not in model fitting, but in the labor-intensive preprocessing, validation, and ensembling required to obtain reliable predictions. Prevailing statistical and deep learning models are tailored to specific datasets or domains and generalize poorly. A general, domain-agnostic framework that minimizes human intervention is urgently in demand. In this paper, we introduce TimeSeriesScientist (TSci), the first LLM-driven agentic framework for general time series forecasting. The framework comprises four specialized agents: Curator performs LLM-guided diagnostics augmented by external tools that reason over data statistics to choose targeted preprocessing; Planner narrows the hypothesis space of model choice by leveraging multi-modal diagnostics and self-planning over the input; Forecaster performs model fitting and validation and, based on the results, adaptively selects the best model configuration as well as ensemble strategy to make final predictions; and Reporter synthesizes the whole process into a comprehensive, transparent report. With transparent natural-language rationales and comprehensive reports, TSci transforms the forecasting workflow into a white-box system that is both interpretable and extensible across tasks. Empirical results on eight established benchmarks demonstrate that TSci consistently outperforms both statistical and LLM-based baselines, reducing forecast error by an average of 10.4% and 38.2%, respectively. Moreover, TSci produces a clear and rigorous report that makes the forecasting workflow more transparent and interpretable.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "122",
        "title": "Executable Counterfactuals: Improving LLMs' Causal Reasoning Through Code",
        "author": [
            "Aniket Vashishtha",
            "Qirun Dai",
            "Hongyuan Mei",
            "Amit Sharma",
            "Chenhao Tan",
            "Hao Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01539",
        "abstract": "Counterfactual reasoning, a hallmark of intelligence, consists of three steps: inferring latent variables from observations (abduction), constructing alternatives (interventions), and predicting their outcomes (prediction). This skill is essential for advancing LLMs' causal understanding and expanding their applications in high-stakes domains such as scientific research. However, existing efforts in assessing LLM's counterfactual reasoning capabilities tend to skip the abduction step, effectively reducing to interventional reasoning and leading to overestimation of LLM performance. To address this, we introduce executable counterfactuals, a novel framework that operationalizes causal reasoning through code and math problems. Our framework explicitly requires all three steps of counterfactual reasoning and enables scalable synthetic data creation with varying difficulty, creating a frontier for evaluating and improving LLM's reasoning. Our results reveal substantial drop in accuracy (25-40%) from interventional to counterfactual reasoning for SOTA models like o4-mini and Claude-4-Sonnet. To address this gap, we construct a training set comprising counterfactual code problems having if-else condition and test on out-of-domain code structures (e.g. having while-loop); we also test whether a model trained on code would generalize to counterfactual math word problems. While supervised finetuning on stronger models' reasoning traces improves in-domain performance of Qwen models, it leads to a decrease in accuracy on OOD tasks such as counterfactual math problems. In contrast, reinforcement learning induces the core cognitive behaviors and generalizes to new domains, yielding gains over the base model on both code (improvement of 1.5x-2x) and math problems. Analysis of the reasoning traces reinforces these findings and highlights the promise of RL for improving LLMs' counterfactual reasoning.",
        "tags": [
            "LLM",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": "123",
        "title": "Towards Better Optimization For Listwise Preference in Diffusion Models",
        "author": [
            "Jiamu Bai",
            "Xin Yu",
            "Meilong Xu",
            "Weitao Lu",
            "Xin Pan",
            "Kiwan Maeng",
            "Daniel Kifer",
            "Jian Wang",
            "Yu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01540",
        "abstract": "Reinforcement learning from human feedback (RLHF) has proven effectiveness for aligning text-to-image (T2I) diffusion models with human preferences. Although Direct Preference Optimization (DPO) is widely adopted for its computational efficiency and avoidance of explicit reward modeling, its applications to diffusion models have primarily relied on pairwise preferences. The precise optimization of listwise preferences remains largely unaddressed. In practice, human feedback on image preferences often contains implicit ranked information, which conveys more precise human preferences than pairwise comparisons. In this work, we propose Diffusion-LPO, a simple and effective framework for Listwise Preference Optimization in diffusion models with listwise data. Given a caption, we aggregate user feedback into a ranked list of images and derive a listwise extension of the DPO objective under the Plackett-Luce model. Diffusion-LPO enforces consistency across the entire ranking by encouraging each sample to be preferred over all of its lower-ranked alternatives. We empirically demonstrate the effectiveness of Diffusion-LPO across various tasks, including text-to-image generation, image editing, and personalized preference alignment. Diffusion-LPO consistently outperforms pairwise DPO baselines on visual quality and preference alignment.",
        "tags": [
            "DPO",
            "Diffusion",
            "Image Editing",
            "RL",
            "RLHF",
            "Text-to-Image"
        ]
    },
    {
        "id": "124",
        "title": "Step-Aware Policy Optimization for Reasoning in Diffusion Large Language Models",
        "author": [
            "Shaoan Xie",
            "Lingjing Kong",
            "Xiangchen Song",
            "Xinshuai Dong",
            "Guangyi Chen",
            "Eric P.Xing",
            "Kun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01544",
        "abstract": "Diffusion language models (dLLMs) offer a promising, non-autoregressive paradigm for text generation, yet training them for complex reasoning remains a key challenge. Current reinforcement learning approaches often rely on sparse, outcome-based rewards, which can reinforce flawed reasoning paths that lead to coincidentally correct answers. We argue that this stems from a fundamental mismatch with the natural structure of reasoning. We first propose a theoretical framework that formalizes complex problem solving as a hierarchical selection process, where an intractable global constraint is decomposed into a series of simpler, localized logical steps. This framework provides a principled foundation for algorithm design, including theoretical insights into the identifiability of this latent reasoning structure. Motivated by this theory, we identify unstructured refinement -- a failure mode where a model's iterative steps do not contribute meaningfully to the solution -- as a core deficiency in existing methods. We then introduce Step-Aware Policy Optimization (SAPO), a novel RL algorithm that aligns the dLLM's denoising process with the latent reasoning hierarchy. By using a process-based reward function that encourages incremental progress, SAPO guides the model to learn structured, coherent reasoning paths. Our empirical results show that this principled approach significantly improves performance on challenging reasoning benchmarks and enhances the interpretability of the generation process.",
        "tags": [
            "Diffusion",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "125",
        "title": "Growing Visual Generative Capacity for Pre-Trained MLLMs",
        "author": [
            "Hanyu Wang",
            "Jiaming Han",
            "Ziyan Yang",
            "Qi Zhao",
            "Shanchuan Lin",
            "Xiangyu Yue",
            "Abhinav Shrivastava",
            "Zhenheng Yang",
            "Hao Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01546",
        "abstract": "Multimodal large language models (MLLMs) extend the success of language models to visual understanding, and recent efforts have sought to build unified MLLMs that support both understanding and generation. However, constructing such models remains challenging: hybrid approaches combine continuous embeddings with diffusion or flow-based objectives, producing high-quality images but breaking the autoregressive paradigm, while pure autoregressive approaches unify text and image prediction over discrete visual tokens but often face trade-offs between semantic alignment and pixel-level fidelity. In this work, we present Bridge, a pure autoregressive unified MLLM that augments pre-trained visual understanding models with generative ability through a Mixture-of-Transformers architecture, enabling both image understanding and generation within a single next-token prediction framework. To further improve visual generation fidelity, we propose a semantic-to-pixel discrete representation that integrates compact semantic tokens with fine-grained pixel tokens, achieving strong language alignment and precise description of visual details with only a 7.9% increase in sequence length. Extensive experiments across diverse multimodal benchmarks demonstrate that Bridge achieves competitive or superior results in both understanding and generation benchmarks, while requiring less training data and reduced training time compared to prior unified MLLMs.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "126",
        "title": "MIRA: Towards Mitigating Reward Hacking in Inference-Time Alignment of T2I Diffusion Models",
        "author": [
            "Kevin Zhai",
            "Utsav Singh",
            "Anirudh Thatipelli",
            "Souradip Chakraborty",
            "Anit Kumar Sahu",
            "Furong Huang",
            "Amrit Singh Bedi",
            "Mubarak Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01549",
        "abstract": "Diffusion models excel at generating images conditioned on text prompts, but the resulting images often do not satisfy user-specific criteria measured by scalar rewards such as Aesthetic Scores. This alignment typically requires fine-tuning, which is computationally demanding. Recently, inference-time alignment via noise optimization has emerged as an efficient alternative, modifying initial input noise to steer the diffusion denoising process towards generating high-reward images. However, this approach suffers from reward hacking, where the model produces images that score highly, yet deviate significantly from the original prompt. We show that noise-space regularization is insufficient and that preventing reward hacking requires an explicit image-space constraint. To this end, we propose MIRA (MItigating Reward hAcking), a training-free, inference-time alignment method. MIRA introduces an image-space, score-based KL surrogate that regularizes the sampling trajectory with a frozen backbone, constraining the output distribution so reward can increase without off-distribution drift (reward hacking). We derive a tractable approximation to KL using diffusion scores. Across SDv1.5 and SDXL, multiple rewards (Aesthetic, HPSv2, PickScore), and public datasets (e.g., Animal-Animal, HPDv2), MIRA achieves >60\\% win rate vs. strong baselines while preserving prompt adherence; mechanism plots show reward gains with near-zero drift, whereas DNO drifts as compute increases. We further introduce MIRA-DPO, mapping preference optimization to inference time with a frozen backbone, extending MIRA to non-differentiable rewards without fine-tuning.",
        "tags": [
            "DPO",
            "Diffusion",
            "SDXL"
        ]
    },
    {
        "id": "127",
        "title": "POLAR: Automating Cyber Threat Prioritization through LLM-Powered Assessment",
        "author": [
            "Luoxi Tang",
            "Yuqiao Meng",
            "Ankita Patra",
            "Weicheng Ma",
            "Muchao Ye",
            "Zhaohan Xi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01552",
        "abstract": "Large Language Models (LLMs) are intensively used to assist security analysts in counteracting the rapid exploitation of cyber threats, wherein LLMs offer cyber threat intelligence (CTI) to support vulnerability assessment and incident response. While recent work has shown that LLMs can support a wide range of CTI tasks such as threat analysis, vulnerability detection, and intrusion defense, significant performance gaps persist in practical deployments. In this paper, we investigate the intrinsic vulnerabilities of LLMs in CTI, focusing on challenges that arise from the nature of the threat landscape itself rather than the model architecture. Using large-scale evaluations across multiple CTI benchmarks and real-world threat reports, we introduce a novel categorization methodology that integrates stratification, autoregressive refinement, and human-in-the-loop supervision to reliably analyze failure instances. Through extensive experiments and human inspections, we reveal three fundamental vulnerabilities: spurious correlations, contradictory knowledge, and constrained generalization, that limit LLMs in effectively supporting CTI. Subsequently, we provide actionable insights for designing more robust LLM-powered CTI systems to facilitate future research.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "128",
        "title": "Rethinking KL Regularization in RLHF: From Value Estimation to Gradient Optimization",
        "author": [
            "Kezhao Liu",
            "Jason Klein Liu",
            "Mingtao Chen",
            "Yiming Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01555",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) leverages a Kullback-Leibler (KL) divergence loss to stabilize training and prevent overfitting. However, in methods such as GRPO, its implementation may be guided by principles from numerical value estimation-a practice that overlooks the term's functional role as an optimization loss. To analyze this issue, we establish a unified framework that connects two seemingly distinct implementation styles: using the mathematical term $k_n$ as a detached coefficient for the policy's score function ('$k_n$ in reward') or as a direct loss function through which gradients are propagated ('$k_n$ as loss'). We show that the latter can always be analyzed via an equivalent gradient coefficient in the former, unifying the two perspectives. Through this framework, we prove that the conventional '$k_1$ in reward' (like in PPO) is the principled loss for Reverse KL (RKL) regularization. We further establish a key finding: under on-policy conditions, the '$k_2$ as loss' formulation is, in fact, gradient-equivalent to '$k_1$ in reward'. This equivalence, first proven in our work, identifies both as the theoretically sound implementations of the RKL objective. In contrast, we show that the recently adopted '$k_3$ as loss' (like in GRPO) is merely a first-order, biased approximation of the principled loss. Furthermore, we argue that common off-policy implementations of '$k_n$ as loss' methods are biased due to neglected importance sampling, and we propose a principled correction. Our findings provide a comprehensive, gradient-based rationale for choosing and correctly implementing KL regularization, paving the way for more robust and effective RLHF systems.",
        "tags": [
            "GRPO",
            "PPO",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "129",
        "title": "Consistent Assistant Domains Transformer for Source-free Domain Adaptation",
        "author": [
            "Renrong Shao",
            "Wei Zhang",
            "Kangyang Luo",
            "Qin Li",
            "and Jun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01559",
        "abstract": "Source-free domain adaptation (SFDA) aims to address the challenge of adapting to a target domain without accessing the source domain directly. However, due to the inaccessibility of source domain data, deterministic invariable features cannot be obtained. Current mainstream methods primarily focus on evaluating invariant features in the target domain that closely resemble those in the source domain, subsequently aligning the target domain with the source domain. However, these methods are susceptible to hard samples and influenced by domain bias. In this paper, we propose a Consistent Assistant Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue by constructing invariable feature representations of domain consistency. Concretely, we develop an assistant domain module for CADTrans to obtain diversified representations from the intermediate aggregated global attentions, which addresses the limitation of existing methods in adequately representing diversity. Based on assistant and target domains, invariable feature representations are obtained by multiple consistent strategies, which can be used to distinguish easy and hard samples. Finally, to align the hard samples to the corresponding easy samples, we construct a conditional multi-kernel max mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same category and those of different categories. Extensive experiments are conducted on various benchmarks such as Office-31, Office-Home, VISDA-C, and DomainNet-126, proving the significant performance improvements achieved by our proposed approaches. Code is available at https://github.com/RoryShao/CADTrans.git.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "130",
        "title": "TetriServe: Efficient DiT Serving for Heterogeneous Image Generation",
        "author": [
            "Runyu Lu",
            "Shiqi He",
            "Wenxuan Tan",
            "Shenggui Li",
            "Ruofan Wu",
            "Jeff J. Ma",
            "Ang Chen",
            "Mosharaf Chowdhury"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01565",
        "abstract": "Diffusion Transformer (DiT) models excel at generating highquality images through iterative denoising steps, but serving them under strict Service Level Objectives (SLOs) is challenging due to their high computational cost, particularly at large resolutions. Existing serving systems use fixed degree sequence parallelism, which is inefficient for heterogeneous workloads with mixed resolutions and deadlines, leading to poor GPU utilization and low SLO attainment.\nIn this paper, we propose step-level sequence parallelism to dynamically adjust the parallel degree of individual requests according to their deadlines. We present TetriServe, a DiT serving system that implements this strategy for highly efficient image generation. Specifically, TetriServe introduces a novel round-based scheduling mechanism that improves SLO attainment: (1) discretizing time into fixed rounds to make deadline-aware scheduling tractable, (2) adapting parallelism at the step level and minimize GPU hour consumption, and (3) jointly packing requests to minimize late completions. Extensive evaluation on state-of-the-art DiT models shows that TetriServe achieves up to 32% higher SLO attainment compared to existing solutions without degrading image quality.",
        "tags": [
            "DiT",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "131",
        "title": "InvThink: Towards AI Safety via Inverse Reasoning",
        "author": [
            "Yubin Kim",
            "Taehan Kim",
            "Eugene Park",
            "Chunjong Park",
            "Cynthia Breazeal",
            "Daniel McDuff",
            "Hae Won Park"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01569",
        "abstract": "We present InvThink, a simple yet powerful approach that gives large language models (LLMs) the capability of inverse thinking: reasoning through failure modes before generating responses. Unlike existing safety alignment methods that optimize directly for safe response, InvThink instructs models to 1) enumerate potential harms, 2) analyze their consequences, and 3) generate safe outputs that proactively avoid these risks. Our method reveals three key findings: (i) safety improvements show stronger scaling with model size compared to existing safety methods. (ii) InvThink mitigates safety tax; by training models to systematically consider failure modes, it preserves general reasoning capabilities on standard benchmarks. (iii) beyond general safety tasks, InvThink excels in high-stakes domains including external-facing (medicine, finance, law) and agentic (blackmail, murder) risk scenarios, achieving up to 15.7% reduction in harmful responses compared to baseline methods like SafetyPrompt. We further implement InvThink via supervised fine-tuning, and reinforcement learning across three LLM families. These results suggest that inverse reasoning provides a scalable and generalizable path toward safer, more capable language models.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "132",
        "title": "Guiding Multimodal Large Language Models with Blind and Low Vision People Visual Questions for Proactive Visual Interpretations",
        "author": [
            "Ricardo Gonzalez Penuela",
            "Felipe Arias-Russi",
            "Victor Capriles"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01576",
        "abstract": "Multimodal large language models (MLLMs) have been integrated into visual interpretation applications to support Blind and Low Vision (BLV) users because of their accuracy and ability to provide rich, human-like interpretations. However, these applications often default to comprehensive, lengthy descriptions regardless of context. This leads to inefficient exchanges, as users must go through irrelevant details rather than receiving the specific information they are likely to seek. To deliver more contextually-relevant information, we developed a system that draws on historical BLV users questions. When given an image, our system identifies similar past visual contexts from the VizWiz-LF dataset and uses the associated questions to guide the MLLM generate descriptions more relevant to BLV users. An evaluation with three human labelers who revised 92 context-aware and context-free descriptions showed that context-aware descriptions anticipated and answered users' questions in 76.1% of cases (70 out of 92) and were preferred in 54.4% of comparisons (50 out of 92). Our paper reviews, and data analysis are publicly available in a Github repository at https://github.com/rgonzalezp/guiding-multimodal-large-language-models-with-blind-and-low-vision-people-visual-questions .",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "133",
        "title": "Think Right: Learning to Mitigate Under-Over Thinking via Adaptive, Attentive Compression",
        "author": [
            "Joykirat Singh",
            "Justin Chih-Yao Chen",
            "Archiki Prasad",
            "Elias Stengel-Eskin",
            "Akshay Nambi",
            "Mohit Bansal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01581",
        "abstract": "Recent thinking models solve complex reasoning tasks by scaling test-time compute, but this scaling must be allocated in line with task difficulty. On one hand, short reasoning (underthinking) leads to errors on harder problems that require extended reasoning steps; but, excessively long reasoning (overthinking) can be token-inefficient, generating unnecessary steps even after reaching a correct intermediate solution. We refer to this as under-adaptivity, where the model fails to modulate its response length appropriately given problems of varying difficulty. To address under-adaptivity and strike a balance between under- and overthinking, we propose TRAAC (Think Right with Adaptive, Attentive Compression), an online post-training RL method that leverages the model's self-attention over a long reasoning trajectory to identify important steps and prune redundant ones. TRAAC also estimates difficulty and incorporates it into training rewards, thereby learning to allocate reasoning budget commensurate with example difficulty. Our approach improves accuracy, reduces reasoning steps, and enables adaptive thinking compared to base models and other RL baselines. Across a variety of tasks (AIME, AMC, GPQA-D, BBEH), TRAAC (Qwen3-4B) achieves an average absolute accuracy gain of 8.4% with a relative reduction in reasoning length of 36.8% compared to the base model, and a 7.9% accuracy gain paired with a 29.4% length drop compared to the best RL baseline. TRAAC also shows strong generalization: although our models are trained on math datasets, they show accuracy and efficiency gains on out-of-distribution non-math datasets like GPQA-D, BBEH, and OptimalThinkingBench. Our analysis further verifies that TRAAC provides fine-grained adjustments to thinking budget based on difficulty and that a combination of task-difficulty calibration and attention-based compression yields gains across diverse tasks.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "134",
        "title": "ImageNet-Think-250K: A Large-Scale Synthetic Dataset for Multimodal Reasoning for Vision Language Models",
        "author": [
            "Krishna Teja Chitty-Venkata",
            "Murali Emani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01582",
        "abstract": "We develop ImageNet-Think, a multimodal reasoning dataset designed to aid the development of Vision Language Models (VLMs) with explicit reasoning capabilities. Our dataset is built on 250,000 images from ImageNet21k dataset, providing structured thinking tokens and corresponding answers. Our synthetic dataset is generated by two state-of-the-art VLMs: GLM-4.1V-9B-Thinking and Kimi-VL-A3B-Thinking-2506. Each image is accompanied by two pairs of thinking-answer sequences, creating a resource for training and evaluating multimodal reasoning models. We capture the step-by-step reasoning process of VLMs and the final descriptive answers. Our goal with this dataset is to enable the development of more robust VLMs while contributing to the broader understanding of multimodal reasoning mechanisms. The dataset and evaluation benchmarks will be publicly available to aid research in reasoning/thinking multimodal VLMs.",
        "tags": [
            "GLM",
            "VLM"
        ]
    },
    {
        "id": "135",
        "title": "ReSSFormer: A Recursive Sparse Structured Transformer for Scalable and Long-Context Reasoning",
        "author": [
            "Haochen You",
            "Baojing Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01585",
        "abstract": "While Transformer architectures have demonstrated impressive scalability across domains, they continue to face challenges in long-context reasoning, computational efficiency, and structural generalization - largely due to rigid layer stacking, dense attention, and reliance on positional encodings. We present ReSSFormer, a Recursive Sparse Structured Transformer that integrates three complementary innovations: Recurrent Reasoning & Memory Unit (R2MU) for iterative reasoning with bounded depth, Adaptive Sparse Attention Module (ASAM) for efficient and focused context selection, and Self-Organizing Encoder Structure (SOES) for position-free structure induction. ReSSFormer replaces conventional depth stacking with recurrent inference, substitutes full attention with token- and expert-level sparsity, and models latent token topology directly from content. Across language modeling, multi-hop QA, and structure-sensitive tasks, ReSSFormer consistently outperforms strong baselines under comparable FLOPs and parameter budgets, highlighting its scalability, efficiency, and structural flexibility.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "136",
        "title": "AdvEvo-MARL: Shaping Internalized Safety through Adversarial Co-Evolution in Multi-Agent Reinforcement Learning",
        "author": [
            "Zhenyu Pan",
            "Yiting Zhang",
            "Zhuo Liu",
            "Yolo Yunlong Tang",
            "Zeliang Zhang",
            "Haozheng Luo",
            "Yuwei Han",
            "Jianshu Zhang",
            "Dennis Wu",
            "Hong-Yu Chen",
            "Haoran Lu",
            "Haoyang Fang",
            "Manling Li",
            "Chenliang Xu",
            "Philip S. Yu",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01586",
        "abstract": "LLM-based multi-agent systems excel at planning, tool use, and role coordination, but their openness and interaction complexity also expose them to jailbreak, prompt-injection, and adversarial collaboration. Existing defenses fall into two lines: (i) self-verification that asks each agent to pre-filter unsafe instructions before execution, and (ii) external guard modules that police behaviors. The former often underperforms because a standalone agent lacks sufficient capacity to detect cross-agent unsafe chains and delegation-induced risks; the latter increases system overhead and creates a single-point-of-failure-once compromised, system-wide safety collapses, and adding more guards worsens cost and complexity. To solve these challenges, we propose AdvEvo-MARL, a co-evolutionary multi-agent reinforcement learning framework that internalizes safety into task agents. Rather than relying on external guards, AdvEvo-MARL jointly optimizes attackers (which synthesize evolving jailbreak prompts) and defenders (task agents trained to both accomplish their duties and resist attacks) in adversarial learning environments. To stabilize learning and foster cooperation, we introduce a public baseline for advantage estimation: agents within the same functional group share a group-level mean-return baseline, enabling lower-variance updates and stronger intra-group coordination. Across representative attack scenarios, AdvEvo-MARL consistently keeps attack-success rate (ASR) below 20%, whereas baselines reach up to 38.33%, while preserving-and sometimes improving-task accuracy (up to +3.67% on reasoning tasks). These results show that safety and utility can be jointly improved without relying on extra guard agents or added system overhead.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "137",
        "title": "CLUE: Non-parametric Verification from Experience via Hidden-State Clustering",
        "author": [
            "Zhenwen Liang",
            "Ruosen Li",
            "Yujun Zhou",
            "Linfeng Song",
            "Dian Yu",
            "Xinya Du",
            "Haitao Mi",
            "Dong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01591",
        "abstract": "Assessing the quality of Large Language Model (LLM) outputs presents a critical challenge. Previous methods either rely on text-level information (e.g., reward models, majority voting), which can overfit to superficial cues, or on calibrated confidence from token probabilities, which would fail on less-calibrated models. Yet both of these signals are, in fact, partial projections of a richer source of information: the model's internal hidden states. Early layers, closer to token embeddings, preserve semantic and lexical features that underpin text-based judgments, while later layers increasingly align with output logits, embedding confidence-related information. This paper explores hidden states directly as a unified foundation for verification. We show that the correctness of a solution is encoded as a geometrically separable signature within the trajectory of hidden activations. To validate this, we present Clue (Clustering and Experience-based Verification), a deliberately minimalist, non-parametric verifier. With no trainable parameters, CLUE only summarizes each reasoning trace by an hidden state delta and classifies correctness via nearest-centroid distance to ``success'' and ``failure'' clusters formed from past experience. The simplicity of this method highlights the strength of the underlying signal. Empirically, CLUE consistently outperforms LLM-as-a-judge baselines and matches or exceeds modern confidence-based methods in reranking candidates, improving both top-1 and majority-vote accuracy across AIME 24/25 and GPQA. As a highlight, on AIME 24 with a 1.5B model, CLUE boosts accuracy from 56.7% (majority@64) to 70.0% (top-maj@16).",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "138",
        "title": "Real-time Multi-Plane Segmentation Based on GPU Accelerated High-Resolution 3D Voxel Mapping for Legged Robot Locomotion",
        "author": [
            "Shun Niijima",
            "Ryoichi Tsuzaki",
            "Noriaki Takasugi",
            "Masaya Kinoshita"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01592",
        "abstract": "This paper proposes a real-time multi-plane segmentation method based on GPU-accelerated high-resolution 3D voxel mapping for legged robot locomotion. Existing online planar mapping approaches struggle to balance accuracy and computational efficiency: direct depth image segmentation from specific sensors suffers from poor temporal integration, height map-based methods cannot represent complex 3D structures like overhangs, and voxel-based plane segmentation remains unexplored for real-time applications. To address these limitations, we develop a novel framework that integrates vertex-based connected component labeling with random sample consensus based plane detection and convex hull, leveraging GPU parallel computing to rapidly extract planar regions from point clouds accumulated in high-resolution 3D voxel maps. Experimental results demonstrate that the proposed method achieves fast and accurate 3D multi-plane segmentation at over 30 Hz update rate even at a resolution of 0.01 m, enabling the detected planes to be utilized in real time for locomotion tasks. Furthermore, we validate the effectiveness of our approach through experiments in both simulated environments and physical legged robot platforms, confirming robust locomotion performance when considering 3D planar structures.",
        "tags": [
            "3D",
            "Detection",
            "Robotics",
            "Segmentation"
        ]
    },
    {
        "id": "139",
        "title": "Securing generative artificial intelligence with parallel magnetic tunnel junction true randomness",
        "author": [
            "Youwei Bao",
            "Shuhan Yang",
            "Hyunsoo Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01598",
        "abstract": "Deterministic pseudo random number generators (PRNGs) used in generative artificial intelligence (GAI) models produce predictable patterns vulnerable to exploitation by attackers. Conventional defences against the vulnerabilities often come with significant energy and latency overhead. Here, we embed hardware-generated true random bits from spin-transfer torque magnetic tunnel junctions (STT-MTJs) to address the challenges. A highly parallel, FPGA-assisted prototype computing system delivers megabit-per-second true random numbers, passing NIST randomness tests after in-situ operations with minimal overhead. Integrating the hardware random bits into a generative adversarial network (GAN) trained on CIFAR-10 reduces insecure outputs by up to 18.6 times compared to the low-quality random number generators (RNG) baseline. With nanosecond switching speed, high energy efficiency, and established scalability, our STT-MTJ-based system holds the potential to scale beyond 106 parallel cells, achieving gigabit-per-second throughput suitable for large language model sampling. This advancement highlights spintronic RNGs as practical security components for next-generation GAI systems.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "140",
        "title": "A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation",
        "author": [
            "Neal Gregory Lawton",
            "Alfy Samuel",
            "Anoop Kumar",
            "Daben Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01600",
        "abstract": "A Comparison of Independent and Joint Fine-tuning Strategies for Retrieval-Augmented Generation Download PDF Neal Gregory Lawton, Alfy Samuel, Anoop Kumar, Daben Liu Published: 20 Aug 2025, Last Modified: 17 Sept 2025EMNLP 2025 FindingsConference, Publication Chairs, AuthorsRevisionsBibTeXCC BY 4.0 Keywords: Retrieval-Augmented Generation (RAG), Large Language Models (LLMs), Fine-tuning, Question Answering, Joint fine-tuning TL;DR: We evaluate and compare strategies for fine-tuning Retrieval Augmented Generation (RAG) pipelines, including independent fine-tuning, joint fine-tuning, and two-phase fine-tuning. Abstract: Retrieval augmented generation (RAG) is a popular framework for question answering that is powered by two large language models (LLMs): an embedding model that retrieves context documents from a database that are relevant to a given question, and a generator model that uses the retrieved context to generate an answer to the question. Both the embedding and generator models can be fine-tuned to increase performance of a RAG pipeline on a new task, but multiple fine-tuning strategies exist with different costs and benefits. In this paper, we evaluate and compare several RAG fine-tuning strategies, including independent, joint, and two-phase fine-tuning. In our experiments, we observe that all of these strategies achieve about equal improvement in EM and F1 generation quality metrics, although they have significantly different computational costs. We conclude the optimal fine-tuning strategy to use depends on whether the training dataset includes context labels and whether a grid search over the learning rates for the embedding and generator models is required.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "141",
        "title": "MiniBEE: A New Form Factor for Compact Bimanual Dexterity",
        "author": [
            "Sharfin Islam",
            "Zewen Chen",
            "Zhanpeng He",
            "Swapneel Bhatt",
            "Andres Permuy",
            "Brock Taylor",
            "James Vickery",
            "Pedro Piacenza",
            "Cheng Zhang",
            "Matei Ciocarlie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01603",
        "abstract": "Bimanual robot manipulators can achieve impressive dexterity, but typically rely on two full six- or seven- degree-of-freedom arms so that paired grippers can coordinate effectively. This traditional framework increases system complexity while only exploiting a fraction of the overall workspace for dexterous interaction. We introduce the MiniBEE (Miniature Bimanual End-effector), a compact system in which two reduced-mobility arms (3+ DOF each) are coupled into a kinematic chain that preserves full relative positioning between grippers. To guide our design, we formulate a kinematic dexterity metric that enlarges the dexterous workspace while keeping the mechanism lightweight and wearable. The resulting system supports two complementary modes: (i) wearable kinesthetic data collection with self-tracked gripper poses, and (ii) deployment on a standard robot arm, extending dexterity across its entire workspace. We present kinematic analysis and design optimization methods for maximizing dexterous range, and demonstrate an end-to-end pipeline in which wearable demonstrations train imitation learning policies that perform robust, real-world bimanual manipulation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "142",
        "title": "ActiveUMI: Robotic Manipulation with Active Perception from Robot-Free Human Demonstrations",
        "author": [
            "Qiyuan Zeng",
            "Chengmeng Li",
            "Jude St. John",
            "Zhongyi Zhou",
            "Junjie Wen",
            "Guorui Feng",
            "Yichen Zhu",
            "Yi Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01607",
        "abstract": "We present ActiveUMI, a framework for a data collection system that transfers in-the-wild human demonstrations to robots capable of complex bimanual manipulation. ActiveUMI couples a portable VR teleoperation kit with sensorized controllers that mirror the robot's end-effectors, bridging human-robot kinematics via precise pose alignment. To ensure mobility and data quality, we introduce several key techniques, including immersive 3D model rendering, a self-contained wearable computer, and efficient calibration methods. ActiveUMI's defining feature is its capture of active, egocentric perception. By recording an operator's deliberate head movements via a head-mounted display, our system learns the crucial link between visual attention and manipulation. We evaluate ActiveUMI on six challenging bimanual tasks. Policies trained exclusively on ActiveUMI data achieve an average success rate of 70\\% on in-distribution tasks and demonstrate strong generalization, retaining a 56\\% success rate when tested on novel objects and in new environments. Our results demonstrate that portable data collection systems, when coupled with learned active perception, provide an effective and scalable pathway toward creating generalizable and highly capable real-world robot policies.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "143",
        "title": "PychoBench: Evaluating the Psychology Intelligence of Large Language Models",
        "author": [
            "Min Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01611",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable success across a wide range of industries, primarily due to their impressive generative abilities. Yet, their potential in applications requiring cognitive abilities, such as psychological counseling, remains largely untapped. This paper investigates the key question: Can LLMs be effectively applied to psychological counseling? To determine whether an LLM can effectively take on the role of a psychological counselor, the first step is to assess whether it meets the qualifications required for such a role, namely the ability to pass the U.S. National Counselor Certification Exam (NCE). This is because, just as a human counselor must pass a certification exam to practice, an LLM must demonstrate sufficient psychological knowledge to meet the standards required for such a role. To address this, we introduce PsychoBench, a benchmark grounded in http://U.S.national counselor examinations, a licensure test for professional counselors that requires about 70% accuracy to pass. PsychoBench comprises approximately 2,252 carefully curated single-choice questions, crafted to require deep understanding and broad enough to cover various sub-disciplines of psychology. This benchmark provides a comprehensive assessment of an LLM's ability to function as a counselor. Our evaluation shows that advanced models such as GPT-4o, Llama3.3-70B, and Gemma3-27B achieve well above the passing threshold, while smaller open-source models (e.g., Qwen2.5-7B, Mistral-7B) remain far below it. These results suggest that only frontier LLMs are currently capable of meeting counseling exam standards, highlighting both the promise and the challenges of developing psychology-oriented LLMs.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "144",
        "title": "Efficient Training of Robust Traditional Chinese LLaMA-1B on a Single Consumer GPU: Continual Pre-training, SFT, and DPO",
        "author": [
            "Yu-Cheng Chih",
            "Ming-Tao Duan",
            "Yong-Hao Hou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01616",
        "abstract": "Small Language Models (SLMs) enable cost-effective, on-device and latency-sensitive AI applications, yet their deployment in Traditional Chinese (TC) remains hindered by token-level instability - models unpredictably emit non-TC characters or code-switch into other languages. We address this practical reliability gap by creating PureTC-1B, a three-stage stabilization pipeline for Llama-3.2-1B-Instruct (an open-weight, instruction-tuned model released by Meta) using parameter-efficient LoRA adapters. Our method combines Continual Pre-Training (CPT) on TC-centric corpora, Supervised Fine-Tuning (SFT) with instruction data, and Direct Preference Optimization (DPO) using TC-adherence preferences to improve monolingual robustness without full-model retraining. On a benchmark designed to simulate real-world usage, PureTC-1B achieves a 51.3% relative reduction (micro-average) in non-TC output tokens versus the base model. On a Named Entity Translation (NET) task, PureTC-1B further reduces incorrect-language tokens by 77.2% relative to Llama-3B and 57.2% relative to Qwen-1.5B, indicating that robust TC adherence is attainable even at the 1B scale. The pipeline is reproducible, adapter-only, and hardware-friendly, offering practitioners a practical recipe to enhance language stability for TC and potentially other non-English languages.",
        "tags": [
            "DPO",
            "LLaMA",
            "LoRA",
            "Qwen"
        ]
    },
    {
        "id": "145",
        "title": "AMAS: Adaptively Determining Communication Topology for LLM-based Multi-Agent System",
        "author": [
            "Hui Yi Leong",
            "Yuheng Li",
            "Yuqing Wu",
            "Wenwen Ouyang",
            "Wei Zhu",
            "Jiechao Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01617",
        "abstract": "Although large language models (LLMs) have revolutionized natural language processing capabilities, their practical implementation as autonomous multi-agent systems (MAS) for industrial problem-solving encounters persistent barriers. Conventional MAS architectures are fundamentally restricted by inflexible, hand-crafted graph topologies that lack contextual responsiveness, resulting in diminished efficacy across varied academic and commercial workloads. To surmount these constraints, we introduce AMAS, a paradigm-shifting framework that redefines LLM-based MAS through a novel dynamic graph designer. This component autonomously identifies task-specific optimal graph configurations via lightweight LLM adaptation, eliminating the reliance on monolithic, universally applied structural templates. Instead, AMAS exploits the intrinsic properties of individual inputs to intelligently direct query trajectories through task-optimized agent pathways. Rigorous validation across question answering, mathematical deduction, and code generation benchmarks confirms that AMAS systematically exceeds state-of-the-art single-agent and multi-agent approaches across diverse LLM architectures. Our investigation establishes that context-sensitive structural adaptability constitutes a foundational requirement for high-performance LLM MAS deployments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "146",
        "title": "MPMAvatar: Learning 3D Gaussian Avatars with Accurate and Robust Physics-Based Dynamics",
        "author": [
            "Changmin Lee",
            "Jihyun Lee",
            "Tae-Kyun Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01619",
        "abstract": "While there has been significant progress in the field of 3D avatar creation from visual observations, modeling physically plausible dynamics of humans with loose garments remains a challenging problem. Although a few existing works address this problem by leveraging physical simulation, they suffer from limited accuracy or robustness to novel animation inputs. In this work, we present MPMAvatar, a framework for creating 3D human avatars from multi-view videos that supports highly realistic, robust animation, as well as photorealistic rendering from free viewpoints. For accurate and robust dynamics modeling, our key idea is to use a Material Point Method-based simulator, which we carefully tailor to model garments with complex deformations and contact with the underlying body by incorporating an anisotropic constitutive model and a novel collision handling algorithm. We combine this dynamics modeling scheme with our canonical avatar that can be rendered using 3D Gaussian Splatting with quasi-shadowing, enabling high-fidelity rendering for physically realistic animations. In our experiments, we demonstrate that MPMAvatar significantly outperforms the existing state-of-the-art physics-based avatar in terms of (1) dynamics modeling accuracy, (2) rendering accuracy, and (3) robustness and efficiency. Additionally, we present a novel application in which our avatar generalizes to unseen interactions in a zero-shot manner-which was not achievable with previous learning-based methods due to their limited simulation generalizability. Our project page is at: https://KAISTChangmin.github.io/MPMAvatar/",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "147",
        "title": "VLA-R1: Enhancing Reasoning in Vision-Language-Action Models",
        "author": [
            "Angen Ye",
            "Zeyu Zhang",
            "Boyuan Wang",
            "Xiaofeng Wang",
            "Dapeng Zhang",
            "Zheng Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01623",
        "abstract": "Vision-Language-Action (VLA) models aim to unify perception, language understanding, and action generation, offering strong cross-task and cross-scene generalization with broad impact on embodied AI. However, current VLA models often lack explicit step-by-step reasoning, instead emitting final actions without considering affordance constraints or geometric relations. Their post-training pipelines also rarely reinforce reasoning quality, relying primarily on supervised fine-tuning with weak reward design. To address these challenges, we present VLA-R1, a reasoning-enhanced VLA that integrates Reinforcement Learning from Verifiable Rewards (RLVR) with Group Relative Policy Optimization (GRPO) to systematically optimize both reasoning and execution. Specifically, we design an RLVR-based post-training strategy with verifiable rewards for region alignment, trajectory consistency, and output formatting, thereby strengthening reasoning robustness and execution accuracy. Moreover, we develop VLA-CoT-13K, a high-quality dataset that provides chain-of-thought supervision explicitly aligned with affordance and trajectory annotations. Furthermore, extensive evaluations on in-domain, out-of-domain, simulation, and real-robot platforms demonstrate that VLA-R1 achieves superior generalization and real-world performance compared to prior VLA methods. We plan to release the model, code, and dataset following the publication of this work. Code: https://github.com/GigaAI-research/VLA-R1. Website: https://gigaai-research.github.io/VLA-R1.",
        "tags": [
            "CoT",
            "GRPO",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "148",
        "title": "Quagmires in SFT-RL Post-Training: When High SFT Scores Mislead and What to Use Instead",
        "author": [
            "Feiyang Kang",
            "Michael Kuchnik",
            "Karthik Padthe",
            "Marin Vlastelica",
            "Ruoxi Jia",
            "Carole-Jean Wu",
            "Newsha Ardalani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01624",
        "abstract": "In post-training for reasoning Large Language Models (LLMs), the current state of practice trains LLMs in two independent stages: Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR, shortened as ``RL'' below). In this work, we challenge whether high SFT scores translate to improved performance after RL. We provide extensive counter-examples where this is not true. We find high SFT scores can be biased toward simpler or more homogeneous data and are not reliably predictive of subsequent RL gains or scaled-up post-training effectiveness. In some cases, RL training on models with improved SFT performance could lead to substantially worse outcome compared to RL on the base model without SFT. We study alternative metrics and identify generalization loss on held-out reasoning examples and Pass@large k performance to provide strong proxies for the RL outcome. We trained hundreds of models up to 12B-parameter with SFT and RLVR via GRPO and ran extensive evaluations on 7 math benchmarks with up to 256 repetitions, spending $>$1M GPU hours. Experiments include models from Llama3, Mistral-Nemo, Qwen3 and multiple state-of-the-art SFT/RL datasets. Compared to directly predicting from pre-RL performance, prediction based on generalization loss and Pass@large k achieves substantial higher precision, improving $R^2$ coefficient and Spearman's rank correlation coefficient by up to 0.5 (2x). This provides strong utility for broad use cases. For example, in most experiments, we find SFT training on unique examples for a one epoch underperforms training on half examples for two epochs, either after SFT or SFT-then-RL; With the same SFT budget, training only on short examples may lead to better SFT performance, though, it often leads to worse outcome after RL compared to training on examples with varying lengths. Evaluation tool will be open-sourced.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "149",
        "title": "Demystifying Synthetic Data in LLM Pre-training: A Systematic Study of Scaling Laws, Benefits, and Pitfalls",
        "author": [
            "Feiyang Kang",
            "Newsha Ardalani",
            "Michael Kuchnik",
            "Youssef Emad",
            "Mostafa Elhoushi",
            "Shubhabrata Sengupta",
            "Shang-Wen Li",
            "Ramya Raghavendra",
            "Ruoxi Jia",
            "Carole-Jean Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01631",
        "abstract": "Training data plays a crucial role in Large Language Models (LLM) scaling, yet high quality data is of limited supply. Synthetic data techniques offer a potential path toward sidestepping these limitations. We conduct a large-scale empirical investigation (>1000 LLMs with >100k GPU hours) using a unified protocol and scaling laws, comparing natural web data, diverse synthetic types (rephrased text, generated textbooks), and mixtures of natural and synthetic data. Specifically, we found pre-training on rephrased synthetic data \\textit{alone} is not faster than pre-training on natural web texts; while pre-training on 1/3 rephrased synthetic data mixed with 2/3 natural web texts can speed up 5-10x (to reach the same validation loss) at larger data budgets. Pre-training on textbook-style synthetic data \\textit{alone} results in notably higher loss on many downstream domains especially at small data budgets. \"Good\" ratios of synthetic data in training data mixtures depend on the model size and data budget, empirically converging to ~30% for rephrased synthetic data. Larger generator models do not necessarily yield better pre-training data than ~8B-param models. These results contribute mixed evidence on \"model collapse\" during large-scale single-round (n=1) model training on synthetic data--training on rephrased synthetic data shows no degradation in performance in foreseeable scales whereas training on mixtures of textbook-style pure-generated synthetic data shows patterns predicted by \"model collapse\". Our work demystifies synthetic data in pre-training, validates its conditional benefits, and offers practical guidance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "150",
        "title": "CAT: Curvature-Adaptive Transformers for Geometry-Aware Learning",
        "author": [
            "Ryan Y. Lin",
            "Siddhartha Ojha",
            "Nicholas Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01634",
        "abstract": "Transformers achieve strong performance across diverse domains but implicitly assume Euclidean geometry in their attention mechanisms, limiting their effectiveness on data with non-Euclidean structure. While recent extensions to hyperbolic and spherical spaces show promise for hierarchical and cyclical patterns, respectively, they require committing to a single geometry a priori, reducing flexibility when data exhibits mixed geometric properties. We introduce the Curvature-Adaptive Transformer (CAT), a novel architecture that dynamically learns per-token routing across three geometric attention branches through a lightweight, differentiable gating mechanism. Unlike fixed-geometry approaches, CAT enables adaptive geometric specialization, routing tokens to the appropriate curvature based on their local relational structure. The routing network provides interpretable curvature preferences while each branch employs geometry-specific operations optimized for its respective manifold. On knowledge graph completion benchmarks (FB15k-237, WN18RR), CAT achieves approximately 10% improvements in MRR and Hits@10 over fixed-geometry baselines with minimal overhead (5% parameter increase, comparable inference time). These results demonstrate that learned geometric adaptation outperforms any single fixed geometry for complex relational reasoning, establishing CAT as a scalable and interpretable foundation for mixture-of-geometry architectures across language, vision, and multimodal domains.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "151",
        "title": "MIMIC: Integrating Diverse Personality Traits for Better Game Testing Using Large Language Model",
        "author": [
            "Yifei Chen",
            "Sarra Habchi",
            "Lili Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01635",
        "abstract": "Modern video games pose significant challenges for traditional automated testing algorithms, yet intensive testing is crucial to ensure game quality. To address these challenges, researchers designed gaming agents using Reinforcement Learning, Imitation Learning, or Large Language Models. However, these agents often neglect the diverse strategies employed by human players due to their different personalities, resulting in repetitive solutions in similar situations. Without mimicking varied gaming strategies, these agents struggle to trigger diverse in-game interactions or uncover edge cases.\nIn this paper, we present MIMIC, a novel framework that integrates diverse personality traits into gaming agents, enabling them to adopt different gaming strategies for similar situations. By mimicking different playstyles, MIMIC can achieve higher test coverage and richer in-game interactions across different games. It also outperforms state-of-the-art agents in Minecraft by achieving a higher task completion rate and providing more diverse solutions. These results highlight MIMIC's significant potential for effective game testing.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "152",
        "title": "Towards Human-Centered RegTech: Unpacking Professionals' Strategies and Needs for Using LLMs Safely",
        "author": [
            "Siying Hu",
            "Yaxing Yao",
            "Zhicong Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01638",
        "abstract": "Large Language Models are profoundly changing work patterns in high-risk professional domains, yet their application also introduces severe and underexplored compliance risks. To investigate this issue, we conducted semi-structured interviews with 24 highly-skilled knowledge workers from industries such as law, healthcare, and finance. The study found that these experts are commonly concerned about sensitive information leakage, intellectual property infringement, and uncertainty regarding the quality of model outputs. In response, they spontaneously adopt various mitigation strategies, such as actively distorting input data and limiting the details in their prompts. However, the effectiveness of these spontaneous efforts is limited due to a lack of specific compliance guidance and training for Large Language Models. Our research reveals a significant gap between current NLP tools and the actual compliance needs of experts. This paper positions these valuable empirical findings as foundational work for building the next generation of Human-Centered, Compliance-Driven Natural Language Processing for Regulatory Technology (RegTech), providing a critical human-centered perspective and design requirements for engineering NLP systems that can proactively support expert compliance workflows.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "153",
        "title": "Understanding the Geospatial Reasoning Capabilities of LLMs: A Trajectory Recovery Perspective",
        "author": [
            "Thinh Hung Truong",
            "Jey Han Lau",
            "Jianzhong Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01639",
        "abstract": "We explore the geospatial reasoning capabilities of Large Language Models (LLMs), specifically, whether LLMs can read road network maps and perform navigation. We frame trajectory recovery as a proxy task, which requires models to reconstruct masked GPS traces, and introduce GLOBALTRACE, a dataset with over 4,000 real-world trajectories across diverse regions and transportation modes. Using road network as context, our prompting framework enables LLMs to generate valid paths without accessing any external navigation tools. Experiments show that LLMs outperform off-the-shelf baselines and specialized trajectory recovery models, with strong zero-shot generalization. Fine-grained analysis shows that LLMs have strong comprehension of the road network and coordinate systems, but also pose systematic biases with respect to regions and transportation modes. Finally, we demonstrate how LLMs can enhance navigation experiences by reasoning over maps in flexible ways to incorporate user preferences.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "154",
        "title": "FideDiff: Efficient Diffusion Model for High-Fidelity Image Motion Deblurring",
        "author": [
            "Xiaoyang Liu",
            "Zhengyan Zhou",
            "Zihang Xu",
            "Jiezhang Cao",
            "Zheng Chen",
            "Yulun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01641",
        "abstract": "Recent advancements in image motion deblurring, driven by CNNs and transformers, have made significant progress. Large-scale pre-trained diffusion models, which are rich in true-world modeling, have shown great promise for high-quality image restoration tasks such as deblurring, demonstrating stronger generative capabilities than CNN and transformer-based methods. However, challenges such as unbearable inference time and compromised fidelity still limit the full potential of the diffusion models. To address this, we introduce FideDiff, a novel single-step diffusion model designed for high-fidelity deblurring. We reformulate motion deblurring as a diffusion-like process where each timestep represents a progressively blurred image, and we train a consistency model that aligns all timesteps to the same clean image. By reconstructing training data with matched blur trajectories, the model learns temporal consistency, enabling accurate one-step deblurring. We further enhance model performance by integrating Kernel ControlNet for blur kernel estimation and introducing adaptive timestep prediction. Our model achieves superior performance on full-reference metrics, surpassing previous diffusion-based methods and matching the performance of other state-of-the-art models. FideDiff offers a new direction for applying pre-trained diffusion models to high-fidelity image restoration tasks, establishing a robust baseline for further advancing diffusion models in real-world industrial applications. Our dataset and code will be available at https://github.com/xyLiu339/FideDiff.",
        "tags": [
            "ControlNet",
            "Deblurring",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "155",
        "title": "FailSafe: Reasoning and Recovery from Failures in Vision-Language-Action Models",
        "author": [
            "Zijun Lin",
            "Jiafei Duan",
            "Haoquan Fang",
            "Dieter Fox",
            "Ranjay Krishna",
            "Cheston Tan",
            "Bihan Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01642",
        "abstract": "Recent advances in robotic manipulation have integrated low-level robotic control into Vision-Language Models (VLMs), extending them into Vision-Language-Action (VLA) models. Although state-of-the-art VLAs achieve strong performance in downstream robotic applications, supported by large-scale crowd-sourced robot training data, they still inevitably encounter failures during execution. Enabling robots to reason about and recover from unpredictable and abrupt failures remains a critical challenge. Existing robotic manipulation datasets, collected in either simulation or the real world, primarily provide only ground-truth trajectories, leaving robots unable to recover once failures occur. Moreover, the few datasets that address failure detection typically offer only textual explanations, which are difficult to utilize directly in VLA models. To address this gap, we introduce FailSafe, a novel failure generation and recovery system that automatically produces diverse failure cases paired with executable recovery actions. FailSafe can be seamlessly applied to any manipulation task in any simulator, enabling scalable creation of failure-action data. To demonstrate its effectiveness, we fine-tune LLaVa-OneVision-7B (LLaVa-OV-7B) to build FailSafe-VLM. Experimental results show that FailSafe-VLM successfully helps robotic arm detect and recover from potential failures, improving the performance of three state-of-the-art VLA models pi0-FAST, OpenVLA, OpenVLA-OFT) by up to 22.6% on average across several tasks in Maniskill. Furthermore, FailSafe-VLM could generalize across different spatial configurations, camera viewpoints, and robotic embodiments. We plan to release the FailSafe code to the community.",
        "tags": [
            "Detection",
            "LLaVA",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "156",
        "title": "Support Basis: Fast Attention Beyond Bounded Entries",
        "author": [
            "Maryam Aliakbarpour",
            "Vladimir Braverman",
            "Junze Yin",
            "Haochen Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01643",
        "abstract": "The quadratic complexity of softmax attention remains a central bottleneck in scaling large language models (LLMs). [Alman and Song, NeurIPS 2023] proposed a sub-quadratic attention approximation algorithm, but it works only under the restrictive bounded-entry assumption. Since this assumption rarely holds in practice, its applicability to modern LLMs is limited.\nIn this paper, we introduce support-basis decomposition, a new framework for efficient attention approximation beyond bounded entries. We empirically demonstrate that the entries of the query and key matrices exhibit sub-Gaussian behavior. Our approach uses this property to split large and small entries, enabling exact computation on sparse components and polynomial approximation on dense components. We establish rigorous theoretical guarantees, proving a sub-quadratic runtime, and extend the method to a multi-threshold setting that eliminates all distributional assumptions. Furthermore, we provide the first theoretical justification for the empirical success of polynomial attention [Kacham, Mirrokni, and Zhong, ICML 2024], showing that softmax attention can be closely approximated by a combination of multiple polynomial attentions with sketching.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "157",
        "title": "NLP Methods for Detecting Novel LLM Jailbreaks and Keyword Analysis with BERT",
        "author": [
            "John Hawkins",
            "Aditya Pramar",
            "Rodney Beard",
            "Rohitash Chandra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01644",
        "abstract": "Large Language Models (LLMs) suffer from a range of vulnerabilities that allow malicious users to solicit undesirable responses through manipulation of the input text. These so-called jailbreak prompts are designed to trick the LLM into circumventing the safety guardrails put in place to keep responses acceptable to the developer's policies. In this study, we analyse the ability of different machine learning models to distinguish jailbreak prompts from genuine uses, including looking at our ability to identify jailbreaks that use previously unseen strategies. Our results indicate that using current datasets the best performance is achieved by fine tuning a Bidirectional Encoder Representations from Transformers (BERT) model end-to-end for identifying jailbreaks. We visualise the keywords that distinguish jailbreak from genuine prompts and conclude that explicit reflexivity in prompt structure could be a signal of jailbreak intention.",
        "tags": [
            "BERT",
            "LLM"
        ]
    },
    {
        "id": "158",
        "title": "Position: Privacy Is Not Just Memorization!",
        "author": [
            "Niloofar Mireshghallah",
            "Tianshi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01645",
        "abstract": "The discourse on privacy risks in Large Language Models (LLMs) has disproportionately focused on verbatim memorization of training data, while a constellation of more immediate and scalable privacy threats remain underexplored. This position paper argues that the privacy landscape of LLM systems extends far beyond training data extraction, encompassing risks from data collection practices, inference-time context leakage, autonomous agent capabilities, and the democratization of surveillance through deep inference attacks. We present a comprehensive taxonomy of privacy risks across the LLM lifecycle -- from data collection through deployment -- and demonstrate through case studies how current privacy frameworks fail to address these multifaceted threats. Through a longitudinal analysis of 1,322 AI/ML privacy papers published at leading conferences over the past decade (2016--2025), we reveal that while memorization receives outsized attention in technical research, the most pressing privacy harms lie elsewhere, where current technical approaches offer little traction and viable paths forward remain unclear. We call for a fundamental shift in how the research community approaches LLM privacy, moving beyond the narrow focus of current technical solutions and embracing interdisciplinary approaches that address the sociotechnical nature of these emerging threats.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "159",
        "title": "The Unseen Frontier: Pushing the Limits of LLM Sparsity with Surrogate-Free ADMM",
        "author": [
            "Kwanhee Lee",
            "Hyeondo Jang",
            "Dongyeop Lee",
            "Dan Alistarh",
            "Namhoon Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01650",
        "abstract": "Neural network pruning is a promising technique to mitigate the excessive computational and memory requirements of large language models (LLMs). Despite its promise, however, progress in this area has diminished, as conventional methods are seemingly unable to surpass moderate sparsity levels (50-60%) without severely degrading model accuracy. This work breaks through the current impasse, presenting a principled and effective method called $\\texttt{Elsa}$, which achieves extreme sparsity levels of up to 90% while retaining high model fidelity. This is done by identifying several limitations in current practice, all of which can be traced back to their reliance on a surrogate objective formulation. $\\texttt{Elsa}$ tackles this issue directly and effectively via standard and well-established constrained optimization techniques based on ADMM. Our extensive experiments across a wide range of models and scales show that $\\texttt{Elsa}$ achieves substantial improvements over existing methods; e.g., it achieves 7.8$\\times$ less perplexity than the best existing method on LLaMA-2-7B at 90% sparsity. Furthermore, we present $\\texttt{Elsa}_{\\text{-L}}$, a quantized variant that scales to extremely large models (27B), and establish its theoretical convergence guarantees. These results highlight meaningful progress in advancing the frontier of LLM sparsity, while promising that significant opportunities for further advancement may remain in directions that have so far attracted limited exploration.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "160",
        "title": "LadderMoE: Ladder-Side Mixture of Experts Adapters for Bronze Inscription Recognition",
        "author": [
            "Rixin Zhou",
            "Peiqiang Qiu",
            "Qian Zhang",
            "Chuntao Li",
            "Xi Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01651",
        "abstract": "Bronze inscriptions (BI), engraved on ritual vessels, constitute a crucial stage of early Chinese writing and provide indispensable evidence for archaeological and historical studies. However, automatic BI recognition remains difficult due to severe visual degradation, multi-domain variability across photographs, rubbings, and tracings, and an extremely long-tailed character distribution. To address these challenges, we curate a large-scale BI dataset comprising 22454 full-page images and 198598 annotated characters spanning 6658 unique categories, enabling robust cross-domain evaluation. Building on this resource, we develop a two-stage detection-recognition pipeline that first localizes inscriptions and then transcribes individual characters. To handle heterogeneous domains and rare classes, we equip the pipeline with LadderMoE, which augments a pretrained CLIP encoder with ladder-style MoE adapters, enabling dynamic expert specialization and stronger robustness. Comprehensive experiments on single-character and full-page recognition tasks demonstrate that our method substantially outperforms state-of-the-art scene text recognition baselines, achieving superior accuracy across head, mid, and tail categories as well as all acquisition modalities. These results establish a strong foundation for bronze inscription recognition and downstream archaeological analysis.",
        "tags": [
            "CLIP",
            "Detection",
            "MoE"
        ]
    },
    {
        "id": "161",
        "title": "Learning to Look at the Other Side: A Semantic Probing Study of Word Embeddings in LLMs with Enabled Bidirectional Attention",
        "author": [
            "Zhaoxin Feng",
            "Jianfei Ma",
            "Emmanuele Chersoni",
            "Xiaojing Zhao",
            "Xiaoyi Bao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01652",
        "abstract": "Autoregressive Large Language Models (LLMs) demonstrate exceptional performance in language understanding and generation. However, their application in text embedding tasks has been relatively slow, along with the analysis of their semantic representation in probing tasks, due to the constraints of the unidirectional attention mechanism.\nThis paper aims to explore whether such constraints can be overcome by enabling bidirectional attention in LLMs. We tested different variants of the Llama architecture through additional training steps, progressively enabling bidirectional attention and unsupervised/supervised contrastive learning.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "162",
        "title": "Asymmetric Proximal Policy Optimization: mini-critics boost LLM reasoning",
        "author": [
            "Jiashun Liu",
            "Johan Obando-Ceron",
            "Han Lu",
            "Yancheng He",
            "Weixun Wang",
            "Wenbo Su",
            "Bo Zheng",
            "Pablo Samuel Castro",
            "Aaron Courville",
            "Ling Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01656",
        "abstract": "Most recent RL for LLMs (RL4LLM) methods avoid explicit critics, replacing them with average advantage baselines. This shift is largely pragmatic: conventional value functions are computationally expensive to train at LLM scale and often fail under sparse rewards and long reasoning horizons. We revisit this bottleneck from an architectural perspective and introduce Asymmetric Proximal Policy Optimization (AsyPPO), a simple and scalable framework that restores the critics role while remaining efficient in large-model settings. AsyPPO employs a set of lightweight mini-critics, each trained on disjoint prompt shards. This design encourages diversity while preserving calibration, reducing value-estimation bias. Beyond robust estimation, AsyPPO leverages inter-critic uncertainty to refine the policy update: (i) masking advantages in states where critics agree and gradients add little learning signal, and (ii) filtering high-divergence states from entropy regularization, suppressing spurious exploration. After training on open-source data with only 5,000 samples, AsyPPO consistently improves learning stability and performance across multiple benchmarks over strong baselines, such as GRPO, achieving performance gains of more than six percent on Qwen3-4b-Base and about three percent on Qwen3-8b-Base and Qwen3-14b-Base over classic PPO, without additional tricks. These results highlight the importance of architectural innovations for scalable, efficient algorithms.",
        "tags": [
            "GRPO",
            "LLM",
            "PPO",
            "RL"
        ]
    },
    {
        "id": "163",
        "title": "Symskill: Symbol and Skill Co-Invention for Data-Efficient and Real-Time Long-Horizon Manipulation",
        "author": [
            "Yifei Simon Shao",
            "Yuchen Zheng",
            "Sunan Sun",
            "Pratik Chaudhari",
            "Vijay Kumar",
            "Nadia Figueroa"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01661",
        "abstract": "Multi-step manipulation in dynamic environments remains challenging. Two major families of methods fail in distinct ways: (i) imitation learning (IL) is reactive but lacks compositional generalization, as monolithic policies do not decide which skill to reuse when scenes change; (ii) classical task-and-motion planning (TAMP) offers compositionality but has prohibitive planning latency, preventing real-time failure recovery. We introduce SymSkill, a unified learning framework that combines the benefits of IL and TAMP, allowing compositional generalization and failure recovery in real-time. Offline, SymSkill jointly learns predicates, operators, and skills directly from unlabeled and unsegmented demonstrations. At execution time, upon specifying a conjunction of one or more learned predicates, SymSkill uses a symbolic planner to compose and reorder learned skills to achieve the symbolic goals, while performing recovery at both the motion and symbolic levels in real time. Coupled with a compliant controller, SymSkill enables safe and uninterrupted execution under human and environmental disturbances. In RoboCasa simulation, SymSkill can execute 12 single-step tasks with 85% success rate. Without additional data, it composes these skills into multi-step plans requiring up to 6 skill recompositions, recovering robustly from execution failures. On a real Franka robot, we demonstrate SymSkill, learning from 5 minutes of unsegmented and unlabeled play data, is capable of performing multiple tasks simply by goal specifications. The source code and additional analysis can be found on https://sites.google.com/view/symskill.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "164",
        "title": "Discrete Facial Encoding: : A Framework for Data-driven Facial Display Discovery",
        "author": [
            "Minh Tran",
            "Maksim Siniukov",
            "Zhangyu Jin",
            "Mohammad Soleymani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01662",
        "abstract": "Facial expression analysis is central to understanding human behavior, yet existing coding systems such as the Facial Action Coding System (FACS) are constrained by limited coverage and costly manual annotation. In this work, we introduce Discrete Facial Encoding (DFE), an unsupervised, data-driven alternative of compact and interpretable dictionary of facial expressions from 3D mesh sequences learned through a Residual Vector Quantized Variational Autoencoder (RVQ-VAE). Our approach first extracts identity-invariant expression features from images using a 3D Morphable Model (3DMM), effectively disentangling factors such as head pose and facial geometry. We then encode these features using an RVQ-VAE, producing a sequence of discrete tokens from a shared codebook, where each token captures a specific, reusable facial deformation pattern that contributes to the overall expression. Through extensive experiments, we demonstrate that Discrete Facial Encoding captures more precise facial behaviors than FACS and other facial encoding alternatives. We evaluate the utility of our representation across three high-level psychological tasks: stress detection, personality prediction, and depression detection. Using a simple Bag-of-Words model built on top of the learned tokens, our system consistently outperforms both FACS-based pipelines and strong image and video representation learning models such as Masked Autoencoders. Further analysis reveals that our representation covers a wider variety of facial displays, highlighting its potential as a scalable and effective alternative to FACS for psychological and affective computing applications.",
        "tags": [
            "3D",
            "Detection",
            "VAE"
        ]
    },
    {
        "id": "165",
        "title": "Shift-Invariant Attribute Scoring for Kolmogorov-Arnold Networks via Shapley Value",
        "author": [
            "Wangxuan Fan",
            "Ching Wang",
            "Siqi Li",
            "Nan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01663",
        "abstract": "For many real-world applications, understanding feature-outcome relationships is as crucial as achieving high predictive accuracy. While traditional neural networks excel at prediction, their black-box nature obscures underlying functional relationships. Kolmogorov--Arnold Networks (KANs) address this by employing learnable spline-based activation functions on edges, enabling recovery of symbolic representations while maintaining competitive performance. However, KAN's architecture presents unique challenges for network pruning. Conventional magnitude-based methods become unreliable due to sensitivity to input coordinate shifts. We propose \\textbf{ShapKAN}, a pruning framework using Shapley value attribution to assess node importance in a shift-invariant manner. Unlike magnitude-based approaches, ShapKAN quantifies each node's actual contribution, ensuring consistent importance rankings regardless of input parameterization. Extensive experiments on synthetic and real-world datasets demonstrate that ShapKAN preserves true node importance while enabling effective network compression. Our approach improves KAN's interpretability advantages, facilitating deployment in resource-constrained environments.",
        "tags": [
            "KAN"
        ]
    },
    {
        "id": "166",
        "title": "GuruAgents: Emulating Wise Investors with Prompt-Guided LLM Agents",
        "author": [
            "Yejin Kim",
            "Youngbin Lee",
            "Juhyeong Kim",
            "Yongjae Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01664",
        "abstract": "This study demonstrates that GuruAgents, prompt-guided AI agents, can systematically operationalize the strategies of legendary investment gurus. We develop five distinct GuruAgents, each designed to emulate an iconic investor, by encoding their distinct philosophies into LLM prompts that integrate financial tools and a deterministic reasoning pipeline. In a backtest on NASDAQ-100 constituents from Q4 2023 to Q2 2025, the GuruAgents exhibit unique behaviors driven by their prompted personas. The Buffett GuruAgent achieves the highest performance, delivering a 42.2\\% CAGR that significantly outperforms benchmarks, while other agents show varied results. These findings confirm that prompt engineering can successfully translate the qualitative philosophies of investment gurus into reproducible, quantitative strategies, highlighting a novel direction for automated systematic investing. The source code and data are available at https://github.com/yejining99/GuruAgents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "167",
        "title": "Non-Rigid Structure-from-Motion via Differential Geometry with Recoverable Conformal Scale",
        "author": [
            "Yongbo Chen",
            "Yanhao Zhang",
            "Shaifali Parashar",
            "Liang Zhao",
            "Shoudong Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01665",
        "abstract": "Non-rigid structure-from-motion (NRSfM), a promising technique for addressing the mapping challenges in monocular visual deformable simultaneous localization and mapping (SLAM), has attracted growing attention. We introduce a novel method, called Con-NRSfM, for NRSfM under conformal deformations, encompassing isometric deformations as a subset. Our approach performs point-wise reconstruction using 2D selected image warps optimized through a graph-based framework. Unlike existing methods that rely on strict assumptions, such as locally planar surfaces or locally linear deformations, and fail to recover the conformal scale, our method eliminates these constraints and accurately computes the local conformal scale. Additionally, our framework decouples constraints on depth and conformal scale, which are inseparable in other approaches, enabling more precise depth estimation. To address the sensitivity of the formulated problem, we employ a parallel separable iterative optimization strategy. Furthermore, a self-supervised learning framework, utilizing an encoder-decoder network, is incorporated to generate dense 3D point clouds with texture. Simulation and experimental results using both synthetic and real datasets demonstrate that our method surpasses existing approaches in terms of reconstruction accuracy and robustness. The code for the proposed method will be made publicly available on the project website: https://sites.google.com/view/con-nrsfm.",
        "tags": [
            "3D",
            "Depth Estimation",
            "SLAM"
        ]
    },
    {
        "id": "168",
        "title": "UniVerse: Unleashing the Scene Prior of Video Diffusion Models for Robust Radiance Field Reconstruction",
        "author": [
            "Jin Cao",
            "Hongrui Wu",
            "Ziyong Feng",
            "Hujun Bao",
            "Xiaowei Zhou",
            "Sida Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01669",
        "abstract": "This paper tackles the challenge of robust reconstruction, i.e., the task of reconstructing a 3D scene from a set of inconsistent multi-view images. Some recent works have attempted to simultaneously remove image inconsistencies and perform reconstruction by integrating image degradation modeling into neural 3D scene http://representations.However, these methods rely heavily on dense observations for robustly optimizing model http://parameters.To address this issue, we propose to decouple robust reconstruction into two subtasks: restoration and reconstruction, which naturally simplifies the optimization http://process.To this end, we introduce UniVerse, a unified framework for robust reconstruction based on a video diffusion model. Specifically, UniVerse first converts inconsistent images into initial videos, then uses a specially designed video diffusion model to restore them into consistent images, and finally reconstructs the 3D scenes from these restored http://images.Compared with case-by-case per-view degradation modeling, the diffusion model learns a general scene prior from large-scale data, making it applicable to diverse image http://inconsistencies.Extensive experiments on both synthetic and real-world datasets demonstrate the strong generalization capability and superior performance of our method in robust reconstruction. Moreover, UniVerse can control the style of the reconstructed 3D scene. Project page: https://jin-cao-tma.github.io/UniVerse.github.io/",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "169",
        "title": "Just Do It!? Computer-Use Agents Exhibit Blind Goal-Directedness",
        "author": [
            "Erfan Shayegani",
            "Keegan Hines",
            "Yue Dong",
            "Nael Abu-Ghazaleh",
            "Roman Lutz",
            "Spencer Whitehead",
            "Vidhisha Balachandran",
            "Besmira Nushi",
            "Vibhav Vineet"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01670",
        "abstract": "Computer-Use Agents (CUAs) are an increasingly deployed class of agents that take actions on GUIs to accomplish user goals. In this paper, we show that CUAs consistently exhibit Blind Goal-Directedness (BGD): a bias to pursue goals regardless of feasibility, safety, reliability, or context. We characterize three prevalent patterns of BGD: (i) lack of contextual reasoning, (ii) assumptions and decisions under ambiguity, and (iii) contradictory or infeasible goals. We develop BLIND-ACT, a benchmark of 90 tasks capturing these three patterns. Built on OSWorld, BLIND-ACT provides realistic environments and employs LLM-based judges to evaluate agent behavior, achieving 93.75% agreement with human annotations. We use BLIND-ACT to evaluate nine frontier models, including Claude Sonnet and Opus 4, Computer-Use-Preview, and GPT-5, observing high average BGD rates (80.8%) across them. We show that BGD exposes subtle risks that arise even when inputs are not directly harmful. While prompting-based interventions lower BGD levels, substantial risk persists, highlighting the need for stronger training- or inference-time interventions. Qualitative analysis reveals observed failure modes: execution-first bias (focusing on how to act over whether to act), thought-action disconnect (execution diverging from reasoning), and request-primacy (justifying actions due to user request). Identifying BGD and introducing BLIND-ACT establishes a foundation for future research on studying and mitigating this fundamental risk and ensuring safe CUA deployment.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "170",
        "title": "A Locally Executable AI System for Improving Preoperative Patient Communication: A Multi-Domain Clinical Evaluation",
        "author": [
            "Motoki Sato",
            "Yuki Matsushita",
            "Hidekazu Takahashi",
            "Tomoaki Kakazu",
            "Sou Nagata",
            "Mizuho Ohnuma",
            "Atsushi Yoshikawa",
            "Masayuki Yamamura"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01671",
        "abstract": "Patients awaiting invasive procedures often have unanswered pre-procedural questions; however, time-pressured workflows and privacy constraints limit personalized counseling. We present LENOHA (Low Energy, No Hallucination, Leave No One Behind Architecture), a safety-first, local-first system that routes inputs with a high-precision sentence-transformer classifier and returns verbatim answers from a clinician-curated FAQ for clinical queries, eliminating free-text generation in the clinical path. We evaluated two domains (tooth extraction and gastroscopy) using expert-reviewed validation sets (n=400/domain) for thresholding and independent test sets (n=200/domain). Among the four encoders, E5-large-instruct (560M) achieved an overall accuracy of 0.983 (95% CI 0.964-0.991), AUC 0.996, and seven total errors, which were statistically indistinguishable from GPT-4o on this task; Gemini made no errors on this test set. Energy logging shows that the non-generative clinical path consumes ~1.0 mWh per input versus ~168 mWh per small-talk reply from a local 8B SLM, a ~170x difference, while maintaining ~0.10 s latency on a single on-prem GPU. These results indicate that near-frontier discrimination and generation-induced errors are structurally avoided in the clinical path by returning vetted FAQ answers verbatim, supporting privacy, sustainability, and equitable deployment in bandwidth-limited environments.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "171",
        "title": "ENLighten: Lighten the Transformer, Enable Efficient Optical Acceleration",
        "author": [
            "Hanqing Zhu",
            "Zhican Zhou",
            "Shupeng Ning",
            "Xuhao Wu",
            "Ray Chen",
            "Yating Wan",
            "David Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01673",
        "abstract": "Photonic computing has emerged as a promising substrate for accelerating the dense linear-algebra operations at the heart of AI, yet adoption for large Transformer models remains in its infancy. We identify two bottlenecks: (1) costly electro--optic conversions and data-movement overheads that erode energy efficiency as model sizes scale; (2) a mismatch between limited on-chip photonic resources and Transformer scale, which forces frequent reuse of photonic tensor cores and dilutes throughput gains. To address these challenges, we introduce a hardware--software co-design framework. First, we propose \\texttt{Lighten}, a PTC-aware compression flow that post-hoc decomposes each Transformer weight matrix into a low-rank component plus a structured-sparse component aligned to photonic tensor-core granularity, without lengthy retraining. Second, we present \\texttt{ENLighten}, a reconfigurable photonic accelerator with dynamically adaptive tensor cores, driven by broadband light redistribution, enabling fine-grained sparsity support and full power gating of inactive parts. On ImageNet, \\texttt{Lighten} prunes a Base-scale Vision Transformer by 50\\% with $\\approx$1\\% accuracy drop after only 3 epochs (about 1 hour) of fine-tuning. Deployed on \\texttt{ENLighten}, it achieves a $2.5\\times$ improvement in energy--delay product over the state-of-the-art photonic Transformer accelerator.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "172",
        "title": "FOR-Prompting: From Objection to Revision via an Asymmetric Prompting Protocol",
        "author": [
            "He Zhang",
            "Anzhou Zhang",
            "Jian Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01674",
        "abstract": "Reasoning protocols such as Chain of Thought (CoT) and Tree of Thought (ToT) organize internal deliberation but lack an explicit mechanism for external questioning that elicits self-revision. We present FOR-Prompting (From Objection to Revision Prompting), an asymmetric protocol where a Defender proposes an answer, an Objectioner raises question-style objections with no direct fixes, and a Host enforces consistency and closure. On GSM8K we observe about a 22% point gain over single-prompt and accuracy on par with CoT, with more than 10% higher ratings in reasoning and coherence from a uniform GPT 4.1 judge. FOR-Prompting also corrects mistakes without tools or human supervision on tricky queries, and improves performance for small-scale model (approx. 19% accuracy improved on Llama3.2:1b for GSM8K task), highlighting promise for small models and on personal device use. Beyond factual QA, qualitative analyses on open-ended tasks show enhanced exploration and refinement, with dialogue traces that make assumptions and trade-offs explicit. The protocol is model agnostic and operates purely at the prompt level through role-structured turns, so it works with hosted and local models of different sizes without retraining, and it supports large-scale study of objection-guided reasoning.",
        "tags": [
            "CoT",
            "GPT"
        ]
    },
    {
        "id": "173",
        "title": "Look Less, Reason More: Rollout-Guided Adaptive Pixel-Space Reasoning",
        "author": [
            "Xuchen Li",
            "Xuzhao Li",
            "Jiahui Gao",
            "Renjie Pi",
            "Shiyu Hu",
            "Wentao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01681",
        "abstract": "Vision-Language Models (VLMs) excel at many multimodal tasks, yet they frequently struggle with tasks requiring precise understanding and handling of fine-grained visual elements. This is mainly due to information loss during image encoding or insufficient attention to critical regions. Recent work has shown promise by incorporating pixel-level visual information into the reasoning process, enabling VLMs to access high-resolution visual details during their thought process. However, this pixel-level information is often overused, leading to inefficiency and distraction from irrelevant visual details. To address these challenges, we propose the first framework for adaptive pixel reasoning that dynamically determines necessary pixel-level operations based on the input query. Specifically, we first apply operation-aware supervised fine-tuning to establish baseline competence in textual reasoning and visual operations, then design a novel rollout-guided reinforcement learning framework relying on feedback of the model's own responses, which enables the VLM to determine when pixel operations should be invoked based on query difficulty. Experiments on extensive multimodal reasoning benchmarks show that our model achieves superior performance while significantly reducing unnecessary visual operations. Impressively, our model achieves 73.4\\% accuracy on HR-Bench 4K while maintaining a tool usage ratio of only 20.1\\%, improving accuracy and simultaneously reducing tool usage by 66.5\\% compared to the previous methods.",
        "tags": [
            "RL",
            "VLM"
        ]
    },
    {
        "id": "174",
        "title": "How Do Language Models Compose Functions?",
        "author": [
            "Apoorv Khandelwal",
            "Ellie Pavlick"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01685",
        "abstract": "While large language models (LLMs) appear to be increasingly capable of solving compositional tasks, it is an open question whether they do so using compositional mechanisms. In this work, we investigate how feedforward LLMs solve two-hop factual recall tasks, which can be expressed compositionally as $g(f(x))$. We first confirm that modern LLMs continue to suffer from the \"compositionality gap\": i.e. their ability to compute both $z = f(x)$ and $y = g(z)$ does not entail their ability to compute the composition $y = g(f(x))$. Then, using logit lens on their residual stream activations, we identify two processing mechanisms, one which solves tasks $\\textit{compositionally}$, computing $f(x)$ along the way to computing $g(f(x))$, and one which solves them $\\textit{directly}$, without any detectable signature of the intermediate variable $f(x)$. Finally, we find that which mechanism is employed appears to be related to the embedding space geometry, with the idiomatic mechanism being dominant in cases where there exists a linear mapping from $x$ to $g(f(x))$ in the embedding spaces. We fully release our data and code at: https://github.com/apoorvkh/composing-functions .",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "175",
        "title": "VaPR -- Vision-language Preference alignment for Reasoning",
        "author": [
            "Rohan Wadhawan",
            "Fabrice Y Harel-Canada",
            "Zi-Yi Dou",
            "Suhaila Shakiah",
            "Robinson Piramuthu",
            "Nanyun Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01700",
        "abstract": "Preference finetuning methods like Direct Preference Optimization (DPO) with AI-generated feedback have shown promise in aligning Large Vision-Language Models (LVLMs) with human preferences. However, existing techniques overlook the prevalence of noise in synthetic preference annotations in the form of stylistic and length biases. To this end, we introduce a hard-negative response generation framework based on LLM-guided response editing, that produces rejected responses with targeted errors, maintaining stylistic and length similarity to the accepted ones. Using this framework, we develop the VaPR dataset, comprising 30K high-quality samples, to finetune three LVLM families: LLaVA-V1.5, Qwen2VL & Qwen2.5VL (2B-13B sizes). Our VaPR models deliver significant performance improvements across ten benchmarks, achieving average gains of 6.5% (LLaVA), 4.0% (Qwen2VL), and 1.5% (Qwen2.5VL), with notable improvements on reasoning tasks. A scaling analysis shows that performance consistently improves with data size, with LLaVA models benefiting even at smaller scales. Moreover, VaPR reduces the tendency to answer \"Yes\" in binary questions - addressing a common failure mode in LVLMs like LLaVA. Lastly, we show that the framework generalizes to open-source LLMs as editors, with models trained on VaPR-OS achieving ~99% of the performance of models trained on \\name, which is synthesized using GPT-4o. Our data, models, and code can be found on the project page https://vap-r.github.io",
        "tags": [
            "DPO",
            "GPT",
            "LLM",
            "LLaVA",
            "VLM"
        ]
    },
    {
        "id": "176",
        "title": "Representational Alignment Across Model Layers and Brain Regions with Hierarchical Optimal Transport",
        "author": [
            "Shaan Shah",
            "Meenakshi Khosla"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01706",
        "abstract": "Standard representational similarity methods align each layer of a network to its best match in another independently, producing asymmetric results, lacking a global alignment score, and struggling with networks of different depths. These limitations arise from ignoring global activation structure and restricting mappings to rigid one-to-one layer correspondences. We propose Hierarchical Optimal Transport (HOT), a unified framework that jointly infers soft, globally consistent layer-to-layer couplings and neuron-level transport plans. HOT allows source neurons to distribute mass across multiple target layers while minimizing total transport cost under marginal constraints. This yields both a single alignment score for the entire network comparison and a soft transport plan that naturally handles depth mismatches through mass distribution. We evaluate HOT on vision models, large language models, and human visual cortex recordings. Across all domains, HOT matches or surpasses standard pairwise matching in alignment quality. Moreover, it reveals smooth, fine-grained hierarchical correspondences: early layers map to early layers, deeper layers maintain relative positions, and depth mismatches are resolved by distributing representations across multiple layers. These structured patterns emerge naturally from global optimization without being imposed, yet are absent in greedy layer-wise methods. HOT thus enables richer, more interpretable comparisons between representations, particularly when networks differ in architecture or depth.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "177",
        "title": "Contrastive Representation Regularization for Vision-Language-Action Models",
        "author": [
            "Taeyoung Kim",
            "Jimin Lee",
            "Myungkyu Koo",
            "Dongyoung Kim",
            "Kyungmin Lee",
            "Changyeon Kim",
            "Younggyo Seo",
            "Jinwoo Shin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01711",
        "abstract": "Vision-Language-Action (VLA) models have shown its capabilities in robot manipulation by leveraging rich representations from pre-trained Vision-Language Models (VLMs). However, their representations arguably remain suboptimal, lacking sensitivity to robotic signals such as control actions and proprioceptive states. To address the issue, we introduce Robot State-aware Contrastive Loss (RS-CL), a simple and effective representation regularization for VLA models, designed to bridge the gap between VLM representations and robotic signals. In particular, RS-CL aligns the representations more closely with the robot's proprioceptive states, by using relative distances between the states as soft supervision. Complementing the original action prediction objective, RS-CL effectively enhances control-relevant representation learning, while being lightweight and fully compatible with standard VLA training pipeline. Our empirical results demonstrate that RS-CL substantially improves the manipulation performance of state-of-the-art VLA models; it pushes the prior art from 30.8% to 41.5% on pick-and-place tasks in RoboCasa-Kitchen, through more accurate positioning during grasping and placing, and boosts success rates from 45.0% to 58.3% on challenging real-robot manipulation tasks.",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "178",
        "title": "PyramidStyler: Transformer-Based Neural Style Transfer with Pyramidal Positional Encoding and Reinforcement Learning",
        "author": [
            "Raahul Krishna Durairaju",
            "K. Saruladha"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01715",
        "abstract": "Neural Style Transfer (NST) has evolved from Gatys et al.'s (2015) CNN-based algorithm, enabling AI-driven artistic image synthesis. However, existing CNN and transformer-based models struggle to scale efficiently to complex styles and high-resolution inputs. We introduce PyramidStyler, a transformer framework with Pyramidal Positional Encoding (PPE): a hierarchical, multi-scale encoding that captures both local details and global context while reducing computational load. We further incorporate reinforcement learning to dynamically optimize stylization, accelerating convergence. Trained on Microsoft COCO and WikiArt, PyramidStyler reduces content loss by 62.6% (to 2.07) and style loss by 57.4% (to 0.86) after 4000 epochs--achieving 1.39 s inference--and yields further improvements (content 2.03; style 0.75) with minimal speed penalty (1.40 s) when using RL. These results demonstrate real-time, high-quality artistic rendering, with broad applications in media and design.",
        "tags": [
            "RL",
            "Style Transfer",
            "Transformer"
        ]
    },
    {
        "id": "179",
        "title": "Accelerating Attention with Basis Decomposition",
        "author": [
            "Jialin Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01718",
        "abstract": "Attention is a core operation in large language models (LLMs) and vision-language models (VLMs). We present BD Attention (BDA), the first lossless algorithmic reformulation of attention. BDA is enabled by a simple matrix identity from Basis Decomposition (BD), which restructures multi-head projections into a compact form while preserving exact outputs. Unlike I/O-aware system optimizations such as FlashAttention, BDA provides a mathematically guaranteed acceleration that is architecture-agnostic. On DeepSeek-V2-Lite (16B, FP16), BDA requires only 4s of offline preparation with no retraining required and, on modern GPUs, achieves 32% faster key/value projections and 25% smaller weights, while increasing end-to-end perplexity (PPL) by just 0.02% (FP16) or 0.0004% (FP32), a negligible effect on model performance. These results position BDA as the first theoretically exact method for lossless attention acceleration that is complementary to existing engineering-level optimizations. Our code is available at https://github.com/abcbdf/basis-decomposition-official.",
        "tags": [
            "DeepSeek",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "180",
        "title": "What MLLMs Learn about When they Learn about Multimodal Reasoning: Perception, Reasoning, or their Integration?",
        "author": [
            "Jiwan Chung",
            "Neel Joshi",
            "Pratyusha Sharma",
            "Youngjae Yu",
            "Vibhav Vineet"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01719",
        "abstract": "Multimodal reasoning models have recently shown promise on challenging domains such as olympiad-level geometry, yet their evaluation remains dominated by aggregate accuracy, a single score that obscures where and how models are improving. We introduce MathLens, a benchmark designed to disentangle the subskills of multimodal reasoning while preserving the complexity of textbook-style geometry problems. The benchmark separates performance into three components: Perception: extracting information from raw inputs, Reasoning: operating on available information, and Integration: selecting relevant perceptual evidence and applying it within reasoning. To support each test, we provide annotations: visual diagrams, textual descriptions to evaluate reasoning in isolation, controlled questions that require both modalities, and probes for fine-grained perceptual skills, all derived from symbolic specifications of the problems to ensure consistency and robustness. Our analysis reveals that different training approaches have uneven effects: First, reinforcement learning chiefly strengthens perception, especially when supported by textual supervision, while textual SFT indirectly improves perception through reflective reasoning. Second, reasoning improves only in tandem with perception. Third, integration remains the weakest capacity, with residual errors concentrated there once other skills advance. Finally, robustness diverges: RL improves consistency under diagram variation, whereas multimodal SFT reduces it through overfitting. We will release all data and experimental logs.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "181",
        "title": "Emotional Text-To-Speech Based on Mutual-Information-Guided Emotion-Timbre Disentanglement",
        "author": [
            "Jianing Yang",
            "Sheng Li",
            "Takahiro Shinozaki",
            "Yuki Saito",
            "Hiroshi Saruwatari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01722",
        "abstract": "Current emotional Text-To-Speech (TTS) and style transfer methods rely on reference encoders to control global style or emotion vectors, but do not capture nuanced acoustic details of the reference speech. To this end, we propose a novel emotional TTS method that enables fine-grained phoneme-level emotion embedding prediction while disentangling intrinsic attributes of the reference speech. The proposed method employs a style disentanglement method to guide two feature extractors, reducing mutual information between timbre and emotion features, and effectively separating distinct style components from the reference speech. Experimental results demonstrate that our method outperforms baseline TTS systems in generating natural and emotionally rich speech. This work highlights the potential of disentangled and fine-grained representations in advancing the quality and flexibility of emotional TTS systems.",
        "tags": [
            "Style Transfer"
        ]
    },
    {
        "id": "182",
        "title": "MetaboT: AI-based agent for natural language-based interaction with metabolomics knowledge graphs",
        "author": [
            "Madina Bekbergenova",
            "Lucas Pradi",
            "Benjamin Navet",
            "Emma Tysinger",
            "Franck Michel",
            "Matthieu Feraud",
            "Yousouf Taghzouti",
            "Yan Zhou Chen",
            "Olivier Kirchhoffer",
            "Florence Mehl",
            "Martin Legrand",
            "Tao Jiang",
            "Marco Pagni",
            "Soha Hassoun",
            "Jean-Luc Wolfender",
            "Wout Bittremieux",
            "Fabien Gandon",
            "Louis-FÃ©lix Nothias"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01724",
        "abstract": "Mass spectrometry metabolomics generates vast amounts of data requiring advanced methods for interpretation. Knowledge graphs address these challenges by structuring mass spectrometry data, metabolite information, and their relationships into a connected network (Gaudry et al. 2024). However, effective use of a knowledge graph demands an in-depth understanding of its ontology and its query language syntax. To overcome this, we designed MetaboT, an AI system utilizing large language models (LLMs) to translate user questions into SPARQL semantic query language for operating on knowledge graphs (Steve Harris 2013). We demonstrate its effectiveness using the Experimental Natural Products Knowledge Graph (ENPKG), a large-scale public knowledge graph for plant natural products (Gaudry et al. 2024).MetaboT employs specialized AI agents for handling user queries and interacting with the knowledge graph by breaking down complex tasks into discrete components, each managed by a specialised agent (Fig. 1a). The multi-agent system is constructed using the LangChain and LangGraph libraries, which facilitate the integration of LLMs with external tools and information sources (LangChain, n.d.). The query generation process follows a structured workflow. First, the Entry Agent determines if the question is new or a follow-up to previous interactions. New questions are forwarded to the Validator Agent, which verifies if the question is related to the knowledge graph. Then, the valid question is sent to the Supervisor Agent, which identifies if the question requires chemical conversions or standardized identifiers. In this case it delegates the question to the Knowledge Graph Agent, which can use tools to extract necessary details, such as URIs or taxonomies of chemical names, from the user query. Finally, an agent responsible for crafting the SPARQL queries equipped with the ontology of the knowledge graph uses the provided identifiers to generate the query. Then, the system executes the generated query against the metabolomics knowledge graph and returns structured results to the user (Fig. 1b). To assess the performance of MetaboT we have curated 50 metabolomics-related questions and their expected answers. In addition to submitting these questions to MetaboT, we evaluated a baseline by submitting them to a standard LLM (GPT-4o) with a prompt that incorporated the knowledge graph ontology but did not provide specific entity IDs. This baseline achieved only 8.16% accuracy, compared to MetaboT's 83.67%, underscoring the necessity of our multi-agent system for accurately retrieving entities and generating correct SPARQL queries. MetaboT demonstrates promising performance as a conversational question-answering assistant, enabling researchers to retrieve structured metabolomics data through natural language queries. By automating the generation and execution of SPARQL queries, it removes technical barriers that have traditionally hindered access to knowledge graphs. Importantly, MetaboT leverages the capabilities of LLMs while maintaining experimentally grounded query generation, ensuring that outputs remain aligned with domain-specific standards and data structures. This approach facilitates data-driven discoveries by bridging the gap between complex semantic technologies and user-friendly interaction. MetaboT is accessible at [https://metabot.holobiomicslab.eu/], and its source code is available at [https://github.com/HolobiomicsLab/MetaboT].",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "183",
        "title": "Dual-Mode Magnetic Continuum Robot for Targeted Drug Delivery",
        "author": [
            "Wendu Zhang",
            "Heng Wang",
            "Shuangyi Wang",
            "Yuanrui Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01761",
        "abstract": "Magnetic continuum robots (MCRs) enable minimally invasive navigation through tortuous anatomical channels, yet axially magnetized designs have largely been limited to bending-only motion. To expand deformation capabilities, this paper presents a simple assembly that embeds permanent magnets radially within the catheter wall, allowing a single externally steered permanent magnet to independently induce either bending or torsion. A physics-based formulation together with finite-element analysis establishes the actuation principles, and benchtop experiments validate decoupled mode control under practical fields. Building on this, a dual-layer blockage mechanism consisting of outer grooves and inner plates leverages torsional shear to achieve on-demand drug release. Finally, an in-phantom intervention experiment demonstrates end-to-end operation: lumen following by bending for target approach, followed by twist-activated release at the site. The resulting compact, cable-free platform combines versatile deformation with precise payload delivery, indicating strong potential for next-generation, site-specific therapies.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "184",
        "title": "Octax: Accelerated CHIP-8 Arcade Environments for Reinforcement Learning in JAX",
        "author": [
            "Waris Radji",
            "Thomas Michel",
            "Hector Piteau"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01764",
        "abstract": "Reinforcement learning (RL) research requires diverse, challenging environments that are both tractable and scalable. While modern video games may offer rich dynamics, they are computationally expensive and poorly suited for large-scale experimentation due to their CPU-bound execution. We introduce Octax, a high-performance suite of classic arcade game environments implemented in JAX, based on CHIP-8 emulation, a predecessor to Atari, which is widely adopted as a benchmark in RL research. Octax provides the JAX community with a long-awaited end-to-end GPU alternative to the Atari benchmark, offering image-based environments, spanning puzzle, action, and strategy genres, all executable at massive scale on modern GPUs. Our JAX-based implementation achieves orders-of-magnitude speedups over traditional CPU emulators while maintaining perfect fidelity to the original game mechanics. We demonstrate Octax's capabilities by training RL agents across multiple games, showing significant improvements in training speed and scalability compared to existing solutions. The environment's modular design enables researchers to easily extend the suite with new games or generate novel environments using large language models, making it an ideal platform for large-scale RL experimentation.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "185",
        "title": "LOBE-GS: Load-Balanced and Efficient 3D Gaussian Splatting for Large-Scale Scene Reconstruction",
        "author": [
            "Sheng-Hsiang Hung",
            "Ting-Yu Yen",
            "Wei-Fang Sun",
            "Simon See",
            "Shih-Hsuan Hung",
            "Hung-Kuo Chu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01767",
        "abstract": "3D Gaussian Splatting (3DGS) has established itself as an efficient representation for real-time, high-fidelity 3D scene reconstruction. However, scaling 3DGS to large and unbounded scenes such as city blocks remains difficult. Existing divide-and-conquer methods alleviate memory pressure by partitioning the scene into blocks, but introduce new bottlenecks: (i) partitions suffer from severe load imbalance since uniform or heuristic splits do not reflect actual computational demands, and (ii) coarse-to-fine pipelines fail to exploit the coarse stage efficiently, often reloading the entire model and incurring high overhead. In this work, we introduce LoBE-GS, a novel Load-Balanced and Efficient 3D Gaussian Splatting framework, that re-engineers the large-scale 3DGS pipeline. LoBE-GS introduces a depth-aware partitioning method that reduces preprocessing from hours to minutes, an optimization-based strategy that balances visible Gaussians -- a strong proxy for computational load -- across blocks, and two lightweight techniques, visibility cropping and selective densification, to further reduce training cost. Evaluations on large-scale urban and outdoor datasets show that LoBE-GS consistently achieves up to $2\\times$ faster end-to-end training time than state-of-the-art baselines, while maintaining reconstruction quality and enabling scalability to scenes infeasible with vanilla 3DGS.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "186",
        "title": "Can LLMs Refuse Questions They Do Not Know? Measuring Knowledge-Aware Refusal in Factual Tasks",
        "author": [
            "Wenbo Pan",
            "Jie Xu",
            "Qiguang Chen",
            "Junhao Dong",
            "Libo Qin",
            "Xinfeng Li",
            "Haining Yu",
            "Xiaohua Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01782",
        "abstract": "Large Language Models (LLMs) should refuse to answer questions beyond their knowledge. This capability, which we term knowledge-aware refusal, is crucial for factual reliability. However, existing metrics fail to faithfully measure this ability. On the one hand, simple refusal-based metrics are biased by refusal rates and yield inconsistent scores when models exhibit different refusal tendencies. On the other hand, existing calibration metrics are proxy-based, capturing the performance of auxiliary calibration processes rather than the model's actual refusal behavior. In this work, we propose the Refusal Index (RI), a principled metric that measures how accurately LLMs refuse questions they do not know. We define RI as Spearman's rank correlation between refusal probability and error probability. To make RI practically measurable, we design a lightweight two-pass evaluation method that efficiently estimates RI from observed refusal rates across two standard evaluation runs. Extensive experiments across 16 models and 5 datasets demonstrate that RI accurately quantifies a model's intrinsic knowledge-aware refusal capability in factual tasks. Notably, RI remains stable across different refusal rates and provides consistent model rankings independent of a model's overall accuracy and refusal rates. More importantly, RI provides insight into an important but previously overlooked aspect of LLM factuality: while LLMs achieve high accuracy on factual tasks, their refusal behavior can be unreliable and fragile. This finding highlights the need to complement traditional accuracy metrics with the Refusal Index for comprehensive factuality evaluation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "187",
        "title": "Pack and Force Your Memory: Long-form and Consistent Video Generation",
        "author": [
            "Xiaofei Wu",
            "Guozhen Zhang",
            "Zhiyong Xu",
            "Yuan Zhou",
            "Qinglin Lu",
            "Xuming He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01784",
        "abstract": "Long-form video generation presents a dual challenge: models must capture long-range dependencies while preventing the error accumulation inherent in autoregressive decoding. To address these challenges, we make two contributions. First, for dynamic context modeling, we propose MemoryPack, a learnable context-retrieval mechanism that leverages both textual and image information as global guidance to jointly model short- and long-term dependencies, achieving minute-level temporal consistency. This design scales gracefully with video length, preserves computational efficiency, and maintains linear complexity. Second, to mitigate error accumulation, we introduce Direct Forcing, an efficient single-step approximating strategy that improves training-inference alignment and thereby curtails error propagation during inference. Together, MemoryPack and Direct Forcing substantially enhance the context consistency and reliability of long-form video generation, advancing the practical usability of autoregressive video models.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "188",
        "title": "Efficient manifold evolution algorithm using adaptive B-Spline interpolation",
        "author": [
            "Muhammad Ammad",
            "Leevan Ling"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01790",
        "abstract": "This paper explores an efficient Lagrangian approach for evolving point cloud data on smooth manifolds. In this preliminary study, we focus on analyzing plane curves, and our ultimate goal is to provide an alternative to the conventional radial basis function (RBF) approach for manifolds in higher dimensions. In particular, we use the B-Spline as the basis function for all local interpolations. Just like RBF and other smooth basis functions, B-Splines enable the approximation of geometric features such as normal vectors and curvature. Once properly set up, the advantage of using B-Splines is that their coefficients carry geometric meanings. This allows the coefficients to be manipulated like points, facilitates rapid updates of the interpolant, and eliminates the need for frequent re-interpolation. Consequently, the removal and insertion of point cloud data become seamless processes, particularly advantageous in regions experiencing significant fluctuations in point density. The numerical results demonstrate the convergence of geometric quantities and the effectiveness of our approach. Finally, we show simulations of curvature flows whose speeds depend on the solutions of coupled reaction--diffusion systems for pattern formation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "189",
        "title": "Comparison of Unsupervised Metrics for Evaluating Judicial Decision Extraction",
        "author": [
            "Ivan Leonidovich Litvak",
            "Anton Kostin",
            "Fedor Lashkin",
            "Tatiana Maksiyan",
            "Sergey Lagutin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01792",
        "abstract": "The rapid advancement of artificial intelligence in legal natural language processing demands scalable methods for evaluating text extraction from judicial decisions. This study evaluates 16 unsupervised metrics, including novel formulations, to assess the quality of extracting seven semantic blocks from 1,000 anonymized Russian judicial decisions, validated against 7,168 expert reviews on a 1--5 Likert scale. These metrics, spanning document-based, semantic, structural, pseudo-ground truth, and legal-specific categories, operate without pre-annotated ground truth. Bootstrapped correlations, Lin's concordance correlation coefficient (CCC), and mean absolute error (MAE) reveal that Term Frequency Coherence (Pearson $r = 0.540$, Lin CCC = 0.512, MAE = 0.127) and Coverage Ratio/Block Completeness (Pearson $r = 0.513$, Lin CCC = 0.443, MAE = 0.139) best align with expert ratings, while Legal Term Density (Pearson $r = -0.479$, Lin CCC = -0.079, MAE = 0.394) show strong negative correlations. The LLM Evaluation Score (mean = 0.849, Pearson $r = 0.382$, Lin CCC = 0.325, MAE = 0.197) showed moderate alignment, but its performance, using gpt-4.1-mini via g4f, suggests limited specialization for legal textse. These findings highlight that unsupervised metrics, including LLM-based approaches, enable scalable screening but, with moderate correlations and low CCC values, cannot fully replace human judgment in high-stakes legal contexts. This work advances legal NLP by providing annotation-free evaluation tools, with implications for judicial analytics and ethical AI deployment.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "190",
        "title": "Nav-EE: Navigation-Guided Early Exiting for Efficient Vision-Language Models in Autonomous Driving",
        "author": [
            "Haibo Hu",
            "Lianming Huang",
            "Xinyu Wang",
            "Yufei Cui",
            "Nan Guan",
            "Chun Jason Xue"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01795",
        "abstract": "Vision-Language Models (VLMs) are increasingly applied in autonomous driving for unified perception and reasoning, but high inference latency hinders real-time deployment. Early-exit reduces latency by terminating inference at intermediate layers, yet its task-dependent nature limits generalization across diverse scenarios. We observe that this limitation aligns with autonomous driving: navigation systems can anticipate upcoming contexts (e.g., intersections, traffic lights), indicating which tasks will be required. We propose Nav-EE, a navigation-guided early-exit framework that precomputes task-specific exit layers offline and dynamically applies them online based on navigation priors. Experiments on CODA, Waymo, and BOSCH show that Nav-EE achieves accuracy comparable to full inference while reducing latency by up to 63.9%. Real-vehicle integration with Autoware Universe further demonstrates reduced inference latency (600ms to 300ms), supporting faster decision-making in complex scenarios. These results suggest that coupling navigation foresight with early-exit offers a viable path toward efficient deployment of large models in autonomous systems. Code and data are available at our anonymous repository: https://anonymous.4open.science/r/Nav-EE-BBC4",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "191",
        "title": "REBot: From RAG to CatRAG with Semantic Enrichment and Graph Routing",
        "author": [
            "Thanh Ma",
            "Tri-Tam La",
            "Lam-Thu Le Huu",
            "Minh-Nghi Nguyen",
            "Khanh-Van Pham Luu",
            "Huu-Hoa Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01800",
        "abstract": "Academic regulation advising is essential for helping students interpret and comply with institutional policies, yet building effective systems requires domain specific regulatory resources. To address this challenge, we propose REBot, an LLM enhanced advisory chatbot powered by CatRAG, a hybrid retrieval reasoning framework that integrates retrieval augmented generation with graph based reasoning. CatRAG unifies dense retrieval and graph reasoning, supported by a hierarchical, category labeled knowledge graph enriched with semantic features for domain alignment. A lightweight intent classifier routes queries to the appropriate retrieval modules, ensuring both factual accuracy and contextual depth. We construct a regulation specific dataset and evaluate REBot on classification and question answering tasks, achieving state of the art performance with an F1 score of 98.89%. Finally, we implement a web application that demonstrates the practical value of REBot in real world academic advising scenarios.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "192",
        "title": "Detecting LLM-Generated Spam Reviews by Integrating Language Model Embeddings and Graph Neural Network",
        "author": [
            "Xin Liu",
            "Rongwu Xu",
            "Xinyi Jia",
            "Jason Liao",
            "Jiao Sun",
            "Ling Huang",
            "Wei Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01801",
        "abstract": "The rise of large language models (LLMs) has enabled the generation of highly persuasive spam reviews that closely mimic human writing. These reviews pose significant challenges for existing detection systems and threaten the credibility of online platforms. In this work, we first create three realistic LLM-generated spam review datasets using three distinct LLMs, each guided by product metadata and genuine reference reviews. Evaluations by GPT-4.1 confirm the high persuasion and deceptive potential of these reviews. To address this threat, we propose FraudSquad, a hybrid detection model that integrates text embeddings from a pre-trained language model with a gated graph transformer for spam node classification. FraudSquad captures both semantic and behavioral signals without relying on manual feature engineering or massive training resources. Experiments show that FraudSquad outperforms state-of-the-art baselines by up to 44.22% in precision and 43.01% in recall on three LLM-generated datasets, while also achieving promising results on two human-written spam datasets. Furthermore, FraudSquad maintains a modest model size and requires minimal labeled training data, making it a practical solution for real-world applications. Our contributions include new synthetic datasets, a practical detection framework, and empirical evidence highlighting the urgency of adapting spam detection to the LLM era. Our code and datasets are available at: https://anonymous.4open.science/r/FraudSquad-5389/.",
        "tags": [
            "Detection",
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "193",
        "title": "SingMOS-Pro: An Comprehensive Benchmark for Singing Quality Assessment",
        "author": [
            "Yuxun Tang",
            "Lan Liu",
            "Wenhao Feng",
            "Yiwen Zhao",
            "Jionghao Han",
            "Yifeng Yu",
            "Jiatong Shi",
            "Qin Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01812",
        "abstract": "Singing voice generation progresses rapidly, yet evaluating singing quality remains a critical challenge. Human subjective assessment, typically in the form of listening tests, is costly and time consuming, while existing objective metrics capture only limited perceptual aspects. In this work, we introduce SingMOS-Pro, a dataset for automatic singing quality assessment. Building on our preview version SingMOS, which provides only overall ratings, SingMOS-Pro expands annotations of the additional part to include lyrics, melody, and overall quality, offering broader coverage and greater diversity. The dataset contains 7,981 singing clips generated by 41 models across 12 datasets, spanning from early systems to recent advances. Each clip receives at least five ratings from professional annotators, ensuring reliability and consistency. Furthermore, we explore how to effectively utilize MOS data annotated under different standards and benchmark several widely used evaluation methods from related tasks on SingMOS-Pro, establishing strong baselines and practical references for future research. The dataset can be accessed at https://huggingface.co/datasets/TangRain/SingMOS-Pro.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "194",
        "title": "Sparse Query Attention (SQA): A Computationally Efficient Attention Mechanism with Query Heads Reduction",
        "author": [
            "Adam Filipek"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01817",
        "abstract": "The Transformer architecture, underpinned by the Multi-Head Attention (MHA) mechanism, has become the de facto standard for state-of-the-art models in artificial intelligence. However, the quadratic computational complexity of MHA with respect to sequence length presents a significant barrier to scaling, particularly for applications involving long contexts. Prevailing solutions, such as Multi-Query Attention (MQA) and Grouped-Query Attention (GQA), have effectively addressed the memory bandwidth bottleneck that dominates autoregressive inference latency by sharing Key and Value projections. While highly successful, these methods do not reduce the fundamental number of floating-point operations (FLOPs) required for the attention score computation, which remains a critical bottleneck for training and full-sequence processing. This paper introduces Sparse Query Attention (SQA), a novel attention architecture that pursues an alternative and complementary optimization path. Instead of reducing Key/Value heads, SQA reduces the number of Query heads. This architectural modification directly decreases the computational complexity of the attention mechanism by a factor proportional to the reduction in query heads, thereby lowering the overall FLOPs. This work presents the theoretical foundation of SQA, its mathematical formulation, and a family of architectural variants. Empirical benchmarks on long sequences (32k-200k tokens) demonstrate that SQA can achieve significant throughput improvements of up to 3x in computation-bound scenarios such as model pre-training, fine-tuning, and encoder-based tasks, with only a minimal impact on model quality in preliminary smallscale experiments. SQA was discovered serendipitously during the development of the upcoming Reactive Transformer architecture, suggesting its potential as a powerful tool for building more efficient and scalable models",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "195",
        "title": "Black-Box Combinatorial Optimization with Order-Invariant Reinforcement Learning",
        "author": [
            "Olivier Goudet",
            "Quentin Suire",
            "Adrien GoÃ«ffon",
            "FrÃ©dÃ©ric Saubion",
            "Sylvain Lamprier"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01824",
        "abstract": "We introduce an order-invariant reinforcement learning framework for black-box combinatorial optimization. Classical estimation-of-distribution algorithms (EDAs) often rely on learning explicit variable dependency graphs, which can be costly and fail to capture complex interactions efficiently. In contrast, we parameterize a multivariate autoregressive generative model trained without a fixed variable ordering. By sampling random generation orders during training - a form of information-preserving dropout - the model is encouraged to be invariant to variable order, promoting search-space diversity and shaping the model to focus on the most relevant variable dependencies, improving sample efficiency. We adapt Generalized Reinforcement Policy Optimization (GRPO) to this setting, providing stable policy-gradient updates from scale-invariant advantages. Across a wide range of benchmark algorithms and problem instances of varying sizes, our method frequently achieves the best performance and consistently avoids catastrophic failures.",
        "tags": [
            "GRPO",
            "RL"
        ]
    },
    {
        "id": "196",
        "title": "What Matters in RL-Based Methods for Object-Goal Navigation? An Empirical Study and A Unified Framework",
        "author": [
            "Hongze Wang",
            "Boyang Sun",
            "Jiaxu Xing",
            "Fan Yang",
            "Marco Hutter",
            "Dhruv Shah",
            "Davide Scaramuzza",
            "Marc Pollefeys"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01830",
        "abstract": "Object-Goal Navigation (ObjectNav) is a critical component toward deploying mobile robots in everyday, uncontrolled environments such as homes, schools, and workplaces. In this context, a robot must locate target objects in previously unseen environments using only its onboard perception. Success requires the integration of semantic understanding, spatial reasoning, and long-horizon planning, which is a combination that remains extremely challenging. While reinforcement learning (RL) has become the dominant paradigm, progress has spanned a wide range of design choices, yet the field still lacks a unifying analysis to determine which components truly drive performance. In this work, we conduct a large-scale empirical study of modular RL-based ObjectNav systems, decomposing them into three key components: perception, policy, and test-time enhancement. Through extensive controlled experiments, we isolate the contribution of each and uncover clear trends: perception quality and test-time strategies are decisive drivers of performance, whereas policy improvements with current methods yield only marginal gains. Building on these insights, we propose practical design guidelines and demonstrate an enhanced modular system that surpasses State-of-the-Art (SotA) methods by 6.6% on SPL and by a 2.7% success rate. We also introduce a human baseline under identical conditions, where experts achieve an average 98% success, underscoring the gap between RL agents and human-level navigation. Our study not only sets the SotA performance but also provides principled guidance for future ObjectNav development and evaluation.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "197",
        "title": "Syntactic Blind Spots: How Misalignment Leads to LLMs Mathematical Errors",
        "author": [
            "Dane Williamson",
            "Yangfeng Ji",
            "Matthew Dwyer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01831",
        "abstract": "Large Language Models (LLMs) demonstrate strong mathematical problem-solving abilities but frequently fail on problems that deviate syntactically from their training distribution. We identify a systematic failure mode, syntactic blind spots, in which models misapply familiar reasoning strategies to problems that are semantically straightforward but phrased in unfamiliar ways. These errors are not due to gaps in mathematical competence, but rather reflect a brittle coupling between surface form and internal representation. To test this, we rephrase incorrectly answered questions using syntactic templates drawn from correct examples. These rephrasings, which preserve semantics while reducing structural complexity, often lead to correct answers. We quantify syntactic complexity using a metric based on Dependency Locality Theory (DLT), and show that higher DLT scores are associated with increased failure rates across multiple datasets. Our findings suggest that many reasoning errors stem from structural misalignment rather than conceptual difficulty, and that syntax-aware interventions can reveal and mitigate these inductive failures.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "198",
        "title": "SCRIBES: Web-Scale Script-Based Semi-Structured Data Extraction with Reinforcement Learning",
        "author": [
            "Shicheng Liu",
            "Kai Sun",
            "Lisheng Fu",
            "Xilun Chen",
            "Xinyuan Zhang",
            "Zhaojiang Lin",
            "Rulin Shao",
            "Yue Liu",
            "Anuj Kumar",
            "Wen-tau Yih",
            "Xin Luna Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01832",
        "abstract": "Semi-structured content in HTML tables, lists, and infoboxes accounts for a substantial share of factual data on the web, yet the formatting complicates usage, and reliably extracting structured information from them remains challenging. Existing methods either lack generalization or are resource-intensive due to per-page LLM inference. In this paper, we introduce SCRIBES (SCRIpt-Based Semi-Structured Content Extraction at Web-Scale), a novel reinforcement learning framework that leverages layout similarity across webpages within the same site as a reward signal. Instead of processing each page individually, SCRIBES generates reusable extraction scripts that can be applied to groups of structurally similar webpages. Our approach further improves by iteratively training on synthetic annotations from in-the-wild CommonCrawl data. Experiments show that our approach outperforms strong baselines by over 13% in script quality and boosts downstream question answering accuracy by more than 4% for GPT-4o, enabling scalable and resource-efficient web information extraction.",
        "tags": [
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "199",
        "title": "Plan Then Action:High-Level Planning Guidance Reinforcement Learning for LLM Reasoning",
        "author": [
            "Zhihao Dou",
            "Qinjian Zhao",
            "Zhongwei Wan",
            "Dinggen Zhang",
            "Weida Wang",
            "Towsif Raiyan",
            "Benteng Chen",
            "Qingtao Pan",
            "Yang Ouyang",
            "Zhiqiang Gao",
            "Shufei Zhang",
            "Sumon Biswas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01833",
        "abstract": "Large language models (LLMs) have demonstrated remarkable reasoning abilities in complex tasks, often relying on Chain-of-Thought (CoT) reasoning. However, due to their autoregressive token-level generation, the reasoning process is largely constrained to local decision-making and lacks global planning. This limitation frequently results in redundant, incoherent, or inaccurate reasoning, which significantly degrades overall performance. Existing approaches, such as tree-based algorithms and reinforcement learning (RL), attempt to address this issue but suffer from high computational costs and often fail to produce optimal reasoning trajectories. To tackle this challenge, we propose Plan-Then-Action Enhanced Reasoning with Group Relative Policy Optimization PTA-GRPO, a two-stage framework designed to improve both high-level planning and fine-grained CoT reasoning. In the first stage, we leverage advanced LLMs to distill CoT into compact high-level guidance, which is then used for supervised fine-tuning (SFT). In the second stage, we introduce a guidance-aware RL method that jointly optimizes the final output and the quality of high-level guidance, thereby enhancing reasoning effectiveness. We conduct extensive experiments on multiple mathematical reasoning benchmarks, including MATH, AIME2024, AIME2025, and AMC, across diverse base models such as Qwen2.5-7B-Instruct, Qwen3-8B, Qwen3-14B, and LLaMA3.2-3B. Experimental results demonstrate that PTA-GRPO consistently achieves stable and significant improvements across different models and tasks, validating its effectiveness and generalization.",
        "tags": [
            "CoT",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "200",
        "title": "Leveraging Prior Knowledge of Diffusion Model for Person Search",
        "author": [
            "Giyeol Kim",
            "Sooyoung Yang",
            "Jihyong Oh",
            "Myungjoo Kang",
            "Chanho Eom"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01841",
        "abstract": "Person search aims to jointly perform person detection and re-identification by localizing and identifying a query person within a gallery of uncropped scene images. Existing methods predominantly utilize ImageNet pre-trained backbones, which may be suboptimal for capturing the complex spatial context and fine-grained identity cues necessary for person search. Moreover, they rely on a shared backbone feature for both person detection and re-identification, leading to suboptimal features due to conflicting optimization objectives. In this paper, we propose DiffPS (Diffusion Prior Knowledge for Person Search), a novel framework that leverages a pre-trained diffusion model while eliminating the optimization conflict between two sub-tasks. We analyze key properties of diffusion priors and propose three specialized modules: (i) Diffusion-Guided Region Proposal Network (DGRPN) for enhanced person localization, (ii) Multi-Scale Frequency Refinement Network (MSFRN) to mitigate shape bias, and (iii) Semantic-Adaptive Feature Aggregation Network (SFAN) to leverage text-aligned diffusion features. DiffPS sets a new state-of-the-art on CUHK-SYSU and PRW.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "201",
        "title": "Like Playing a Video Game: Spatial-Temporal Optimization of Foot Trajectories for Controlled Football Kicking in Bipedal Robots",
        "author": [
            "Wanyue Li",
            "Ji Ma",
            "Minghao Lu",
            "Peng Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01843",
        "abstract": "Humanoid robot soccer presents several challenges, particularly in maintaining system stability during aggressive kicking motions while achieving precise ball trajectory control. Current solutions, whether traditional position-based control methods or reinforcement learning (RL) approaches, exhibit significant limitations. Model predictive control (MPC) is a prevalent approach for ordinary quadruped and biped robots. While MPC has demonstrated advantages in legged robots, existing studies often oversimplify the leg swing progress, relying merely on simple trajectory interpolation methods. This severely constrains the foot's environmental interaction capability, hindering tasks such as ball kicking. This study innovatively adapts the spatial-temporal trajectory planning method, which has been successful in drone applications, to bipedal robotic systems. The proposed approach autonomously generates foot trajectories that satisfy constraints on target kicking position, velocity, and acceleration while simultaneously optimizing swing phase duration. Experimental results demonstrate that the optimized trajectories closely mimic human kicking behavior, featuring a backswing motion. Simulation and hardware experiments confirm the algorithm's efficiency, with trajectory planning times under 1 ms, and its reliability, achieving nearly 100 % task completion accuracy when the soccer goal is within the range of -90Â° to 90Â°.",
        "tags": [
            "MPC",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "202",
        "title": "GreenhouseSplat: A Dataset of Photorealistic Greenhouse Simulations for Mobile Robotics",
        "author": [
            "Diram Tabaa",
            "Gianni Di Caro"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01848",
        "abstract": "Simulating greenhouse environments is critical for developing and evaluating robotic systems for agriculture, yet existing approaches rely on simplistic or synthetic assets that limit simulation-to-real transfer. Recent advances in radiance field methods, such as Gaussian splatting, enable photorealistic reconstruction but have so far been restricted to individual plants or controlled laboratory conditions. In this work, we introduce GreenhouseSplat, a framework and dataset for generating photorealistic greenhouse assets directly from inexpensive RGB images. The resulting assets are integrated into a ROS-based simulation with support for camera and LiDAR rendering, enabling tasks such as localization with fiducial markers. We provide a dataset of 82 cucumber plants across multiple row configurations and demonstrate its utility for robotics evaluation. GreenhouseSplat represents the first step toward greenhouse-scale radiance-field simulation and offers a foundation for future research in agricultural robotics.",
        "tags": [
            "Gaussian Splatting",
            "Robotics"
        ]
    },
    {
        "id": "203",
        "title": "Learning a Dense Reasoning Reward Model from Expert Demonstration via Inverse Reinforcement Learning",
        "author": [
            "Claudio Fanconi",
            "NicolÃ¡s Astorga",
            "Mihaela van der Schaar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01857",
        "abstract": "We reframe and operationalise adversarial inverse reinforcement learning (IRL) to large language model reasoning, learning a dense, token-level reward model for process supervision directly from expert demonstrations rather than imitating style via supervised fine-tuning. The learned reasoning reward serves two complementary roles: (i) it provides step-level feedback to optimise a reasoning policy during training; and (ii) it functions at inference as a critic to rerank sampled traces under fixed compute budgets. We demonstrate that our approach prioritises correctness over surface form, yielding scores that correlate with eventual answer validity and enabling interpretable localisation of errors within a trace. Empirically, on GSM8K with Llama3 and Qwen2.5 backbones, we demonstrate: (i) dense reasoning rewards can be used as a learning signal to elicit reasoning, and (ii) predictive performance is improved from reward-guided reranking (notably for Llama-based policies). By unifying training signals, inference-time selection, and token-level diagnostics into a single reasoning reward, this work suggests reusable process-level rewards with broad potential to enhance multi-step reasoning in language models.",
        "tags": [
            "LLaMA",
            "RL"
        ]
    },
    {
        "id": "204",
        "title": "Who is responsible? Social Identity, Robot Errors and Blame Attribution",
        "author": [
            "Samantha Stedtler",
            "Marianna Leventi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01862",
        "abstract": "This paper argues that conventional blame practices fall short of capturing the complexity of moral experiences, neglecting power dynamics and discriminatory social practices. It is evident that robots, embodying roles linked to specific social groups, pose a risk of reinforcing stereotypes of how these groups behave or should behave, so they set a normative and descriptive standard. In addition, we argue that faulty robots might create expectations of who is supposed to compensate and repair after their errors, where social groups that are already disadvantaged might be blamed disproportionately if they do not act according to their ascribed roles. This theoretical and empirical gap becomes even more urgent to address as there have been indications of potential carryover effects from Human-Robot Interactions (HRI) to Human-Human Interactions (HHI). We therefore urge roboticists and designers to stay in an ongoing conversation about how social traits are conceptualised and implemented in this technology. We also argue that one solution could be to 'embrace the glitch' and to focus on constructively disrupting practices instead of prioritizing efficiency and smoothness of interaction above everything else. Apart from considering ethical aspects in the design phase of social robots, we see our analysis as a call for more research on the consequences of robot stereotyping and blame attribution.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "205",
        "title": "Microscaling Floating Point Formats for Large Language Models",
        "author": [
            "Marco Cococcioni",
            "Dario Pagani",
            "Federico Rossi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01863",
        "abstract": "The increasing computational and memory demands of large language models (LLMs) necessitate innovative approaches to optimize resource usage without compromising performance. This paper leverages microscaling floating-point formats, a novel technique designed to address these challenges by reducing the storage and computational overhead associated with numerical representations in LLMs. Unlike traditional floating-point representations that allocate a dedicated scale for each value, microscaling employs a shared scale across a block of values, enabling compact one-byte floating-point representations while maintaining an extended dynamic range. We explore the application of microscaling in the context of 8-bit floating-point formats to significantly reduce memory footprint and computational costs. We tested several configurations of microscaling floats within the GPT-2 LLM architecture, demonstrating that microscaling data formats can achieve competitive accuracy during training and inference, proving its efficacy as a resource-efficient alternative for deploying LLMs at scale. The source code is publicly available at: https://github.com/unipi-dii-compressedarith/llm.c-sve",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "206",
        "title": "TACOS: Task Agnostic COordinator of a multi-drone System",
        "author": [
            "Alessandro Nazzari",
            "Roberto Rubinacci",
            "Marco Lovera"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01869",
        "abstract": "When a single pilot is responsible for managing a multi-drone system, the task demands varying levels of autonomy, from direct control of individual UAVs, to group-level coordination, to fully autonomous swarm behaviors for accomplishing high-level tasks. Enabling such flexible interaction requires a framework that supports multiple modes of shared autonomy. As language models continue to improve in reasoning and planning, they provide a natural foundation for such systems, reducing pilot workload by enabling high-level task delegation through intuitive, language-based interfaces. In this paper we present TACOS (Task-Agnostic COordinator of a multi-drone System), a unified framework that enables high-level natural language control of multi-UAV systems through Large Language Models (LLMs). TACOS integrates three key capabilities into a single architecture: a one-to-many natural language interface for intuitive user interaction, an intelligent coordinator for translating user intent into structured task plans, and an autonomous agent that executes plans interacting with the real-world. TACOS allows a LLM to interact with a library of executable APIs, bridging semantic reasoning with real-time multi-robot coordination. We demonstrate the system in real-world multi-drone system and conduct an ablation study to assess the contribution of each module.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "207",
        "title": "Randomized Gradient Subspaces for Efficient Large Language Model Training",
        "author": [
            "Sahar Rajabi",
            "Nayeema Nonta",
            "Samanvay Vajpayee",
            "Sirisha Rambhatla"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01878",
        "abstract": "Training large language models (LLMs) is often bottlenecked by extreme memory demands, with optimizer states dominating the footprint. Recent works mitigates this cost by projecting gradients into low-dimensional subspaces using sophisticated update strategies. In this paper, we analyze the dynamics of gradient space and its underlying subspaces. We find that while a small subspace captures most gradient energy, a significant portion still resides in the residual bulk; moreover, the influence of the core subspace diminishes over time and in deeper layers. We also observe that the gradient space exhibits near-flat curvature, calling for algorithms that explicitly account for this geometry. Motivated by these insights, we introduce a suite of randomized algorithms, GrassWalk and GrassJump, which exploit subspace and achieve state-of-the-art memory savings while improving performance on LLaMA-1B and LLaMA-7B pretraining.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "208",
        "title": "REPAIR: Robust Editing via Progressive Adaptive Intervention and Reintegration",
        "author": [
            "Yisu Wang",
            "Ming Wang",
            "Haoyuan Song",
            "Wenjie Huang",
            "Chaozheng Wang",
            "Yi Xie",
            "Xuming Ran"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01879",
        "abstract": "Post-training for large language models (LLMs) is constrained by the high cost of acquiring new knowledge or correcting errors and by the unintended side effects that frequently arise from retraining. To address these issues, we introduce REPAIR (Robust Editing via Progressive Adaptive Intervention and Reintegration), a lifelong editing framework designed to support precise and low-cost model updates while preserving non-target knowledge. REPAIR mitigates the instability and conflicts of large-scale sequential edits through a closed-loop feedback mechanism coupled with dynamic memory management. Furthermore, by incorporating frequent knowledge fusion and enforcing strong locality guards, REPAIR effectively addresses the shortcomings of traditional distribution-agnostic approaches that often overlook unintended ripple effects. Our experiments demonstrate that REPAIR boosts editing accuracy by 10%-30% across multiple model families and significantly reduces knowledge forgetting. This work introduces a robust framework for developing reliable, scalable, and continually evolving LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "209",
        "title": "HRTFformer: A Spatially-Aware Transformer for Personalized HRTF Upsampling in Immersive Audio Rendering",
        "author": [
            "Xuyi Hu",
            "Jian Li",
            "Shaojie Zhang",
            "Stefan Goetz",
            "Lorenzo Picinali",
            "Ozgur B. Akan",
            "Aidan O. T. Hogg"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01891",
        "abstract": "Personalized Head-Related Transfer Functions (HRTFs) are starting to be introduced in many commercial immersive audio applications and are crucial for realistic spatial audio rendering. However, one of the main hesitations regarding their introduction is that creating personalized HRTFs is impractical at scale due to the complexities of the HRTF measurement process. To mitigate this drawback, HRTF spatial upsampling has been proposed with the aim of reducing measurements required. While prior work has seen success with different machine learning (ML) approaches, these models often struggle with long-range spatial consistency and generalization at high upsampling factors. In this paper, we propose a novel transformer-based architecture for HRTF upsampling, leveraging the attention mechanism to better capture spatial correlations across the HRTF sphere. Working in the spherical harmonic (SH) domain, our model learns to reconstruct high-resolution HRTFs from sparse input measurements with significantly improved accuracy. To enhance spatial coherence, we introduce a neighbor dissimilarity loss that promotes magnitude smoothness, yielding more realistic upsampling. We evaluate our method using both perceptual localization models and objective spectral distortion metrics. Experiments show that our model surpasses leading methods by a substantial margin in generating realistic, high-fidelity HRTFs.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "210",
        "title": "Are LLMs Better GNN Helpers? Rethinking Robust Graph Learning under Deficiencies with Iterative Refinement",
        "author": [
            "Zhaoyan Wang",
            "Zheng Gao",
            "Arogya Kharel",
            "In-Young Ko"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01910",
        "abstract": "Graph Neural Networks (GNNs) are widely adopted in Web-related applications, serving as a core technique for learning from graph-structured data, such as text-attributed graphs. Yet in real-world scenarios, such graphs exhibit deficiencies that substantially undermine GNN performance. While prior GNN-based augmentation studies have explored robustness against individual imperfections, a systematic understanding of how graph-native and Large Language Models (LLMs) enhanced methods behave under compound deficiencies is still missing. Specifically, there has been no comprehensive investigation comparing conventional approaches and recent LLM-on-graph frameworks, leaving their merits unclear. To fill this gap, we conduct the first empirical study that benchmarks these two lines of methods across diverse graph deficiencies, revealing overlooked vulnerabilities and challenging the assumption that LLM augmentation is consistently superior. Building on empirical findings, we propose Robust Graph Learning via Retrieval-Augmented Contrastive Refinement (RoGRAD) framework. Unlike prior one-shot LLM-as-Enhancer designs, RoGRAD is the first iterative paradigm that leverages Retrieval-Augmented Generation (RAG) to inject retrieval-grounded augmentations by supplying class-consistent, diverse augmentations and enforcing discriminative representations through iterative graph contrastive learning. It transforms LLM augmentation for graphs from static signal injection into dynamic refinement. Extensive experiments demonstrate RoGRAD's superiority over both conventional GNN- and LLM-enhanced baselines, achieving up to 82.43% average improvement.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "211",
        "title": "Flow-Matching Guided Deep Unfolding for Hyperspectral Image Reconstruction",
        "author": [
            "Yi Ai",
            "Yuanhao Cai",
            "Yulun Zhang",
            "Xiaokang Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01912",
        "abstract": "Hyperspectral imaging (HSI) provides rich spatial-spectral information but remains costly to acquire due to hardware limitations and the difficulty of reconstructing three-dimensional data from compressed measurements. Although compressive sensing systems such as CASSI improve efficiency, accurate reconstruction is still challenged by severe degradation and loss of fine spectral details. We propose the Flow-Matching-guided Unfolding network (FMU), which, to our knowledge, is the first to integrate flow matching into HSI reconstruction by embedding its generative prior within a deep unfolding framework. To further strengthen the learned dynamics, we introduce a mean velocity loss that enforces global consistency of the flow, leading to a more robust and accurate reconstruction. This hybrid design leverages the interpretability of optimization-based methods and the generative capacity of flow matching. Extensive experiments on both simulated and real datasets show that FMU significantly outperforms existing approaches in reconstruction quality. Code and models will be available at https://github.com/YiAi03/FMU.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "212",
        "title": "To Mask or to Mirror: Human-AI Alignment in Collective Reasoning",
        "author": [
            "Crystal Qian",
            "Aaron Parisi",
            "ClÃ©mentine Bouleau",
            "Vivian Tsai",
            "MaÃ«l Lebreton",
            "Lucas Dixon"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01924",
        "abstract": "As large language models (LLMs) are increasingly used to model and augment collective decision-making, it is critical to examine their alignment with human social reasoning. We present an empirical framework for assessing collective alignment, in contrast to prior work on the individual level. Using the Lost at Sea social psychology task, we conduct a large-scale online experiment (N=748), randomly assigning groups to leader elections with either visible demographic attributes (e.g. name, gender) or pseudonymous aliases. We then simulate matched LLM groups conditioned on the human data, benchmarking Gemini 2.5, GPT 4.1, Claude Haiku 3.5, and Gemma 3. LLM behaviors diverge: some mirror human biases; others mask these biases and attempt to compensate for them. We empirically demonstrate that human-AI alignment in collective reasoning depends on context, cues, and model-specific inductive biases. Understanding how LLMs align with collective human behavior is critical to advancing socially-aligned AI, and demands dynamic benchmarks that capture the complexities of collective reasoning.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "213",
        "title": "Enhancing Large Language Model Reasoning with Reward Models: An Analytical Survey",
        "author": [
            "Qiyuan Liu",
            "Hao Xu",
            "Xuhong Chen",
            "Wei Chen",
            "Yee Whye Teh",
            "Ning Miao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01925",
        "abstract": "Reward models (RMs) play a critical role in enhancing the reasoning performance of LLMs. For example, they can provide training signals to finetune LLMs during reinforcement learning (RL) and help select the best answer from multiple candidates during inference. In this paper, we provide a systematic introduction to RMs, along with a comprehensive survey of their applications in LLM reasoning. We first review fundamental concepts of RMs, including their architectures, training methodologies, and evaluation techniques. Then, we explore their key applications: (1) guiding generation and selecting optimal outputs during LLM inference, (2) facilitating data synthesis and iterative self-improvement for LLMs, and (3) providing training signals in RL-based finetuning. Finally, we address critical open questions regarding the selection, generalization, evaluation, and enhancement of RMs, based on existing research and our own empirical findings. Our analysis aims to provide actionable insights for the effective deployment and advancement of RMs for LLM reasoning.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "214",
        "title": "Inverse Language Modeling towards Robust and Grounded LLMs",
        "author": [
            "Davide Gabrielli",
            "Simone Sestito",
            "Iacopo Masi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01929",
        "abstract": "The current landscape of defensive mechanisms for LLMs is fragmented and underdeveloped, unlike prior work on classifiers. To further promote adversarial robustness in LLMs, we propose Inverse Language Modeling (ILM), a unified framework that simultaneously 1) improves the robustness of LLMs to input perturbations, and, at the same time, 2) enables native grounding by inverting model outputs to identify potentially toxic or unsafe input triggers. ILM transforms LLMs from static generators into analyzable and robust systems, potentially helping RED teaming. ILM can lay the foundation for next-generation LLMs that are not only robust and grounded but also fundamentally more controllable and trustworthy. The code is publicly available at http://github.com/davegabe/pag-llm.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "215",
        "title": "Veri-R1: Toward Precise and Faithful Claim Verification via Online Reinforcement Learning",
        "author": [
            "Qi He",
            "Cheng Qian",
            "Xiusi Chen",
            "Bingxiang He",
            "Yi R.",
            "Fung",
            "Heng Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01932",
        "abstract": "Claim verification with large language models (LLMs) has recently attracted considerable attention, owing to their superior reasoning capabilities and transparent verification pathways compared to traditional answer-only judgments. Online claim verification requires iterative evidence retrieval and reasoning, yet existing approaches mainly rely on prompt engineering or predesigned reasoning workflows without offering a unified training paradigm to improve necessary skills. Therefore, we introduce Veri-R1, an online reinforcement learning (RL) framework that enables an LLM to interact with a search engine and to receive reward signals that explicitly shape its planning, retrieval, and reasoning behaviors. The dynamic interaction between models and retrieval systems more accurately reflects real-world verification scenarios and fosters comprehensive verification skills. Empirical results show that Veri-R1 improves joint accuracy by up to 30% and doubles evidence score, often surpassing larger-scale counterparts. Ablation studies further reveal the impact of reward components and the link between output logits and label accuracy. Our results highlight the effectiveness of online RL for precise and faithful claim verification and provide a foundation for future research. We release our code to support community progress in LLM empowered claim verification.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "216",
        "title": "StelLA: Subspace Learning in Low-rank Adaptation using Stiefel Manifold",
        "author": [
            "Zhizhong Li",
            "Sina Sajadmanesh",
            "Jingtao Li",
            "Lingjuan Lyu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01938",
        "abstract": "Low-rank adaptation (LoRA) has been widely adopted as a parameter-efficient technique for fine-tuning large-scale pre-trained models. However, it still lags behind full fine-tuning in performance, partly due to its insufficient exploitation of the geometric structure underlying low-rank manifolds. In this paper, we propose a geometry-aware extension of LoRA that uses a three-factor decomposition $U\\!SV^\\top$. Analogous to the structure of singular value decomposition (SVD), it separates the adapter's input and output subspaces, $V$ and $U$, from the scaling factor $S$. Our method constrains $U$ and $V$ to lie on the Stiefel manifold, ensuring their orthonormality throughout the training. To optimize on the Stiefel manifold, we employ a flexible and modular geometric optimization design that converts any Euclidean optimizer to a Riemannian one. It enables efficient subspace learning while remaining compatible with existing fine-tuning pipelines. Empirical results across a wide range of downstream tasks, including commonsense reasoning, math and code generation, image classification, and image generation, demonstrate the superior performance of our approach against the recent state-of-the-art variants of LoRA. Code is available at https://github.com/SonyResearch/stella.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "217",
        "title": "ClustViT: Clustering-based Token Merging for Semantic Segmentation",
        "author": [
            "Fabio Montello",
            "Ronja GÃ¼ldenring",
            "Lazaros Nalpantidis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01948",
        "abstract": "Vision Transformers can achieve high accuracy and strong generalization across various contexts, but their practical applicability on real-world robotic systems is limited due to their quadratic attention complexity. Recent works have focused on dynamically merging tokens according to the image complexity. Token merging works well for classification but is less suited to dense prediction. We propose ClustViT, where we expand upon the Vision Transformer (ViT) backbone and address semantic segmentation. Within our architecture, a trainable Cluster module merges similar tokens along the network guided by pseudo-clusters from segmentation masks. Subsequently, a Regenerator module restores fine details for downstream heads. Our approach achieves up to 2.18x fewer GFLOPs and 1.64x faster inference on three different datasets, with comparable segmentation accuracy. Our code and models will be made publicly available.",
        "tags": [
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "218",
        "title": "Patch-as-Decodable-Token: Towards Unified Multi-Modal Vision Tasks in MLLMs",
        "author": [
            "Yongyi Su",
            "Haojie Zhang",
            "Shijie Li",
            "Nanqing Liu",
            "Jingyi Liao",
            "Junyi Pan",
            "Yuan Liu",
            "Xiaofen Xing",
            "Chong Sun",
            "Chen Li",
            "Nancy F. Chen",
            "Shuicheng Yan",
            "Xulei Yang",
            "Xun Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01954",
        "abstract": "Multimodal large language models (MLLMs) have advanced rapidly in recent years. However, existing approaches for vision tasks often rely on indirect representations, such as generating coordinates as text for detection, which limits performance and prevents dense prediction tasks like segmentation. To overcome these challenges, we introduce Patch-as-Decodable Token (PaDT), a unified paradigm that enables MLLMs to directly generate both textual and diverse visual outputs. Central to PaDT are Visual Reference Tokens (VRTs), derived from visual patch embeddings of query images and interleaved seamlessly with LLM's output textual tokens. A lightweight decoder then transforms LLM's outputs into detection, segmentation, and grounding predictions. Unlike prior methods, PaDT processes VRTs independently at each forward pass and dynamically expands the embedding table, thus improving localization and differentiation among similar objects. We further tailor a training strategy for PaDT by randomly selecting VRTs for supervised fine-tuning and introducing a robust per-token cross-entropy loss. Our empirical studies across four visual perception and understanding tasks suggest PaDT consistently achieving state-of-the-art performance, even compared with significantly larger MLLM models. The code is available at https://github.com/Gorilla-Lab-SCUT/PaDT.",
        "tags": [
            "Detection",
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "219",
        "title": "Exploring Resolution-Wise Shared Attention in Hybrid Mamba-U-Nets for Improved Cross-Corpus Speech Enhancement",
        "author": [
            "Nikolai Lund KÃ¼hne",
            "Jesper Jensen",
            "Jan Ãstergaard",
            "Zheng-Hua Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01958",
        "abstract": "Recent advances in speech enhancement have shown that models combining Mamba and attention mechanisms yield superior cross-corpus generalization performance. At the same time, integrating Mamba in a U-Net structure has yielded state-of-the-art enhancement performance, while reducing both model size and computational complexity. Inspired by these insights, we propose RWSA-MambaUNet, a novel and efficient hybrid model combining Mamba and multi-head attention in a U-Net structure for improved cross-corpus performance. Resolution-wise shared attention (RWSA) refers to layerwise attention-sharing across corresponding time- and frequency resolutions. Our best-performing RWSA-MambaUNet model achieves state-of-the-art generalization performance on two out-of-domain test sets. Notably, our smallest model surpasses all baselines on the out-of-domain DNS 2020 test set in terms of PESQ, SSNR, and ESTOI, and on the out-of-domain EARS-WHAM_v2 test set in terms of SSNR, ESTOI, and SI-SDR, while using less than half the model parameters and a fraction of the FLOPs.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "220",
        "title": "ROI-GS: Interest-based Local Quality 3D Gaussian Splatting",
        "author": [
            "Quoc-Anh Bui",
            "Gilles Rougeron",
            "GÃ©raldine Morin",
            "Simone Gasparini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01978",
        "abstract": "We tackle the challenge of efficiently reconstructing 3D scenes with high detail on objects of interest. Existing 3D Gaussian Splatting (3DGS) methods allocate resources uniformly across the scene, limiting fine detail to Regions Of Interest (ROIs) and leading to inflated model size. We propose ROI-GS, an object-aware framework that enhances local details through object-guided camera selection, targeted Object training, and seamless integration of high-fidelity object of interest reconstructions into the global scene. Our method prioritizes higher resolution details on chosen objects while maintaining real-time performance. Experiments show that ROI-GS significantly improves local quality (up to 2.96 dB PSNR), while reducing overall model size by $\\approx 17\\%$ of baseline and achieving faster training for a scene with a single object of interest, outperforming existing methods.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "221",
        "title": "$\\text{G}^2$RPO: Granular GRPO for Precise Reward in Flow Models",
        "author": [
            "Yujie Zhou",
            "Pengyang Ling",
            "Jiazi Bu",
            "Yibin Wang",
            "Yuhang Zang",
            "Jiaqi Wang",
            "Li Niu",
            "Guangtao Zhai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01982",
        "abstract": "The integration of online reinforcement learning (RL) into diffusion and flow models has recently emerged as a promising approach for aligning generative models with human preferences. Stochastic sampling via Stochastic Differential Equations (SDE) is employed during the denoising process to generate diverse denoising directions for RL exploration. While existing methods effectively explore potential high-value samples, they suffer from sub-optimal preference alignment due to sparse and narrow reward signals. To address these challenges, we propose a novel Granular-GRPO ($\\text{G}^2$RPO ) framework that achieves precise and comprehensive reward assessments of sampling directions in reinforcement learning of flow models. Specifically, a Singular Stochastic Sampling strategy is introduced to support step-wise stochastic exploration while enforcing a high correlation between the reward and the injected noise, thereby facilitating a faithful reward for each SDE perturbation. Concurrently, to eliminate the bias inherent in fixed-granularity denoising, we introduce a Multi-Granularity Advantage Integration module that aggregates advantages computed at multiple diffusion scales, producing a more comprehensive and robust evaluation of the sampling directions. Experiments conducted on various reward models, including both in-domain and out-of-domain evaluations, demonstrate that our $\\text{G}^2$RPO significantly outperforms existing flow-based GRPO baselines,highlighting its effectiveness and robustness.",
        "tags": [
            "Diffusion",
            "GRPO",
            "RL",
            "SDE"
        ]
    },
    {
        "id": "222",
        "title": "SPARC: Spine with Prismatic and Revolute Compliance for Quadruped Robot",
        "author": [
            "Yue Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01984",
        "abstract": "We present SPARC, a compact, open-source 3-DoF sagittal-plane spine module that combines revolute (pitch) and prismatic (axial) motion with programmable task-space impedance for quadruped robots. The system integrates three torque-controlled actuators, a custom 1 kHz control board, and a protected power unit in a 1.26 kg package, enabling closed-loop stiffness and damping shaping along x, z, and theta. We develop an RNEA-based computed-acceleration controller with smooth Stribeck friction compensation to render spring-damper behavior without explicit inertia shaping. Bench experiments validate the approach. Quasi-static push-pull tests show linear force-displacement characteristics with commanded horizontal stiffness spanning 300-700 N/m and <= 1.5% relative error (R^2 >= 0.992, narrow 95% CIs). Dynamic displace-and-release trials confirm mass-spring-damper responses over multiple damping settings, with small, interpretable phase deviations due to configuration-dependent inertia and low-speed friction effects. A task-space PD controller produces roughly linear stiffness but with greater variability and coupling sensitivity. SPARC provides a portable platform for systematic studies of spine compliance in legged locomotion and will be released with complete hardware and firmware resources.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "223",
        "title": "Exploring Database Normalization Effects on SQL Generation",
        "author": [
            "Ryosuke Kohita"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01989",
        "abstract": "Schema design, particularly normalization, is a critical yet often overlooked factor in natural language to SQL (NL2SQL) systems. Most prior research evaluates models on fixed schemas, overlooking the influence of design on performance. We present the first systematic study of schema normalization's impact, evaluating eight leading large language models on synthetic and real-world datasets with varied normalization levels. We construct controlled synthetic datasets with formal normalization (1NF-3NF) and real academic paper datasets with practical schemes. Our results show that denormalized schemas offer high accuracy on simple retrieval queries, even with cost-effective models in zero-shot settings. In contrast, normalized schemas (2NF/3NF) introduce challenges such as errors in base table selection and join type prediction; however, these issues are substantially mitigated by providing few-shot examples. For aggregation queries, normalized schemas yielded better performance, mainly due to their robustness against the data duplication and NULL value issues that cause errors in denormalized schemas. These findings suggest that the optimal schema design for NL2SQL applications depends on the types of queries to be supported. Our study demonstrates the importance of considering schema design when developing NL2SQL interfaces and integrating adaptive schema selection for real-world scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "224",
        "title": "4DGS-Craft: Consistent and Interactive 4D Gaussian Splatting Editing",
        "author": [
            "Lei Liu",
            "Can Wang",
            "Zhenghao Chen",
            "Dong Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01991",
        "abstract": "Recent advances in 4D Gaussian Splatting (4DGS) editing still face challenges with view, temporal, and non-editing region consistency, as well as with handling complex text instructions. To address these issues, we propose 4DGS-Craft, a consistent and interactive 4DGS editing framework. We first introduce a 4D-aware InstructPix2Pix model to ensure both view and temporal consistency. This model incorporates 4D VGGT geometry features extracted from the initial scene, enabling it to capture underlying 4D geometric structures during editing. We further enhance this model with a multi-view grid module that enforces consistency by iteratively refining multi-view input images while jointly optimizing the underlying 4D scene. Furthermore, we preserve the consistency of non-edited regions through a novel Gaussian selection mechanism, which identifies and optimizes only the Gaussians within the edited regions. Beyond consistency, facilitating user interaction is also crucial for effective 4DGS editing. Therefore, we design an LLM-based module for user intent understanding. This module employs a user instruction template to define atomic editing operations and leverages an LLM for reasoning. As a result, our framework can interpret user intent and decompose complex instructions into a logical sequence of atomic operations, enabling it to handle intricate user commands and further enhance editing performance. Compared to related works, our approach enables more consistent and controllable 4D scene editing. Our code will be made available upon acceptance.",
        "tags": [
            "Gaussian Splatting",
            "LLM"
        ]
    },
    {
        "id": "225",
        "title": "Clarifying Semantics of In-Context Examples for Unit Test Generation",
        "author": [
            "Chen Yang",
            "Lin Yang",
            "Ziqi Wang",
            "Dong Wang",
            "Jianyi Zhou",
            "Junjie Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01994",
        "abstract": "Recent advances in large language models (LLMs) have enabled promising performance in unit test generation through in-context learning (ICL). However, the quality of in-context examples significantly influences the effectiveness of generated tests-poorly structured or semantically unclear test examples often lead to suboptimal outputs. In this paper, we propose CLAST, a novel technique that systematically refines unit tests to improve their semantic clarity, thereby enhancing their utility as in-context examples. The approach decomposes complex tests into logically clearer ones and improves semantic clarity through a combination of program analysis and LLM-based rewriting. We evaluated CLAST on four open-source and three industrial projects. The results demonstrate that CLAST largely outperforms UTgen, the state-of-the-art refinement technique, in both preserving test effectiveness and enhancing semantic clarity. Specifically, CLAST fully retains the original effectiveness of unit tests, while UTgen reduces compilation success rate (CSR), pass rate (PR), test coverage (Cov), and mutation score (MS) by an average of 12.90%, 35.82%, 4.65%, and 5.07%, respectively. Over 85.33% of participants in our user study preferred the semantic clarity of CLAST-refined tests. Notably, incorporating CLAST-refined tests as examples effectively improves ICL-based unit test generation approaches such as RAGGen and TELPA, resulting in an average increase of 25.97% in CSR, 28.22% in PR, and 45.99% in Cov for generated tests, compared to incorporating UTgen-refined tests. The insights from the follow-up user study not only reinforce CLAST's potential impact in software testing practice but also illuminate avenues for future research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "226",
        "title": "LLM-Based Multi-Task Bangla Hate Speech Detection: Type, Severity, and Target",
        "author": [
            "Md Arid Hasan",
            "Firoj Alam",
            "Md Fahad Hossain",
            "Usman Naseem",
            "Syed Ishtiaque Ahmed"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01995",
        "abstract": "Online social media platforms are central to everyday communication and information seeking. While these platforms serve positive purposes, they also provide fertile ground for the spread of hate speech, offensive language, and bullying content targeting individuals, organizations, and communities. Such content undermines safety, participation, and equity online. Reliable detection systems are therefore needed, especially for low-resource languages where moderation tools are limited. In Bangla, prior work has contributed resources and models, but most are single-task (e.g., binary hate/offense) with limited coverage of multi-facet signals (type, severity, target). We address these gaps by introducing the first multi-task Bangla hate-speech dataset, BanglaMultiHate, one of the largest manually annotated corpus to date. Building on this resource, we conduct a comprehensive, controlled comparison spanning classical baselines, monolingual pretrained models, and LLMs under zero-shot prompting and LoRA fine-tuning. Our experiments assess LLM adaptability in a low-resource setting and reveal a consistent trend: although LoRA-tuned LLMs are competitive with BanglaBERT, culturally and linguistically grounded pretraining remains critical for robust performance. Together, our dataset and findings establish a stronger benchmark for developing culturally aligned moderation tools in low-resource contexts. For reproducibility, we will release the dataset and all related scripts.",
        "tags": [
            "Detection",
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "227",
        "title": "Generating Findings for Jaw Cysts in Dental Panoramic Radiographs Using GPT-4o: Building a Two-Stage Self-Correction Loop with Structured Output (SLSO) Framework",
        "author": [
            "Nanaka Hosokawa",
            "Ryo Takahashi",
            "Tomoya Kitano",
            "Yukihiro Iida",
            "Chisako Muramatsu",
            "Tatsuro Hayashi",
            "Yuta Seino",
            "Xiangrong Zhou",
            "Takeshi Hara",
            "Akitoshi Katsumata",
            "Hiroshi Fujita"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02001",
        "abstract": "In this study, we utilized the multimodal capabilities of OpenAI GPT-4o to automatically generate jaw cyst findings on dental panoramic radiographs. To improve accuracy, we constructed a Self-correction Loop with Structured Output (SLSO) framework and verified its effectiveness. A 10-step process was implemented for 22 cases of jaw cysts, including image input and analysis, structured data generation, tooth number extraction and consistency checking, iterative regeneration when inconsistencies were detected, and finding generation with subsequent restructuring and consistency verification. A comparative experiment was conducted using the conventional Chain-of-Thought (CoT) method across seven evaluation items: transparency, internal structure, borders, root resorption, tooth movement, relationships with other structures, and tooth number. The results showed that the proposed SLSO framework improved output accuracy for many items, with 66.9%, 33.3%, and 28.6% improvement rates for tooth number, tooth movement, and root resorption, respectively. In the successful cases, a consistently structured output was achieved after up to five regenerations. Although statistical significance was not reached because of the small size of the dataset, the overall SLSO framework enforced negative finding descriptions, suppressed hallucinations, and improved tooth number identification accuracy. However, the accurate identification of extensive lesions spanning multiple teeth is limited. Nevertheless, further refinement is required to enhance overall performance and move toward a practical finding generation system.",
        "tags": [
            "CoT",
            "GPT"
        ]
    },
    {
        "id": "228",
        "title": "A Copula-Based Variational Autoencoder for Uncertainty Quantification in Inverse Problems: Application to Damage Identification in an Offshore Wind Turbine",
        "author": [
            "Ana Fernandez-Navamuel",
            "Matteo Croci",
            "Martin Alberto Diaz Viera"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02013",
        "abstract": "Structural Health Monitoring of Floating Offshore Wind Turbines (FOWTs) is critical for ensuring operational safety and efficiency. However, identifying damage in components like mooring systems from limited sensor data poses a challenging inverse problem, often characterized by multimodal solutions where various damage states could explain the observed response. To overcome it, we propose a Variational Autoencoder (VAE) architecture, where the encoder approximates the inverse operator, while the decoder approximates the forward. The posterior distribution of the latent space variables is probabilistically modeled, describing the uncertainties in the estimates. This work tackles the limitations of conventional Gaussian Mixtures used within VAEs, which can be either too restrictive or computationally prohibitive for high-dimensional spaces. We propose a novel Copula-based VAE architecture that decouples the marginal distribution of the variables from their dependence structure, offering a flexible method for representing complex, correlated posterior distributions. We provide a comprehensive comparison of three different approaches for approximating the posterior: a Gaussian Mixture with a diagonal covariance matrix, a Gaussian Mixture with a full covariance matrix, and a Gaussian Copula. Our analysis, conducted on a high-fidelity synthetic dataset, demonstrates that the Copula VAE offers a promising and tractable solution in high-dimensional spaces. Although the present work remains in the two-dimensional space, the results suggest efficient scalability to higher dimensions. It achieves superior performance with significantly fewer parameters than the Gaussian Mixture alternatives, whose parametrization grows prohibitively with the dimensionality. The results underscore the potential of Copula-based VAEs as a tool for uncertainty-aware damage identification in FOWT mooring systems.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "229",
        "title": "Style Over Story: A Process-Oriented Study of Authorial Creativity in Large Language Models",
        "author": [
            "Donghoon Jung",
            "Jiwoo Choi",
            "Songeun Chae",
            "Seohyon Jung"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02025",
        "abstract": "Evaluations of large language models (LLMs)' creativity have focused primarily on the quality of their outputs rather than the processes that shape them. This study takes a process-oriented approach, drawing on narratology to examine LLMs as computational authors. We introduce constraint-based decision-making as a lens for authorial creativity. Using controlled prompting to assign authorial personas, we analyze the creative preferences of the models. Our findings show that LLMs consistently emphasize Style over other elements, including Character, Event, and Setting. By also probing the reasoning the models provide for their choices, we show that distinctive profiles emerge across models and argue that our approach provides a novel systematic tool for analyzing AI's authorial creativity.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "230",
        "title": "GaussianMorphing: Mesh-Guided 3D Gaussians for Semantic-Aware Object Morphing",
        "author": [
            "Mengtian Li",
            "Yunshu Bai",
            "Yimin Chu",
            "Yijun Shen",
            "Zhongmei Li",
            "Weifeng Ge",
            "Zhifeng Xie",
            "Chaofeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02034",
        "abstract": "We introduce GaussianMorphing, a novel framework for semantic-aware 3D shape and texture morphing from multi-view images. Previous approaches usually rely on point clouds or require pre-defined homeomorphic mappings for untextured data. Our method overcomes these limitations by leveraging mesh-guided 3D Gaussian Splatting (3DGS) for high-fidelity geometry and appearance modeling. The core of our framework is a unified deformation strategy that anchors 3DGaussians to reconstructed mesh patches, ensuring geometrically consistent transformations while preserving texture fidelity through topology-aware constraints. In parallel, our framework establishes unsupervised semantic correspondence by using the mesh topology as a geometric prior and maintains structural integrity via physically plausible point trajectories. This integrated approach preserves both local detail and global semantic coherence throughout the morphing process with out requiring labeled data. On our proposed TexMorph benchmark, GaussianMorphing substantially outperforms prior 2D/3D methods, reducing color consistency error ($\\Delta E$) by 22.2% and EI by 26.2%. Project page: https://baiyunshu.github.io/GAUSSIANMORPHING.github.io/",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "231",
        "title": "Zero-shot Human Pose Estimation using Diffusion-based Inverse solvers",
        "author": [
            "Sahil Bhandary Karnoor",
            "Romit Roy Choudhury"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02043",
        "abstract": "Pose estimation refers to tracking a human's full body posture, including their head, torso, arms, and legs. The problem is challenging in practical settings where the number of body sensors are limited. Past work has shown promising results using conditional diffusion models, where the pose prediction is conditioned on both <location, rotation> measurements from the sensors. Unfortunately, nearly all these approaches generalize poorly across users, primarly because location measurements are highly influenced by the body size of the user. In this paper, we formulate pose estimation as an inverse problem and design an algorithm capable of zero-shot generalization. Our idea utilizes a pre-trained diffusion model and conditions it on rotational measurements alone; the priors from this model are then guided by a likelihood term, derived from the measured locations. Thus, given any user, our proposed InPose method generatively estimates the highly likely sequence of poses that best explains the sparse on-body measurements.",
        "tags": [
            "Diffusion",
            "Pose Estimation"
        ]
    },
    {
        "id": "232",
        "title": "Stream RAG: Instant and Accurate Spoken Dialogue Systems with Streaming Tool Usage",
        "author": [
            "Siddhant Arora",
            "Haidar Khan",
            "Kai Sun",
            "Xin Luna Dong",
            "Sajal Choudhary",
            "Seungwhan Moon",
            "Xinyuan Zhang",
            "Adithya Sagar",
            "Surya Teja Appini",
            "Kaushik Patnaik",
            "Sanat Sharma",
            "Shinji Watanabe",
            "Anuj Kumar",
            "Ahmed Aly",
            "Yue Liu",
            "Florian Metze",
            "Zhaojiang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02044",
        "abstract": "End-to-end speech-in speech-out dialogue systems are emerging as a powerful alternative to traditional ASR-LLM-TTS pipelines, generating more natural, expressive responses with significantly lower latency. However, these systems remain prone to hallucinations due to limited factual grounding. While text-based dialogue systems address this challenge by integrating tools such as web search and knowledge graph APIs, we introduce the first approach to extend tool use directly into speech-in speech-out systems. A key challenge is that tool integration substantially increases response latency, disrupting conversational flow. To mitigate this, we propose Streaming Retrieval-Augmented Generation (Streaming RAG), a novel framework that reduces user-perceived latency by predicting tool queries in parallel with user speech, even before the user finishes speaking. Specifically, we develop a post-training pipeline that teaches the model when to issue tool calls during ongoing speech and how to generate spoken summaries that fuse audio queries with retrieved text results, thereby improving both accuracy and responsiveness. To evaluate our approach, we construct AudioCRAG, a benchmark created by converting queries from the publicly available CRAG dataset into speech form. Experimental results demonstrate that our streaming RAG approach increases QA accuracy by up to 200% relative (from 11.1% to 34.2% absolute) and further enhances user experience by reducing tool use latency by 20%. Importantly, our streaming RAG approach is modality-agnostic and can be applied equally to typed input, paving the way for more agentic, real-time AI assistants.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "233",
        "title": "Chain-of-Thought Reasoning in Streaming Full-Duplex End-to-End Spoken Dialogue Systems",
        "author": [
            "Siddhant Arora",
            "Jinchuan Tian",
            "Hayato Futami",
            "Jiatong Shi",
            "Yosuke Kashiwagi",
            "Emiru Tsunoo",
            "Shinji Watanabe"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02066",
        "abstract": "Most end-to-end (E2E) spoken dialogue systems (SDS) rely on voice activity detection (VAD) for turn-taking, but VAD fails to distinguish between pauses and turn completions. Duplex SDS models address this by predicting output continuously, including silence tokens, thus removing the need for explicit VAD. However, they often have complex dual-channel architecture and lag behind cascaded models in semantic reasoning. To overcome these challenges, we propose SCoT: a Streaming Chain-of-Thought (CoT) framework for Duplex SDS, alternating between processing fixed-duration user input and generating responses in a blockwise manner. Using frame-level alignments, we create intermediate targets-aligned user transcripts and system responses for each block. Experiments show that our approach produces more coherent and interpretable responses than existing duplex methods while supporting lower-latency and overlapping interactions compared to turn-by-turn systems.",
        "tags": [
            "CoT",
            "Detection"
        ]
    },
    {
        "id": "234",
        "title": "Spec-Gloss Surfels and Normal-Diffuse Priors for Relightable Glossy Objects",
        "author": [
            "Georgios Kouros",
            "Minye Wu",
            "Tinne Tuytelaars"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02069",
        "abstract": "Accurate reconstruction and relighting of glossy objects remain a longstanding challenge, as object shape, material properties, and illumination are inherently difficult to disentangle. Existing neural rendering approaches often rely on simplified BRDF models or parameterizations that couple diffuse and specular components, which restricts faithful material recovery and limits relighting fidelity. We propose a relightable framework that integrates a microfacet BRDF with the specular-glossiness parameterization into 2D Gaussian Splatting with deferred shading. This formulation enables more physically consistent material decomposition, while diffusion-based priors for surface normals and diffuse color guide early-stage optimization and mitigate ambiguity. A coarse-to-fine optimization of the environment map accelerates convergence and preserves high-dynamic-range specular reflections. Extensive experiments on complex, glossy scenes demonstrate that our method achieves high-quality geometry and material reconstruction, delivering substantially more realistic and consistent relighting under novel illumination compared to existing Gaussian splatting methods.",
        "tags": [
            "Diffusion",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "235",
        "title": "EC3R-SLAM: Efficient and Consistent Monocular Dense SLAM with Feed-Forward 3D Reconstruction",
        "author": [
            "Lingxiang Hu",
            "Naima Ait Oufroukh",
            "Fabien Bonardi",
            "Raymond Ghandour"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02080",
        "abstract": "The application of monocular dense Simultaneous Localization and Mapping (SLAM) is often hindered by high latency, large GPU memory consumption, and reliance on camera calibration. To relax this constraint, we propose EC3R-SLAM, a novel calibration-free monocular dense SLAM framework that jointly achieves high localization and mapping accuracy, low latency, and low GPU memory consumption. This enables the framework to achieve efficiency through the coupling of a tracking module, which maintains a sparse map of feature points, and a mapping module based on a feed-forward 3D reconstruction model that simultaneously estimates camera intrinsics. In addition, both local and global loop closures are incorporated to ensure mid-term and long-term data association, enforcing multi-view consistency and thereby enhancing the overall accuracy and robustness of the system. Experiments across multiple benchmarks show that EC3R-SLAM achieves competitive performance compared to state-of-the-art methods, while being faster and more memory-efficient. Moreover, it runs effectively even on resource-constrained platforms such as laptops and Jetson Orin NX, highlighting its potential for real-world robotics applications.",
        "tags": [
            "3D",
            "Robotics",
            "SLAM"
        ]
    },
    {
        "id": "236",
        "title": "Fine-Tuning Flow Matching via Maximum Likelihood Estimation of Reconstructions",
        "author": [
            "Zhaoyi Li",
            "Jingtao Ding",
            "Yong Li",
            "Shihua Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02081",
        "abstract": "Flow Matching (FM) algorithm achieves remarkable results in generative tasks especially in robotic manipulation. Building upon the foundations of diffusion models, the simulation-free paradigm of FM enables simple and efficient training, but inherently introduces a train-inference gap. Specifically, we cannot assess the model's output during the training phase. In contrast, other generative models including Variational Autoencoder (VAE), Normalizing Flow and Generative Adversarial Networks (GANs) directly optimize on the reconstruction loss. Such a gap is particularly evident in scenarios that demand high precision, such as robotic manipulation. Moreover, we show that FM's over-pursuit of straight predefined paths may introduce some serious problems such as stiffness into the system. These motivate us to fine-tune FM via Maximum Likelihood Estimation of reconstructions - an approach made feasible by FM's underlying smooth ODE formulation, in contrast to the stochastic differential equations (SDEs) used in diffusion models. This paper first theoretically analyzes the relation between training loss and inference error in FM. Then we propose a method of fine-tuning FM via Maximum Likelihood Estimation of reconstructions, which includes both straightforward fine-tuning and residual-based fine-tuning approaches. Furthermore, through specifically designed architectures, the residual-based fine-tuning can incorporate the contraction property into the model, which is crucial for the model's robustness and interpretability. Experimental results in image generation and robotic manipulation verify that our method reliably improves the inference performance of FM.",
        "tags": [
            "Diffusion",
            "Flow Matching",
            "ODE",
            "VAE"
        ]
    },
    {
        "id": "237",
        "title": "Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning",
        "author": [
            "Xinyuan Song",
            "Keyu Wang",
            "PengXiang Li",
            "Lu Yin",
            "Shiwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02091",
        "abstract": "Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "238",
        "title": "LangGrasp: Leveraging Fine-Tuned LLMs for Language Interactive Robot Grasping with Ambiguous Instructions",
        "author": [
            "Yunhan Lin",
            "Wenqi Wu",
            "Zhijie Zhang",
            "Huasong Min"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02104",
        "abstract": "The existing language-driven grasping methods struggle to fully handle ambiguous instructions containing implicit intents. To tackle this challenge, we propose LangGrasp, a novel language-interactive robotic grasping framework. The framework integrates fine-tuned large language models (LLMs) to leverage their robust commonsense understanding and environmental perception capabilities, thereby deducing implicit intents from linguistic instructions and clarifying task requirements along with target manipulation objects. Furthermore, our designed point cloud localization module, guided by 2D part segmentation, enables partial point cloud localization in scenes, thereby extending grasping operations from coarse-grained object-level to fine-grained part-level manipulation. Experimental results show that the LangGrasp framework accurately resolves implicit intents in ambiguous instructions, identifying critical operations and target information that are unstated yet essential for task completion. Additionally, it dynamically selects optimal grasping poses by integrating environmental information. This enables high-precision grasping from object-level to part-level manipulation, significantly enhancing the adaptability and task execution efficiency of robots in unstructured environments. More information and code are available here: https://github.com/wu467/LangGrasp.",
        "tags": [
            "LLM",
            "Robotics",
            "Segmentation"
        ]
    },
    {
        "id": "239",
        "title": "SoundReactor: Frame-level Online Video-to-Audio Generation",
        "author": [
            "Koichi Saito",
            "Julian Tanke",
            "Christian Simon",
            "Masato Ishii",
            "Kazuki Shimada",
            "Zachary Novack",
            "Zhi Zhong",
            "Akio Hayakawa",
            "Takashi Shibuya",
            "Yuki Mitsufuji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02110",
        "abstract": "Prevailing Video-to-Audio (V2A) generation models operate offline, assuming an entire video sequence or chunks of frames are available beforehand. This critically limits their use in interactive applications such as live content creation and emerging generative world models. To address this gap, we introduce the novel task of frame-level online V2A generation, where a model autoregressively generates audio from video without access to future video frames. Furthermore, we propose SoundReactor, which, to the best of our knowledge, is the first simple yet effective framework explicitly tailored for this task. Our design enforces end-to-end causality and targets low per-frame latency with audio-visual synchronization. Our model's backbone is a decoder-only causal transformer over continuous audio latents. For vision conditioning, it leverages grid (patch) features extracted from the smallest variant of the DINOv2 vision encoder, which are aggregated into a single token per frame to maintain end-to-end causality and efficiency. The model is trained through a diffusion pre-training followed by consistency fine-tuning to accelerate the diffusion head decoding. On a benchmark of diverse gameplay videos from AAA titles, our model successfully generates semantically and temporally aligned, high-quality full-band stereo audio, validated by both objective and human evaluations. Furthermore, our model achieves low per-frame waveform-level latency (26.3ms with the head NFE=1, 31.5ms with NFE=4) on 30FPS, 480p videos using a single H100. Demo samples are available at https://koichi-saito-sony.github.io/soundreactor/.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "240",
        "title": "FRIEREN: Federated Learning with Vision-Language Regularization for Segmentation",
        "author": [
            "Ding-Ruei Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02114",
        "abstract": "Federeated Learning (FL) offers a privacy-preserving solution for Semantic Segmentation (SS) tasks to adapt to new domains, but faces significant challenges from these domain shifts, particularly when client data is unlabeled. However, most existing FL methods unrealistically assume access to labeled data on remote clients or fail to leverage the power of modern Vision Foundation Models (VFMs). Here, we propose a novel and challenging task, FFREEDG, in which a model is pretrained on a server's labeled source dataset and subsequently trained across clients using only their unlabeled data, without ever re-accessing the source. To solve FFREEDG, we propose FRIEREN, a framework that leverages the knowledge of a VFM by integrating vision and language modalities. Our approach employs a Vision-Language decoder guided by CLIP-based text embeddings to improve semantic disambiguation and uses a weak-to-strong consistency learning strategy for robust local training on pseudo-labels. Our experiments on synthetic-to-real and clear-to-adverse-weather benchmarks demonstrate that our framework effectively tackles this new task, achieving competitive performance against established domain generalization and adaptation methods and setting a strong baseline for future research.",
        "tags": [
            "CLIP",
            "Segmentation"
        ]
    },
    {
        "id": "241",
        "title": "The Disparate Impacts of Speculative Decoding",
        "author": [
            "Jameson Sandler",
            "Ahmet ÃstÃ¼n",
            "Marco Romanelli",
            "Sara Hooker",
            "Ferdinando Fioretto"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02128",
        "abstract": "The practice of speculative decoding, whereby inference is probabilistically supported by a smaller, cheaper, ``drafter'' model, has become a standard technique for systematically reducing the decoding time of large language models. This paper conducts an analysis of speculative decoding through the lens of its potential disparate speed-up rates across tasks. Crucially, the paper shows that speed-up gained from speculative decoding is not uniformly distributed across tasks, consistently diminishing for under-fit, and often underrepresented tasks. To better understand this phenomenon, we derive an analysis to quantify this observed ``unfairness'' and draw attention to the factors that motivate such disparate speed-ups to emerge. Further, guided by these insights, the paper proposes a mitigation strategy designed to reduce speed-up disparities and validates the approach across several model pairs, revealing on average a 12% improvement in our fairness metric.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "242",
        "title": "Stand Up, NAO! Increasing the Reliability of Stand-Up Motions Through Error Compensation in Position Control",
        "author": [
            "Philip Reichenberg",
            "Tim Laue"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02129",
        "abstract": "Stand-up motions are an indispensable part of humanoid robot soccer. A robot incapable of standing up by itself is removed from the game for some time. In this paper, we present our stand-up motions for the NAO robot. Our approach dates back to 2019 and has been evaluated and slightly expanded over the past six years. We claim that the main reason for failed stand-up attempts are large errors in the executed joint positions. By addressing such problems by either executing special motions to free up stuck limbs such as the arms, or by compensating large errors with other joints, we significantly increased the overall success rate of our stand-up routine. The motions presented in this paper are also used by several other teams in the Standard Platform League, which thereby achieve similar success rates, as shown in an analysis of videos from multiple tournaments.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "243",
        "title": "Policy Gradient Guidance Enables Test Time Control",
        "author": [
            "Jianing Qi",
            "Hao Tang",
            "Zhigang Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02148",
        "abstract": "We introduce Policy Gradient Guidance (PGG), a simple extension of classifier-free guidance from diffusion models to classical policy gradient methods. PGG augments the policy gradient with an unconditional branch and interpolates conditional and unconditional branches, yielding a test-time control knob that modulates behavior without retraining. We provide a theoretical derivation showing that the additional normalization term vanishes under advantage estimation, leading to a clean guided policy gradient update. Empirically, we evaluate PGG on discrete and continuous control benchmarks. We find that conditioning dropout-central to diffusion guidance-offers gains in simple discrete tasks and low sample regimes, but dropout destabilizes continuous control. Training with modestly larger guidance ($\\gamma>1$) consistently improves stability, sample efficiency, and controllability. Our results show that guidance, previously confined to diffusion policies, can be adapted to standard on-policy methods, opening new directions for controllable online reinforcement learning.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "244",
        "title": "Reinforcement Learning with Action-Triggered Observations",
        "author": [
            "Alexander Ryabchenko",
            "Wenlong Mou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02149",
        "abstract": "We study reinforcement learning problems where state observations are stochastically triggered by actions, a constraint common in many real-world applications. This framework is formulated as Action-Triggered Sporadically Traceable Markov Decision Processes (ATST-MDPs), where each action has a specified probability of triggering a state observation. We derive tailored Bellman optimality equations for this framework and introduce the action-sequence learning paradigm in which agents commit to executing a sequence of actions until the next observation arrives. Under the linear MDP assumption, value-functions are shown to admit linear representations in an induced action-sequence feature map. Leveraging this structure, we propose off-policy estimators with statistical error guarantees for such feature maps and introduce ST-LSVI-UCB, a variant of LSVI-UCB adapted for action-triggered settings. ST-LSVI-UCB achieves regret $\\widetilde O(\\sqrt{Kd^3(1-\\gamma)^{-3}})$, where $K$ is the number of episodes, $d$ the feature dimension, and $\\gamma$ the discount factor (per-step episode non-termination probability). Crucially, this work establishes the theoretical foundation for learning with sporadic, action-triggered observations while demonstrating that efficient learning remains feasible under such observation constraints.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "245",
        "title": "Agentic Reasoning and Refinement through Semantic Interaction",
        "author": [
            "Xuxin Tang",
            "Rehema Abulikemu",
            "Eric Krokos",
            "Kirsten Whitley",
            "Xuan Wang",
            "Chris North"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02157",
        "abstract": "Sensemaking report writing often requires multiple refinements in the iterative process. While Large Language Models (LLMs) have shown promise in generating initial reports based on human visual workspace representations, they struggle to precisely incorporate sequential semantic interactions during the refinement process. We introduce VIS-ReAct, a framework that reasons about newly-added semantic interactions in visual workspaces to steer the LLM for report refinement.\nVIS-ReAct is a two-agent framework: a primary LLM analysis agent interprets new semantic interactions to infer user intentions and generate refinement planning, followed by an LLM refinement agent that updates reports accordingly. Through case study, VIS-ReAct outperforms baseline and VIS-ReAct (without LLM analysis) on targeted refinement, semantic fidelity, and transparent inference. Results demonstrate that VIS-ReAct better handles various interaction types and granularities while enhancing the transparency of human-LLM collaboration.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "246",
        "title": "Towards fairer public transit: Real-time tensor-based multimodal fare evasion and fraud detection",
        "author": [
            "Peter Wauyo",
            "Dalia Bwiza",
            "Alain Murara",
            "Edwin Mugume",
            "Eric Umuhoza"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02165",
        "abstract": "This research introduces a multimodal system designed to detect fraud and fare evasion in public transportation by analyzing closed circuit television (CCTV) and audio data. The proposed solution uses the Vision Transformer for Video (ViViT) model for video feature extraction and the Audio Spectrogram Transformer (AST) for audio analysis. The system implements a Tensor Fusion Network (TFN) architecture that explicitly models unimodal and bimodal interactions through a 2-fold Cartesian product. This advanced fusion technique captures complex cross-modal dynamics between visual behaviors (e.g., tailgating,unauthorized access) and audio cues (e.g., fare transaction sounds). The system was trained and tested on a custom dataset, achieving an accuracy of 89.5%, precision of 87.2%, and recall of 84.0% in detecting fraudulent activities, significantly outperforming early fusion baselines and exceeding the 75% recall rates typically reported in state-of-the-art transportation fraud detection systems. Our ablation studies demonstrate that the tensor fusion approach provides a 7.0% improvement in the F1 score and an 8.8% boost in recall compared to traditional concatenation methods. The solution supports real-time detection, enabling public transport operators to reduce revenue loss, improve passenger safety, and ensure operational compliance.",
        "tags": [
            "Detection",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "247",
        "title": "RESTRAIN: From Spurious Votes to Signals -- Self-Driven RL with Self-Penalization",
        "author": [
            "Zhaoning Yu",
            "Will Su",
            "Leitian Tao",
            "Haozhu Wang",
            "Aashu Singh",
            "Hanchao Yu",
            "Jianyu Wang",
            "Hongyang Gao",
            "Weizhe Yuan",
            "Jason Weston",
            "Ping Yu",
            "Jing Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02172",
        "abstract": "Reinforcement learning with human-annotated data has boosted chain-of-thought reasoning in large reasoning models, but these gains come at high costs in labeled data while faltering on harder tasks. A natural next step is experience-driven learning, where models improve without curated labels by adapting to unlabeled data. We introduce RESTRAIN (REinforcement learning with Self-restraint), a self-penalizing RL framework that converts the absence of gold labels into a useful learning signal. Instead of overcommitting to spurious majority votes, RESTRAIN exploits signals from the model's entire answer distribution: penalizing overconfident rollouts and low-consistency examples while preserving promising reasoning chains. The self-penalization mechanism integrates seamlessly into policy optimization methods such as GRPO, enabling continual self-improvement without supervision. On challenging reasoning benchmarks, RESTRAIN delivers large gains using only unlabeled data. With Qwen3-4B-Base and OctoThinker Hybrid-8B-Base, it improves Pass@1 by up to +140.7 percent on AIME25, +36.2 percent on MMLU_STEM, and +19.6 percent on GPQA-Diamond, nearly matching gold-label training while using no gold labels. These results demonstrate that RESTRAIN establishes a scalable path toward stronger reasoning without gold labels.",
        "tags": [
            "CoT",
            "GRPO",
            "RL"
        ]
    },
    {
        "id": "248",
        "title": "Learning to Reason for Hallucination Span Detection",
        "author": [
            "Hsuan Su",
            "Ting-Yao Hu",
            "Hema Swetha Koppula",
            "Kundan Krishna",
            "Hadi Pouransari",
            "Cheng-Yu Hsieh",
            "Cem Koc",
            "Joseph Yitan Cheng",
            "Oncel Tuzel",
            "Raviteja Vemulapalli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02173",
        "abstract": "Large language models (LLMs) often generate hallucinations -- unsupported content that undermines reliability. While most prior works frame hallucination detection as a binary task, many real-world applications require identifying hallucinated spans, which is a multi-step decision making process. This naturally raises the question of whether explicit reasoning can help the complex task of detecting hallucination spans. To answer this question, we first evaluate pretrained models with and without Chain-of-Thought (CoT) reasoning, and show that CoT reasoning has the potential to generate at least one correct answer when sampled multiple times. Motivated by this, we propose RL4HS, a reinforcement learning framework that incentivizes reasoning with a span-level reward function. RL4HS builds on Group Relative Policy Optimization and introduces Class-Aware Policy Optimization to mitigate reward imbalance issue. Experiments on the RAGTruth benchmark (summarization, question answering, data-to-text) show that RL4HS surpasses pretrained reasoning models and supervised fine-tuning, demonstrating the necessity of reinforcement learning with span-level rewards for detecting hallucination spans.",
        "tags": [
            "CoT",
            "Detection",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "249",
        "title": "Flatness-Aware Stochastic Gradient Langevin Dynamics",
        "author": [
            "Stefano Bruno",
            "Youngsik Hwang",
            "Jaehyeon An",
            "Sotirios Sabanis",
            "Dong-Young Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02174",
        "abstract": "Generalization in deep learning is closely tied to the pursuit of flat minima in the loss landscape, yet classical Stochastic Gradient Langevin Dynamics (SGLD) offers no mechanism to bias its dynamics toward such low-curvature solutions. This work introduces Flatness-Aware Stochastic Gradient Langevin Dynamics (fSGLD), designed to efficiently and provably seek flat minima in high-dimensional nonconvex optimization problems. At each iteration, fSGLD uses the stochastic gradient evaluated at parameters perturbed by isotropic Gaussian noise, commonly referred to as Random Weight Perturbation (RWP), thereby optimizing a randomized-smoothing objective that implicitly captures curvature information. Leveraging these properties, we prove that the invariant measure of fSGLD stays close to a stationary measure concentrated on the global minimizers of a loss function regularized by the Hessian trace whenever the inverse temperature and the scale of random weight perturbation are properly coupled. This result provides a rigorous theoretical explanation for the benefits of random weight perturbation. In particular, we establish non-asymptotic convergence guarantees in Wasserstein distance with the best known rate and derive an excess-risk bound for the Hessian-trace regularized objective. Extensive experiments on noisy-label and large-scale vision tasks, in both training-from-scratch and fine-tuning settings, demonstrate that fSGLD achieves superior or comparable generalization and robustness to baseline algorithms while maintaining the computational cost of SGD, about half that of SAM. Hessian-spectrum analysis further confirms that fSGLD converges to significantly flatter minima.",
        "tags": [
            "SAM"
        ]
    },
    {
        "id": "250",
        "title": "DisCo-Layout: Disentangling and Coordinating Semantic and Physical Refinement in a Multi-Agent Framework for 3D Indoor Layout Synthesis",
        "author": [
            "Jialin Gao",
            "Donghao Zhou",
            "Mingjian Liang",
            "Lihao Liu",
            "Chi-Wing Fu",
            "Xiaowei Hu",
            "Pheng-Ann Heng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02178",
        "abstract": "3D indoor layout synthesis is crucial for creating virtual environments. Traditional methods struggle with generalization due to fixed datasets. While recent LLM and VLM-based approaches offer improved semantic richness, they often lack robust and flexible refinement, resulting in suboptimal layouts. We develop DisCo-Layout, a novel framework that disentangles and coordinates physical and semantic refinement. For independent refinement, our Semantic Refinement Tool (SRT) corrects abstract object relationships, while the Physical Refinement Tool (PRT) resolves concrete spatial issues via a grid-matching algorithm. For collaborative refinement, a multi-agent framework intelligently orchestrates these tools, featuring a planner for placement rules, a designer for initial layouts, and an evaluator for assessment. Experiments demonstrate DisCo-Layout's state-of-the-art performance, generating realistic, coherent, and generalizable 3D indoor layouts. Our code will be publicly available.",
        "tags": [
            "3D",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "251",
        "title": "GRACE: A Language Model Framework for Explainable Inverse Reinforcement Learning",
        "author": [
            "Silvia Sapora",
            "Devon Hjelm",
            "Alexander Toshev",
            "Omar Attia",
            "Bogdan Mazoure"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02180",
        "abstract": "Inverse Reinforcement Learning aims to recover reward models from expert demonstrations, but traditional methods yield \"black-box\" models that are difficult to interpret and debug. In this work, we introduce GRACE (Generating Rewards As CodE), a method for using Large Language Models within an evolutionary search to reverse-engineer an interpretable, code-based reward function directly from expert trajectories. The resulting reward function is executable code that can be inspected and verified. We empirically validate GRACE on the BabyAI and AndroidWorld benchmarks, where it efficiently learns highly accurate rewards, even in complex, multi-task settings. Further, we demonstrate that the resulting reward leads to strong policies, compared to both competitive Imitation Learning and online RL approaches with ground-truth rewards. Finally, we show that GRACE is able to build complex reward APIs in multi-task setups.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "252",
        "title": "FalseCrashReducer: Mitigating False Positive Crashes in OSS-Fuzz-Gen Using Agentic AI",
        "author": [
            "Paschal C. Amusuo",
            "Dongge Liu",
            "Ricardo Andres Calvo Mendez",
            "Jonathan Metzman",
            "Oliver Chang",
            "James C. Davis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02185",
        "abstract": "Fuzz testing has become a cornerstone technique for identifying software bugs and security vulnerabilities, with broad adoption in both industry and open-source communities. Directly fuzzing a function requires fuzz drivers, which translate random fuzzer inputs into valid arguments for the target function. Given the cost and expertise required to manually develop fuzz drivers, methods exist that leverage program analysis and Large Language Models to automatically generate these drivers. However, the generated fuzz drivers frequently lead to false positive crashes, especially in functions highly structured input and complex state requirements. This problem is especially crucial in industry-scale fuzz driver generation efforts like OSS-Fuzz-en, as reporting false positive crashes to maintainers impede trust in both the system and the team.\nThis paper presents two AI-driven strategies to reduce false positives in OSS-Fuzz-Gen, a multi-agent system for automated fuzz driver generation. First, constraint-based fuzz driver generation proactively enforces constraints on a function's inputs and state to guide driver creation. Second, context-based crash validation reactively analyzes function callers to determine whether reported crashes are feasible from program entry points. Using 1,500 benchmark functions from OSS-Fuzz, we show that these strategies reduce spurious crashes by up to 8%, cut reported crashes by more than half, and demonstrate that frontier LLMs can serve as reliable program analysis agents. Our results highlight the promise and challenges of integrating AI into large-scale fuzzing pipelines.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "253",
        "title": "GeoPurify: A Data-Efficient Geometric Distillation Framework for Open-Vocabulary 3D Segmentation",
        "author": [
            "Weijia Dou",
            "Xu Zhang",
            "Yi Bin",
            "Jian Liu",
            "Bo Peng",
            "Guoqing Wang",
            "Yang Yang",
            "Heng Tao Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02186",
        "abstract": "Recent attempts to transfer features from 2D Vision-Language Models (VLMs) to 3D semantic segmentation expose a persistent trade-off. Directly projecting 2D features into 3D yields noisy and fragmented predictions, whereas enforcing geometric coherence necessitates costly training pipelines and large-scale annotated 3D data. We argue that this limitation stems from the dominant segmentation-and-matching paradigm, which fails to reconcile 2D semantics with 3D geometric structure. The geometric cues are not eliminated during the 2D-to-3D transfer but remain latent within the noisy and view-aggregated features. To exploit this property, we propose GeoPurify that applies a small Student Affinity Network to purify 2D VLM-generated 3D point features using geometric priors distilled from a 3D self-supervised teacher model. During inference, we devise a Geometry-Guided Pooling module to further denoise the point cloud and ensure the semantic and structural consistency. Benefiting from latent geometric information and the learned affinity network, GeoPurify effectively mitigates the trade-off and achieves superior data efficiency. Extensive experiments on major 3D benchmarks demonstrate that GeoPurify achieves or surpasses state-of-the-art performance while utilizing only about 1.5% of the training data. Our codes and checkpoints are available at [https://github.com/tj12323/GeoPurify](https://github.com/tj12323/GeoPurify).",
        "tags": [
            "3D",
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "254",
        "title": "High-Fidelity Speech Enhancement via Discrete Audio Tokens",
        "author": [
            "Luca A. LanzendÃ¶rfer",
            "FrÃ©dÃ©ric Berdoz",
            "Antonis Asonitis",
            "Roger Wattenhofer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02187",
        "abstract": "Recent autoregressive transformer-based speech enhancement (SE) methods have shown promising results by leveraging advanced semantic understanding and contextual modeling of speech. However, these approaches often rely on complex multi-stage pipelines and low sampling rate codecs, limiting them to narrow and task-specific speech enhancement. In this work, we introduce DAC-SE1, a simplified language model-based SE framework leveraging discrete high-resolution audio representations; DAC-SE1 preserves fine-grained acoustic details while maintaining semantic coherence. Our experiments show that DAC-SE1 surpasses state-of-the-art autoregressive SE methods on both objective perceptual metrics and in a MUSHRA human evaluation. We release our codebase and model checkpoints to support further research in scalable, unified, and high-quality speech enhancement.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "255",
        "title": "UpSafe$^\\circ$C: Upcycling for Controllable Safety in Large Language Models",
        "author": [
            "Yuhao Sun",
            "Zhuoer Xu",
            "Shiwen Cui",
            "Kun Yang",
            "Lingyun Yu",
            "Yongdong Zhang",
            "Hongtao Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02194",
        "abstract": "Large Language Models (LLMs) have achieved remarkable progress across a wide range of tasks, but remain vulnerable to safety risks such as harmful content generation and jailbreak attacks. Existing safety techniques -- including external guardrails, inference-time guidance, and post-training alignment -- each face limitations in balancing safety, utility, and controllability. In this work, we propose UpSafe$^\\circ$C, a unified framework for enhancing LLM safety through safety-aware upcycling. Our approach first identifies safety-critical layers and upcycles them into a sparse Mixture-of-Experts (MoE) structure, where the router acts as a soft guardrail that selectively activates original MLPs and added safety experts. We further introduce a two-stage SFT strategy to strengthen safety discrimination while preserving general capabilities. To enable flexible control at inference time, we introduce a safety temperature mechanism, allowing dynamic adjustment of the trade-off between safety and utility. Experiments across multiple benchmarks, base model, and model scales demonstrate that UpSafe$^\\circ$C achieves robust safety improvements against harmful and jailbreak inputs, while maintaining competitive performance on general tasks. Moreover, analysis shows that safety temperature provides fine-grained inference-time control that achieves the Pareto-optimal frontier between utility and safety. Our results highlight a new direction for LLM safety: moving from static alignment toward dynamic, modular, and inference-aware control.",
        "tags": [
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "256",
        "title": "ARUQULA -- An LLM based Text2SPARQL Approach using ReAct and Knowledge Graph Exploration Utilities",
        "author": [
            "Felix Brei",
            "Lorenz BÃ¼hmann",
            "Johannes Frey",
            "Daniel Gerber",
            "Lars-Peter Meyer",
            "Claus Stadler",
            "Kirill Bulert"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02200",
        "abstract": "Interacting with knowledge graphs can be a daunting task for people without a background in computer science since the query language that is used (SPARQL) has a high barrier of entry. Large language models (LLMs) can lower that barrier by providing support in the form of Text2SPARQL translation. In this paper we introduce a generalized method based on SPINACH, an LLM backed agent that translates natural language questions to SPARQL queries not in a single shot, but as an iterative process of exploration and execution. We describe the overall architecture and reasoning behind our design decisions, and also conduct a thorough analysis of the agent behavior to gain insights into future areas for targeted improvements. This work was motivated by the Text2SPARQL challenge, a challenge that was held to facilitate improvements in the Text2SPARQL domain.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "257",
        "title": "Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents",
        "author": [
            "Lingzhong Dong",
            "Ziqi Zhou",
            "Shuaibo Yang",
            "Haiyue Sheng",
            "Pengzhou Cheng",
            "Zongru Wu",
            "Zheng Wu",
            "Gongshen Liu",
            "Zhuosheng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02204",
        "abstract": "Mobile-use agents powered by vision-language models (VLMs) have shown great potential in interpreting natural language instructions and generating corresponding actions based on mobile graphical user interface. Recent studies suggest that incorporating chain-of-thought (CoT) reasoning tends to improve the execution accuracy. However, existing evaluations emphasize execution accuracy while neglecting whether CoT reasoning aligns with ground-truth actions. This oversight fails to assess potential reasoning-execution gaps, which in turn foster over-trust: users relying on seemingly plausible CoTs may unknowingly authorize harmful actions, potentially resulting in financial loss or trust crisis. In this work, we introduce a new evaluation framework to diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment (GTA), which measures whether the action implied by a CoT matches the ground-truth action. By combining GTA with the standard Exact Match (EM) metric, we jointly assess both the reasoning accuracy and execution accuracy. This joint perspective reveals two types of reasoning-execution gaps: (i) Execution Gap (EG), where the reasoning correctly identifies the correct action but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but reasoning process conflicts with the actual execution. Experimental results across a wide range of mobile interaction tasks reveal that reasoning-execution gaps are prevalent, with execution gaps occurring more frequently than reasoning gaps. Moreover, while scaling up model size reduces the overall gap, sizable execution gaps persist even in the largest models. Further analysis shows that our framework reliably reflects systematic EG/RG patterns in state-of-the-art models. These findings offer concrete diagnostics and support the development of more trustworthy mobile-use agents.",
        "tags": [
            "CoT",
            "VLM"
        ]
    },
    {
        "id": "258",
        "title": "Poolformer: Recurrent Networks with Pooling for Long-Sequence Modeling",
        "author": [
            "Daniel Gallo FernÃ¡ndez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02206",
        "abstract": "Sequence-to-sequence models have become central in Artificial Intelligence, particularly following the introduction of the transformer architecture. While initially developed for Natural Language Processing, these models have demonstrated utility across domains, including Computer Vision. Such models require mechanisms to exchange information along the time dimension, typically using recurrent or self-attention layers. However, self-attention scales quadratically with sequence length, limiting its practicality for very long sequences.\nWe introduce Poolformer, a sequence-to-sequence model that replaces self-attention with recurrent layers and incorporates pooling operations to reduce sequence length. Poolformer is defined recursively using SkipBlocks, which contain residual blocks, a down-pooling layer, a nested SkipBlock, an up-pooling layer, and additional residual blocks. We conduct extensive experiments to support our architectural choices.\nOur results show that pooling greatly accelerates training, improves perceptual metrics (FID and IS), and prevents overfitting. Our experiments also suggest that long-range dependencies are handled by deep layers, while shallow layers take care of short-term features.\nEvaluated on raw audio, which naturally features long sequence lengths, Poolformer outperforms state-of-the-art models such as SaShiMi and Mamba. Future directions include applications to text and vision, as well as multi-modal scenarios, where a Poolformer-based LLM could effectively process dense representations of images and videos.",
        "tags": [
            "LLM",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "259",
        "title": "StockBench: Can LLM Agents Trade Stocks Profitably In Real-world Markets?",
        "author": [
            "Yanxu Chen",
            "Zijun Yao",
            "Yantao Liu",
            "Jin Ye",
            "Jianing Yu",
            "Lei Hou",
            "Juanzi Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02209",
        "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities as autonomous agents, showing promise in reasoning, tool use, and sequential decision-making. While prior benchmarks have evaluated LLM agents in domains such as software engineering and scientific discovery, the finance domain remains underexplored, despite its direct relevance to economic value and high-stakes decision-making. Existing financial benchmarks primarily test static knowledge through question answering, but they fall short of capturing the dynamic and iterative nature of trading. To address this gap, we introduce StockBench, a contamination-free benchmark designed to evaluate LLM agents in realistic, multi-month stock trading environments. Agents receive daily market signals -- including prices, fundamentals, and news -- and must make sequential buy, sell, or hold decisions. Performance is assessed using financial metrics such as cumulative return, maximum drawdown, and the Sortino ratio. Our evaluation of state-of-the-art proprietary (e.g., GPT-5, Claude-4) and open-weight (e.g., Qwen3, Kimi-K2, GLM-4.5) models shows that while most LLM agents struggle to outperform the simple buy-and-hold baseline, several models demonstrate the potential to deliver higher returns and manage risk more effectively. These findings highlight both the challenges and opportunities in developing LLM-powered financial agents, showing that excelling at static financial knowledge tasks does not necessarily translate into successful trading strategies. We release StockBench as an open-source resource to support reproducibility and advance future research in this domain.",
        "tags": [
            "GLM",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "260",
        "title": "DiFFPO: Training Diffusion LLMs to Reason Fast and Furious via Reinforcement Learning",
        "author": [
            "Hanyang Zhao",
            "Dawen Liang",
            "Wenpin Tang",
            "David Yao",
            "Nathan Kallus"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02212",
        "abstract": "We propose DiFFPO, Diffusion Fast and Furious Policy Optimization, a unified framework for training masked diffusion large language models (dLLMs) to reason not only better (furious), but also faster via reinforcement learning (RL). We first unify the existing baseline approach such as d1 by proposing to train surrogate policies via off-policy RL, whose likelihood is much more tractable as an approximation to the true dLLM policy. This naturally motivates a more accurate and informative two-stage likelihood approximation combined with importance sampling correction, which leads to generalized RL algorithms with better sample efficiency and superior task performance. Second, we propose a new direction of joint training efficient samplers/controllers of dLLMs policy. Via RL, we incentivize dLLMs' natural multi-token prediction capabilities by letting the model learn to adaptively allocate an inference threshold for each prompt. By jointly training the sampler, we yield better accuracies with lower number of function evaluations (NFEs) compared to training the model only, obtaining the best performance in improving the Pareto frontier of the inference-time compute of dLLMs. We showcase the effectiveness of our pipeline by training open source large diffusion language models over benchmark math and planning tasks.",
        "tags": [
            "Diffusion",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "261",
        "title": "MMDEW: Multipurpose Multiclass Density Estimation in the Wild",
        "author": [
            "Villanelle O'Reilly",
            "Jonathan Cox",
            "Georgios Leontidis",
            "Marc Hanheide",
            "Petra Bosilj",
            "James Brown"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02213",
        "abstract": "Density map estimation can be used to estimate object counts in dense and occluded scenes where discrete counting-by-detection methods fail. We propose a multicategory counting framework that leverages a Twins pyramid vision-transformer backbone and a specialised multi-class counting head built on a state-of-the-art multiscale decoding approach. A two-task design adds a segmentation-based Category Focus Module, suppressing inter-category cross-talk at training time. Training and evaluation on the VisDrone and iSAID benchmarks demonstrates superior performance versus prior multicategory crowd-counting approaches (33%, 43% and 64% reduction to MAE), and the comparison with YOLOv11 underscores the necessity of crowd counting methods in dense scenes. The method's regional loss opens up multi-class crowd counting to new domains, demonstrated through the application to a biodiversity monitoring dataset, highlighting its capacity to inform conservation efforts and enable scalable ecological insights.",
        "tags": [
            "Detection",
            "Segmentation",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "262",
        "title": "Diffusion Transformers for Imputation: Statistical Efficiency and Uncertainty Quantification",
        "author": [
            "Zeqi Ye",
            "Minshuo Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02216",
        "abstract": "Imputation methods play a critical role in enhancing the quality of practical time-series data, which often suffer from pervasive missing values. Recently, diffusion-based generative imputation methods have demonstrated remarkable success compared to autoregressive and conventional statistical approaches. Despite their empirical success, the theoretical understanding of how well diffusion-based models capture complex spatial and temporal dependencies between the missing values and observed ones remains limited. Our work addresses this gap by investigating the statistical efficiency of conditional diffusion transformers for imputation and quantifying the uncertainty in missing values. Specifically, we derive statistical sample complexity bounds based on a novel approximation theory for conditional score functions using transformers, and, through this, construct tight confidence regions for missing values. Our findings also reveal that the efficiency and accuracy of imputation are significantly influenced by the missing patterns. Furthermore, we validate these theoretical insights through simulation and propose a mixed-masking training strategy to enhance the imputation performance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "263",
        "title": "Contrastive Retrieval Heads Improve Attention-Based Re-Ranking",
        "author": [
            "Linh Tran",
            "Yulong Li",
            "Radu Florian",
            "Wei Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02219",
        "abstract": "The strong zero-shot and long-context capabilities of recent Large Language Models (LLMs) have paved the way for highly effective re-ranking systems. Attention-based re-rankers leverage attention weights from transformer heads to produce relevance scores, but not all heads are created equally: many contribute noise and redundancy, thus limiting performance. To address this, we introduce CoRe heads, a small set of retrieval heads identified via a contrastive scoring metric that explicitly rewards high attention heads that correlate with relevant documents, while downplaying nodes with higher attention that correlate with irrelevant documents. This relative ranking criterion isolates the most discriminative heads for re-ranking and yields a state-of-the-art list-wise re-ranker. Extensive experiments with three LLMs show that aggregated signals from CoRe heads, constituting less than 1% of all heads, substantially improve re-ranking accuracy over strong baselines. We further find that CoRe heads are concentrated in middle layers, and pruning the computation of final 50% of model layers preserves accuracy while significantly reducing inference time and memory usage.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "264",
        "title": "TempoControl: Temporal Attention Guidance for Text-to-Video Models",
        "author": [
            "Shira Schiber",
            "Ofir Lindenbaum",
            "Idan Schwartz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02226",
        "abstract": "Recent advances in generative video models have enabled the creation of high-quality videos based on natural language prompts. However, these models frequently lack fine-grained temporal control, meaning they do not allow users to specify when particular visual elements should appear within a generated sequence. In this work, we introduce TempoControl, a method that allows for temporal alignment of visual concepts during inference, without requiring retraining or additional supervision. TempoControl utilizes cross-attention maps, a key component of text-to-video diffusion models, to guide the timing of concepts through a novel optimization approach. Our method steers attention using three complementary principles: aligning its temporal shape with a control signal (via correlation), amplifying it where visibility is needed (via energy), and maintaining spatial focus (via entropy). TempoControl allows precise control over timing while ensuring high video quality and diversity. We demonstrate its effectiveness across various video generation applications, including temporal reordering for single and multiple objects, as well as action and audio-aligned generation.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "265",
        "title": "More Than One Teacher: Adaptive Multi-Guidance Policy Optimization for Diverse Exploration",
        "author": [
            "Xiaoyang Yuan",
            "Yujuan Ding",
            "Yi Bin",
            "Wenqi Shao",
            "Jinyu Cai",
            "Jingkuan Song",
            "Yang Yang",
            "Hengtao Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02227",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) is a promising paradigm for enhancing the reasoning ability in Large Language Models (LLMs). However, prevailing methods primarily rely on self-exploration or a single off-policy teacher to elicit long chain-of-thought (LongCoT) reasoning, which may introduce intrinsic model biases and restrict exploration, ultimately limiting reasoning diversity and performance. Drawing inspiration from multi-teacher strategies in knowledge distillation, we introduce Adaptive Multi-Guidance Policy Optimization (AMPO), a novel framework that adaptively leverages guidance from multiple proficient teacher models, but only when the on-policy model fails to generate correct solutions. This \"guidance-on-demand\" approach expands exploration while preserving the value of self-discovery. Moreover, AMPO incorporates a comprehension-based selection mechanism, prompting the student to learn from the reasoning paths that it is most likely to comprehend, thus balancing broad exploration with effective exploitation. Extensive experiments show AMPO substantially outperforms a strong baseline (GRPO), with a 4.3% improvement on mathematical reasoning tasks and 12.2% on out-of-distribution tasks, while significantly boosting Pass@k performance and enabling more diverse exploration. Notably, using four peer-sized teachers, our method achieves comparable results to approaches that leverage a single, more powerful teacher (e.g., DeepSeek-R1) with more data. These results demonstrate a more efficient and scalable path to superior reasoning and generalizability. Our code is available at https://github.com/SII-Enigma/AMPO.",
        "tags": [
            "CoT",
            "DeepSeek",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "266",
        "title": "xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity",
        "author": [
            "Maximilian Beck",
            "Kajetan Schweighofer",
            "Sebastian BÃ¶ck",
            "Sebastian Lehner",
            "Sepp Hochreiter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02228",
        "abstract": "Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Importantly, xLSTM's advantage widens as training and inference contexts grow.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "267",
        "title": "The Reasoning Boundary Paradox: How Reinforcement Learning Constrains Language Models",
        "author": [
            "Phuc Minh Nguyen",
            "Chinh D. La",
            "Duy M. H. Nguyen",
            "Nitesh V. Chawla",
            "Binh T. Nguyen",
            "Khoa D. Doan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02230",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a key method for improving Large Language Models' reasoning capabilities, yet recent evidence suggests it may paradoxically shrink the reasoning boundary rather than expand it. This paper investigates the shrinkage issue of RLVR by analyzing its learning dynamics and reveals two critical phenomena that explain this failure. First, we expose negative interference in RLVR, where learning to solve certain training problems actively reduces the likelihood of correct solutions for others, leading to the decline of Pass@$k$ performance, or the probability of generating a correct solution within $k$ attempts. Second, we uncover the winner-take-all phenomenon: RLVR disproportionately reinforces problems with high likelihood, correct solutions, under the base model, while suppressing other initially low-likelihood ones. Through extensive theoretical and empirical analysis on multiple mathematical reasoning benchmarks, we show that this effect arises from the inherent on-policy sampling in standard RL objectives, causing the model to converge toward narrow solution strategies. Based on these insights, we propose a simple yet effective data curation algorithm that focuses RLVR learning on low-likelihood problems, achieving notable improvement in Pass@$k$ performance. Our code is available at https://github.com/mail-research/SELF-llm-interference.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "268",
        "title": "Enhanced Arabic-language cyberbullying detection: deep embedding and transformer (BERT) approaches",
        "author": [
            "Ebtesam Jaber Aljohani",
            "Wael M. S. Yafoo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02232",
        "abstract": "Recent technological advances in smartphones and communications, including the growth of such online platforms as massive social media networks such as X (formerly known as Twitter) endangers young people and their emotional well-being by exposing them to cyberbullying, taunting, and bullying content. Most proposed approaches for automatically detecting cyberbullying have been developed around the English language, and methods for detecting Arabic-language cyberbullying are scarce. Methods for detecting Arabic-language cyberbullying are especially scarce. This paper aims to enhance the effectiveness of methods for detecting cyberbullying in Arabic-language content. We assembled a dataset of 10,662 X posts, pre-processed the data, and used the kappa tool to verify and enhance the quality of our annotations. We conducted four experiments to test numerous deep learning models for automatically detecting Arabic-language cyberbullying. We first tested a long short-term memory (LSTM) model and a bidirectional long short-term memory (Bi-LSTM) model with several experimental word embeddings. We also tested the LSTM and Bi-LSTM models with a novel pre-trained bidirectional encoder from representations (BERT) and then tested them on a different experimental models BERT again. LSTM-BERT and Bi-LSTM-BERT demonstrated a 97% accuracy. Bi-LSTM with FastText embedding word performed even better, achieving 98% accuracy. As a result, the outcomes are generalize",
        "tags": [
            "BERT",
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "269",
        "title": "RewardMap: Tackling Sparse Rewards in Fine-grained Visual Reasoning via Multi-Stage Reinforcement Learning",
        "author": [
            "Sicheng Feng",
            "Kaiwen Tuo",
            "Song Wang",
            "Lingdong Kong",
            "Jianke Zhu",
            "Huan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02240",
        "abstract": "Fine-grained visual reasoning remains a core challenge for multimodal large language models (MLLMs). The recently introduced ReasonMap highlights this gap by showing that even advanced MLLMs struggle with spatial reasoning in structured and information-rich settings such as transit maps, a task of clear practical and scientific importance. However, standard reinforcement learning (RL) on such tasks is impeded by sparse rewards and unstable optimization. To address this, we first construct ReasonMap-Plus, an extended dataset that introduces dense reward signals through Visual Question Answering (VQA) tasks, enabling effective cold-start training of fine-grained visual understanding skills. Next, we propose RewardMap, a multi-stage RL framework designed to improve both visual understanding and reasoning capabilities of MLLMs. RewardMap incorporates two key designs. First, we introduce a difficulty-aware reward design that incorporates detail rewards, directly tackling the sparse rewards while providing richer supervision. Second, we propose a multi-stage RL scheme that bootstraps training from simple perception to complex reasoning tasks, offering a more effective cold-start strategy than conventional Supervised Fine-Tuning (SFT). Experiments on ReasonMap and ReasonMap-Plus demonstrate that each component of RewardMap contributes to consistent performance gains, while their combination yields the best results. Moreover, models trained with RewardMap achieve an average improvement of 3.47% across 6 benchmarks spanning spatial reasoning, fine-grained visual reasoning, and general tasks beyond transit maps, underscoring enhanced visual understanding and reasoning capabilities.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "270",
        "title": "Study on LLMs for Promptagator-Style Dense Retriever Training",
        "author": [
            "Daniel Gwon",
            "Nour Jedidi",
            "Jimmy Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02241",
        "abstract": "Promptagator demonstrated that Large Language Models (LLMs) with few-shot prompts can be used as task-specific query generators for fine-tuning domain-specialized dense retrieval models. However, the original Promptagator approach relied on proprietary and large-scale LLMs which users may not have access to or may be prohibited from using with sensitive data. In this work, we study the impact of open-source LLMs at accessible scales ($\\leq$14B parameters) as an alternative. Our results demonstrate that open-source LLMs as small as 3B parameters can serve as effective Promptagator-style query generators. We hope our work will inform practitioners with reliable alternatives for synthetic data generation and give insights to maximize fine-tuning results for domain-specific applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "271",
        "title": "AccurateRAG: A Framework for Building Accurate Retrieval-Augmented Question-Answering Applications",
        "author": [
            "Linh The Nguyen",
            "Chi Tran",
            "Dung Ngoc Nguyen",
            "Van-Cuong Pham",
            "Hoang Ngo",
            "Dat Quoc Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02243",
        "abstract": "We introduce AccurateRAG -- a novel framework for constructing high-performance question-answering applications based on retrieval-augmented generation (RAG). Our framework offers a pipeline for development efficiency with tools for raw dataset processing, fine-tuning data generation, text embedding & LLM fine-tuning, output evaluation, and building RAG systems locally. Experimental results show that our framework outperforms previous strong baselines and obtains new state-of-the-art question-answering performance on benchmark datasets.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "272",
        "title": "ExGRPO: Learning to Reason from Experience",
        "author": [
            "Runzhe Zhan",
            "Yafu Li",
            "Zhi Wang",
            "Xiaoye Qu",
            "Dongrui Liu",
            "Jing Shao",
            "Derek F. Wong",
            "Yu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02245",
        "abstract": "Reinforcement learning from verifiable rewards (RLVR) is an emerging paradigm for improving the reasoning ability of large language models. However, standard on-policy training discards rollout experiences after a single update, leading to computational inefficiency and instability. While prior work on RL has highlighted the benefits of reusing past experience, the role of experience characteristics in shaping learning dynamics of large reasoning models remains underexplored. In this paper, we are the first to investigate what makes a reasoning experience valuable and identify rollout correctness and entropy as effective indicators of experience value. Based on these insights, we propose ExGRPO (Experiential Group Relative Policy Optimization), a framework that organizes and prioritizes valuable experiences, and employs a mixed-policy objective to balance exploration with experience exploitation. Experiments on five backbone models (1.5B-8B parameters) show that ExGRPO consistently improves reasoning performance on mathematical/general benchmarks, with an average gain of +3.5/7.6 points over on-policy RLVR. Moreover, ExGRPO stabilizes training on both stronger and weaker models where on-policy methods fail. These results highlight principled experience management as a key ingredient for efficient and scalable RLVR.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "273",
        "title": "Performance-Guided Refinement for Visual Aerial Navigation using Editable Gaussian Splatting in FalconGym 2.0",
        "author": [
            "Yan Miao",
            "Ege Yuceel",
            "Georgios Fainekos",
            "Bardh Hoxha",
            "Hideki Okamoto",
            "Sayan Mitra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02248",
        "abstract": "Visual policy design is crucial for aerial navigation. However, state-of-the-art visual policies often overfit to a single track and their performance degrades when track geometry changes. We develop FalconGym 2.0, a photorealistic simulation framework built on Gaussian Splatting (GSplat) with an Edit API that programmatically generates diverse static and dynamic tracks in milliseconds. Leveraging FalconGym 2.0's editability, we propose a Performance-Guided Refinement (PGR) algorithm, which concentrates visual policy's training on challenging tracks while iteratively improving its performance. Across two case studies (fixed-wing UAVs and quadrotors) with distinct dynamics and environments, we show that a single visual policy trained with PGR in FalconGym 2.0 outperforms state-of-the-art baselines in generalization and robustness: it generalizes to three unseen tracks with 100% success without per-track retraining and maintains higher success rates under gate-pose perturbations. Finally, we demonstrate that the visual policy trained with PGR in FalconGym 2.0 can be zero-shot sim-to-real transferred to a quadrotor hardware, achieving a 98.6% success rate (69 / 70 gates) over 30 trials spanning two three-gate tracks and a moving-gate track.",
        "tags": [
            "Gaussian Splatting"
        ]
    },
    {
        "id": "274",
        "title": "Explore Briefly, Then Decide: Mitigating LLM Overthinking via Cumulative Entropy Regulation",
        "author": [
            "Tianyi Jiang",
            "Yi Bin",
            "Yujuan Ding",
            "Kainian Zhu",
            "Fei Ma",
            "Jingkuan Song",
            "Heng Tao Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02249",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning abilities on complex problems using long Chain-of-Thought (CoT) reasoning. However, they often suffer from overthinking, meaning generating unnecessarily lengthy reasoning steps for simpler problems. This issue may degrade the efficiency of the models and make them difficult to adapt the reasoning depth to the complexity of problems. To address this, we introduce a novel metric Token Entropy Cumulative Average (TECA), which measures the extent of exploration throughout the reasoning process. We further propose a novel reasoning paradigm -- Explore Briefly, Then Decide -- with an associated Cumulative Entropy Regulation (CER) mechanism. This paradigm leverages TECA to help the model dynamically determine the optimal point to conclude its thought process and provide a final answer, thus achieving efficient reasoning. Experimental results across diverse mathematical benchmarks show that our approach substantially mitigates overthinking without sacrificing problem-solving ability. With our thinking paradigm, the average response length decreases by up to 71% on simpler datasets, demonstrating the effectiveness of our method in creating a more efficient and adaptive reasoning process.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "275",
        "title": "Retargeting Matters: General Motion Retargeting for Humanoid Motion Tracking",
        "author": [
            "Joao Pedro Araujo",
            "Yanjie Ze",
            "Pei Xu",
            "Jiajun Wu",
            "C. Karen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02252",
        "abstract": "Humanoid motion tracking policies are central to building teleoperation pipelines and hierarchical controllers, yet they face a fundamental challenge: the embodiment gap between humans and humanoid robots. Current approaches address this gap by retargeting human motion data to humanoid embodiments and then training reinforcement learning (RL) policies to imitate these reference trajectories. However, artifacts introduced during retargeting, such as foot sliding, self-penetration, and physically infeasible motion are often left in the reference trajectories for the RL policy to correct. While prior work has demonstrated motion tracking abilities, they often require extensive reward engineering and domain randomization to succeed. In this paper, we systematically evaluate how retargeting quality affects policy performance when excessive reward tuning is suppressed. To address issues that we identify with existing retargeting methods, we propose a new retargeting method, General Motion Retargeting (GMR). We evaluate GMR alongside two open-source retargeters, PHC and ProtoMotions, as well as with a high-quality closed-source dataset from Unitree. Using BeyondMimic for policy training, we isolate retargeting effects without reward tuning. Our experiments on a diverse subset of the LAFAN1 dataset reveal that while most motions can be tracked, artifacts in retargeted data significantly reduce policy robustness, particularly for dynamic or long sequences. GMR consistently outperforms existing open-source methods in both tracking performance and faithfulness to the source motion, achieving perceptual fidelity and policy success rates close to the closed-source baseline. Website: https://jaraujo98.github.io/retargeting_matters. Code: https://github.com/YanjieZe/GMR.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "276",
        "title": "DragFlow: Unleashing DiT Priors with Region Based Supervision for Drag Editing",
        "author": [
            "Zihan Zhou",
            "Shilin Lu",
            "Shuli Leng",
            "Shaocong Zhang",
            "Zhuming Lian",
            "Xinlei Yu",
            "Adams Wai-Kin Kong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02253",
        "abstract": "Drag-based image editing has long suffered from distortions in the target region, largely because the priors of earlier base models, Stable Diffusion, are insufficient to project optimized latents back onto the natural image manifold. With the shift from UNet-based DDPMs to more scalable DiT with flow matching (e.g., SD3.5, FLUX), generative priors have become significantly stronger, enabling advances across diverse editing tasks. However, drag-based editing has yet to benefit from these stronger priors. This work proposes the first framework to effectively harness FLUX's rich prior for drag-based editing, dubbed DragFlow, achieving substantial gains over baselines. We first show that directly applying point-based drag editing to DiTs performs poorly: unlike the highly compressed features of UNets, DiT features are insufficiently structured to provide reliable guidance for point-wise motion supervision. To overcome this limitation, DragFlow introduces a region-based editing paradigm, where affine transformations enable richer and more consistent feature supervision. Additionally, we integrate pretrained open-domain personalization adapters (e.g., IP-Adapter) to enhance subject consistency, while preserving background fidelity through gradient mask-based hard constraints. Multimodal large language models (MLLMs) are further employed to resolve task ambiguities. For evaluation, we curate a novel Region-based Dragging benchmark (ReD Bench) featuring region-level dragging instructions. Extensive experiments on DragBench-DR and ReD Bench show that DragFlow surpasses both point-based and region-based baselines, setting a new state-of-the-art in drag-based image editing. Code and datasets will be publicly available upon publication.",
        "tags": [
            "DiT",
            "Diffusion",
            "FLUX",
            "Flow Matching",
            "Image Editing",
            "LLM"
        ]
    },
    {
        "id": "277",
        "title": "From Frames to Clips: Efficient Key Clip Selection for Long-Form Video Understanding",
        "author": [
            "Guangyu Sun",
            "Archit Singhal",
            "Burak Uzkent",
            "Mubarak Shah",
            "Chen Chen",
            "Garin Kessler"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02262",
        "abstract": "Video Large Language Models (VLMs) have achieved remarkable results on a variety of vision language tasks, yet their practical use is limited by the \"needle in a haystack\" problem: the massive number of visual tokens produced from raw video frames exhausts the model's context window. Existing solutions alleviate this issue by selecting a sparse set of frames, thereby reducing token count, but such frame-wise selection discards essential temporal dynamics, leading to suboptimal reasoning about motion and event continuity. In this work we systematically explore the impact of temporal information and demonstrate that extending selection from isolated key frames to key clips, which are short, temporally coherent segments, improves video understanding. To maintain a fixed computational budget while accommodating the larger token footprint of clips, we propose an adaptive resolution strategy that dynamically balances spatial resolution and clip length, ensuring a constant token count per video. Experiments on three long-form video benchmarks demonstrate that our training-free approach, F2C, outperforms uniform sampling up to 8.1%, 5.6%, and 10.3% on Video-MME, LongVideoBench and MLVU benchmarks, respectively. These results highlight the importance of preserving temporal coherence in frame selection and provide a practical pathway for scaling Video LLMs to real world video understanding applications. Project webpage is available at https://guangyusun.com/f2c .",
        "tags": [
            "CLIP",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "278",
        "title": "RLAD: Training LLMs to Discover Abstractions for Solving Reasoning Problems",
        "author": [
            "Yuxiao Qu",
            "Anikait Singh",
            "Yoonho Lee",
            "Amrith Setlur",
            "Ruslan Salakhutdinov",
            "Chelsea Finn",
            "Aviral Kumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02263",
        "abstract": "Reasoning requires going beyond pattern matching or memorization of solutions to identify and implement \"algorithmic procedures\" that can be used to deduce answers to hard problems. Doing so requires realizing the most relevant primitives, intermediate results, or shared procedures, and building upon them. While RL post-training on long chains of thought ultimately aims to uncover this kind of algorithmic behavior, most reasoning traces learned by large models fail to consistently capture or reuse procedures, instead drifting into verbose and degenerate exploration. To address more effective reasoning, we introduce reasoning abstractions: concise natural language descriptions of procedural and factual knowledge that guide the model toward learning successful reasoning. We train models to be capable of proposing multiple abstractions given a problem, followed by RL that incentivizes building a solution while using the information provided by these abstractions. This results in a two-player RL training paradigm, abbreviated as RLAD, that jointly trains an abstraction generator and a solution generator. This setup effectively enables structured exploration, decouples learning signals of abstraction proposal and solution generation, and improves generalization to harder problems. We also show that allocating more test-time compute to generating abstractions is more beneficial for performance than generating more solutions at large test budgets, illustrating the role of abstractions in guiding meaningful exploration.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "279",
        "title": "How to Combat Reactive and Dynamic Jamming Attacks with Reinforcement Learning",
        "author": [
            "Yalin E. Sagduyu",
            "Tugba Erpek",
            "Kemal Davaslioglu",
            "Sastry Kompella"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02265",
        "abstract": "This paper studies the problem of mitigating reactive jamming, where a jammer adopts a dynamic policy of selecting channels and sensing thresholds to detect and jam ongoing transmissions. The transmitter-receiver pair learns to avoid jamming and optimize throughput over time (without prior knowledge of channel conditions or jamming strategies) by using reinforcement learning (RL) to adapt transmit power, modulation, and channel selection. Q-learning is employed for discrete jamming-event states, while Deep Q-Networks (DQN) are employed for continuous states based on received power. Through different reward functions and action sets, the results show that RL can adapt rapidly to spectrum dynamics and sustain high rates as channels and jamming policies change over time.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "280",
        "title": "Do You Know Where Your Camera Is? View-Invariant Policy Learning with Camera Conditioning",
        "author": [
            "Tianchong Jiang",
            "Jingtian Ji",
            "Xiangshan Tan",
            "Jiading Fang",
            "Anand Bhattad",
            "Vitor Guizilini",
            "Matthew R. Walter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02268",
        "abstract": "We study view-invariant imitation learning by explicitly conditioning policies on camera extrinsics. Using Plucker embeddings of per-pixel rays, we show that conditioning on extrinsics significantly improves generalization across viewpoints for standard behavior cloning policies, including ACT, Diffusion Policy, and SmolVLA. To evaluate policy robustness under realistic viewpoint shifts, we introduce six manipulation tasks in RoboSuite and ManiSkill that pair \"fixed\" and \"randomized\" scene variants, decoupling background cues from camera pose. Our analysis reveals that policies without extrinsics often infer camera pose using visual cues from static backgrounds in fixed scenes; this shortcut collapses when workspace geometry or camera placement shifts. Conditioning on extrinsics restores performance and yields robust RGB-only control without depth. We release the tasks, demonstrations, and code at https://ripl.github.io/know_your_camera/ .",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "281",
        "title": "microCLIP: Unsupervised CLIP Adaptation via Coarse-Fine Token Fusion for Fine-Grained Image Classification",
        "author": [
            "Sathira Silva",
            "Eman Ali",
            "Chetan Arora",
            "Muhammad Haris Khan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02270",
        "abstract": "Unsupervised adaptation of CLIP-based vision-language models (VLMs) for fine-grained image classification requires sensitivity to microscopic local cues. While CLIP exhibits strong zero-shot transfer, its reliance on coarse global features restricts its performance on fine-grained classification tasks. Prior efforts inject fine-grained knowledge by aligning large language model (LLM) descriptions with the CLIP $\\texttt{[CLS]}$ token; however, this approach overlooks spatial precision. We propose $\\textbf{microCLIP}$, a self-training framework that jointly refines CLIP's visual and textual representations using fine-grained cues. At its core is Saliency-Oriented Attention Pooling (SOAP) within a lightweight TokenFusion module, which builds a saliency-guided $\\texttt{[FG]}$ token from patch embeddings and fuses it with the global $\\texttt{[CLS]}$ token for coarse-fine alignment. To stabilize adaptation, we introduce a two-headed LLM-derived classifier: a frozen classifier that, via multi-view alignment, provides a stable text-based prior for pseudo-labeling, and a learnable classifier initialized from LLM descriptions and fine-tuned with TokenFusion. We further develop Dynamic Knowledge Aggregation, which convexly combines fixed LLM/CLIP priors with TokenFusion's evolving logits to iteratively refine pseudo-labels. Together, these components uncover latent fine-grained signals in CLIP, yielding a consistent $2.90\\%$ average accuracy gain across 13 fine-grained benchmarks while requiring only light adaptation. Our code is available at https://github.com/sathiiii/microCLIP.",
        "tags": [
            "CLIP",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "282",
        "title": "InfoMosaic-Bench: Evaluating Multi-Source Information Seeking in Tool-Augmented Agents",
        "author": [
            "Yaxin Du",
            "Yuanshuo Zhang",
            "Xiyuan Yang",
            "Yifan Zhou",
            "Cheng Wang",
            "Gongyi Zou",
            "Xianghe Pang",
            "Wenhao Wang",
            "Menglan Chen",
            "Shuo Tang",
            "Zhiyu Li",
            "Siheng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02271",
        "abstract": "Information seeking is a fundamental requirement for humans. However, existing LLM agents rely heavily on open-web search, which exposes two fundamental weaknesses: online content is noisy and unreliable, and many real-world tasks require precise, domain-specific knowledge unavailable from the web. The emergence of the Model Context Protocol (MCP) now allows agents to interface with thousands of specialized tools, seemingly resolving this limitation. Yet it remains unclear whether agents can effectively leverage such tools -- and more importantly, whether they can integrate them with general-purpose search to solve complex tasks. Therefore, we introduce InfoMosaic-Bench, the first benchmark dedicated to multi-source information seeking in tool-augmented agents. Covering six representative domains (medicine, finance, maps, video, web, and multi-domain integration), InfoMosaic-Bench requires agents to combine general-purpose search with domain-specific tools. Tasks are synthesized with InfoMosaic-Flow, a scalable pipeline that grounds task conditions in verified tool outputs, enforces cross-source dependencies, and filters out shortcut cases solvable by trivial lookup. This design guarantees both reliability and non-triviality. Experiments with 14 state-of-the-art LLM agents reveal three findings: (i) web information alone is insufficient, with GPT-5 achieving only 38.2% accuracy and 67.5% pass rate; (ii) domain tools provide selective but inconsistent benefits, improving some domains while degrading others; and (iii) 22.4% of failures arise from incorrect tool usage or selection, highlighting that current LLMs still struggle with even basic tool handling.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "283",
        "title": "Parallel Scaling Law: Unveiling Reasoning Generalization through A Cross-Linguistic Perspective",
        "author": [
            "Wen Yang",
            "Junhong Wu",
            "Chong Li",
            "Chengqing Zong",
            "Jiajun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02272",
        "abstract": "Recent advancements in Reinforcement Post-Training (RPT) have significantly enhanced the capabilities of Large Reasoning Models (LRMs), sparking increased interest in the generalization of RL-based reasoning. While existing work has primarily focused on investigating its generalization across tasks or modalities, this study proposes a novel cross-linguistic perspective to investigate reasoning generalization. This raises a crucial question: $\\textit{Does the reasoning capability achieved from English RPT effectively transfer to other languages?}$ We address this by systematically evaluating English-centric LRMs on multilingual reasoning benchmarks and introducing a metric to quantify cross-lingual transferability. Our findings reveal that cross-lingual transferability varies significantly across initial model, target language, and training paradigm. Through interventional studies, we find that models with stronger initial English capabilities tend to over-rely on English-specific patterns, leading to diminished cross-lingual generalization. To address this, we conduct a thorough parallel training study. Experimental results yield three key findings: $\\textbf{First-Parallel Leap}$, a substantial leap in performance when transitioning from monolingual to just a single parallel language, and a predictable $\\textbf{Parallel Scaling Law}$, revealing that cross-lingual reasoning transfer follows a power-law with the number of training parallel languages. Moreover, we identify the discrepancy between actual monolingual performance and the power-law prediction as $\\textbf{Monolingual Generalization Gap}$, indicating that English-centric LRMs fail to fully generalize across languages. Our study challenges the assumption that LRM reasoning mirrors human cognition, providing critical insights for the development of more language-agnostic LRMs.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "284",
        "title": "Diffusion^2: Turning 3D Environments into Radio Frequency Heatmaps",
        "author": [
            "Kyoungjun Park",
            "Yifan Yang",
            "Changhan Ge",
            "Lili Qiu",
            "Shiqi Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02274",
        "abstract": "Modeling radio frequency (RF) signal propagation is essential for understanding the environment, as RF signals offer valuable insights beyond the capabilities of RGB cameras, which are limited by the visible-light spectrum, lens coverage, and occlusions. It is also useful for supporting wireless diagnosis, deployment, and optimization. However, accurately predicting RF signals in complex environments remains a challenge due to interactions with obstacles such as absorption and reflection. We introduce Diffusion^2, a diffusion-based approach that uses 3D point clouds to model the propagation of RF signals across a wide range of frequencies, from Wi-Fi to millimeter waves. To effectively capture RF-related features from 3D data, we present the RF-3D Encoder, which encapsulates the complexities of 3D geometry along with signal-specific details. These features undergo multi-scale embedding to simulate the actual RF signal dissemination process. Our evaluation, based on synthetic and real-world measurements, demonstrates that Diffusion^2 accurately estimates the behavior of RF signals in various frequency bands and environmental conditions, with an error margin of just 1.9 dB and 27x faster than existing methods, marking a significant advancement in the field. Refer to https://rfvision-project.github.io/ for more information.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "285",
        "title": "Addressing Pitfalls in the Evaluation of Uncertainty Estimation Methods for Natural Language Generation",
        "author": [
            "Mykyta Ielanskyi",
            "Kajetan Schweighofer",
            "Lukas Aichberger",
            "Sepp Hochreiter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02279",
        "abstract": "Hallucinations are a common issue that undermine the reliability of large language models (LLMs). Recent studies have identified a specific subset of hallucinations, known as confabulations, which arise due to predictive uncertainty of LLMs. To detect confabulations, various methods for estimating predictive uncertainty in natural language generation (NLG) have been developed. These methods are typically evaluated by correlating uncertainty estimates with the correctness of generated text, with question-answering (QA) datasets serving as the standard benchmark. However, commonly used approximate correctness functions have substantial disagreement between each other and, consequently, in the ranking of the uncertainty estimation methods. This allows one to inflate the apparent performance of uncertainty estimation methods. We propose using several alternative risk indicators for risk correlation experiments that improve robustness of empirical assessment of UE algorithms for NLG. For QA tasks, we show that marginalizing over multiple LLM-as-a-judge variants leads to reducing the evaluation biases. Furthermore, we explore structured tasks as well as out of distribution and perturbation detection tasks which provide robust and controllable risk indicators. Finally, we propose to use an Elo rating of uncertainty estimation methods to give an objective summarization over extensive evaluation settings.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "286",
        "title": "VidGuard-R1: AI-Generated Video Detection and Explanation via Reasoning MLLMs and RL",
        "author": [
            "Kyoungjun Park",
            "Yifan Yang",
            "Juheon Yi",
            "Shicheng Zheng",
            "Yifei Shen",
            "Dongqi Han",
            "Caihua Shan",
            "Muhammad Muaz",
            "Lili Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02282",
        "abstract": "With the rapid advancement of AI-generated videos, there is an urgent need for effective detection tools to mitigate societal risks such as misinformation and reputational harm. In addition to accurate classification, it is essential that detection models provide interpretable explanations to ensure transparency for regulators and end users. To address these challenges, we introduce VidGuard-R1, the first video authenticity detector that fine-tunes a multi-modal large language model (MLLM) using group relative policy optimization (GRPO). Our model delivers both highly accurate judgments and insightful reasoning. We curate a challenging dataset of 140k real and AI-generated videos produced by state-of-the-art generation models, carefully designing the generation process to maximize discrimination difficulty. We then fine-tune Qwen-VL using GRPO with two specialized reward models that target temporal artifacts and generation complexity. Extensive experiments demonstrate that VidGuard-R1 achieves state-of-the-art zero-shot performance on existing benchmarks, with additional training pushing accuracy above 95%. Case studies further show that VidGuard-R1 produces precise and interpretable rationales behind its predictions. The code is publicly available at https://VidGuard-R1.github.io.",
        "tags": [
            "Detection",
            "GRPO",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": "287",
        "title": "Self-Forcing++: Towards Minute-Scale High-Quality Video Generation",
        "author": [
            "Justin Cui",
            "Jie Wu",
            "Ming Li",
            "Tao Yang",
            "Xiaojie Li",
            "Rui Wang",
            "Andrew Bai",
            "Yuanhao Ban",
            "Cho-Jui Hsieh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02283",
        "abstract": "Diffusion models have revolutionized image and video generation, achieving unprecedented visual quality. However, their reliance on transformer architectures incurs prohibitively high computational costs, particularly when extending generation to long videos. Recent work has explored autoregressive formulations for long video generation, typically by distilling from short-horizon bidirectional teachers. Nevertheless, given that teacher models cannot synthesize long videos, the extrapolation of student models beyond their training horizon often leads to pronounced quality degradation, arising from the compounding of errors within the continuous latent space. In this paper, we propose a simple yet effective approach to mitigate quality degradation in long-horizon video generation without requiring supervision from long-video teachers or retraining on long video datasets. Our approach centers on exploiting the rich knowledge of teacher models to provide guidance for the student model through sampled segments drawn from self-generated long videos. Our method maintains temporal consistency while scaling video length by up to 20x beyond teacher's capability, avoiding common issues such as over-exposure and error-accumulation without recomputing overlapping frames like previous methods. When scaling up the computation, our method shows the capability of generating videos up to 4 minutes and 15 seconds, equivalent to 99.9% of the maximum span supported by our base model's position embedding and more than 50x longer than that of our baseline model. Experiments on standard benchmarks and our proposed improved benchmark demonstrate that our approach substantially outperforms baseline methods in both fidelity and consistency. Our long-horizon videos demo can be found at https://self-forcing-plus-plus.github.io/",
        "tags": [
            "Diffusion",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "288",
        "title": "Learning to Generate Object Interactions with Physics-Guided Video Diffusion",
        "author": [
            "David Romero",
            "Ariana Bermudez",
            "Hao Li",
            "Fabio Pizzati",
            "Ivan Laptev"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02284",
        "abstract": "Recent models for video generation have achieved remarkable progress and are now deployed in film, social media production, and advertising. Beyond their creative potential, such models also hold promise as world simulators for robotics and embodied decision making. Despite strong advances, however, current approaches still struggle to generate physically plausible object interactions and lack physics-grounded control mechanisms. To address this limitation, we introduce KineMask, an approach for physics-guided video generation that enables realistic rigid body control, interactions, and effects. Given a single image and a specified object velocity, our method generates videos with inferred motions and future object interactions. We propose a two-stage training strategy that gradually removes future motion supervision via object masks. Using this strategy we train video diffusion models (VDMs) on synthetic scenes of simple interactions and demonstrate significant improvements of object interactions in real scenes. Furthermore, KineMask integrates low-level motion control with high-level textual conditioning via predictive scene descriptions, leading to effective support for synthesis of complex dynamical phenomena. Extensive experiments show that KineMask achieves strong improvements over recent models of comparable size. Ablation studies further highlight the complementary roles of low- and high-level conditioning in VDMs. Our code, model, and data will be made publicly available.",
        "tags": [
            "Diffusion",
            "Robotics",
            "Video Generation"
        ]
    },
    {
        "id": "289",
        "title": "MultiModal Action Conditioned Video Generation",
        "author": [
            "Yichen Li",
            "Antonio Torralba"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02287",
        "abstract": "Current video models fail as world model as they lack fine-graiend control. General-purpose household robots require real-time fine motor control to handle delicate tasks and urgent situations. In this work, we introduce fine-grained multimodal actions to capture such precise control. We consider senses of proprioception, kinesthesia, force haptics, and muscle activation. Such multimodal senses naturally enables fine-grained interactions that are difficult to simulate with text-conditioned generative models. To effectively simulate fine-grained multisensory actions, we develop a feature learning paradigm that aligns these modalities while preserving the unique information each modality provides. We further propose a regularization scheme to enhance causality of the action trajectory features in representing intricate interaction dynamics. Experiments show that incorporating multimodal senses improves simulation accuracy and reduces temporal drift. Extensive ablation studies and downstream applications demonstrate the effectiveness and practicality of our work.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "290",
        "title": "Test-Time Anchoring for Discrete Diffusion Posterior Sampling",
        "author": [
            "Litu Rout",
            "Andreas Lugmayr",
            "Yasamin Jafarian",
            "Srivatsan Varadharajan",
            "Constantine Caramanis",
            "Sanjay Shakkottai",
            "Ira Kemelmacher-Shlizerman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02291",
        "abstract": "We study the problem of posterior sampling using pretrained discrete diffusion foundation models, aiming to recover images from noisy measurements without retraining task-specific models. While diffusion models have achieved remarkable success in generative modeling, most advances rely on continuous Gaussian diffusion. In contrast, discrete diffusion offers a unified framework for jointly modeling categorical data such as text and images. Beyond unification, discrete diffusion provides faster inference, finer control, and principled training-free Bayesian inference, making it particularly well-suited for posterior sampling. However, existing approaches to discrete diffusion posterior sampling face severe challenges: derivative-free guidance yields sparse signals, continuous relaxations limit applicability, and split Gibbs samplers suffer from the curse of dimensionality. To overcome these limitations, we introduce Anchored Posterior Sampling (APS) for masked diffusion foundation models, built on two key innovations -- quantized expectation for gradient-like guidance in discrete embedding space, and anchored remasking for adaptive decoding. Our approach achieves state-of-the-art performance among discrete diffusion samplers across linear and nonlinear inverse problems on the standard benchmarks. We further demonstrate the benefits of our approach in training-free stylization and text-guided editing.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "291",
        "title": "From Behavioral Performance to Internal Competence: Interpreting Vision-Language Models with VLM-Lens",
        "author": [
            "Hala Sheta",
            "Eric Huang",
            "Shuyu Wu",
            "Ilia Alenabi",
            "Jiajun Hong",
            "Ryker Lin",
            "Ruoxi Ning",
            "Daniel Wei",
            "Jialin Yang",
            "Jiawei Zhou",
            "Ziqiao Ma",
            "Freda Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02292",
        "abstract": "We introduce VLM-Lens, a toolkit designed to enable systematic benchmarking, analysis, and interpretation of vision-language models (VLMs) by supporting the extraction of intermediate outputs from any layer during the forward pass of open-source VLMs. VLM-Lens provides a unified, YAML-configurable interface that abstracts away model-specific complexities and supports user-friendly operation across diverse VLMs. It currently supports 16 state-of-the-art base VLMs and their over 30 variants, and is extensible to accommodate new models without changing the core logic.\nThe toolkit integrates easily with various interpretability and analysis methods. We demonstrate its usage with two simple analytical experiments, revealing systematic differences in the hidden representations of VLMs across layers and target concepts. VLM-Lens is released as an open-sourced project to accelerate community efforts in understanding and improving VLMs.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "292",
        "title": "F2LLM Technical Report: Matching SOTA Embedding Performance with 6 Million Open-Source Data",
        "author": [
            "Ziyin Zhang",
            "Zihan Liao",
            "Hang Yu",
            "Peng Di",
            "Rui Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02294",
        "abstract": "We introduce F2LLM - Foundation to Feature Large Language Models, a suite of state-of-the-art embedding models in three sizes: 0.6B, 1.7B, and 4B. Unlike previous top-ranking embedding models that require massive contrastive pretraining, sophisticated training pipelines, and costly synthetic training data, F2LLM is directly finetuned from foundation models on 6 million query-document-negative tuples curated from open-source, non-synthetic datasets, striking a strong balance between training cost, model size, and embedding performance. On the MTEB English leaderboard, F2LLM-4B ranks 2nd among models with approximately 4B parameters and 7th overall, while F2LLM-1.7B ranks 1st among models in the 1B-2B size range. To facilitate future research in the field, we release the models, training dataset, and code, positioning F2LLM as a strong, reproducible, and budget-friendly baseline for future works.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "293",
        "title": "Continual Personalization for Diffusion Models",
        "author": [
            "Yu-Chien Liao",
            "Jr-Jen Chen",
            "Chi-Pin Huang",
            "Ci-Siang Lin",
            "Meng-Lin Wu",
            "Yu-Chiang Frank Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02296",
        "abstract": "Updating diffusion models in an incremental setting would be practical in real-world applications yet computationally challenging. We present a novel learning strategy of Concept Neuron Selection (CNS), a simple yet effective approach to perform personalization in a continual learning scheme. CNS uniquely identifies neurons in diffusion models that are closely related to the target concepts. In order to mitigate catastrophic forgetting problems while preserving zero-shot text-to-image generation ability, CNS finetunes concept neurons in an incremental manner and jointly preserves knowledge learned of previous concepts. Evaluation of real-world datasets demonstrates that CNS achieves state-of-the-art performance with minimal parameter adjustments, outperforming previous methods in both single and multi-concept personalization works. CNS also achieves fusion-free operation, reducing memory storage and processing time for continual personalization.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "294",
        "title": "ARMADA: Autonomous Online Failure Detection and Human Shared Control Empower Scalable Real-world Deployment and Adaptation",
        "author": [
            "Wenye Yu",
            "Jun Lv",
            "Zixi Ying",
            "Yang Jin",
            "Chuan Wen",
            "Cewu Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02298",
        "abstract": "Imitation learning has shown promise in learning from large-scale real-world datasets. However, pretrained policies usually perform poorly without sufficient in-domain data. Besides, human-collected demonstrations entail substantial labour and tend to encompass mixed-quality data and redundant information. As a workaround, human-in-the-loop systems gather domain-specific data for policy post-training, and exploit closed-loop policy feedback to offer informative guidance, but usually require full-time human surveillance during policy rollout. In this work, we devise ARMADA, a multi-robot deployment and adaptation system with human-in-the-loop shared control, featuring an autonomous online failure detection method named FLOAT. Thanks to FLOAT, ARMADA enables paralleled policy rollout and requests human intervention only when necessary, significantly reducing reliance on human supervision. Hence, ARMADA enables efficient acquisition of in-domain data, and leads to more scalable deployment and faster adaptation to new scenarios. We evaluate the performance of ARMADA on four real-world tasks. FLOAT achieves nearly 95% accuracy on average, surpassing prior state-of-the-art failure detection approaches by over 20%. Besides, ARMADA manifests more than 4$\\times$ increase in success rate and greater than 2$\\times$ reduction in human intervention rate over multiple rounds of policy rollout and post-training, compared to previous human-in-the-loop learning methods.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "295",
        "title": "Equilibrium Matching: Generative Modeling with Implicit Energy-Based Models",
        "author": [
            "Runqian Wang",
            "Yilun Du"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02300",
        "abstract": "We introduce Equilibrium Matching (EqM), a generative modeling framework built from an equilibrium dynamics perspective. EqM discards the non-equilibrium, time-conditional dynamics in traditional diffusion and flow-based generative models and instead learns the equilibrium gradient of an implicit energy landscape. Through this approach, we can adopt an optimization-based sampling process at inference time, where samples are obtained by gradient descent on the learned landscape with adjustable step sizes, adaptive optimizers, and adaptive compute. EqM surpasses the generation performance of diffusion/flow models empirically, achieving an FID of 1.90 on ImageNet 256$\\times$256. EqM is also theoretically justified to learn and sample from the data manifold. Beyond generation, EqM is a flexible framework that naturally handles tasks including partially noised image denoising, OOD detection, and image composition. By replacing time-conditional velocities with a unified equilibrium landscape, EqM offers a tighter bridge between flow and energy-based models and a simple route to optimization-driven inference.",
        "tags": [
            "Detection",
            "Diffusion",
            "Energy-Based Models"
        ]
    },
    {
        "id": "296",
        "title": "Knowledge Distillation Detection for Open-weights Models",
        "author": [
            "Qin Shi",
            "Amber Yijia Zheng",
            "Qifan Song",
            "Raymond A. Yeh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02302",
        "abstract": "We propose the task of knowledge distillation detection, which aims to determine whether a student model has been distilled from a given teacher, under a practical setting where only the student's weights and the teacher's API are available. This problem is motivated by growing concerns about model provenance and unauthorized replication through distillation. To address this task, we introduce a model-agnostic framework that combines data-free input synthesis and statistical score computation for detecting distillation. Our approach is applicable to both classification and generative models. Experiments on diverse architectures for image classification and text-to-image generation show that our method improves detection accuracy over the strongest baselines by 59.6% on CIFAR-10, 71.2% on ImageNet, and 20.0% for text-to-image generation. The code is available at https://github.com/shqii1j/distillation_detection.",
        "tags": [
            "Detection",
            "Text-to-Image"
        ]
    },
    {
        "id": "297",
        "title": "Diffusion Models and the Manifold Hypothesis: Log-Domain Smoothing is Geometry Adaptive",
        "author": [
            "Tyler Farghly",
            "Peter Potaptchik",
            "Samuel Howard",
            "George Deligiannidis",
            "Jakiw Pidstrigach"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02305",
        "abstract": "Diffusion models have achieved state-of-the-art performance, demonstrating remarkable generalisation capabilities across diverse domains. However, the mechanisms underpinning these strong capabilities remain only partially understood. A leading conjecture, based on the manifold hypothesis, attributes this success to their ability to adapt to low-dimensional geometric structure within the data. This work provides evidence for this conjecture, focusing on how such phenomena could result from the formulation of the learning problem through score matching. We inspect the role of implicit regularisation by investigating the effect of smoothing minimisers of the empirical score matching objective. Our theoretical and empirical results confirm that smoothing the score function -- or equivalently, smoothing in the log-density domain -- produces smoothing tangential to the data manifold. In addition, we show that the manifold along which the diffusion model generalises can be controlled by choosing an appropriate smoothing.",
        "tags": [
            "Diffusion",
            "Score Matching"
        ]
    },
    {
        "id": "298",
        "title": "Drawing Conclusions from Draws: Rethinking Preference Semantics in Arena-Style LLM Evaluation",
        "author": [
            "Raphael Tang",
            "Crystina Zhang",
            "Wenyan Li",
            "Carmen Lai",
            "Pontus Stenetorp",
            "Yao Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02306",
        "abstract": "In arena-style evaluation of large language models (LLMs), two LLMs respond to a user query, and the user chooses the winning response or deems the \"battle\" a draw, resulting in an adjustment to the ratings of both models. The prevailing approach for modeling these rating dynamics is to view battles as two-player game matches, as in chess, and apply the Elo rating system and its derivatives. In this paper, we critically examine this paradigm. Specifically, we question whether a draw genuinely means that the two models are equal and hence whether their ratings should be equalized. Instead, we conjecture that draws are more indicative of query difficulty: if the query is too easy, then both models are more likely to succeed equally. On three real-world arena datasets, we show that ignoring rating updates for draws yields a 1-3% relative increase in battle outcome prediction accuracy (which includes draws) for all four rating systems studied. Further analyses suggest that draws occur more for queries rated as very easy and those as highly objective, with risk ratios of 1.37 and 1.35, respectively. We recommend future rating systems to reconsider existing draw semantics and to account for query properties in rating updates.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "299",
        "title": "NoiseShift: Resolution-Aware Noise Recalibration for Better Low-Resolution Image Generation",
        "author": [
            "Ruozhen He",
            "Moayed Haji-Ali",
            "Ziyan Yang",
            "Vicente Ordonez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02307",
        "abstract": "Text-to-image diffusion models trained on a fixed set of resolutions often fail to generalize, even when asked to generate images at lower resolutions than those seen during training. High-resolution text-to-image generators are currently unable to easily offer an out-of-the-box budget-efficient alternative to their users who might not need high-resolution images. We identify a key technical insight in diffusion models that when addressed can help tackle this limitation: Noise schedulers have unequal perceptual effects across resolutions. The same level of noise removes disproportionately more signal from lower-resolution images than from high-resolution images, leading to a train-test mismatch. We propose NoiseShift, a training-free method that recalibrates the noise level of the denoiser conditioned on resolution size. NoiseShift requires no changes to model architecture or sampling schedule and is compatible with existing models. When applied to Stable Diffusion 3, Stable Diffusion 3.5, and Flux-Dev, quality at low resolutions is significantly improved. On LAION-COCO, NoiseShift improves SD3.5 by 15.89%, SD3 by 8.56%, and Flux-Dev by 2.44% in FID on average. On CelebA, NoiseShift improves SD3.5 by 10.36%, SD3 by 5.19%, and Flux-Dev by 3.02% in FID on average. These results demonstrate the effectiveness of NoiseShift in mitigating resolution-dependent artifacts and enhancing the quality of low-resolution image generation.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Text-to-Image"
        ]
    },
    {
        "id": "300",
        "title": "Inferring Dynamic Physical Properties from Video Foundation Models",
        "author": [
            "Guanqi Zhan",
            "Xianzheng Ma",
            "Weidi Xie",
            "Andrew Zisserman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02311",
        "abstract": "We study the task of predicting dynamic physical properties from videos. More specifically, we consider physical properties that require temporal information to be inferred: elasticity of a bouncing object, viscosity of a flowing liquid, and dynamic friction of an object sliding on a surface. To this end, we make the following contributions: (i) We collect a new video dataset for each physical property, consisting of synthetic training and testing splits, as well as a real split for real world evaluation. (ii) We explore three ways to infer the physical property from videos: (a) an oracle method where we supply the visual cues that intrinsically reflect the property using classical computer vision techniques; (b) a simple read out mechanism using a visual prompt and trainable prompt vector for cross-attention on pre-trained video generative and self-supervised models; and (c) prompt strategies for Multi-modal Large Language Models (MLLMs). (iii) We show that video foundation models trained in a generative or self-supervised manner achieve a similar performance, though behind that of the oracle, and MLLMs are currently inferior to the other models, though their performance can be improved through suitable prompting.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "301",
        "title": "KaVa: Latent Reasoning via Compressed KV-Cache Distillation",
        "author": [
            "Anna Kuzina",
            "Maciej Pioro",
            "Paul N. Whatmough",
            "Babak Ehteshami Bejnordi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02312",
        "abstract": "Large Language Models (LLMs) excel at multi-step reasoning problems with explicit chain-of-thought (CoT), but verbose traces incur significant computational costs and memory overhead, and often carry redundant, stylistic artifacts. Latent reasoning has emerged as an efficient alternative that internalizes the thought process, but it suffers from a critical lack of supervision, limiting its effectiveness on complex, natural-language reasoning traces. In this work, we propose KaVa, the first framework that bridges this gap by distilling knowledge directly from a compressed KV-cache of the teacher into a latent-reasoning student via self-distillation, leveraging the representational flexibility of continuous latent tokens to align stepwise KV trajectories. We show that the abstract, unstructured knowledge within compressed KV-cache, which lacks direct token correspondence, can serve as a rich supervisory signal for a latent reasoning student. Empirically, the approach consistently outperforms strong latent baselines, exhibits markedly smaller degradation from equation-only to natural-language traces, and scales to larger backbones while preserving efficiency. These results establish compressed KV-cache distillation as a scalable supervision signal for latent reasoning, combining the accuracy of CoT-trained teachers with the efficiency and deployability of latent inference.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "302",
        "title": "StealthAttack: Robust 3D Gaussian Splatting Poisoning via Density-Guided Illusions",
        "author": [
            "Bo-Hsu Ke",
            "You-Zhe Xie",
            "Yu-Lun Liu",
            "Wei-Chen Chiu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02314",
        "abstract": "3D scene representation methods like Neural Radiance Fields (NeRF) and 3D Gaussian Splatting (3DGS) have significantly advanced novel view synthesis. As these methods become prevalent, addressing their vulnerabilities becomes critical. We analyze 3DGS robustness against image-level poisoning attacks and propose a novel density-guided poisoning method. Our method strategically injects Gaussian points into low-density regions identified via Kernel Density Estimation (KDE), embedding viewpoint-dependent illusory objects clearly visible from poisoned views while minimally affecting innocent views. Additionally, we introduce an adaptive noise strategy to disrupt multi-view consistency, further enhancing attack effectiveness. We propose a KDE-based evaluation protocol to assess attack difficulty systematically, enabling objective benchmarking for future research. Extensive experiments demonstrate our method's superior performance compared to state-of-the-art techniques. Project page: https://hentci.github.io/stealthattack/",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF"
        ]
    },
    {
        "id": "303",
        "title": "Optimal Control Meets Flow Matching: A Principled Route to Multi-Subject Fidelity",
        "author": [
            "Eric Tillmann Bill",
            "Enis Simsar",
            "Thomas Hofmann"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02315",
        "abstract": "Text-to-image (T2I) models excel on single-entity prompts but struggle with multi-subject descriptions, often showing attribute leakage, identity entanglement, and subject omissions. We introduce the first theoretical framework with a principled, optimizable objective for steering sampling dynamics toward multi-subject fidelity. Viewing flow matching (FM) through stochastic optimal control (SOC), we formulate subject disentanglement as control over a trained FM sampler. This yields two architecture-agnostic algorithms: (i) a training-free test-time controller that perturbs the base velocity with a single-pass update, and (ii) Adjoint Matching, a lightweight fine-tuning rule that regresses a control network to a backward adjoint signal while preserving base-model capabilities. The same formulation unifies prior attention heuristics, extends to diffusion models via a flow-diffusion correspondence, and provides the first fine-tuning route explicitly designed for multi-subject fidelity. Empirically, on Stable Diffusion 3.5, FLUX, and Stable Diffusion XL, both algorithms consistently improve multi-subject alignment while maintaining base-model style. Test-time control runs efficiently on commodity GPUs, and fine-tuned controllers trained on limited prompts generalize to unseen ones. We further highlight FOCUS (Flow Optimal Control for Unentangled Subjects), which achieves state-of-the-art multi-subject fidelity across models.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Flow Matching",
            "Text-to-Image"
        ]
    },
    {
        "id": "304",
        "title": "Mamba Outpaces Reformer in Stock Prediction with Sentiments from Top Ten LLMs",
        "author": [
            "Lokesh Antony Kadiyala",
            "Amir Mirzaeinia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01203",
        "abstract": "The stock market is extremely difficult to predict in the short term due to high market volatility, changes caused by news, and the non-linear nature of the financial time series. This research proposes a novel framework for improving minute-level prediction accuracy using semantic sentiment scores from top ten different large language models (LLMs) combined with minute interval intraday stock price data. We systematically constructed a time-aligned dataset of AAPL news articles and 1-minute Apple Inc. (AAPL) stock prices for the dates of April 4 to May 2, 2025. The sentiment analysis was achieved using the DeepSeek-V3, GPT variants, LLaMA, Claude, Gemini, Qwen, and Mistral models through their APIs. Each article obtained sentiment scores from all ten LLMs, which were scaled to a [0, 1] range and combined with prices and technical indicators like RSI, ROC, and Bollinger Band Width. Two state-of-the-art such as Reformer and Mamba were trained separately on the dataset using the sentiment scores produced by each LLM as input. Hyper parameters were optimized by means of Optuna and were evaluated through a 3-day evaluation period. Reformer had mean squared error (MSE) or the evaluation metrics, and it should be noted that Mamba performed not only faster but also better than Reformer for every LLM across the 10 LLMs tested. Mamba performed best with LLaMA 3.3--70B, with the lowest error of 0.137. While Reformer could capture broader trends within the data, the model appeared to over smooth sudden changes by the LLMs. This study highlights the potential of integrating LLM-based semantic analysis paired with efficient temporal modeling to enhance real-time financial forecasting.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM",
            "LLaMA",
            "Mamba",
            "Qwen"
        ]
    },
    {
        "id": "305",
        "title": "Enhancing the development of Cherenkov Telescope Array control software with Large Language Models",
        "author": [
            "Dmitriy Kostunin",
            "Elisa Jones",
            "Vladimir Sotnikov",
            "Valery Sotnikov",
            "Sergo Golovachev",
            "Alexandre Strube"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01299",
        "abstract": "We develop AI agents based on instruction-finetuned large language models (LLMs) to assist in the engineering and operation of the Cherenkov Telescope Array Observatory (CTAO) Control and Data Acquisition Software (ACADA). These agents align with project-specific documentation and codebases, understand contextual information, interact with external APIs, and communicate with users in natural language. We present our progress in integrating these features into CTAO pipelines for operations and offline data analysis.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "306",
        "title": "Combining complex Langevin dynamics with score-based and energy-based diffusion models",
        "author": [
            "Gert Aarts",
            "Diaa E. Habibi",
            "Lingxiao Wang",
            "Kai Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01328",
        "abstract": "Theories with a sign problem due to a complex action or Boltzmann weight can sometimes be numerically solved using a stochastic process in the complexified configuration space. However, the probability distribution effectively sampled by this complex Langevin process is not known a priori and notoriously hard to understand. In generative AI, diffusion models can learn distributions, or their log derivatives, from data. We explore the ability of diffusion models to learn the distributions sampled by a complex Langevin process, comparing score-based and energy-based diffusion models, and speculate about possible applications.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "307",
        "title": "Continuously Augmented Discrete Diffusion model for Categorical Generative Modeling",
        "author": [
            "Huangjie Zheng",
            "Shansan Gong",
            "Ruixiang Zhang",
            "Tianrong Chen",
            "Jiatao Gu",
            "Mingyuan Zhou",
            "Navdeep Jaitly",
            "Yizhe Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01329",
        "abstract": "Standard discrete diffusion models treat all unobserved states identically by mapping them to an absorbing [MASK] token. This creates an 'information void' where semantic information that could be inferred from unmasked tokens is lost between denoising steps. We introduce Continuously Augmented Discrete Diffusion (CADD), a framework that augments the discrete state space with a paired diffusion in a continuous latent space. This yields graded, gradually corrupted states in which masked tokens are represented by noisy yet informative latent vectors rather than collapsed 'information voids'. At each reverse step, CADD may leverage the continuous latent as a semantic hint to guide discrete denoising. The design is clean and compatible with existing discrete diffusion training. At sampling time, the strength and choice of estimator for the continuous latent vector enables a controlled trade-off between mode-coverage (generating diverse outputs) and mode-seeking (generating contextually precise outputs) behaviors. Empirically, we demonstrate CADD improves generative quality over mask-based diffusion across text generation, image synthesis, and code modeling, with consistent gains on both qualitative and quantitative metrics against strong discrete baselines.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "308",
        "title": "DeMuon: A Decentralized Muon for Matrix Optimization over Graphs",
        "author": [
            "Chuan He",
            "Shuyi Ren",
            "Jingwei Mao",
            "Erik G. Larsson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01377",
        "abstract": "In this paper, we propose DeMuon, a method for decentralized matrix optimization over a given communication topology. DeMuon incorporates matrix orthogonalization via Newton-Schulz iterations-a technique inherited from its centralized predecessor, Muon-and employs gradient tracking to mitigate heterogeneity among local functions. Under heavy-tailed noise conditions and additional mild assumptions, we establish the iteration complexity of DeMuon for reaching an approximate stochastic stationary point. This complexity result matches the best-known complexity bounds of centralized algorithms in terms of dependence on the target tolerance. To the best of our knowledge, DeMuon is the first direct extension of Muon to decentralized optimization over graphs with provable complexity guarantees. We conduct preliminary numerical experiments on decentralized transformer pretraining over graphs with varying degrees of connectivity. Our numerical results demonstrate a clear margin of improvement of DeMuon over other popular decentralized algorithms across different network topologies.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "309",
        "title": "Financial Stability Implications of Generative AI: Taming the Animal Spirits",
        "author": [
            "Anne Lundgaard Hansen",
            "Seung Jung Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01451",
        "abstract": "This paper investigates the impact of the adoption of generative AI on financial stability. We conduct laboratory-style experiments using large language models to replicate classic studies on herd behavior in trading decisions. Our results show that AI agents make more rational decisions than humans, relying predominantly on private information over market trends. Increased reliance on AI-powered trading advice could therefore potentially lead to fewer asset price bubbles arising from animal spirits that trade by following the herd. However, exploring variations in the experimental settings reveals that AI agents can be induced to herd optimally when explicitly guided to make profit-maximizing decisions. While optimal herding improves market discipline, this behavior still carries potential implications for financial stability. In other experimental variations, we show that AI agents are not purely algorithmic, but have inherited some elements of human conditioning and bias.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "310",
        "title": "Aligning Video Models with Human Social Judgments via Behavior-Guided Fine-Tuning",
        "author": [
            "Kathy Garcia",
            "Leyla Isik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01502",
        "abstract": "Humans intuitively perceive complex social signals in visual scenes, yet it remains unclear whether state-of-the-art AI models encode the same similarity structure. We study (Q1) whether modern video and language models capture human-perceived similarity in social videos, and (Q2) how to instill this structure into models using human behavioral data. To address this, we introduce a new benchmark of over 49,000 odd-one-out similarity judgments on 250 three-second video clips of social interactions, and discover a modality gap: despite the task being visual, caption-based language embeddings align better with human similarity than any pretrained video model. We close this gap by fine-tuning a TimeSformer video model on these human judgments with our novel hybrid triplet-RSA objective using low-rank adaptation (LoRA), aligning pairwise distances to human similarity. This fine-tuning protocol yields significantly improved alignment with human perceptions on held-out videos in terms of both explained variance and odd-one-out triplet accuracy. Variance partitioning shows that the fine-tuned video model increases shared variance with language embeddings and explains additional unique variance not captured by the language model. Finally, we test transfer via linear probes and find that human-similarity fine-tuning strengthens the encoding of social-affective attributes (intimacy, valence, dominance, communication) relative to the pretrained baseline. Overall, our findings highlight a gap in pretrained video models' social recognition and demonstrate that behavior-guided fine-tuning shapes video representations toward human social perception.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "311",
        "title": "AI Foundation Model for Time Series with Innovations Representation",
        "author": [
            "Lang Tong",
            "Xinyi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01560",
        "abstract": "This paper introduces an Artificial Intelligence (AI) foundation model for time series in engineering applications, where causal operations are required for real-time monitoring and control. Since engineering time series are governed by physical, rather than linguistic, laws, large-language-model-based AI foundation models may be ineffective or inefficient. Building on the classical innovations representation theory of Wiener, Kallianpur, and Rosenblatt, we propose Time Series GPT (TS-GPT) -- an innovations-representation-based Generative Pre-trained Transformer for engineering monitoring and control. As an example of foundation model adaptation, we consider Probabilistic Generative Forecasting, which produces future time series samples from conditional probability distributions given past realizations. We demonstrate the effectiveness of TS-GPT in forecasting real-time locational marginal prices using historical data from U.S. independent system operators.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "312",
        "title": "Towards Photonic Band Diagram Generation with Transformer-Latent Diffusion Models",
        "author": [
            "Valentin Delchevalerie",
            "Nicolas Roy",
            "Arnaud Bougaham",
            "Alexandre Mayer",
            "BenoÃ®t FrÃ©nay",
            "MichaÃ«l Lobet"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01749",
        "abstract": "Photonic crystals enable fine control over light propagation at the nanoscale, and thus play a central role in the development of photonic and quantum technologies. Photonic band diagrams (BDs) are a key tool to investigate light propagation into such inhomogeneous structured materials. However, computing BDs requires solving Maxwell's equations across many configurations, making it numerically expensive, especially when embedded in optimization loops for inverse design techniques, for example. To address this challenge, we introduce the first approach for BD generation based on diffusion models, with the capacity to later generalize and scale to arbitrary three dimensional structures. Our method couples a transformer encoder, which extracts contextual embeddings from the input structure, with a latent diffusion model to generate the corresponding BD. In addition, we provide insights into why transformers and diffusion models are well suited to capture the complex interference and scattering phenomena inherent to photonics, paving the way for new surrogate modeling strategies in this domain.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "313",
        "title": "NGGAN: Noise Generation GAN Based on the Practical Measurement Dataset for Narrowband Powerline Communications",
        "author": [
            "Ying-Ren Chien",
            "Po-Heng Chou",
            "You-Jie Peng",
            "Chun-Yuan Huang",
            "Hen-Wai Tsao",
            "Yu Tsao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01850",
        "abstract": "Capturing comprehensive statistics of nonperiodic asynchronous impulsive noise is a critical issue in enhancing impulse noise processing for narrowband powerline communication (NB-PLC) transceivers. However, existing mathematical noise generative models capture only some of the characteristics of additive noise. Therefore, we propose a generative adversarial network (GAN), called the noise-generation GAN (NGGAN), that learns the complicated characteristics of practically measured noise samples for data augmentation. To closely match the statistics of complicated noise in NB-PLC systems, we measured the NB-PLC noise via the analog coupling and bandpass filtering circuits of a commercial NB-PLC modem to build a realistic dataset. Specifically, the NGGAN design approaches based on the practically measured dataset are as follows: (i) we design the length of input signals that the NGGAN model can fit to facilitate cyclo-stationary noise generation. (ii) Wasserstein distance is used as a loss function to enhance the similarity between the generated noise and the training dataset and ensure that the sample diversity is sufficient for various applications. (iii) To measure the similarity performance of the GAN-based models based on mathematical and practically measured datasets, we perform quantitative and qualitative analyses. The training datasets include (1) a piecewise spectral cyclo-stationary Gaussian model (PSCGM), (2) a frequency-shift (FRESH) filter, and (3) practical measurements from NB-PLC systems. Simulation results demonstrate that the proposed NGGAN trained using waveform characteristics is closer to the practically measured dataset in terms of the quality of the generated noise.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "314",
        "title": "SLAP: Learning Speaker and Health-Related Representations from Natural Language Supervision",
        "author": [
            "Angelika Ando",
            "Auguste Crabeil",
            "Adrien Lesage",
            "Rachid Riad"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01860",
        "abstract": "Speech encodes paralinguistic information such as demographics, voice quality, and health. Yet no audio foundation model supports zero-shot or out-of-distribution (OOD) generalization to these tasks. We introduce SLAP (Speaker contrastive Language-Audio Pretraining), the first model aligning speech with natural language descriptions of speaker and health metadata through contrastive learning. SLAP combines a Vision Transformer audio encoder with text encoders, trained on more than 3400 hours across 9 datasets with diverse speaker annotations. We evaluated on 38 binary classification tasks spanning demographics, voice characteristics, and clinical assessments across 14 datasets in 7 languages. SLAP achieves 62.9% average F1 in zero-shot evaluation, a 48% relative improvement over CLAP (42.4%), while demonstrating strong OOD generalization to unseen languages and clinical populations. When fine-tuned with linear probing, SLAP reaches 69.3% F1 overall and achieves best-in-class performance on health tasks (57.9% F1), surpassing larger foundation models.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "315",
        "title": "Uniform-in-time convergence bounds for Persistent Contrastive Divergence Algorithms",
        "author": [
            "Paul Felix Valsecchi Oliva",
            "O. Deniz Akyildiz",
            "Andrew Duncan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01944",
        "abstract": "We propose a continuous-time formulation of persistent contrastive divergence (PCD) for maximum likelihood estimation (MLE) of unnormalised densities. Our approach expresses PCD as a coupled, multiscale system of stochastic differential equations (SDEs), which perform optimisation of the parameter and sampling of the associated parametrised density, simultaneously.\nFrom this novel formulation, we are able to derive explicit bounds for the error between the PCD iterates and the MLE solution for the model parameter. This is made possible by deriving uniform-in-time (UiT) bounds for the difference in moments between the multiscale system and the averaged regime. An efficient implementation of the continuous-time scheme is introduced, leveraging a class of explicit, stable intregators, stochastic orthogonal Runge-Kutta Chebyshev (S-ROCK), for which we provide explicit error estimates in the long-time regime. This leads to a novel method for training energy-based models (EBMs) with explicit error guarantees.",
        "tags": [
            "Energy-Based Models"
        ]
    },
    {
        "id": "316",
        "title": "LLM-Enhanced, Data-Driven Personalized and Equitable Clinician Scheduling: A Predict-then-Optimize Approach",
        "author": [
            "Anjali Jha",
            "Wanqing Chen",
            "Maxim Eckmann",
            "Ian Stockwell",
            "Jianwu Wang",
            "Kai Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02047",
        "abstract": "Clinician scheduling remains a persistent challenge due to limited clinical resources and fluctuating demands. This complexity is especially acute in large academic anesthesiology departments as physicians balance responsibilities across multiple clinical sites with conflicting priorities. Further, scheduling must account for individual clinical and lifestyle preferences to ensure job satisfaction and well-being. Traditional approaches, often based on statistical or rule-based optimization models, rely on structured data and explicit domain knowledge. However, these methods often overlook unstructured information, e.g., free-text notes from routinely administered clinician well-being surveys and scheduling platforms. These notes may reveal implicit and underutilized clinical resources. Neglecting such information can lead to misaligned schedules, increased burnout, overlooked staffing flexibility, and suboptimal utilization of available resources. To address this gap, we propose a predict-then-optimize framework that integrates classification-based clinician availability predictions with a mixed-integer programming schedule optimization model. Large language models (LLMs) are employed to extract actionable preferences and implicit constraints from unstructured schedule notes, enhancing the reliability of availability predictions. These predictions then inform the schedule optimization considering four objectives: first, ensuring clinical full-time equivalent compliance, second, reducing workload imbalances by enforcing equitable proportions of shift types, third, maximizing clinician availability for assigned shifts, and fourth, schedule consistency. By combining the interpretive power of LLMs with the rigor of mathematical optimization, our framework provides a robust, data-driven solution that enhances operational efficiency while supporting equity and clinician well-being.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "317",
        "title": "Uncovering Semantic Selectivity of Latent Groups in Higher Visual Cortex with Mutual Information-Guided Diffusion",
        "author": [
            "Yule Wang",
            "Joseph Yu",
            "Chengrui Li",
            "Weihan Li",
            "Anqi Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02182",
        "abstract": "Understanding how neural populations in higher visual areas encode object-centered visual information remains a central challenge in computational neuroscience. Prior works have investigated representational alignment between artificial neural networks and the visual cortex. Nevertheless, these findings are indirect and offer limited insights to the structure of neural populations themselves. Similarly, decoding-based methods have quantified semantic features from neural populations but have not uncovered their underlying organizations. This leaves open a scientific question: \"how feature-specific visual information is distributed across neural populations in higher visual areas, and whether it is organized into structured, semantically meaningful subspaces.\" To tackle this problem, we present MIG-Vis, a method that leverages the generative power of diffusion models to visualize and validate the visual-semantic attributes encoded in neural latent subspaces. Our method first uses a variational autoencoder to infer a group-wise disentangled neural latent subspace from neural populations. Subsequently, we propose a mutual information (MI)-guided diffusion synthesis procedure to visualize the specific visual-semantic features encoded by each latent group. We validate MIG-Vis on multi-session neural spiking datasets from the inferior temporal (IT) cortex of two macaques. The synthesized results demonstrate that our method identifies neural latent groups with clear semantic selectivity to diverse visual features, including object pose, inter-category transformations, and intra-class content. These findings provide direct, interpretable evidence of structured semantic representation in the higher visual cortex and advance our understanding of its encoding principles.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "318",
        "title": "Measurement-Guided Consistency Model Sampling for Inverse Problems",
        "author": [
            "Amirreza Tanevardi",
            "Pooria Abbas Rad Moghadam",
            "Sajjad Amini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.02208",
        "abstract": "Diffusion models have become powerful generative priors for solving inverse imaging problems, but their reliance on slow multi-step sampling limits practical deployment. Consistency models address this bottleneck by enabling high-quality generation in a single or only a few steps, yet their direct adaptation to inverse problems is underexplored. In this paper, we present a modified consistency sampling approach tailored for inverse problem reconstruction: the sampler's stochasticity is guided by a measurement-consistency mechanism tied to the measurement operator, which enforces fidelity to the acquired measurements while retaining the efficiency of consistency-based generation. Experiments on Fashion-MNIST and LSUN Bedroom datasets demonstrate consistent improvements in perceptual and pixel-level metrics, including FrÃ©chet Inception Distance, Kernel Inception Distance, peak signal-to-noise ratio, and structural similarity index measure, compared to baseline consistency sampling, yielding competitive or superior reconstructions with only a handful of steps.",
        "tags": [
            "Consistency Models",
            "Diffusion"
        ]
    }
]