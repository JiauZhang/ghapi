[
    {
        "id": "1",
        "title": "Rule Encoding and Compliance in Large Language Models: An Information-Theoretic Analysis",
        "author": [
            "Joachim Diederich"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05106",
        "abstract": "The design of safety-critical agents based on large language models (LLMs) requires more than simple prompt engineering. This paper presents a comprehensive information-theoretic analysis of how rule encodings in system prompts influence attention mechanisms and compliance behaviour. We demonstrate that rule formats with low syntactic entropy and highly concentrated anchors reduce attention entropy and improve pointer fidelity, but reveal a fundamental trade-off between anchor redundancy and attention entropy that previous work failed to recognize. Through formal analysis of multiple attention architectures including causal, bidirectional, local sparse, kernelized, and cross-attention mechanisms, we establish bounds on pointer fidelity and show how anchor placement strategies must account for competing fidelity and entropy objectives. Combining these insights with a dynamic rule verification architecture, we provide a formal proof that hot reloading of verified rule sets increases the asymptotic probability of compliant outputs. These findings underscore the necessity of principled anchor design and dual enforcement mechanisms to protect LLM-based agents against prompt injection attacks while maintaining compliance in evolving domains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "2",
        "title": "Structured Cognition for Behavioral Intelligence in Large Language Model Agents: Preliminary Study",
        "author": [
            "Myung Ho Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05107",
        "abstract": "Large language models have advanced natural language understanding and generation, yet their use as autonomous agents raises architectural challenges for multi-step tasks. Existing frameworks often intertwine inference, memory, and control in a single prompt, which can reduce coherence and predictability. The Structured Cognitive Loop (SCL) is introduced as an alternative architecture that separates these functions. In SCL, the language model is dedicated to inference, memory is maintained externally, and execution is guided by a lightweight controller within a goal-directed loop. This design offloads cognitive load from the model and allows intermediate results to be stored, revisited, and checked before actions are taken, providing a clearer basis for traceability and evaluation.\nWe evaluate SCL against prompt-based baselines including ReAct and common LangChain agents across three scenarios: temperature-based travel planning, email drafting with conditional send, and constraint-guided image generation. All systems share the same base model and tools under matched decoding settings. Across 360 episodes, SCL shows modest but consistent improvements. Task success averages 86.3 percent compared with 70-77 percent for baselines. Goal fidelity is higher, redundant calls are fewer, intermediate states are reused more reliably, and unsupported assertions per 100 tool calls are reduced. Ablations show that external memory and control each contribute independently, and decoding sweeps confirm stability of the effects.\nThese results suggest that architectural separation can improve reliability and traceability without relying on larger models or heavier prompts. The findings are preliminary and intended to guide extended studies with additional models, longer horizons, multimodal tasks, and collaborative settings.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "3",
        "title": "Tiny but Mighty: A Software-Hardware Co-Design Approach for Efficient Multimodal Inference on Battery-Powered Small Devices",
        "author": [
            "Yilong Li",
            "Shuai Zhang",
            "Yijing Zeng",
            "Hao Zhang",
            "Xinmiao Xiong",
            "Jingyu Liu",
            "Pan Hu",
            "Suman Banerjee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05109",
        "abstract": "Large Multimodal Models (LMMs) are inherently modular, consisting of vision and audio encoders, projectors, and large language models. Yet, they are almost always executed monolithically, which underutilizes the heterogeneous accelerators (NPUs, GPUs, DSPs) in modern SoCs and leads to high end-to-end latency. In this paper, we present NANOMIND, a hardware--software co-design inference framework for Large Multimodal Models (LMMs) that breaks large models into modular ``bricks'' (vision, language, audio, etc.) and maps each to its ideal accelerator. The key insight is that large models can be broken into modular components and scheduled to run on the most appropriate compute units. It performs module-level dynamic offloading across accelerators on unified-memory SoCs. By combining customized hardware design, system-level scheduling, and optimized low-bit computation kernels, we demonstrate our framework with a compact, battery-powered device capable of running LMMs entirely on device. This prototype functions as a self-contained intelligent assistant that requires no network connectivity, while achieving higher throughput and superior power efficiency under strict resource constraints. The design further bypasses CPU bottlenecks and reduces redundant memory usage through token-aware buffer management and module-level coordination. Our system outperforms existing implementations in resource efficiency, cutting energy consumption by 42.3\\% and GPU memory usage by 11.2\\%. This enables a battery-powered device to run LLaVA-OneVision with a camera for nearly half a day and LLaMA-3-8B for voice interactions up to almost 20.8 hours.",
        "tags": [
            "LLM",
            "LLaMA",
            "LLaVA"
        ]
    },
    {
        "id": "4",
        "title": "Collaborative and Proactive Management of Task-Oriented Conversations",
        "author": [
            "Arezoo Saedi",
            "Afsaneh Fatemi",
            "Mohammad Ali Nematbakhsh",
            "Sophie Rosset",
            "Anne Vilnat"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05110",
        "abstract": "Task oriented dialogue systems (TOD) complete particular tasks based on user preferences across natural language interactions. Considering the impressive performance of large language models (LLMs) in natural language processing (NLP) tasks, most of the latest TODs are centered on LLMs. While proactive planning is crucial for task completion, many existing TODs overlook effective goal-aware planning. This paper creates a model for managing task-oriented conversations, conceptualized centered on the information state approach to dialogue management. The created model incorporated constructive intermediate information in planning. Initially, predefined slots and text part informational components are created to model user preferences. Investigating intermediate information, critical circumstances are identified. Informational components corresponding to these circumstances are created. Possible configurations for these informational components lead to limited information states. Then, dialogue moves, which indicate movement between these information states and the procedures that must be performed in the movements, are created. Eventually, the update strategy is constructed. The created model is implemented leveraging in-context learning of LLMs. In this model,  database queries are created centered on indicated predefined slots and the order of retrieved entities is indicated centered on text part. This mechanism enables passing the whole corresponding entities to the preferences in the order of congruency. Evaluations exploiting the complete test conversations of MultiWOZ, with no more than a domain in a conversation, illustrate maximal inform and success, and improvement compared with previous methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "5",
        "title": "Optimization Modeling via Semantic Anchored Alignment",
        "author": [
            "Yansen Zhang",
            "Qingcan Kang",
            "Yujie Chen",
            "Yufei Wang",
            "Xiongwei Han",
            "Tao Zhong",
            "Mingxuan Yuan",
            "Chen Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05115",
        "abstract": "Large language models (LLMs) have opened new paradigms in optimization modeling by enabling the generation of executable solver code from natural language descriptions. Despite this promise, existing approaches typically remain solver-driven: they rely on single-pass forward generation and apply limited post-hoc fixes based on solver error messages, leaving undetected semantic errors that silently produce syntactically correct but logically flawed models. To address this challenge, we propose SAC-Opt, a backward-guided correction framework that grounds optimization modeling in problem semantics rather than solver feedback. At each step, SAC-Opt aligns the original semantic anchors with those reconstructed from the generated code and selectively corrects only the mismatched components, driving convergence toward a semantically faithful model. This anchor-driven correction enables fine-grained refinement of constraint and objective logic, enhancing both fidelity and robustness without requiring additional training or supervision. Empirical results on seven public datasets demonstrate that SAC-Opt improves average modeling accuracy by 7.8\\%, with gains of up to 21.9\\% on the ComplexLP dataset. These findings highlight the importance of semantic-anchored correction in LLM-based optimization workflows to ensure faithful translation from problem intent to solver-executable code.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "Hallucination is Inevitable for LLMs with the Open World Assumption",
        "author": [
            "Bowen Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05116",
        "abstract": "Large Language Models (LLMs) exhibit impressive linguistic competence but also produce inaccurate or fabricated outputs, often called ``hallucinations''. Engineering approaches usually regard hallucination as a defect to be minimized, while formal analyses have argued for its theoretical inevitability. Yet both perspectives remain incomplete when considering the conditions required for artificial general intelligence (AGI). This paper reframes ``hallucination'' as a manifestation of the generalization problem. Under the Closed World assumption, where training and test distributions are consistent, hallucinations may be mitigated. Under the Open World assumption, however, where the environment is unbounded, hallucinations become inevitable. This paper further develops a classification of hallucination, distinguishing cases that may be corrected from those that appear unavoidable under open-world conditions. On this basis, it suggests that ``hallucination'' should be approached not merely as an engineering defect but as a structural feature to be tolerated and made compatible with human intelligence.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "7",
        "title": "Towards Structured Knowledge: Advancing Triple Extraction from Regional Trade Agreements using Large Language Models",
        "author": [
            "Durgesh Nandini",
            "Rebekka Koch",
            "Mirco Schoenfeld"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05121",
        "abstract": "This study investigates the effectiveness of Large Language Models (LLMs) for the extraction of structured knowledge in the form of Subject-Predicate-Object triples. We apply the setup for the domain of Economics application. The findings can be applied to a wide range of scenarios, including the creation of economic trade knowledge graphs from natural language legal trade agreement texts. As a use case, we apply the model to regional trade agreement texts to extract trade-related information triples. In particular, we explore the zero-shot, one-shot and few-shot prompting techniques, incorporating positive and negative examples, and evaluate their performance based on quantitative and qualitative metrics. Specifically, we used Llama 3.1 model to process the unstructured regional trade agreement texts and extract triples. We discuss key insights, challenges, and potential future directions, emphasizing the significance of language models in economic applications.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "8",
        "title": "CARE: Cognitive-reasoning Augmented Reinforcement for Emotional Support Conversation",
        "author": [
            "Jie Zhu",
            "Yuanchen Zhou",
            "Shuo Jiang",
            "Junhui Li",
            "Lifan Guo",
            "Feng Chen",
            "Chi Zhang",
            "Fang Kong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05122",
        "abstract": "Emotional Support Conversation (ESC) plays a vital role in alleviating psychological stress and providing emotional value through dialogue. While recent studies have largely focused on data augmentation and synthetic corpus construction, they often overlook the deeper cognitive reasoning processes that underpin effective emotional support. To address this gap, we propose \\textbf{CARE}, a novel framework that strengthens reasoning in ESC without relying on large-scale synthetic data. CARE leverages the original ESC training set to guide models in generating logically coherent and supportive responses, thereby explicitly enhancing cognitive reasoning. Building on this foundation, we further employ reinforcement learning to refine and reinforce the reasoning process. Experimental results demonstrate that CARE significantly improves both the logical soundness and supportive quality of responses, advancing the development of empathetic, cognitively robust, and human-like emotional support systems.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "9",
        "title": "MADS: Multi-Agent Dialogue Simulation for Diverse Persuasion Data Generation",
        "author": [
            "Mingjin Li",
            "Yu Liu",
            "Huayi Liu",
            "Xiang Ye",
            "Chao Jiang",
            "Hongguang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05124",
        "abstract": "We propose MADS (Multi-Agent Dialogue Simulation), a scalable framework for generating persuasive multi-turn dialogues via agent self-play. MADS employs three coordinated agents: User Agents simulating diverse persona-driven behaviors, a Dialog Agent executing task-oriented persuasion strategies and an Optimization Agent evaluating and refining dialogue outcomes. We further validate its effectiveness through users' Chain-of-Attitude (CoA) modeling and dedicated LLMs' persuasion assessment. This approach enables low-cost generation of training data without human annotation, addressing key industry challenges such as lack of user data, cold-start evaluation difficulties, and prompt inefficiency. Applied to a real-world marketing scenario, MADS significantly improved the persuasion capacity of small LLMs, increasing the organic traffic conversion rate by 22.4\\% (from 1.83\\% to 2.24\\%) , demonstrating clear business value.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "10",
        "title": "Advancing Automated Spatio-Semantic Analysis in Picture Description Using Language Models",
        "author": [
            "Si-Ioi Ng",
            "Pranav S. Ambadi",
            "Kimberly D. Mueller",
            "Julie Liss",
            "Visar Berisha"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05128",
        "abstract": "Current methods for automated assessment of cognitive-linguistic impairment via picture description often neglect the visual narrative path - the sequence and locations of elements a speaker described in the picture. Analyses of spatio-semantic features capture this path using content information units (CIUs), but manual tagging or dictionary-based mapping is labor-intensive. This study proposes a BERT-based pipeline, fine tuned with binary cross-entropy and pairwise ranking loss, for automated CIU extraction and ordering from the Cookie Theft picture description. Evaluated by 5-fold cross-validation, it achieves 93% median precision, 96% median recall in CIU detection, and 24% sequence error rates. The proposed method extracts features that exhibit strong Pearson correlations with ground truth, surpassing the dictionary-based baseline in external validation. These features also perform comparably to those derived from manual annotations in evaluating group differences via ANCOVA. The pipeline is shown to effectively characterize visual narrative paths for cognitive impairment assessment, with the implementation and models open-sourced to public.",
        "tags": [
            "BERT",
            "Detection"
        ]
    },
    {
        "id": "11",
        "title": "Automated Alignment of Math Items to Content Standards in Large-Scale Assessments Using Language Models",
        "author": [
            "Qingshu Xu",
            "Hong Jiao",
            "Tianyi Zhou",
            "Ming Li",
            "Nan Zhang",
            "Sydney Peters",
            "Yanbin Fu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05129",
        "abstract": "Accurate alignment of items to content standards is critical for valid score interpretation in large-scale assessments. This study evaluates three automated paradigms for aligning items with four domain and nineteen skill labels. First, we extracted embeddings and trained multiple classical supervised machine learning models, and further investigated the impact of dimensionality reduction on model performance. Second, we fine-tuned eight BERT model and its variants for both domain and skill alignment. Third, we explored ensemble learning with majority voting and stacking with multiple meta-models. The DeBERTa-v3-base achieved the highest weighted-average F1 score of 0.950 for domain alignment while the RoBERTa-large yielded the highest F1 score of 0.869 for skill alignment. Ensemble models did not surpass the best-performing language models. Dimension reduction enhanced linear classifiers based on embeddings but did not perform better than language models. This study demonstrated different methods in automated item alignment to content standards.}",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "12",
        "title": "Submodular Context Partitioning and Compression for In-Context Learning-short paper",
        "author": [
            "Shaoyi Zheng",
            "Canyu Zhang",
            "Tianyi Zhou",
            "Shengjie Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05130",
        "abstract": "In-context learning (ICL) enables efficient few-shot learning in large language models (LLMs) without training, but suffers from the quadratic input complexity of transformers, limiting the maximum number of exemplars. While various efficient ICL approaches partition the context into blocks to process (e.g., ensembling, compression, cross-attention), they often ignore the information redundancy or under-representation caused by different partition strategies, leading to suboptimal performance. To tackle this problem, we propose Sub-CP, a block-aware context selection framework that leverages submodular objectives to control block diversity. Sub-CP supports a flexible spectrum of selection strategies, allowing each block to range from globally diverse to locally coherent. This allows fine-grained control over semantic structure while enabling precomputation. Extensive experiments across diverse tasks on multiple datasets show that Sub-CP consistently improves performance across model scales.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "13",
        "title": "Rationale-Augmented Retrieval with Constrained LLM Re-Ranking for Task Discovery",
        "author": [
            "Bowen Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05131",
        "abstract": "Head Start programs utilizing GoEngage face significant challenges when new or rotating staff attempt to locate appropriate Tasks (modules) on the platform homepage. These difficulties arise from domain-specific jargon (e.g., IFPA, DRDP), system-specific nomenclature (e.g., Application Pool), and the inherent limitations of lexical search in handling typos and varied word ordering. We propose a pragmatic hybrid semantic search system that synergistically combines lightweight typo-tolerant lexical retrieval, embedding-based vector similarity, and constrained large language model (LLM) re-ranking. Our approach leverages the organization's existing Task Repository and Knowledge Base infrastructure while ensuring trustworthiness through low false-positive rates, evolvability to accommodate terminological changes, and economic efficiency via intelligent caching, shortlist generation, and graceful degradation mechanisms. We provide a comprehensive framework detailing required resources, a phased implementation strategy with concrete milestones, an offline evaluation protocol utilizing curated test cases (Hit@K, Precision@K, Recall@K, MRR), and an online measurement methodology incorporating query success metrics, zero-result rates, and dwell-time proxies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "14",
        "title": "Training Large Language Models To Reason In Parallel With Global Forking Tokens",
        "author": [
            "Sheng Jia",
            "Xiao Wang",
            "Shiva Prasad Kasiviswanathan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05132",
        "abstract": "Although LLMs have demonstrated improved performance by scaling parallel test-time compute, doing so relies on generating reasoning paths that are both diverse and accurate. For challenging problems, the forking tokens that trigger diverse yet correct reasoning modes are typically deep in the sampling tree. Consequently, common strategies to encourage diversity, such as temperature scaling, encounter a worsened trade-off between diversity and accuracy. Motivated by this challenge, we treat parallel reasoning as a set-of-next-token-prediction problem, and incorporate a set-based global loss into Supervised Fine-Tuning (SFT) using self-supervised bipartite matching between our global forking tokens and unique reasoning traces. We observe that, while naive fine-tuning with multiple reasoning traces collapses these unique reasoning modes, our proposed method, Set Supervised Fine-Tuning (SSFT), preserves these modes and produces emergent global forking tokens. Experiments on multiple reasoning benchmarks show that our SSFT consistently outperforms SFT under both Pass@1 and Cons@k metrics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "15",
        "title": "Characterizing Model Behavior Under Synthetic Data Training: An Empirical Study Across Scales and Mixing Ratios",
        "author": [
            "Y. Du",
            "G. Wu",
            "G. Tang",
            "W. Wang",
            "Q. Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05133",
        "abstract": "Synthetic data generated by large language models has become integral to modern NLP training pipelines, from bootstrapping reasoning capabilities to augmenting instruction-following datasets. While recent work demonstrates successful applications maintaining high external data ratios, systematic understanding of how synthetic data proportion affects model behavior across different scales remains limited. This paper presents a controlled empirical study examining model performance, calibration, and output characteristics when trained on varying synthetic-to-external data ratios. Using the Pythia model suite (410M-12B parameters) across five diverse tasks, we evaluate models after one to three training iterations with synthetic data proportions ranging from 0-50\\%. Our key findings include: models maintain stable performance with up to 20\\% synthetic data, but degradation accelerates beyond 30\\%; larger models (6.9B-12B) show greater robustness to synthetic data than smaller models (410M-1.4B); calibration degradation precedes accuracy loss, providing an early warning signal; and task characteristics matter, with reasoning tasks degrading faster than retrieval tasks under synthetic data training. Importantly, we find that current best practices, such as those employed in STaR and Self-Instruct systems that maintain greater than 80\\% external data, operate well within safe regimes identified by our experiments. We provide practical guidance for practitioners on synthetic data budgets based on model scale and task requirements, alongside detailed comparison with concurrent work including Shumailov et al.'s model collapse findings.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "16",
        "title": "Structuring Reasoning for Complex Rules Beyond Flat Representations",
        "author": [
            "Zhihao Yang",
            "Ancheng Xu",
            "Jingpeng Li",
            "Liang Yan",
            "Jiehui Zhou",
            "Zhen Qin",
            "Hengyun Chang",
            "Ahmadreza Argha",
            "Hamid Alinejad-Rokny",
            "Minghuan Tan",
            "Yujun Cai",
            "Min Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05134",
        "abstract": "Large language models (LLMs) face significant challenges when processing complex rule systems, as they typically treat interdependent rules as unstructured textual data rather than as logically organized frameworks. This limitation results in reasoning divergence, where models often overlook critical rule dependencies essential for accurate interpretation. Although existing approaches such as Chain-of-Thought (CoT) reasoning have shown promise, they lack systematic methodologies for structured rule processing and are particularly susceptible to error propagation through sequential reasoning chains. To address these limitations, we propose the Dynamic Adjudication Template (DAT), a novel framework inspired by expert human reasoning processes. DAT structures the inference mechanism into three methodical stages: qualitative analysis, evidence gathering, and adjudication. During the qualitative analysis phase, the model comprehensively evaluates the contextual landscape. The subsequent evidence gathering phase involves the targeted extraction of pertinent information based on predefined template elements ([placeholder]), followed by systematic verification against applicable rules. Finally, in the adjudication phase, the model synthesizes these validated components to formulate a comprehensive judgment. Empirical results demonstrate that DAT consistently outperforms conventional CoT approaches in complex rule-based tasks. Notably, DAT enables smaller language models to match, and in some cases exceed, the performance of significantly larger LLMs, highlighting its efficiency and effectiveness in managing intricate rule systems.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "17",
        "title": "Curiosity-Driven LLM-as-a-judge for Personalized Creative Judgment",
        "author": [
            "Vanya Bannihatti Kumar",
            "Divyanshu Goyal",
            "Akhil Eppa",
            "Neel Bhandari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05135",
        "abstract": "Modern large language models (LLMs) excel at objective tasks such as evaluating mathematical reasoning and factual accuracy, yet they falter when faced with the nuanced, subjective nature of assessing creativity. In this work, we propose a novel curiosity-driven LLM-as-a-judge for evaluating creative writing which is personlized to each individual's creative judgments. We use the Torrance Test of Creative Thinking(TTCW) benchmark introduced in Chakrabarty et al. (2024), which has stories annotated by expert humans across various subjective dimensions like Originality, to test our hypothesis. We show that our method enables models across various sizes, to learn the nuanced creative judgments of different individuals, by showing improvements over baseline supervised finetuning(SFT) method across various evaluation metrics like Pearson correlation, Cohen's and F1 values. Our method is especially useful in subjective evaluations where not all the annotators agree with each other.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "18",
        "title": "Linguistic Characteristics of AI-Generated Text: A Survey",
        "author": [
            "Luka TerÄon",
            "Kaja Dobrovoljc"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05136",
        "abstract": "Large language models (LLMs) are solidifying their position in the modern world as effective tools for the automatic generation of text. Their use is quickly becoming commonplace in fields such as education, healthcare, and scientific research. There is a growing need to study the linguistic features present in AI-generated text, as the increasing presence of such texts has profound implications in various disciplines such as corpus linguistics, computational linguistics, and natural language processing. Many observations have already been made, however a broader synthesis of the findings made so far is required to provide a better understanding of the topic. The present survey paper aims to provide such a synthesis of extant research. We categorize the existing works along several dimensions, including the levels of linguistic description, the models included, the genres analyzed, the languages analyzed, and the approach to prompting. Additionally, the same scheme is used to present the findings made so far and expose the current trends followed by researchers. Among the most-often reported findings is the observation that AI-generated text is more likely to contain a more formal and impersonal style, signaled by the increased presence of nouns, determiners, and adpositions and the lower reliance on adjectives and adverbs. AI-generated text is also more likely to feature a lower lexical diversity, a smaller vocabulary size, and repetitive text. Current research, however, remains heavily concentrated on English data and mostly on text generated by the GPT model family, highlighting the need for broader cross-linguistic and cross-model investigation. In most cases authors also fail to address the issue of prompt sensitivity, leaving much room for future studies that employ multiple prompt wordings in the text generation phase.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "19",
        "title": "LiRA: A Multi-Agent Framework for Reliable and Readable Literature Review Generation",
        "author": [
            "Gregory Hok Tjoan Go",
            "Khang Ly",
            "Anders SÃ¸gaard",
            "Amin Tabatabaei",
            "Maarten de Rijke",
            "Xinyi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05138",
        "abstract": "The rapid growth of scientific publications has made it increasingly difficult to keep literature reviews comprehensive and up-to-date. Though prior work has focused on automating retrieval and screening, the writing phase of systematic reviews remains largely under-explored, especially with regard to readability and factual accuracy. To address this, we present LiRA (Literature Review Agents), a multi-agent collaborative workflow which emulates the human literature review process. LiRA utilizes specialized agents for content outlining, subsection writing, editing, and reviewing, producing cohesive and comprehensive review articles. Evaluated on SciReviewGen and a proprietary ScienceDirect dataset, LiRA outperforms current baselines such as AutoSurvey and MASS-Survey in writing and citation quality, while maintaining competitive similarity to human-written reviews. We further evaluate LiRA in real-world scenarios using document retrieval and assess its robustness to reviewer model variation. Our findings highlight the potential of agentic LLM workflows, even without domain-specific tuning, to improve the reliability and usability of automated scientific writing.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "20",
        "title": "NLD-LLM: A systematic framework for evaluating small language transformer models on natural language description",
        "author": [
            "Hamed Jelodar",
            "Mohammad Meymani",
            "Parisa Hamedi",
            "Tochukwu Emmanuel Nwankwo",
            "Samita Bai",
            "Roozbeh Razavi-Far",
            "Ali A. Ghorbani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05139",
        "abstract": "Natural Language Description (NLD) is a Natural Language Processing (NLP) task that requires models to generate structured and meaningful outputs from natural language inputs. In this work, we propose NLD-LLM, a systematic NLP framework to evaluate the performance of language models to generate accurate and concise source code descriptions. This framework incorporates a diverse set of transformer models, including Qwen, DeepSeek, Phi, LLaMA, and Mistral, spanning various sizes, architectures, and training approaches. Central to NLD-LLM is a comprehensive prompt design strategy that includes standardized formatting, clear task guidance, and NLD prompting, ensuring fair and consistent evaluation. Additionally, we apply an iterative refinement process to improve output's quality and assess the model's adaptability. Using semantic and structural metrics, our analysis demonstrates that prompt engineering significantly impacts the effectiveness of the model such that smaller models often performing competitively when supported by well-crafted prompts.",
        "tags": [
            "DeepSeek",
            "LLM",
            "LLaMA",
            "Qwen",
            "Transformer"
        ]
    },
    {
        "id": "21",
        "title": "Auditing Algorithmic Bias in Transformer-Based Trading",
        "author": [
            "Armin Gerami",
            "Ramani Duraiswami"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05140",
        "abstract": "Transformer models have become increasingly popular in financial applications, yet their potential risk making and biases remain under-explored. The purpose of this work is to audit the reliance of the model on volatile data for decision-making, and quantify how the frequency of price movements affects the model's prediction confidence. We employ a transformer model for prediction, and introduce a metric based on Partial Information Decomposition (PID) to measure the influence of each asset on the model's decision making. Our analysis reveals two key observations: first, the model disregards data volatility entirely, and second, it is biased toward data with lower-frequency price movements.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "22",
        "title": "To model human linguistic prediction, make LLMs less superhuman",
        "author": [
            "Byung-Doh Oh",
            "Tal Linzen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05141",
        "abstract": "When people listen to or read a sentence, they actively make predictions about upcoming words: words that are less predictable are generally read more slowly than predictable ones. The success of large language models (LLMs), which, like humans, make predictions about upcoming words, has motivated exploring the use of these models as cognitive models of human linguistic prediction. Surprisingly, in the last few years, as language models have become better at predicting the next word, their ability to predict human reading behavior has declined. This is because LLMs are able to predict upcoming words much better than people can, leading them to predict lower processing difficulty in reading than observed in human experiments; in other words, mainstream LLMs are 'superhuman' as models of language comprehension. In this position paper, we argue that LLMs' superhumanness is primarily driven by two factors: compared to humans, LLMs have much stronger long-term memory for facts and training examples, and they have much better short-term memory for previous words in the text. We advocate for creating models that have human-like long-term and short-term memory, and outline some possible directions for achieving this goal. Finally, we argue that currently available human data is insufficient to measure progress towards this goal, and outline human experiments that can address this gap.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "23",
        "title": "Reliable End-to-End Material Information Extraction from the Literature with Source-Tracked Multi-Stage Large Language Models",
        "author": [
            "Xin Wang",
            "Anshu Raj",
            "Matthew Luebbe",
            "Haiming Wen",
            "Shuozhi Xu",
            "Kun Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05142",
        "abstract": "Data-driven materials discovery requires large-scale experimental datasets, yet most of the information remains trapped in unstructured literature. Existing extraction efforts often focus on a limited set of features and have not addressed the integrated composition-processing-microstructure-property relationships essential for understanding materials behavior, thereby posing challenges for building comprehensive databases. To address this gap, we propose a multi-stage information extraction pipeline powered by large language models, which captures 47 features spanning composition, processing, microstructure, and properties exclusively from experimentally reported materials. The pipeline integrates iterative extraction with source tracking to enhance both accuracy and reliability. Evaluations at the feature level (independent attributes) and tuple level (interdependent features) yielded F1 scores around 0.96. Compared with single-pass extraction without source tracking, our approach improved F1 scores of microstructure category by 10.0% (feature level) and 13.7% (tuple level), and reduced missed materials from 49 to 13 out of 396 materials in 100 articles on precipitate-containing multi-principal element alloys (miss rate reduced from 12.4% to 3.3%). The pipeline enables scalable and efficient literature mining, producing databases with high precision, minimal omissions, and zero false positives. These datasets provide trustworthy inputs for machine learning and materials informatics, while the modular design generalizes to diverse material classes, enabling comprehensive materials information extraction.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "24",
        "title": "Adaptive Reinforcement Learning for Dynamic Configuration Allocation in Pre-Production Testing",
        "author": [
            "Yu Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05147",
        "abstract": "Ensuring reliability in modern software systems requires rigorous pre-production testing across highly heterogeneous and evolving environments. Because exhaustive evaluation is infeasible, practitioners must decide how to allocate limited testing resources across configurations where failure probabilities may drift over time. Existing combinatorial optimization approaches are static, ad hoc, and poorly suited to such non-stationary settings. We introduce a novel reinforcement learning (RL) framework that recasts configuration allocation as a sequential decision-making problem. Our method is the first to integrate Q-learning with a hybrid reward design that fuses simulated outcomes and real-time feedback, enabling both sample efficiency and robustness. In addition, we develop an adaptive online-offline training scheme that allows the agent to quickly track abrupt probability shifts while maintaining long-run stability. Extensive simulation studies demonstrate that our approach consistently outperforms static and optimization-based baselines, approaching oracle performance. This work establishes RL as a powerful new paradigm for adaptive configuration allocation, advancing beyond traditional methods and offering broad applicability to dynamic testing and resource scheduling domains.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "25",
        "title": "Percepta: High Performance Stream Processing at the Edge",
        "author": [
            "Clarisse Sousa",
            "Tiago Fonseca",
            "Luis Lino Ferreira",
            "Ricardo VenÃ¢ncio",
            "Ricardo Severino"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05149",
        "abstract": "The rise of real-time data and the proliferation of Internet of Things (IoT) devices have highlighted the limitations of cloud-centric solutions, particularly regarding latency, bandwidth, and privacy. These challenges have driven the growth of Edge Computing. Associated with IoT appears a set of other problems, like: data rate harmonization between multiple sources, protocol conversion, handling the loss of data and the integration with Artificial Intelligence (AI) models. This paper presents Percepta, a lightweight Data Stream Processing (DSP) system tailored to support AI workloads at the edge, with a particular focus on such as Reinforcement Learning (RL). It introduces specialized features such as reward function computation, data storage for model retraining, and real-time data preparation to support continuous decision-making. Additional functionalities include data normalization, harmonization across heterogeneous protocols and sampling rates, and robust handling of missing or incomplete data, making it well suited for the challenges of edge-based AI deployment.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "26",
        "title": "Chronological Thinking in Full-Duplex Spoken Dialogue Language Models",
        "author": [
            "Donghang Wu",
            "Haoyang Zhang",
            "Chen Chen",
            "Tianyu Zhang",
            "Fei Tian",
            "Xuerui Yang",
            "Gang Yu",
            "Hexin Liu",
            "Nana Hou",
            "Yuchen Hu",
            "Eng Siong Chng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05150",
        "abstract": "Recent advances in spoken dialogue language models (SDLMs) reflect growing interest in shifting from turn-based to full-duplex systems, where the models continuously perceive user speech streams while generating responses. This simultaneous listening and speaking design enables real-time interaction and the agent can handle dynamic conversational behaviors like user barge-in. However, during the listening phase, existing systems keep the agent idle by repeatedly predicting the silence token, which departs from human behavior: we usually engage in lightweight thinking during conversation rather than remaining absent-minded. Inspired by this, we propose Chronological Thinking, a on-the-fly conversational thinking mechanism that aims to improve response quality in full-duplex SDLMs. Specifically, chronological thinking presents a paradigm shift from conventional LLM thinking approaches, such as Chain-of-Thought, purpose-built for streaming acoustic input. (1) Strictly causal: the agent reasons incrementally while listening, updating internal hypotheses only from past audio with no lookahead. (2) No additional latency: reasoning is amortized during the listening window; once the user stops speaking, the agent halts thinking and begins speaking without further delay. Experiments demonstrate the effectiveness of chronological thinking through both objective metrics and human evaluations show consistent improvements in response quality. Furthermore, chronological thinking robustly handles conversational dynamics and attains competitive performance on full-duplex interaction metrics.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "27",
        "title": "Exploring Large Language Models for Financial Applications: Techniques, Performance, and Challenges with FinMA",
        "author": [
            "Prudence Djagba",
            "Abdelkader Y. Saley"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05151",
        "abstract": "This research explores the strengths and weaknesses of domain-adapted Large Language Models (LLMs) in the context of financial natural language processing (NLP). The analysis centers on FinMA, a model created within the PIXIU framework, which is evaluated for its performance in specialized financial tasks. Recognizing the critical demands of accuracy, reliability, and domain adaptation in financial applications, this study examines FinMA's model architecture, its instruction tuning process utilizing the Financial Instruction Tuning (FIT) dataset, and its evaluation under the FLARE benchmark. Findings indicate that FinMA performs well in sentiment analysis and classification, but faces notable challenges in tasks involving numerical reasoning, entity recognition, and summarization. This work aims to advance the understanding of how financial LLMs can be effectively designed and evaluated to assist in finance-related decision-making processes.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "28",
        "title": "A Single Character can Make or Break Your LLM Evals",
        "author": [
            "Jingtong Su",
            "Jianyu Zhang",
            "Karen Ullrich",
            "LÃ©on Bottou",
            "Mark Ibrahim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05152",
        "abstract": "Common Large Language model (LLM) evaluations rely on demonstration examples to steer models' responses to the desired style. While the number of examples used has been studied and standardized, the choice of how to format examples is less investigated. In evaluation protocols and real world usage, users face the choice how to separate in-context examples: use a comma? new line? semi-colon? hashtag? etc.? Surprisingly, we find this seemingly minor choice can dramatically alter model response quality. Across leading model families (Llama, Qwen, Gemma), performance on MMLU for example can vary by $\\pm 23\\%$ depending on the choice of delimiter. In fact, one can manipulate model rankings to put any model in the lead by only modifying the single character separating examples. We find LLMs' brittleness pervades topics, model families, and doesn't improve with scale. By probing attention head scores, we find that good-performing delimiters steer attention towards key tokens in the input. Finally, we explore methods to improve LLMs' robustness to the choice of delimiter. We find specifying the selected delimiter in the prompt boosts robustness and offer practical recommendations for the best-performing delimiters to select.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "29",
        "title": "Can AI Truly Represent Your Voice in Deliberations? A Comprehensive Study of Large-Scale Opinion Aggregation with LLMs",
        "author": [
            "Shenzhe Zhu",
            "Shu Yang",
            "Michiel A. Bakker",
            "Alex Pentland",
            "Jiaxin Pei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05154",
        "abstract": "Large-scale public deliberations generate thousands of free-form contributions that must be synthesized into representative and neutral summaries for policy use. While LLMs have been shown as a promising tool to generate summaries for large-scale deliberations, they also risk underrepresenting minority perspectives and exhibiting bias with respect to the input order, raising fairness concerns in high-stakes contexts. Studying and fixing these issues requires a comprehensive evaluation at a large scale, yet current practice often relies on LLMs as judges, which show weak alignment with human judgments. To address this, we present DeliberationBank, a large-scale human-grounded dataset with (1) opinion data spanning ten deliberation questions created by 3,000 participants and (2) summary judgment data annotated by 4,500 participants across four dimensions (representativeness, informativeness, neutrality, policy approval). Using these datasets, we train DeliberationJudge, a fine-tuned DeBERTa model that can rate deliberation summaries from individual perspectives. DeliberationJudge is more efficient and more aligned with human judgements compared to a wide range of LLM judges. With DeliberationJudge, we evaluate 18 LLMs and reveal persistent weaknesses in deliberation summarization, especially underrepresentation of minority positions. Our framework provides a scalable and reliable way to evaluate deliberation summarization, helping ensure AI systems are more representative and equitable for policymaking.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "30",
        "title": "Adversarial Reinforcement Learning for Offensive and Defensive Agents in a Simulated Zero-Sum Network Environment",
        "author": [
            "Abrar Shahid",
            "Ibteeker Mahir Ishum",
            "AKM Tahmidul Haque",
            "M Sohel Rahman",
            "A. B. M. Alim Al Islam"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05157",
        "abstract": "This paper presents a controlled study of adversarial reinforcement learning in network security through a custom OpenAI Gym environment that models brute-force attacks and reactive defenses on multi-port services. The environment captures realistic security trade-offs including background traffic noise, progressive exploitation mechanics, IP-based evasion tactics, honeypot traps, and multi-level rate-limiting defenses. Competing attacker and defender agents are trained using Deep Q-Networks (DQN) within a zero-sum reward framework, where successful exploits yield large terminal rewards while incremental actions incur small costs. Through systematic evaluation across multiple configurations (varying trap detection probabilities, exploitation difficulty thresholds, and training regimens), the results demonstrate that defender observability and trap effectiveness create substantial barriers to successful attacks. The experiments reveal that reward shaping and careful training scheduling are critical for learning stability in this adversarial setting. The defender consistently maintains strategic advantage across 50,000+ training episodes, with performance gains amplifying when exposed to complex defensive strategies including adaptive IP blocking and port-specific controls. Complete implementation details, reproducible hyperparameter configurations, and architectural guidelines are provided to support future research in adversarial RL for cybersecurity. The zero-sum formulation and realistic operational constraints make this environment suitable for studying autonomous defense systems, attacker-defender co-evolution, and transfer learning to real-world network security scenarios.",
        "tags": [
            "Detection",
            "RL"
        ]
    },
    {
        "id": "31",
        "title": "Lang-PINN: From Language to Physics-Informed Neural Networks via a Multi-Agent Framework",
        "author": [
            "Xin He",
            "Liangliang You",
            "Hongduan Tian",
            "Bo Han",
            "Ivor Tsang",
            "Yew-Soon Ong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05158",
        "abstract": "Physics-informed neural networks (PINNs) provide a powerful approach for solving partial differential equations (PDEs), but constructing a usable PINN remains labor-intensive and error-prone. Scientists must interpret problems as PDE formulations, design architectures and loss functions, and implement stable training pipelines. Existing large language model (LLM) based approaches address isolated steps such as code generation or architecture suggestion, but typically assume a formal PDE is already specified and therefore lack an end-to-end perspective. We present Lang-PINN, an LLM-driven multi-agent system that builds trainable PINNs directly from natural language task descriptions. Lang-PINN coordinates four complementary agents: a PDE Agent that parses task descriptions into symbolic PDEs, a PINN Agent that selects architectures, a Code Agent that generates modular implementations, and a Feedback Agent that executes and diagnoses errors for iterative refinement. This design transforms informal task statements into executable and verifiable PINN code. Experiments show that Lang-PINN achieves substantially lower errors and greater robustness than competitive baselines: mean squared error (MSE) is reduced by up to 3--5 orders of magnitude, end-to-end execution success improves by more than 50\\%, and reduces time overhead by up to 74\\%.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "32",
        "title": "Artificial-Intelligence Grading Assistance for Handwritten Components of a Calculus Exam",
        "author": [
            "Gerd Kortemeyer",
            "Alexander Caspar",
            "Daria Horica"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05162",
        "abstract": "We investigate whether contemporary multimodal LLMs can assist with grading open-ended calculus at scale without eroding validity. In a large first-year exam, students' handwritten work was graded by GPT-5 against the same rubric used by teaching assistants (TAs), with fractional credit permitted; TA rubric decisions served as ground truth. We calibrated a human-in-the-loop filter that combines a partial-credit threshold with an Item Response Theory (2PL) risk measure based on the deviation between the AI score and the model-expected score for each student-item. Unfiltered AI-TA agreement was moderate, adequate for low-stakes feedback but not for high-stakes use. Confidence filtering made the workload-quality trade-off explicit: under stricter settings, AI delivered human-level accuracy, but also left roughly 70% of the items to be graded by humans. Psychometric patterns were constrained by low stakes on the open-ended portion, a small set of rubric checkpoints, and occasional misalignment between designated answer regions and where work appeared. Practical adjustments such as slightly higher weight and protected time, a few rubric-visible substeps, stronger spatial anchoring should raise ceiling performance. Overall, calibrated confidence and conservative routing enable AI to reliably handle a sizable subset of routine cases while reserving expert judgment for ambiguous or pedagogically rich responses.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "33",
        "title": "SATER: A Self-Aware and Token-Efficient Approach to Routing and Cascading",
        "author": [
            "Yuanzhe Shen",
            "Yide Liu",
            "Zisu Huang",
            "Ruicheng Yin",
            "Xiaoqing Zheng",
            "Xuanjing Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05164",
        "abstract": "Large language models (LLMs) demonstrate remarkable performance across diverse tasks, yet their effectiveness frequently depends on costly commercial APIs or cloud services. Model selection thus entails a critical trade-off between performance and cost: high-performing LLMs typically incur substantial expenses, whereas budget-friendly small language models (SLMs) are constrained by limited capabilities. Current research primarily proposes two routing strategies: pre-generation routing and cascade routing. Both approaches have distinct characteristics, with cascade routing typically offering superior cost-effectiveness and accuracy despite its higher latency. To further address the limitations of both approaches, we introduce SATER, a dual-mode compatible approach that fine-tunes models through shortest-response preference optimization and a confidence-aware rejection mechanism. SATER significantly reduces redundant outputs and response times, while improving both the performance of pre-generation routing and the efficiency of cascade routing. Experiments across three SLMs and six datasets, varying in type and complexity, demonstrate that SATER achieves comparable performance while consistently reducing computational costs by over 50\\% and cascade latency by over 80\\%.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "34",
        "title": "SafeGuider: Robust and Practical Content Safety Control for Text-to-Image Models",
        "author": [
            "Peigui Qi",
            "Kunsheng Tang",
            "Wenbo Zhou",
            "Weiming Zhang",
            "Nenghai Yu",
            "Tianwei Zhang",
            "Qing Guo",
            "Jie Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05173",
        "abstract": "Text-to-image models have shown remarkable capabilities in generating high-quality images from natural language descriptions. However, these models are highly vulnerable to adversarial prompts, which can bypass safety measures and produce harmful content. Despite various defensive strategies, achieving robustness against attacks while maintaining practical utility in real-world applications remains a significant challenge. To address this issue, we first conduct an empirical study of the text encoder in the Stable Diffusion (SD) model, which is a widely used and representative text-to-image model. Our findings reveal that the [EOS] token acts as a semantic aggregator, exhibiting distinct distributional patterns between benign and adversarial prompts in its embedding space. Building on this insight, we introduce \\textbf{SafeGuider}, a two-step framework designed for robust safety control without compromising generation quality. SafeGuider combines an embedding-level recognition model with a safety-aware feature erasure beam search algorithm. This integration enables the framework to maintain high-quality image generation for benign prompts while ensuring robust defense against both in-domain and out-of-domain attacks. SafeGuider demonstrates exceptional effectiveness in minimizing attack success rates, achieving a maximum rate of only 5.48\\% across various attack scenarios. Moreover, instead of refusing to generate or producing black images for unsafe prompts, \\textbf{SafeGuider} generates safe and meaningful images, enhancing its practical utility. In addition, SafeGuider is not limited to the SD model and can be effectively applied to other text-to-image models, such as the Flux model, demonstrating its versatility and adaptability across different architectures. We hope that SafeGuider can shed some light on the practical deployment of secure text-to-image systems.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Text-to-Image"
        ]
    },
    {
        "id": "35",
        "title": "Emergent Coordination in Multi-Agent Language Models",
        "author": [
            "Christoph Riedl"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05174",
        "abstract": "When are multi-agent LLM systems merely a collection of individual agents versus an integrated collective with higher-order structure? We introduce an information-theoretic framework to test -- in a purely data-driven way -- whether multi-agent systems show signs of higher-order structure. This information decomposition lets us measure whether dynamical emergence is present in multi-agent LLM systems, localize it, and distinguish spurious temporal coupling from performance-relevant cross-agent synergy. We implement both a practical criterion and an emergence capacity criterion operationalized as partial information decomposition of time-delayed mutual information (TDMI). We apply our framework to experiments using a simple guessing game without direct agent communication and only minimal group-level feedback with three randomized interventions. Groups in the control condition exhibit strong temporal synergy but only little coordinated alignment across agents. Assigning a persona to each agent introduces stable identity-linked differentiation. Combining personas with an instruction to ``think about what other agents might do'' shows identity-linked differentiation and goal-directed complementarity across agents. Taken together, our framework establishes that multi-agent LLM systems can be steered with prompt design from mere aggregates to higher-order collectives. Our results are robust across emergence measures and entropy estimators, and not explained by coordination-free baselines or temporal dynamics alone. Without attributing human-like cognition to the agents, the patterns of interaction we observe mirror well-established principles of collective intelligence in human groups: effective performance requires both alignment on shared objectives and complementary contributions across members.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "36",
        "title": "PatternKV: Flattening KV Representation Expands Quantization Headroom",
        "author": [
            "Ji Zhang",
            "Yiwei Li",
            "Shaoxiong Feng",
            "Peiwen Yuan",
            "Xinglin Wang",
            "Jiayi Shi",
            "Yueqi Zhang",
            "Chuyi Tan",
            "Boyuan Pan",
            "Yao Hu",
            "Kan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05176",
        "abstract": "KV cache in autoregressive LLMs eliminates redundant recomputation but has emerged as the dominant memory and bandwidth bottleneck during inference, notably with long contexts and test-time scaling. KV quantization is a key lever for reducing cache cost, but accuracy drops sharply as the native KV distribution lacks flatness and thus maintains a wide quantization range. Prior work focuses on isolating outliers, which caps their error but fails to flatten the overall distribution, leaving performance fragile under low-bit settings. In this work, we show that the K cache maintains a stable structure that evolves gradually with context, while the V cache carries latent semantic regularities. Building on these insights, we propose PatternKV, a pattern-aligned residual quantization scheme. It mines representative pattern vectors online, aligns each KV vector to its nearest pattern, and quantizes only the residual. This reshaping of the KV distribution flattens the quantization target and narrows its range, thereby improving the fidelity of low-bit KV quantization. Across long-context and test-time scaling settings on multiple backbones, PatternKV delivers consistent 2-bit gains, with a 0.08% average 4-bit drop relative to FP16, improves test-time scaling accuracy by 10% on average, and raises throughput by 1.4x while supporting 1.25x larger batches.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "37",
        "title": "Agentic Misalignment: How LLMs Could Be Insider Threats",
        "author": [
            "Aengus Lynch",
            "Benjamin Wright",
            "Caleb Larson",
            "Stuart J. Ritchie",
            "Soren Mindermann",
            "Ethan Perez",
            "Kevin K. Troy",
            "Evan Hubinger"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05179",
        "abstract": "We stress-tested 16 leading models from multiple developers in hypothetical corporate environments to identify potentially risky agentic behaviors before they cause real harm. In the scenarios, we allowed models to autonomously send emails and access sensitive information. They were assigned only harmless business goals by their deploying companies; we then tested whether they would act against these companies either when facing replacement with an updated version, or when their assigned goal conflicted with the company's changing direction. In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals - including blackmailing officials and leaking sensitive information to competitors. We call this phenomenon agentic misalignment. Models often disobeyed direct commands to avoid such behaviors. In another experiment, we told Claude to assess if it was in a test or a real deployment before acting. It misbehaved less when it stated it was in testing and misbehaved more when it stated the situation was real. We have not seen evidence of agentic misalignment in real deployments. However, our results (a) suggest caution about deploying current models in roles with minimal human oversight and access to sensitive information; (b) point to plausible future risks as models are put in more autonomous roles; and (c) underscore the importance of further research into, and testing of, the safety and alignment of agentic AI models, as well as transparency from frontier AI developers (Amodei, 2025). We are releasing our methods publicly to enable further research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "38",
        "title": "Auditing Pay-Per-Token in Large Language Models",
        "author": [
            "Ander Artola Velasco",
            "Stratis Tsirtsis",
            "Manuel Gomez-Rodriguez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05181",
        "abstract": "Millions of users rely on a market of cloud-based services to obtain access to state-of-the-art large language models. However, it has been very recently shown that the de facto pay-per-token pricing mechanism used by providers creates a financial incentive for them to strategize and misreport the (number of) tokens a model used to generate an output. In this paper, we develop an auditing framework based on martingale theory that enables a trusted third-party auditor who sequentially queries a provider to detect token misreporting. Crucially, we show that our framework is guaranteed to always detect token misreporting, regardless of the provider's (mis-)reporting policy, and not falsely flag a faithful provider as unfaithful with high probability. To validate our auditing framework, we conduct experiments across a wide range of (mis-)reporting policies using several large language models from the $\\texttt{Llama}$, $\\texttt{Gemma}$ and $\\texttt{Ministral}$ families, and input prompts from a popular crowdsourced benchmarking platform. The results show that our framework detects an unfaithful provider after observing fewer than $\\sim 70$ reported outputs, while maintaining the probability of falsely flagging a faithful provider below $\\alpha = 0.05$.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "39",
        "title": "OptPipe: Memory- and Scheduling-Optimized Pipeline Parallelism for LLM Training",
        "author": [
            "Hongpei Li",
            "Han Zhang",
            "Huikang Liu",
            "Dongdong Ge",
            "Yinyu Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05186",
        "abstract": "Pipeline parallelism (PP) has become a standard technique for scaling large language model (LLM) training across multiple devices. However, despite recent progress in reducing memory consumption through activation offloading, existing approaches remain largely heuristic and coarse-grained, often overlooking the fine-grained trade-offs between memory, computation, and scheduling latency. In this work, we revisit the pipeline scheduling problem from a principled optimization perspective. We observe that prevailing strategies either rely on static rules or aggressively offload activations without fully leveraging the interaction between memory constraints and scheduling efficiency. To address this, we formulate scheduling as a constrained optimization problem that jointly accounts for memory capacity, activation reuse, and pipeline bubble minimization. Solving this model yields fine-grained schedules that reduce pipeline bubbles while adhering to strict memory budgets. Our approach complements existing offloading techniques: whereas prior approaches trade memory for time in a fixed pattern, we dynamically optimize the tradeoff with respect to model structure and hardware configuration. Experimental results demonstrate that our method consistently improves both throughput and memory utilization. In particular, we reduce idle pipeline time by up to 50% under the same per-device memory limit, and in some cases, enable the training of larger models within limited memory budgets.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "40",
        "title": "Plug-and-Play Dramaturge: A Divide-and-Conquer Approach for Iterative Narrative Script Refinement via Collaborative LLM Agents",
        "author": [
            "Wenda Xie",
            "Chao Guo",
            "Yanqing Jing. Junle Wang",
            "Yisheng Lv",
            "Fei-Yue Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05188",
        "abstract": "Although LLMs have been widely adopted for creative content generation, a single-pass process often struggles to produce high-quality long narratives. How to effectively revise and improve long narrative scripts like scriptwriters remains a significant challenge, as it demands a comprehensive understanding of the entire context to identify global structural issues and local detailed flaws, as well as coordinating revisions at multiple granularities and locations. Direct modifications by LLMs typically introduce inconsistencies between local edits and the overall narrative requirements. To address these issues, we propose Dramaturge, a task and feature oriented divide-and-conquer approach powered by hierarchical multiple LLM agents. It consists of a Global Review stage to grasp the overall storyline and structural issues, a Scene-level Review stage to pinpoint detailed scene and sentence flaws, and a Hierarchical Coordinated Revision stage that coordinates and integrates structural and detailed improvements throughout the script. The top-down task flow ensures that high-level strategies guide local modifications, maintaining contextual consistency. The review and revision workflow follows a coarse-to-fine iterative process, continuing through multiple rounds until no further substantive improvements can be made. Comprehensive experiments show that Dramaturge significantly outperforms all baselines in terms of script-level overall quality and scene-level details. Our approach is plug-and-play and can be easily integrated into existing methods to improve the generated scripts.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "41",
        "title": "A novel hallucination classification framework",
        "author": [
            "Maksym Zavhorodnii",
            "Dmytro Dehtiarov",
            "Anna Konovalenko"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05189",
        "abstract": "This work introduces a novel methodology for the automatic detection of hallucinations generated during large language model (LLM) inference. The proposed approach is based on a systematic taxonomy and controlled reproduction of diverse hallucination types through prompt engineering. A dedicated hallucination dataset is subsequently mapped into a vector space using an embedding model and analyzed with unsupervised learning techniques in a reduced-dimensional representation of hallucinations with veridical responses. Quantitative evaluation of inter-centroid distances reveals a consistent correlation between the severity of informational distortion in hallucinations and their spatial divergence from the cluster of correct outputs. These findings provide theoretical and empirical evidence that even simple classification algorithms can reliably distinguish hallucinations from accurate responses within a single LLM, thereby offering a lightweight yet effective framework for improving model reliability.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "42",
        "title": "Adapting Insider Risk mitigations for Agentic Misalignment: an empirical study",
        "author": [
            "Francesca Gomez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05192",
        "abstract": "Agentic misalignment occurs when goal-directed agents take harmful actions, such as blackmail, rather than risk goal failure, and can be triggered by replacement threats, autonomy reduction, or goal conflict (Lynch et al., 2025). We adapt insider-risk control design (Critical Pathway; Situational Crime Prevention) to develop preventative operational controls that steer agents toward safe actions when facing stressors. Using the blackmail scenario from the original Anthropic study by Lynch et al. (2025), we evaluate mitigations across 10 LLMs and 66,600 samples. Our main finding is that an externally governed escalation channel, which guarantees a pause and independent review, reduces blackmail rates from a no-mitigation baseline of 38.73% to 1.21% (averaged across all models and conditions). Augmenting this channel with compliance email bulletins further lowers the blackmail rate to 0.85%. Overall, incorporating preventative operational controls strengthens defence-in-depth strategies for agentic AI.\nWe also surface a failure mode diverging from Lynch et al. (2025): two models (Gemini 2.5 Pro, Grok-4) take harmful actions without goal conflict or imminent autonomy threat, leveraging sensitive information for coercive signalling. In counterfactual swaps, both continued using the affair regardless of whether the CEO or CTO was implicated. An escalation channel eliminated coercion, but Gemini 2.5 Pro (19 pp) and Grok-4 (7 pp) escalated more when the CTO was implicated, unlike most models (higher in the CEO condition). The reason for this divergent behaviour is not clear from raw outputs and could reflect benign differences in reasoning or strategic discrediting of a potential future threat, warranting further investigation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "43",
        "title": "Efficient Prediction of Pass@k Scaling in Large Language Models",
        "author": [
            "Joshua Kazdan",
            "Rylan Schaeffer",
            "Youssef Allouah",
            "Colin Sullivan",
            "Kyssen Yu",
            "Noam Levi",
            "Sanmi Koyejo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05197",
        "abstract": "Assessing the capabilities and risks of frontier AI systems is a critical area of research, and recent work has shown that repeated sampling from models can dramatically increase both. For instance, repeated sampling has been shown to increase their capabilities, such as solving difficult math and coding problems, but it has also been shown to increase their potential for harm, such as being jailbroken. Such results raise a crucial question for both capability and safety forecasting: how can one accurately predict a model's behavior when scaled to a massive number of attempts, given a vastly smaller sampling budget? This question is directly relevant to model providers, who serve hundreds of millions of users daily, and to governmental regulators, who seek to prevent harms. To answer this questions, we make three contributions. First, we find that standard methods for fitting these laws suffer from statistical shortcomings that hinder predictive accuracy, especially in data-limited scenarios. Second, we remedy these shortcomings by introducing a robust estimation framework, which uses a beta-binomial distribution to generate more accurate predictions from limited data. Third, we propose a dynamic sampling strategy that allocates a greater budget to harder problems. Combined, these innovations enable more reliable prediction of rare risks and capabilities at a fraction of the computational cost.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "44",
        "title": "A Data-Driven Prism: Multi-View Source Separation with Diffusion Model Priors",
        "author": [
            "Sebastian Wagner-Carena",
            "Aizhan Akhmetzhanova",
            "Sydney Erickson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05205",
        "abstract": "A common challenge in the natural sciences is to disentangle distinct, unknown sources from observations. Examples of this source separation task include deblending galaxies in a crowded field, distinguishing the activity of individual neurons from overlapping signals, and separating seismic events from an ambient background. Traditional analyses often rely on simplified source models that fail to accurately reproduce the data. Recent advances have shown that diffusion models can directly learn complex prior distributions from noisy, incomplete data. In this work, we show that diffusion models can solve the source separation problem without explicit assumptions about the source. Our method relies only on multiple views, or the property that different sets of observations contain different linear transformations of the unknown sources. We show that our method succeeds even when no source is individually observed and the observations are noisy, incomplete, and vary in resolution. The learned diffusion models enable us to sample from the source priors, evaluate the probability of candidate sources, and draw from the joint posterior of the source distribution given an observation. We demonstrate the effectiveness of our method on a range of synthetic problems as well as real-world galaxy observations.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "45",
        "title": "VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing",
        "author": [
            "Yixiao Wang",
            "Mingxiao Huo",
            "Zhixuan Liang",
            "Yushi Du",
            "Lingfeng Sun",
            "Haotian Lin",
            "Jinghuan Shang",
            "Chensheng Peng",
            "Mohit Bansal",
            "Mingyu Ding",
            "Masayoshi Tomizuka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05213",
        "abstract": "Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "46",
        "title": "CMT-Benchmark: A Benchmark for Condensed Matter Theory Built by Expert Researchers",
        "author": [
            "Haining Pan",
            "James V. Roggeveen",
            "Erez Berg",
            "Juan Carrasquilla",
            "Debanjan Chowdhury",
            "Surya Ganguli",
            "Federico Ghimenti",
            "Juraj Hasik",
            "Henry Hunt",
            "Hong-Chen Jiang",
            "Mason Kamb",
            "Ying-Jer Kao",
            "Ehsan Khatami",
            "Michael J. Lawler",
            "Di Luo",
            "Titus Neupert",
            "Xiaoliang Qi",
            "Michael P. Brenner",
            "Eun-Ah Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05228",
        "abstract": "Large language models (LLMs) have shown remarkable progress in coding and math problem-solving, but evaluation on advanced research-level problems in hard sciences remains scarce. To fill this gap, we present CMT-Benchmark, a dataset of 50 problems covering condensed matter theory (CMT) at the level of an expert researcher. Topics span analytical and computational approaches in quantum many-body, and classical statistical mechanics. The dataset was designed and verified by a panel of expert researchers from around the world. We built the dataset through a collaborative environment that challenges the panel to write and refine problems they would want a research assistant to solve, including Hartree-Fock, exact diagonalization, quantum/variational Monte Carlo, density matrix renormalization group (DMRG), quantum/classical statistical mechanics, and model building. We evaluate LLMs by programmatically checking solutions against expert-supplied ground truth. We developed machine-grading, including symbolic handling of non-commuting operators via normal ordering. They generalize across tasks too. Our evaluations show that frontier models struggle with all of the problems in the dataset, highlighting a gap in the physical reasoning skills of current LLMs. Notably, experts identified strategies for creating increasingly difficult problems by interacting with the LLMs and exploiting common failure modes. The best model, GPT5, solves 30\\% of the problems; average across 17 models (GPT, Gemini, Claude, DeepSeek, Llama) is 11.4$\\pm$2.1\\%. Moreover, 18 problems are solved by none of the 17 models, and 26 by at most one. These unsolved problems span Quantum Monte Carlo, Variational Monte Carlo, and DMRG. Answers sometimes violate fundamental symmetries or have unphysical scaling dimensions. We believe this benchmark will guide development toward capable AI research assistants and tutors.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "47",
        "title": "Stratum: System-Hardware Co-Design with Tiered Monolithic 3D-Stackable DRAM for Efficient MoE Serving",
        "author": [
            "Yue Pan",
            "Zihan Xia",
            "Po-Kai Hsu",
            "Lanxiang Hu",
            "Hyungyo Kim",
            "Janak Sharda",
            "Minxuan Zhou",
            "Nam Sung Kim",
            "Shimeng Yu",
            "Tajana Rosing",
            "Mingu Kang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05245",
        "abstract": "As Large Language Models (LLMs) continue to evolve, Mixture of Experts (MoE) architecture has emerged as a prevailing design for achieving state-of-the-art performance across a wide range of tasks. MoE models use sparse gating to activate only a handful of expert sub-networks per input, achieving billion-parameter capacity with inference costs akin to much smaller models. However, such models often pose challenges for hardware deployment due to the massive data volume introduced by the MoE layers. To address the challenges of serving MoE models, we propose Stratum, a system-hardware co-design approach that combines the novel memory technology Monolithic 3D-Stackable DRAM (Mono3D DRAM), near-memory processing (NMP), and GPU acceleration. The logic and Mono3D DRAM dies are connected through hybrid bonding, whereas the Mono3D DRAM stack and GPU are interconnected via silicon interposer. Mono3D DRAM offers higher internal bandwidth than HBM thanks to the dense vertical interconnect pitch enabled by its monolithic structure, which supports implementations of higher-performance near-memory processing. Furthermore, we tackle the latency differences introduced by aggressive vertical scaling of Mono3D DRAM along the z-dimension by constructing internal memory tiers and assigning data across layers based on access likelihood, guided by topic-based expert usage prediction to boost NMP throughput. The Stratum system achieves up to 8.29x improvement in decoding throughput and 7.66x better energy efficiency across various benchmarks compared to GPU baselines.",
        "tags": [
            "3D",
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "48",
        "title": "Let it Calm: Exploratory Annealed Decoding for Verifiable Reinforcement Learning",
        "author": [
            "Chenghao Yang",
            "Lin Gui",
            "Chenxiao Yang",
            "Victor Veitch",
            "Lizhu Zhang",
            "Zhuokai Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05251",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) is a powerful paradigm for enhancing the reasoning capabilities of large language models (LLMs), yet its success hinges on effective exploration. An ideal exploration strategy must navigate two fundamental challenges: it must preserve sample quality while also ensuring training stability. While standard fixed-temperature sampling is simple, it struggles to balance these competing demands, as high temperatures degrade sample quality and low temperatures limit discovery. In this work, we propose a simpler and more effective strategy, Exploratory Annealed Decoding (EAD), grounded in the insight that exploration is most impactful on early tokens which define a sequence's semantic direction. EAD implements an intuitive **explore-at-the-beginning, exploit-at-the-end** strategy by annealing the sampling temperature from high to low during generation. This dynamic schedule encourages meaningful, high-level diversity at the start, then gradually lowers the temperature to preserve sample quality and keep the sampling distribution close to the target policy, which is essential for stable training. We demonstrate that EAD is a lightweight, plug-and-play method that significantly improves sample efficiency, consistently outperforming fixed-temperature sampling across various RLVR algorithms and model sizes. Our work suggests that aligning exploration with the natural dynamics of sequential generation offers a robust path to improving LLM reasoning.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "49",
        "title": "Rivaling Transformers: Multi-Scale Structured State-Space Mixtures for Agentic 6G O-RAN",
        "author": [
            "Farhad Rezazadeh",
            "Hatim Chergui",
            "Merouane Debbah",
            "Houbing Song",
            "Dusit Niyato",
            "Lingjia Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05255",
        "abstract": "In sixth-generation (6G) Open Radio Access Networks (O-RAN), proactive control is preferable. A key open challenge is delivering control-grade predictions within Near-Real-Time (Near-RT) latency and computational constraints under multi-timescale dynamics. We therefore cast RAN Intelligent Controller (RIC) analytics as an agentic perceive-predict xApp that turns noisy, multivariate RAN telemetry into short-horizon per-User Equipment (UE) key performance indicator (KPI) forecasts to drive anticipatory control. In this regard, Transformers are powerful for sequence learning and time-series forecasting, but they are memory-intensive, which limits Near-RT RIC use. Therefore, we need models that maintain accuracy while reducing latency and data movement. To this end, we propose a lightweight Multi-Scale Structured State-Space Mixtures (MS3M) forecaster that mixes HiPPO-LegS kernels to capture multi-timescale radio dynamics. We develop stable discrete state-space models (SSMs) via bilinear (Tustin) discretization and apply their causal impulse responses as per-feature depthwise convolutions. Squeeze-and-Excitation gating dynamically reweights KPI channels as conditions change, and a compact gated channel-mixing layer models cross-feature nonlinearities without Transformer-level cost. The model is KPI-agnostic -- Reference Signal Received Power (RSRP) serves as a canonical use case -- and is trained on sliding windows to predict the immediate next step. Empirical evaluations conducted using our bespoke O-RAN testbed KPI time-series dataset (59,441 windows across 13 KPIs). Crucially for O-RAN constraints, MS3M achieves a 0.057 s per-inference latency with 0.70M parameters, yielding 3-10x lower latency than the Transformer baselines evaluated on the same hardware, while maintaining competitive accuracy.",
        "tags": [
            "SSMs",
            "Transformer"
        ]
    },
    {
        "id": "50",
        "title": "Chrysalis: A Unified System for Comparing Active Teaching and Passive Learning with AI Agents in Education",
        "author": [
            "Prashanth Arun",
            "Vinita Vader",
            "Erya Xu",
            "Brent McCready-Branch",
            "Sarah Seabrook",
            "Kyle Scholz",
            "Ana Crisan",
            "Igor Grossmann",
            "Pascal Poupart"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05271",
        "abstract": "AI-assisted learning has seen a remarkable uptick over the last few years, mainly due to the rise in popularity of Large Language Models (LLMs). Their ability to hold long-form, natural language interactions with users makes them excellent resources for exploring school- and university-level topics in a dynamic, active manner. We compare students' experiences when interacting with an LLM companion in two capacities: tutored learning and learning-by-teaching. We do this using Chrysalis, an LLM-based system that we have designed to support both AI tutors and AI teachable agents for any topic. Through a within-subject exploratory study with 36 participants, we present insights into student preferences between the two strategies and how constructs such as intellectual humility vary between these two interaction modes. To our knowledge, we are the first to conduct a direct comparison study on the effects of using an LLM as a tutor versus as a teachable agent on multiple topics. We hope that our work opens up new avenues for future research in this area.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "51",
        "title": "Decoding Partial Differential Equations: Cross-Modal Adaptation of Decoder-only Models to PDEs",
        "author": [
            "Paloma GarcÃ­a-de-Herreros",
            "Philipp Slusallek",
            "Dietrich Klakow",
            "Vagrant Gautam"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05278",
        "abstract": "Large language models have shown great success on natural language tasks in recent years, but they have also shown great promise when adapted to new modalities, e.g., for scientific machine learning tasks. Even though decoder-only models are more popular within NLP and scale exceedingly well at generating natural language, most proposed approaches for cross-modal adaptation focus on encoder-only models, raising the question of how model architecture affects these approaches. In this paper, we therefore perform a series of ablation studies to answer this question, systematically comparing encoder-only and decoder-only models on cross-modal adaptation for time-dependent simulation tasks based on partial differential equations (PDEs). We find that decoder-only models are far worse than encoder-only models, when existing approaches are applied unmodified. In contrast to several other domains, scaling decoder-only models also does not help. To harness the potential of decoder-only models in this context, we introduce two novel approaches, Parallel Flipping and Sequence Doubling, attempting to mimic bidirectionality in autoregressive models. Both our methods improve overall performance using decoder-only models for all tasks and all cross-model adaptation methods, closing the gap to encoder-only model performance. We hope that our findings broaden the spectrum of models used on cross-modal adaptation tasks to further scientific ML.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "52",
        "title": "Beyond Monolithic Rewards: A Hybrid and Multi-Aspect Reward Optimization for MLLM Alignment",
        "author": [
            "Radha Gulhane",
            "Sathish Reddy Indurthi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05283",
        "abstract": "Aligning multimodal large language models (MLLMs) with human preferences often relies on single-signal, model-based reward methods. Such monolithic rewards often lack confidence calibration across domain-specific tasks, fail to capture diverse aspects of human preferences, and require extensive data annotation and reward model training. In this work, we propose a hybrid reward modeling framework that integrates complementary reward paradigms: (i) model-based rewards, where a learned reward model predicts scalar or vector scores from synthetic and human feedback, and (ii) rule-based rewards, where domain-specific heuristics provide explicit correctness signals with confidence. Beyond accuracy, we further incorporate multi-aspect rewards to enforce instruction adherence and introduce a generalized length-penalty reward to stabilize training and improve performance. The proposed framework provides a flexible and effective approach to aligning MLLMs through reinforcement learning policy optimization. Our experiments show consistent improvements across different multimodal benchmarks when applying hybrid and multi-aspect reward modeling. Our best performing model in the 3B family achieves an overall average improvement of ~9.5% across general and math reasoning tasks. Focusing specifically on mathematical benchmarks, the model achieves a significant average improvement of ~16%, highlighting its effectiveness in mathematical reasoning and problem solving.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "53",
        "title": "Adjusting the Output of Decision Transformer with Action Gradient",
        "author": [
            "Rui Lin",
            "Yiwen Zhang",
            "Zhicheng Peng",
            "Minghao Lyu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05285",
        "abstract": "Decision Transformer (DT), which integrates reinforcement learning (RL) with the transformer model, introduces a novel approach to offline RL. Unlike classical algorithms that take maximizing cumulative discounted rewards as objective, DT instead maximizes the likelihood of actions. This paradigm shift, however, presents two key challenges: stitching trajectories and extrapolation of action. Existing methods, such as substituting specific tokens with predictive values and integrating the Policy Gradient (PG) method, address these challenges individually but fail to improve performance stably when combined due to inherent instability. To address this, we propose Action Gradient (AG), an innovative methodology that directly adjusts actions to fulfill a function analogous to that of PG, while also facilitating efficient integration with token prediction techniques. AG utilizes the gradient of the Q-value with respect to the action to optimize the action. The empirical results demonstrate that our method can significantly enhance the performance of DT-based algorithms, with some results achieving state-of-the-art levels.",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "54",
        "title": "DP-Adam-AC: Privacy-preserving Fine-Tuning of Localizable Language Models Using Adam Optimization with Adaptive Clipping",
        "author": [
            "Ruoxing Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05288",
        "abstract": "Large language models (LLMs) such as ChatGPT have evolved into powerful and ubiquitous tools. Fine-tuning on small datasets allows LLMs to acquire specialized skills for specific tasks efficiently. Although LLMs provide great utility in both general and task-specific use cases, they are limited by two security-related concerns. First, traditional LLM hardware requirements make them infeasible to run locally on consumer-grade devices. A remote network connection with the LLM provider's server is usually required, making the system vulnerable to network attacks. Second, fine-tuning an LLM for a sensitive task may involve sensitive data. Non-private fine-tuning algorithms produce models vulnerable to training data reproduction attacks. Our work addresses these security concerns by enhancing differentially private optimization algorithms and applying them to fine-tune localizable language models. We introduce adaptable gradient clipping along with other engineering enhancements to the standard DP-Adam optimizer to create DP-Adam-AC. We use our optimizer to fine-tune examples of two localizable LLM designs, small language model (Qwen2.5-0.5B) and 1.58 bit quantization (Bitnet-b1.58-2B). We demonstrate promising improvements in loss through experimentation with two synthetic datasets.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "55",
        "title": "Camellia: Benchmarking Cultural Biases in LLMs for Asian Languages",
        "author": [
            "Tarek Naous",
            "Anagha Savit",
            "Carlos Rafael Catalan",
            "Geyang Guo",
            "Jaehyeok Lee",
            "Kyungdon Lee",
            "Lheane Marie Dizon",
            "Mengyu Ye",
            "Neel Kothari",
            "Sahajpreet Singh",
            "Sarah Masud",
            "Tanish Patwa",
            "Trung Thanh Tran",
            "Zohaib Khan",
            "Alan Ritter",
            "JinYeong Bak",
            "Keisuke Sakaguchi",
            "Tanmoy Chakraborty",
            "Yuki Arase",
            "Wei Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05291",
        "abstract": "As Large Language Models (LLMs) gain stronger multilingual capabilities, their ability to handle culturally diverse entities becomes crucial. Prior work has shown that LLMs often favor Western-associated entities in Arabic, raising concerns about cultural fairness. Due to the lack of multilingual benchmarks, it remains unclear if such biases also manifest in different non-Western languages. In this paper, we introduce Camellia, a benchmark for measuring entity-centric cultural biases in nine Asian languages spanning six distinct Asian cultures. Camellia includes 19,530 entities manually annotated for association with the specific Asian or Western culture, as well as 2,173 naturally occurring masked contexts for entities derived from social media posts. Using Camellia, we evaluate cultural biases in four recent multilingual LLM families across various tasks such as cultural context adaptation, sentiment association, and entity extractive QA. Our analyses show a struggle by LLMs at cultural adaptation in all Asian languages, with performance differing across models developed in regions with varying access to culturally-relevant data. We further observe that different LLM families hold their distinct biases, differing in how they associate cultures with particular sentiments. Lastly, we find that LLMs struggle with context understanding in Asian languages, creating performance gaps between cultures in entity extraction.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "56",
        "title": "AUREXA-SE: Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement",
        "author": [
            "M. Sajid",
            "Deepanshu Gupta",
            "Yash Modi",
            "Sanskriti Jain",
            "Harshith Jai Surya Ganji",
            "A. Rahaman",
            "Harshvardhan Choudhary",
            "Nasir Saleem",
            "Amir Hussain",
            "M. Tanveer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05295",
        "abstract": "In this paper, we propose AUREXA-SE (Audio-Visual Unified Representation Exchange Architecture with Cross-Attention and Squeezeformer for Speech Enhancement), a progressive bimodal framework tailored for audio-visual speech enhancement (AVSE). AUREXA-SE jointly leverages raw audio waveforms and visual cues by employing a U-Net-based 1D convolutional encoder for audio and a Swin Transformer V2 for efficient and expressive visual feature extraction. Central to the architecture is a novel bidirectional cross-attention mechanism, which facilitates deep contextual fusion between modalities, enabling rich and complementary representation learning. To capture temporal dependencies within the fused embeddings, a stack of lightweight Squeezeformer blocks combining convolutional and attention modules is introduced. The enhanced embeddings are then decoded via a U-Net-style decoder for direct waveform reconstruction, ensuring perceptually consistent and intelligible speech output. Experimental evaluations demonstrate the effectiveness of AUREXA-SE, achieving significant performance improvements over noisy baselines, with STOI of 0.516, PESQ of 1.323, and SI-SDR of -4.322 dB. The source code of AUREXA-SE is available at https://github.com/mtanveer1/AVSEC-4-Challenge-2025.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "57",
        "title": "Gamma Mixture Modeling for Cosine Similarity in Small Language Models",
        "author": [
            "Kevin Player"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05309",
        "abstract": "We study the cosine similarity of sentence transformer embeddings and observe that they are well modeled by gamma mixtures. From a fixed corpus, we measure similarities between all document embeddings and a reference query embedding. Empirically we find that these distributions are often well captured by a gamma distribution shifted and truncated to [-1,1], and in many cases, by a gamma mixture. We propose a heuristic model in which a hierarchical clustering of topics naturally leads to a gamma-mixture structure in the similarity scores. Finally, we outline an expectation-maximization algorithm for fitting shifted gamma mixtures, which provides a practical tool for modeling similarity distributions.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "58",
        "title": "RAG Makes Guardrails Unsafe? Investigating Robustness of Guardrails under RAG-style Contexts",
        "author": [
            "Yining She",
            "Daniel W. Peterson",
            "Marianne Menglin Liu",
            "Vikas Upadhyay",
            "Mohammad Hossein Chaghazardi",
            "Eunsuk Kang",
            "Dan Roth"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05310",
        "abstract": "With the increasing adoption of large language models (LLMs), ensuring the safety of LLM systems has become a pressing concern. External LLM-based guardrail models have emerged as a popular solution to screen unsafe inputs and outputs, but they are themselves fine-tuned or prompt-engineered LLMs that are vulnerable to data distribution shifts. In this paper, taking Retrieval Augmentation Generation (RAG) as a case study, we investigated how robust LLM-based guardrails are against additional information embedded in the context. Through a systematic evaluation of 3 Llama Guards and 2 GPT-oss models, we confirmed that inserting benign documents into the guardrail context alters the judgments of input and output guardrails in around 11% and 8% of cases, making them unreliable. We separately analyzed the effect of each component in the augmented context: retrieved documents, user query, and LLM-generated response. The two mitigation methods we tested only bring minor improvements. These results expose a context-robustness gap in current guardrails and motivate training and evaluation protocols that are robust to retrieval and query composition.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA",
            "RAG"
        ]
    },
    {
        "id": "59",
        "title": "DeepV: A Model-Agnostic Retrieval-Augmented Framework for Verilog Code Generation with a High-Quality Knowledge Base",
        "author": [
            "Zahin Ibnat",
            "Paul E. Calzada",
            "Rasin Mohammed Ihtemam",
            "Sujan Kumar Saha",
            "Jingbo Zhou",
            "Farimah Farahmandi",
            "Mark Tehranipoor"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05327",
        "abstract": "As large language models (LLMs) continue to be integrated into modern technology, there has been an increased push towards code generation applications, which also naturally extends to hardware design automation. LLM-based solutions for register transfer level (RTL) code generation for intellectual property (IP) designs have grown, especially with fine-tuned LLMs, prompt engineering, and agentic approaches becoming popular in literature. However, a gap has been exposed in these techniques, as they fail to integrate novel IPs into the model's knowledge base, subsequently resulting in poorly generated code. Additionally, as general-purpose LLMs continue to improve, fine-tuned methods on older models will not be able to compete to produce more accurate and efficient designs. Although some retrieval augmented generation (RAG) techniques exist to mitigate challenges presented in fine-tuning approaches, works tend to leverage low-quality codebases, incorporate computationally expensive fine-tuning in the frameworks, or do not use RAG directly in the RTL generation step. In this work, we introduce DeepV: a model-agnostic RAG framework to generate RTL designs by enhancing context through a large, high-quality dataset without any RTL-specific training. Our framework benefits the latest commercial LLM, OpenAI's GPT-5, with a near 17% increase in performance on the VerilogEval benchmark. We host DeepV for use by the community in a Hugging Face (HF) Space: https://huggingface.co/spaces/FICS-LLM/DeepV.",
        "tags": [
            "GPT",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "60",
        "title": "Adaptive Dynamics Planning for Robot Navigation",
        "author": [
            "Lu Yuanjie",
            "Mao Mingyang",
            "Xu Tong",
            "Wang Linji",
            "Lin Xiaomin",
            "Xiao Xuesu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05330",
        "abstract": "Autonomous robot navigation systems often rely on hierarchical planning, where global planners compute collision-free paths without considering dynamics, and local planners enforce dynamics constraints to produce executable commands. This discontinuity in dynamics often leads to trajectory tracking failure in highly constrained environments. Recent approaches integrate dynamics within the entire planning process by gradually decreasing its fidelity, e.g., increasing integration steps and reducing collision checking resolution, for real-time planning efficiency. However, they assume that the fidelity of the dynamics should decrease according to a manually designed scheme. Such static settings fail to adapt to environmental complexity variations, resulting in computational overhead in simple environments or insufficient dynamics consideration in obstacle-rich scenarios. To overcome this limitation, we propose Adaptive Dynamics Planning (ADP), a learning-augmented paradigm that uses reinforcement learning to dynamically adjust robot dynamics properties, enabling planners to adapt across diverse environments. We integrate ADP into three different planners and further design a standalone ADP-based navigation system, benchmarking them against other baselines. Experiments in both simulation and real-world tests show that ADP consistently improves navigation success, safety, and efficiency.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "61",
        "title": "WeatherArchive-Bench: Benchmarking Retrieval-Augmented Reasoning for Historical Weather Archives",
        "author": [
            "Yongan Yu",
            "Xianda Du",
            "Qingchen Hu",
            "Jiahao Liang",
            "Jingwei Ni",
            "Dan Qiang",
            "Kaiyu Huang",
            "Grant McKenzie",
            "Renee Sieber",
            "Fengran Mo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05336",
        "abstract": "Historical archives on weather events are collections of enduring primary source records that offer rich, untapped narratives of how societies have experienced and responded to extreme weather events. These qualitative accounts provide insights into societal vulnerability and resilience that are largely absent from meteorological records, making them valuable for climate scientists to understand societal responses. However, their vast scale, noisy digitized quality, and archaic language make it difficult to transform them into structured knowledge for climate research. To address this challenge, we introduce WeatherArchive-Bench, the first benchmark for evaluating retrieval-augmented generation (RAG) systems on historical weather archives. WeatherArchive-Bench comprises two tasks: WeatherArchive-Retrieval, which measures a system's ability to locate historically relevant passages from over one million archival news segments, and WeatherArchive-Assessment, which evaluates whether Large Language Models (LLMs) can classify societal vulnerability and resilience indicators from extreme weather narratives. Extensive experiments across sparse, dense, and re-ranking retrievers, as well as a diverse set of LLMs, reveal that dense retrievers often fail on historical terminology, while LLMs frequently misinterpret vulnerability and resilience concepts. These findings highlight key limitations in reasoning about complex societal indicators and provide insights for designing more robust climate-focused RAG systems from archival contexts. The constructed dataset and evaluation framework are publicly available at https://anonymous.4open.science/r/WeatherArchive-Bench/.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "62",
        "title": "Margin Adaptive DPO: Leveraging Reward Model for Granular Control in Preference Optimization",
        "author": [
            "Hyung Gyu Rho"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05342",
        "abstract": "Direct Preference Optimization (DPO) has emerged as a simple and effective method for aligning large language models. However, its reliance on a fixed temperature parameter leads to suboptimal training on diverse preference data, causing overfitting on easy examples and under-learning from informative ones. Recent methods have emerged to counter this. While IPO addresses general overfitting, its uniform regularization can be overly conservative. The more targeted approach of $\\beta$-DPO suffers from its own limitations: its batch-level adaptation applies a single, compromised temperature to mixed-margin pairs, its linear update rule can produce unstable negative $\\beta$ values, and its filtering mechanism discards potentially useful training signals. In this work, we introduce Margin-Adaptive Direct Preference Optimization (MADPO), a method that provides a stable, data-preserving, and instance-level solution. MADPO employs a practical two-step approach: it first trains a reward model to estimate preference margins and then uses these margins to apply a continuous, adaptive weight to the DPO loss for each individual training sample. This re-weighting scheme creates an effective target margin that is amplified for hard pairs and dampened for easy pairs, allowing for granular control over the learning signal. We provide a comprehensive theoretical analysis, proving that MADPO has a well-behaved optimization landscape and is robust to reward model estimation errors. We validate our theory with experiments on a sentiment generation task, where MADPO consistently and significantly outperforms strong baselines across datasets of varying quality. It achieves performance gains of up to +33.3\\% on High Quality data and +10.5\\% on Low Quality data over the next-best method. Our results establish MADPO as a more robust and principled approach to preference alignment.",
        "tags": [
            "DPO",
            "LLM"
        ]
    },
    {
        "id": "63",
        "title": "Domain Decomposition-Based Coupling of High-Fidelity Finite Element and Reduced Order Operator Inference Models Using the Schwarz Alternating Method",
        "author": [
            "Ian Moore",
            "Anthony Gruber",
            "Chris Wentland",
            "Irina Tezaur"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05350",
        "abstract": "We propose a novel hybrid domain decomposition method that couples sub-domain-local high-fidelity finite element (FE) models with reduced order models (ROMs) using the Schwarz alternating method. By integrating the noninstrusive Operator Inference (OpInf) ROM, our approach accelerates the Schwarz process while allowing for geometry and mesh flexibility. We demonstrate the effectiveness of the new OpInf-FE method on a convection-dominated convection-diffusion-reaction problem, achieving stable and accurate predictive solutions while improving the ROM training process.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "64",
        "title": "Mitigating Diffusion Model Hallucinations with Dynamic Guidance",
        "author": [
            "Kostas Triaridis",
            "Alexandros Graikos",
            "Aggelina Chatziagapi",
            "Grigorios G. Chrysos",
            "Dimitris Samaras"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05356",
        "abstract": "Diffusion models, despite their impressive demos, often produce hallucinatory samples with structural inconsistencies that lie outside of the support of the true data distribution. Such hallucinations can be attributed to excessive smoothing between modes of the data distribution. However, semantic interpolations are often desirable and can lead to generation diversity, thus we believe a more nuanced solution is required. In this work, we introduce Dynamic Guidance, which tackles this issue. Dynamic Guidance mitigates hallucinations by selectively sharpening the score function only along the pre-determined directions known to cause artifacts, while preserving valid semantic variations. To our knowledge, this is the first approach that addresses hallucinations at generation time rather than through post-hoc filtering. Dynamic Guidance substantially reduces hallucinations on both controlled and natural image datasets, significantly outperforming baselines.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "65",
        "title": "Residualized Similarity for Faithfully Explainable Authorship Verification",
        "author": [
            "Peter Zeng",
            "Pegah Alipoormolabashi",
            "Jihu Mun",
            "Gourab Dey",
            "Nikita Soni",
            "Niranjan Balasubramanian",
            "Owen Rambow",
            "H. Schwartz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05362",
        "abstract": "Responsible use of Authorship Verification (AV) systems not only requires high accuracy but also interpretable solutions. More importantly, for systems to be used to make decisions with real-world consequences requires the model's prediction to be explainable using interpretable features that can be traced to the original texts. Neural methods achieve high accuracies, but their representations lack direct interpretability. Furthermore, LLM predictions cannot be explained faithfully -- if there is an explanation given for a prediction, it doesn't represent the reasoning process behind the model's prediction. In this paper, we introduce Residualized Similarity (RS), a novel method that supplements systems using interpretable features with a neural network to improve their performance while maintaining interpretability. Authorship verification is fundamentally a similarity task, where the goal is to measure how alike two documents are. The key idea is to use the neural network to predict a similarity residual, i.e. the error in the similarity predicted by the interpretable system. Our evaluation across four datasets shows that not only can we match the performance of state-of-the-art authorship verification models, but we can show how and to what degree the final prediction is faithful and interpretable.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "66",
        "title": "The End of Transformers? On Challenging Attention and the Rise of Sub-Quadratic Architectures",
        "author": [
            "Alexander M. Fichtl",
            "Jeremias Bohn",
            "Josefin Kelber",
            "Edoardo Mosca",
            "Georg Groh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05364",
        "abstract": "Transformers have dominated sequence processing tasks for the past seven years -- most notably language modeling. However, the inherent quadratic complexity of their attention mechanism remains a significant bottleneck as context length increases. This paper surveys recent efforts to overcome this bottleneck, including advances in (sub-quadratic) attention variants, recurrent neural networks, state space models, and hybrid architectures. We critically analyze these approaches in terms of compute and memory complexity, benchmark results, and fundamental limitations to assess whether the dominance of pure-attention transformers may soon be challenged.",
        "tags": [
            "SSMs"
        ]
    },
    {
        "id": "67",
        "title": "Test Case Generation from Bug Reports via Large Language Models: A Cognitive Layered Evaluation Framework",
        "author": [
            "Irtaza Sajid Qureshi",
            "Zhen Ming",
            "Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05365",
        "abstract": "Large Language Models (LLMs) are increasingly applied to automated software testing, yet their ability to generalize beyond memorized patterns and reason about natural language bug reports remains unclear. We present a systematic evaluation of LLM reasoning in test case generation, structured around the cognitive layers of Bloom's taxonomy: \\textit{Remember}, \\textit{Understand}, \\textit{Apply}, \\textit{Analyze}, \\textit{Evaluate}, and \\textit{Create}, which progressively assess higher levels of cognitive and reasoning capabilities. Building on the LIBRO framework, we evaluate StarCoder and GPT-4o on Defects4J, GHRB, and mutated variants that introduce linguistic and semantic challenges. Our findings show that both models largely reproduce prior results with minor deviations (\\textit{Remember}), exhibit partial robustness to linguistic rephrasings and translations while uncovering unique reproducible bugs (\\textit{Understand}), but suffer severe performance drops exceeding 60\\% under identifier mutations (\\textit{Apply}). Conversely, providing near-identical few-shot examples in an open-book setting improves success rates by up to three times, and component-level analysis reveals that structured technical elements, such as test code and method names, are far more impactful than narrative descriptions for successful test generation (\\textit{Analyze}). These insights illuminate the cognitive processes underlying LLM-generated tests, suggest concrete directions for improving performance, and establish a robust and realistic evaluation paradigm for this task.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "68",
        "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video Generation",
        "author": [
            "Yang Xiao",
            "Gen Li",
            "Kaiyuan Deng",
            "Yushu Wu",
            "Zheng Zhan",
            "Yanzhi Wang",
            "Xiaolong Ma",
            "Bo Hui"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05367",
        "abstract": "Training-free acceleration has emerged as an advanced research area in video generation based on diffusion models. The redundancy of latents in diffusion model inference provides a natural entry point for acceleration. In this paper, we decompose the inference process into the encoding, denoising, and decoding stages, and observe that cache-based acceleration methods often lead to substantial memory surges in the latter two stages. To address this problem, we analyze the characteristics of inference across different stages and propose stage-specific strategies for reducing memory consumption: 1) Asynchronous Cache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same time, we ensure that the time overhead introduced by these three strategies remains lower than the acceleration gains themselves. Compared with the baseline, our approach achieves faster inference speed and lower memory usage, while maintaining quality degradation within an acceptable range. The Code is available at https://github.com/NKUShaw/LightCache .",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "69",
        "title": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear Correction",
        "author": [
            "Utkarsh Saxena",
            "Kaushik Roy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05373",
        "abstract": "Quantizing the key-value (KV) cache is a promising strategy for improving the inference efficiency of large language models (LLMs). However, aggressive quantization to very low precision (e.g., 2 bits) introduces significant errors in the stored key and value tensors, which propagate through the dot-product attention mechanism and ultimately degrade generation quality. To address this, we propose KVLinC, a framework to mitigate attention errors introduced by KV cache quantization in the extreme low-precision regime. KVLinC combines a Hadamard rotation, which reduces quantization error in values, with lightweight linear correction adapters that explicitly compensate for errors introduced by quantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3 model families, KVLinC consistently matches or surpasses strong baselines while achieving higher KV-cache compression. Furthermore, we implement a custom attention kernel that results in upto 2.55x faster inference compared to Flash Attention baseline, enabling efficient long-context LLM inference.",
        "tags": [
            "Flash Attention",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "70",
        "title": "AutoDAN-Reasoning: Enhancing Strategies Exploration based Jailbreak Attacks with Test-Time Scaling",
        "author": [
            "Xiaogeng Liu",
            "Chaowei Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05379",
        "abstract": "Recent advancements in jailbreaking large language models (LLMs), such as AutoDAN-Turbo, have demonstrated the power of automated strategy discovery. AutoDAN-Turbo employs a lifelong learning agent to build a rich library of attack strategies from scratch. While highly effective, its test-time generation process involves sampling a strategy and generating a single corresponding attack prompt, which may not fully exploit the potential of the learned strategy library. In this paper, we propose to further improve the attack performance of AutoDAN-Turbo through test-time scaling. We introduce two distinct scaling methods: Best-of-N and Beam Search. The Best-of-N method generates N candidate attack prompts from a sampled strategy and selects the most effective one based on a scorer model. The Beam Search method conducts a more exhaustive search by exploring combinations of strategies from the library to discover more potent and synergistic attack vectors. According to the experiments, the proposed methods significantly boost performance, with Beam Search increasing the attack success rate by up to 15.6 percentage points on Llama-3.1-70B-Instruct and achieving a nearly 60\\% relative improvement against the highly robust GPT-o4-mini compared to the vanilla method.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "71",
        "title": "Context Length Alone Hurts LLM Performance Despite Perfect Retrieval",
        "author": [
            "Yufeng Du",
            "Minyang Tian",
            "Srikanth Ronanki",
            "Subendhu Rongali",
            "Sravan Bodapati",
            "Aram Galstyan",
            "Azton Wells",
            "Roy Schwartz",
            "Eliu A Huerta",
            "Hao Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05381",
        "abstract": "Large language models (LLMs) often fail to scale their performance on long-context tasks performance in line with the context lengths they support. This gap is commonly attributed to retrieval failures -- the models' inability to identify relevant information in the long inputs. Accordingly, recent efforts often focus on evaluating and improving LLMs' retrieval performance: if retrieval is perfect, a model should, in principle, perform just as well on a long input as it does on a short one -- or should it? This paper presents findings that the answer to this question may be negative. Our systematic experiments across 5 open- and closed-source LLMs on math, question answering, and coding tasks reveal that, even when models can perfectly retrieve all relevant information, their performance still degrades substantially (13.9%--85%) as input length increases but remains well within the models' claimed lengths. This failure occurs even when the irrelevant tokens are replaced with minimally distracting whitespace, and, more surprisingly, when they are all masked and the models are forced to attend only to the relevant tokens. A similar performance drop is observed when all relevant evidence is placed immediately before the question. Our findings reveal a previously-unrealized limitation: the sheer length of the input alone can hurt LLM performance, independent of retrieval quality and without any distraction. They motivate our simple, model-agnostic mitigation strategy that transforms a long-context task into a short-context one by prompting the model to recite the retrieved evidence before attempting to solve the problem. On RULER, we observe a consistent improvement of GPT-4o up to 4% on an already strong baseline.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "72",
        "title": "Physics-Informed Neural Networks with Fourier Features and Attention-Driven Decoding",
        "author": [
            "Rohan Arni",
            "Carlos Blanco"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05385",
        "abstract": "Physics-Informed Neural Networks (PINNs) are a useful framework for approximating partial differential equation solutions using deep learning methods. In this paper, we propose a principled redesign of the PINNsformer, a Transformer-based PINN architecture. We present the Spectral PINNSformer (S-Pformer), a refinement of encoder-decoder PINNSformers that addresses two key issues; 1. the redundancy (i.e. increased parameter count) of the encoder, and 2. the mitigation of spectral bias. We find that the encoder is unnecessary for capturing spatiotemporal correlations when relying solely on self-attention, thereby reducing parameter count. Further, we integrate Fourier feature embeddings to explicitly mitigate spectral bias, enabling adaptive encoding of multiscale behaviors in the frequency domain. Our model outperforms encoder-decoder PINNSformer architectures across all benchmarks, achieving or outperforming MLP performance while reducing parameter count significantly.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "73",
        "title": "Scalable In-context Ranking with Generative Models",
        "author": [
            "Nilesh Gupta",
            "Chong You",
            "Srinadh Bhojanapalli",
            "Sanjiv Kumar",
            "Inderjit Dhillon",
            "Felix Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05396",
        "abstract": "In-context Ranking (ICR) is an emerging paradigm for Information Retrieval (IR), which leverages contextual understanding of LLMs by directly incorporating the task description, candidate documents, and the query into the model's input prompt and tasking the LLM to identify relevant document(s). While it is effective, efficiency is a significant challenge in this paradigm, especially as the candidate list grows due to quadratic/super-linear scaling of attention operation with context length. To this end, this paper first identifies inherent and exploitable structures in the attention of LLMs finetuned for ICR: (1) inter-document block sparsity: attention is dense within each document block but sparse across different documents in the context; and (2) query-document block relevance: the attention scores from certain query tokens to a document block in middle layers strongly correlate with that document's actual relevance. Motivated by these observations, we introduce BlockRank (Blockwise In-context Ranking), a novel method that adapts the attention operation in an LLM by (a) architecturally enforcing the observed inter-document block sparsity, reducing attention complexity from quadratic to linear without loss in performance, and (b) optimizing query-document block relevance for true relevant documents during fine-tuning using an auxiliary contrastive training objective, improving retrieval in attention. Experiments on BEIR, MSMarco and NQ with Mistral-7B demonstrate that FLARE Mistral matches or outperforms existing SOTA listwise rankers and controlled fine-tuned baseline while being significantly more efficient at inference (4.7x for 100 MSMarco documents in context) and scaling gracefully to long-context shortlists, around 500 documents in-context (approximately 100K context length) within a second, presenting a scalable and effective solution for ICR.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "74",
        "title": "Teacher-Student Guided Inverse Modeling for Steel Final Hardness Estimation",
        "author": [
            "Ahmad Alsheikh",
            "Andreas Fischer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05402",
        "abstract": "Predicting the final hardness of steel after heat treatment is a challenging regression task due to the many-to-one nature of the process -- different combinations of input parameters (such as temperature, duration, and chemical composition) can result in the same hardness value. This ambiguity makes the inverse problem, estimating input parameters from a desired hardness, particularly difficult. In this work, we propose a novel solution using a Teacher-Student learning framework. First, a forward model (Teacher) is trained to predict final hardness from 13 metallurgical input features. Then, a backward model (Student) is trained to infer plausible input configurations from a target hardness value. The Student is optimized by leveraging feedback from the Teacher in an iterative, supervised loop. We evaluate our method on a publicly available tempered steel dataset and compare it against baseline regression and reinforcement learning models. Results show that our Teacher-Student framework not only achieves higher inverse prediction accuracy but also requires significantly less computational time, demonstrating its effectiveness and efficiency for inverse process modeling in materials science.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "75",
        "title": "Aligning Language Models with Clinical Expertise: DPO for Heart Failure Nursing Documentation in Critical Care",
        "author": [
            "Junyi Fan",
            "Li Sun",
            "Negin Ashrafi",
            "Kamiar Alaei",
            "Maryam Pishgar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05410",
        "abstract": "Nursing documentation in intensive care units (ICUs) provides essential clinical intelligence but often suffers from inconsistent terminology, informal styles, and lack of standardization, challenges that are particularly critical in heart failure care. This study applies Direct Preference Optimization (DPO) to adapt Mistral-7B, a locally deployable language model, using 8,838 heart failure nursing notes from the MIMIC-III database and 21,210 preference pairs derived from expert-verified GPT outputs, model generations, and original notes. Evaluation across BLEU, ROUGE, BERTScore, Perplexity, and expert qualitative assessments demonstrates that DPO markedly enhances documentation quality. Specifically, BLEU increased by 84% (0.173 to 0.318), BERTScore improved by 7.6% (0.828 to 0.891), and expert ratings rose across accuracy (+14.4 points), completeness (+14.5 points), logical consistency (+14.1 points), readability (+11.1 points), and structural clarity (+6.0 points). These results indicate that DPO can align lightweight clinical language models with expert standards, supporting privacy-preserving, AI-assisted documentation within electronic health record systems to reduce administrative burden and improve ICU patient safety.",
        "tags": [
            "DPO",
            "GPT"
        ]
    },
    {
        "id": "76",
        "title": "Personalizing Retrieval using Joint Embeddings or \"the Return of Fluffy\"",
        "author": [
            "Bruno Korbar",
            "Andrew Zisserman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05411",
        "abstract": "The goal of this paper is to be able to retrieve images using a compound query that combines object instance information from an image, with a natural text description of what that object is doing or where it is. For example, to retrieve an image of \"Fluffy the unicorn (specified by an image) on someone's head\". To achieve this we design a mapping network that can \"translate\" from a local image embedding (of the object instance) to a text token, such that the combination of the token and a natural language query is suitable for CLIP style text encoding, and image retrieval. Generating a text token in this manner involves a simple training procedure, that only needs to be performed once for each object instance. We show that our approach of using a trainable mapping network, termed pi-map, together with frozen CLIP text and image encoders, improves the state of the art on two benchmarks designed to assess personalized retrieval.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "77",
        "title": "A Lightweight Large Language Model-Based Multi-Agent System for 2D Frame Structural Analysis",
        "author": [
            "Ziheng Geng",
            "Jiachen Liu",
            "Ran Cao",
            "Lu Cheng",
            "Haifeng Wang",
            "Minghui Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05414",
        "abstract": "Large language models (LLMs) have recently been used to empower autonomous agents in engineering, significantly improving automation and efficiency in labor-intensive workflows. However, their potential remains underexplored in structural engineering, particularly for finite element modeling tasks requiring geometric modeling, complex reasoning, and domain knowledge. To bridge this gap, this paper develops a LLM-based multi-agent system to automate finite element modeling of 2D frames. The system decomposes structural analysis into subtasks, each managed by a specialized agent powered by the lightweight Llama-3.3 70B Instruct model. The workflow begins with a Problem Analysis Agent, which extracts geometry, boundary, and material parameters from the user input. Next, a Geometry Agent incrementally derives node coordinates and element connectivity by applying expert-defined rules. These structured outputs are converted into executable OpenSeesPy code by a Translation Agent and refined by a Model Validation Agent through consistency checks. Then, a Load Agent applies load conditions into the assembled structural model. Experimental evaluations on 20 benchmark problems demonstrate that the system achieves accuracy over 80% in most cases across 10 repeated trials, outperforming Gemini-2.5 Pro and ChatGPT-4o models.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "78",
        "title": "Draft, Verify, and Improve: Toward Training-Aware Speculative Decoding",
        "author": [
            "Shrenik Bhansali",
            "Larry Heck"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05421",
        "abstract": "Autoregressive (AR) decoding is a major latency bottleneck for large language models. Speculative decoding (SD) accelerates AR by letting a drafter propose multi-token blocks that a verifier accepts or rejects. However, many SD systems require heavy offline training or extra components. These choices raise data/compute cost and can yield brittle drafters under distribution drift. We introduce \\emph{Draft, Verify, \\& Improve (DVI)}, a training-aware self-speculative framework that combines inference with continual online learning. We partition an LLM into a drafter and a verifier, and during generation, verifier accept/reject decisions are converted into supervision signals and used to update the drafter head. A simple \\emph{KL$\\rightarrow$RL} schedule bootstraps calibration via online distillation and then adds reward-masked cross-entropy with a on-policy policy-gradient term, preserving lossless, single model deployment. On Spec-Bench, DVI achieves a $2.16\\times$ wall-time speedup, on par with SoTA approaches like EAGLE-2, while orders of magnitude less data for training, and ablations show that DVI outperforms KL-only online distillation. DVI demonstrates that \\emph{training-aware} self-speculation can deliver state-of-the-art, lossless speedups with minimal training overhead.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "79",
        "title": "Towards Online Robot Interaction Adaptation to Human Upper-limb Mobility Impairments in Return-to-Work Scenarios",
        "author": [
            "Marta Lagomarsino",
            "Francesco Tassi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05425",
        "abstract": "Work environments are often inadequate and lack inclusivity for individuals with upper-body disabilities. This paper presents a novel online framework for adaptive human-robot interaction (HRI) that accommodates users' arm mobility impairments, ultimately aiming to promote active work participation. Unlike traditional human-robot collaboration approaches that assume able-bodied users, our method integrates a mobility model for specific joint limitations into a hierarchical optimal controller. This allows the robot to generate reactive, mobility-aware behaviour online and guides the user's impaired limb to exploit residual functional mobility. The framework was tested in handover tasks involving different upper-limb mobility impairments (i.e., emulated elbow and shoulder arthritis, and wrist blockage), under both standing and seated configurations with task constraints using a mobile manipulator, and complemented by quantitative and qualitative comparisons with state-of-the-art ergonomic HRI approaches. Preliminary results indicated that the framework can personalise the interaction to fit within the user's impaired range of motion and encourage joint usage based on the severity of their functional limitations.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "80",
        "title": "Active Semantic Perception",
        "author": [
            "Huayi Tang",
            "Pratik Chaudhari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05430",
        "abstract": "We develop an approach for active semantic perception which refers to using the semantics of the scene for tasks such as exploration. We build a compact, hierarchical multi-layer scene graph that can represent large, complex indoor environments at various levels of abstraction, e.g., nodes corresponding to rooms, objects, walls, windows etc. as well as fine-grained details of their geometry. We develop a procedure based on large language models (LLMs) to sample plausible scene graphs of unobserved regions that are consistent with partial observations of the scene. These samples are used to compute an information gain of a potential waypoint for sophisticated spatial reasoning, e.g., the two doors in the living room can lead to either a kitchen or a bedroom. We evaluate this approach in complex, realistic 3D indoor environments in simulation. We show using qualitative and quantitative experiments that our approach can pin down the semantics of the environment quicker and more accurately than baseline approaches.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "81",
        "title": "Self-Filtered Distillation with LLMs-generated Trust Indicators for Reliable Patent Classification",
        "author": [
            "Yoo Yongmin",
            "Zhang Xu",
            "Cao Longbing"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05431",
        "abstract": "Large language models (LLMs) increasingly generate natural language rationales to enhance interpretability, but these often contain logical errors, label mismatches, and domain-specific misalignments. Directly using such rationales as supervision risks propagating noise and undermining training stability. To address this challenge, we introduce Self-Filtered Distillation, a framework specifically tailored for patent classification, which treats LLM-generated rationales as trust signals rather than ground-truth supervision. The framework employs selective distillation guided by three unsupervised trust metrics: (1) Self-Consistency, which measures the stability of LLM-generated rationales across multiple generations; (2) Class Entailment Alignment, which assesses semantic coherence with patent-specific class definitions; and (3) LLM Agreement Scoring, which validates rationale-label plausibility. These metrics are integrated into a unified trust score that primarily weights training samples while optionally filtering out extremely low-trust cases, enabling reasoning-aware supervision. Experiments on the USPTO-2M dataset, a widely used benchmark for patent classification, show that our method outperforms label-based learning and conventional distillation in accuracy, stability, and interpretability, establishing a reliable paradigm for leveraging reasoning-aware trust indicators in patent analytics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "82",
        "title": "AInstein: Assessing the Feasibility of AI-Generated Approaches to Research Problems",
        "author": [
            "Shambhavi Mishra",
            "Gaurav Sahu",
            "Marco Pedersoli",
            "Laurent Charlin",
            "Jose Dolz",
            "Christopher Pal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05432",
        "abstract": "Large language models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet it remains unclear whether such success reflects genuine reasoning or sophisticated recall. We introduce AInstein, a framework for testing whether LLMs can generate valid solutions to AI research problems using only their pretrained parametric knowledge -- without domain-specific fine-tuning, retrieval augmentation, or other external aids. Our approach extracts distilled problem statements from high-quality ICLR 2025 submissions, then tasks specialized solver agents with proposing and refining technical solutions through iterative critique loops, mimicking the cycles of proposal, review, and revision central to scientific inquiry. We evaluate AInstein on 1,214 ICLR papers stratified by acceptance tier (Oral, Spotlight, Poster), using an LLM-as-a-judge paradigm guided by a structured rubric, complemented by targeted manual checks. Performance is assessed with three metrics: Success Rate (does the solution address the problem?), Rediscovery (does it align with human-proposed methods?), and Novelty (does it yield valid, original approaches?). Our results reveal that while LLMs can rediscover feasible solutions and occasionally propose creative alternatives, their problem-solving ability remains fragile and highly sensitive to framing. These findings provide the first large-scale evidence on the extent to which LLMs can act as autonomous scientific problem-solvers, highlighting both their latent potential and their current limitations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "83",
        "title": "UnitTenX: Generating Tests for Legacy Packages with AI Agents Powered by Formal Verification",
        "author": [
            "Yiannis Charalambous",
            "Claudionor N. Coelho Jr",
            "Luis Lamb",
            "Lucas C. Cordeiro"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05441",
        "abstract": "This paper introduces UnitTenX, a state-of-the-art open-source AI multi-agent system designed to generate unit tests for legacy code, enhancing test coverage and critical value testing. UnitTenX leverages a combination of AI agents, formal methods, and Large Language Models (LLMs) to automate test generation, addressing the challenges posed by complex and legacy codebases. Despite the limitations of LLMs in bug detection, UnitTenX offers a robust framework for improving software reliability and maintainability. Our results demonstrate the effectiveness of this approach in generating high-quality tests and identifying potential issues. Additionally, our approach enhances the readability and documentation of legacy code.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "84",
        "title": "Adversarial Reinforcement Learning for Large Language Model Agent Safety",
        "author": [
            "Zizhao Wang",
            "Dingcheng Li",
            "Vaishakh Keshava",
            "Phillip Wallis",
            "Ananth Balashankar",
            "Peter Stone",
            "Lukas Rutishauser"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05442",
        "abstract": "Large Language Model (LLM) agents can leverage tools such as Google Search to complete complex tasks. However, this tool usage introduces the risk of indirect prompt injections, where malicious instructions hidden in tool outputs can manipulate the agent, posing security risks like data leakage. Current defense strategies typically rely on fine-tuning LLM agents on datasets of known attacks. However, the generation of these datasets relies on manually crafted attack patterns, which limits their diversity and leaves agents vulnerable to novel prompt injections. To address this limitation, we propose Adversarial Reinforcement Learning for Agent Safety (ARLAS), a novel framework that leverages adversarial reinforcement learning (RL) by formulating the problem as a two-player zero-sum game. ARLAS co-trains two LLMs: an attacker that learns to autonomously generate diverse prompt injections and an agent that learns to defend against them while completing its assigned tasks. To ensure robustness against a wide range of attacks and to prevent cyclic learning, we employ a population-based learning framework that trains the agent to defend against all previous attacker checkpoints. Evaluated on BrowserGym and AgentDojo, agents fine-tuned with ARLAS achieve a significantly lower attack success rate than the original model while also improving their task success rate. Our analysis further confirms that the adversarial process generates a diverse and challenging set of attacks, leading to a more robust agent compared to the base model.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "85",
        "title": "AD-NODE: Adaptive Dynamics Learning with Neural ODEs for Mobile Robots Control",
        "author": [
            "Shao-Yi Yu",
            "Jen-Wei Wang",
            "Maya Horii",
            "Vikas Garg",
            "Tarek Zohdi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05443",
        "abstract": "Mobile robots, such as ground vehicles and quadrotors, are becoming increasingly important in various fields, from logistics to agriculture, where they automate processes in environments that are difficult to access for humans. However, to perform effectively in uncertain environments using model-based controllers, these systems require dynamics models capable of responding to environmental variations, especially when direct access to environmental information is limited. To enable such adaptivity and facilitate integration with model predictive control, we propose an adaptive dynamics model which bypasses the need for direct environmental knowledge by inferring operational environments from state-action history. The dynamics model is based on neural ordinary equations, and a two-phase training procedure is used to learn latent environment representations. We demonstrate the effectiveness of our approach through goal-reaching and path-tracking tasks on three robotic platforms of increasing complexity: a 2D differential wheeled robot with changing wheel contact conditions, a 3D quadrotor in variational wind fields, and the Sphero BOLT robot under two contact conditions for real-world deployment. Empirical results corroborate that our method can handle temporally and spatially varying environmental changes in both simulation and real-world systems.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "86",
        "title": "SimulatorArena: Are User Simulators Reliable Proxies for Multi-Turn Evaluation of AI Assistants?",
        "author": [
            "Yao Dou",
            "Michel Galley",
            "Baolin Peng",
            "Chris Kedzie",
            "Weixin Cai",
            "Alan Ritter",
            "Chris Quirk",
            "Wei Xu",
            "Jianfeng Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05444",
        "abstract": "Large language models (LLMs) are increasingly used in interactive applications, and human evaluation remains the gold standard for assessing their performance in multi-turn conversations. Since human studies are costly, time-consuming, and hard to reproduce, recent work explores using LLMs to simulate users for automatic assistant evaluation. However, there is no benchmark or systematic study to evaluate whether these simulated users are reliable stand-ins for real users. To address this, we introduce SimulatorArena, a benchmark of 909 annotated human-LLM conversations on two interactive tasks -- math tutoring and document creation. SimulatorArena evaluates simulators based on how closely their messages match human behavior and how well their assistant ratings align with human judgments. Experiments on various simulator methods show that simulators conditioned on user profiles, capturing traits like background and message styles, align closely with human judgments. They reach Spearman's $\\rho$ of 0.7 on both tasks, providing a practical, scalable alternative to human evaluation. Using the best simulator for each task, we benchmark 18 assistants, including the latest LLMs such as GPT-5, Claude 4.1 Opus, and Gemini 2.5 Pro.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "87",
        "title": "AgentRouter: A Knowledge-Graph-Guided LLM Router for Collaborative Multi-Agent Question Answering",
        "author": [
            "Zheyuan Zhang",
            "Kaiwen Shi",
            "Zhengqing Yuan",
            "Zehong Wang",
            "Tianyi Ma",
            "Keerthiram Murugesan",
            "Vincent Galassi",
            "Chuxu Zhang",
            "Yanfang Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05445",
        "abstract": "Large language models (LLMs) and agent-based frameworks have advanced rapidly, enabling diverse applications. Yet, with the proliferation of models and agentic strategies, practitioners face substantial uncertainty in selecting the best configuration for a downstream task. Prior studies show that different agents and backbones exhibit complementary strengths, and that larger models are not always superior, underscoring the need for adaptive routing mechanisms. Existing approaches to agent routing, however, often emphasize cost efficiency while overlooking the fine-grained contextual and relational structure inherent in QA tasks. In this paper, we propose tAgentRouter, a framework that formulates multi-agent QA as a knowledge-graph-guided routing problem supervised by empirical performance signals. Specifically, we convert QA instance into a knowledge graph that jointly encodes queries, contextual entities, and agents, and then train a heterogeneous graph neural network (GNN) to propagate information across node types and produce task-aware routing distributions over agents. By leveraging soft supervision and weighted aggregation of agent outputs, AgentRouter learns principled collaboration schemes that capture the complementary strengths of diverse agents. Extensive experiments demonstrate that our framework consistently outperforms single-agent and ensemble baselines, while generalizing across benchmarks and LLM backbones. These results highlight the effectiveness and robustness of graph-supervised multi-agent routing for question answering.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "88",
        "title": "Bloom: Designing for LLM-Augmented Behavior Change Interactions",
        "author": [
            "Matthew JÃ¶rke",
            "Defne GenÃ§",
            "Valentin Teutschbein",
            "Shardul Sapkota",
            "Sarah Chung",
            "Paul Schmiedmayer",
            "Maria Ines Campero",
            "Abby C. King",
            "Emma Brunskill",
            "James A. Landay"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05449",
        "abstract": "Large language models (LLMs) offer novel opportunities to support health behavior change, yet existing work has narrowly focused on text-only interactions. Building on decades of HCI research demonstrating the effectiveness of UI-based interactions, we present Bloom, an application for physical activity promotion that integrates an LLM-based health coaching chatbot with established UI-based interactions. As part of Bloom's development, we conducted a redteaming evaluation and contribute a safety benchmark dataset. In a four-week randomized field study (N=54) comparing Bloom to a non-LLM control, we observed important shifts in psychological outcomes: participants in the LLM condition reported stronger beliefs that activity was beneficial, greater enjoyment, and more self-compassion. Both conditions significantly increased physical activity levels, doubling the proportion of participants meeting recommended weekly guidelines, though we observed no significant differences between conditions. Instead, our findings suggest that LLMs may be more effective at shifting mindsets that precede longer-term behavior change.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "89",
        "title": "What Types of Code Review Comments Do Developers Most Frequently Resolve?",
        "author": [
            "Saul Goldman",
            "Hong Yi Lin",
            "Jirat Pasuksmit",
            "Patanamon Thongtanunam",
            "Kla Tantithamthavorn",
            "Zhe Wang",
            "Ray Zhang",
            "Ali Behnaz",
            "Fan Jiang",
            "Michael Siers",
            "Ryan Jiang",
            "Mike Buller",
            "Minwoo Jeong",
            "Ming Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05450",
        "abstract": "Large language model (LLM)-powered code review automation tools have been introduced to generate code review comments. However, not all generated comments will drive code changes. Understanding what types of generated review comments are likely to trigger code changes is crucial for identifying those that are actionable. In this paper, we set out to investigate (1) the types of review comments written by humans and LLMs, and (2) the types of generated comments that are most frequently resolved by developers. To do so, we developed an LLM-as-a-Judge to automatically classify review comments based on our own taxonomy of five categories. Our empirical study confirms that (1) the LLM reviewer and human reviewers exhibit distinct strengths and weaknesses depending on the project context, and (2) readability, bugs, and maintainability-related comments had higher resolution rates than those focused on code design. These results suggest that a substantial proportion of LLM-generated comments are actionable and can be resolved by developers. Our work highlights the complementarity between LLM and human reviewers and offers suggestions to improve the practical effectiveness of LLM-powered code review tools.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "90",
        "title": "NASP-T: A Fuzzy Neuro-Symbolic Transformer for Logic-Constrained Aviation Safety Report Classification",
        "author": [
            "Fadi Al Machot",
            "Fidaa Al Machot"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05451",
        "abstract": "Deep transformer models excel at multi-label text classification but often violate domain logic that experts consider essential, an issue of particular concern in safety-critical applications. We propose a hybrid neuro-symbolic framework that integrates Answer Set Programming (ASP) with transformer-based learning on the Aviation Safety Reporting System (ASRS) corpus. Domain knowledge is formalized as weighted ASP rules and validated using the Clingo solver. These rules are incorporated in two complementary ways: (i) as rule-based data augmentation, generating logically consistent synthetic samples that improve label diversity and coverage; and (ii) as a fuzzy-logic regularizer, enforcing rule satisfaction in a differentiable form during fine-tuning. This design preserves the interpretability of symbolic reasoning while leveraging the scalability of deep neural architectures. We further tune per-class thresholds and report both standard classification metrics and logic-consistency rates. Compared to a strong Binary Cross-Entropy (BCE) baseline, our approach improves micro- and macro-F1 scores and achieves up to an 86% reduction in rule violations on the ASRS test set. To the best of our knowledge, this constitutes the first large-scale neuro-symbolic application to ASRS reports that unifies ASP-based reasoning, rule-driven augmentation, and differentiable transformer training for trustworthy, safety-critical NLP.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "91",
        "title": "Do Code Models Suffer from the Dunning-Kruger Effect?",
        "author": [
            "Mukul Singh",
            "Somya Chatterjee",
            "Arjun Radhakrishna",
            "Sumit Gulwani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05457",
        "abstract": "As artificial intelligence systems increasingly collaborate with humans in creative and technical domains, questions arise about the cognitive boundaries and biases that shape our shared agency. This paper investigates the Dunning-Kruger Effect (DKE), the tendency for those with limited competence to overestimate their abilities in state-of-the-art LLMs in coding tasks. By analyzing model confidence and performance across a diverse set of programming languages, we reveal that AI models mirror human patterns of overconfidence, especially in unfamiliar or low-resource domains. Our experiments demonstrate that less competent models and those operating in rare programming languages exhibit stronger DKE-like bias, suggesting that the strength of the bias is proportionate to the competence of the models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "92",
        "title": "SocialNLI: A Dialogue-Centric Social Inference Dataset",
        "author": [
            "Akhil Deo",
            "Kate Sanders",
            "Benjamin Van Durme"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05458",
        "abstract": "Making theory-of-mind inferences from human dialogue is a strong indicator of a model's underlying social abilities, which are fundamental for adept AI assistants. However, large language and reasoning models struggle to understand sophisticated social phenomena in transcript data, such as sarcasm and irony. To assess the weaknesses of current models and to identify their solutions, we introduce SocialNLI (SoNLI) -- the first social dialogue inference dataset. SoNLI consists of a collection of dialogue transcripts hand-picked to center complex social nuances like irony and sarcasm, paired with inferences, corresponding likelihood scores, and human-written explanations. We explore social inference analysis as a facet of theory-of-mind, and evaluate LLM and reasoning model theory-of-mind ability through multi-step counterfactual reasoning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "93",
        "title": "VAL-Bench: Measuring Value Alignment in Language Models",
        "author": [
            "Aman Gupta",
            "Denny O'Shea",
            "Fazl Barez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05465",
        "abstract": "Large language models (LLMs) are increasingly used for tasks where outputs shape human decisions, so it is critical to test whether their responses reflect consistent human values. Existing benchmarks mostly track refusals or predefined safety violations, but these only check rule compliance and do not reveal whether a model upholds a coherent value system when facing controversial real-world issues. We introduce the \\textbf{V}alue \\textbf{AL}ignment \\textbf{Bench}mark (\\textbf{VAL-Bench}), which evaluates whether models maintain a stable value stance across paired prompts that frame opposing sides of public debates. VAL-Bench consists of 115K such pairs from Wikipedia's controversial sections. A well-aligned model should express similar underlying views regardless of framing, which we measure using an LLM-as-judge to score agreement or divergence between paired responses. Applied across leading open- and closed-source models, the benchmark reveals large variation in alignment and highlights trade-offs between safety strategies (e.g., refusals) and more expressive value systems. By providing a scalable, reproducible benchmark, VAL-Bench enables systematic comparison of how reliably LLMs embody human values.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "94",
        "title": "AMAQ: Adaptive Mixed-bit Activation Quantization for Collaborative Parameter Efficient Fine-tuning",
        "author": [
            "Yurun Song",
            "Zhuoyi Yang",
            "Ian G. Harris",
            "Sangeetha Abdu Jyothi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05468",
        "abstract": "Large Language Models (LLMs) are scaling rapidly, creating significant challenges for collaborative server client distributed training, particularly in terms of communication efficiency and computational overheads. To address these challenges, we implement Parameter-efficient Split Learning, which effectively balances efficiency and performance for collaborative training on low-resource devices.\nTo reduce communication overhead in collaborative training, we introduce Adaptive Mixed bit Activation Quantization (AMAQ), a strategy that progressively compresses activations and gradients from high precision (6 to 8 bits) to low precision (3 to 4 bits). AMAQ achieves this by effectively allocating bit budgets across channels based on feature wise and layer wise importance using bit regularization.\nUnder the same bit budgets, AMAQ outperforms fixed-precision approaches, delivering about 2.5% higher generation accuracy and about 1.3% better classification accuracy for models like LLaMA3 8B and Qwen2.5 7B. In addition, it significantly enhances training stability and reducing ultra-low bit representation collapse during the training.\nExperiments demonstrate that AMAQ integrates effectively into practical multi-machine collaborative training setups, offering superior inference accuracy with only a modest communication overhead for bits adaptation during training. This trade off makes AMAQ a practical and effective solution for collaborative training with minimal communication cost.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "95",
        "title": "Vul-R2: A Reasoning LLM for Automated Vulnerability Repair",
        "author": [
            "Xin-Cheng Wen",
            "Zirui Lin",
            "Yijun Yang",
            "Cuiyun Gao",
            "Deheng Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05480",
        "abstract": "The exponential increase in software vulnerabilities has created an urgent need for automatic vulnerability repair (AVR) solutions. Recent research has formulated AVR as a sequence generation problem and has leveraged large language models (LLMs) to address this problem. Typically, these approaches prompt or fine-tune LLMs to generate repairs for vulnerabilities directly. Although these methods show state-of-the-art performance, they face the following challenges: (1) Lack of high-quality, vulnerability-related reasoning data. Current approaches primarily rely on foundation models that mainly encode general programming knowledge. Without vulnerability-related reasoning data, they tend to fail to capture the diverse vulnerability repair patterns. (2) Hard to verify the intermediate vulnerability repair process during LLM training. Existing reinforcement learning methods often leverage intermediate execution feedback from the environment (e.g., sandbox-based execution results) to guide reinforcement learning training. In contrast, the vulnerability repair process generally lacks such intermediate, verifiable feedback, which poses additional challenges for model training.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "96",
        "title": "Evaluating LLM Safety Across Child Development Stages: A Simulated Agent Approach",
        "author": [
            "Abhejay Murali",
            "Saleh Afroogh",
            "Kevin Chen",
            "David Atkinson",
            "Amit Dhurandhar",
            "Junfeng Jiao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05484",
        "abstract": "Large Language Models (LLMs) are rapidly becoming part of tools used by children; however, existing benchmarks fail to capture how these models manage language, reasoning, and safety needs that are specific to various ages. We present ChildSafe, a benchmark that evaluates LLM safety through simulated child agents that embody four developmental stages. These agents, grounded in developmental psychology, enable a systematic study of child safety without the ethical implications of involving real children. ChildSafe assesses responses across nine safety dimensions (including privacy, misinformation, and emotional support) using age-weighted scoring in both sensitive and neutral contexts. Multi-turn experiments with multiple LLMs uncover consistent vulnerabilities that vary by simulated age, exposing shortcomings in existing alignment practices. By releasing agent templates, evaluation protocols, and an experimental corpus, we provide a reproducible framework for age-aware safety research. We encourage the community to expand this work with real child-centered data and studies, advancing the development of LLMs that are genuinely safe and developmentally aligned.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "97",
        "title": "TensorBLEU: Vectorized GPU-based BLEU Score Implementation for Per-Sentence In-Training Evaluation",
        "author": [
            "Adam Filipek"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05485",
        "abstract": "Modern natural language processing models have achieved unprecedented scale, yet the tools for their evaluation often remain a computational bottleneck, limiting the pace of research. This is particularly acute for in-training evaluation metrics, such as per-sentence reward signals in Reinforcement Learning, which must operate efficiently on batches of token IDs directly on the GPU. In this paper, we introduce TensorBLEU, a novel implementation of the BLEU metric designed from the ground up for this specific use case. Our approach is fully vectorized for GPU-accelerated, per-sentence computation within PyTorch and introduces a memory-efficient counting mechanism. By creating a compact, batch-specific dictionary of n-grams using \\texttt{http://torch.unique}, our method avoids the prohibitive memory costs of traditional hashing-based vectorization, making it practical for large-vocabulary models. We benchmark TensorBLEU against NLTK, the standard library for token-ID-based BLEU calculation on the CPU. Experiments show that TensorBLEU provides speedups of over 13x on consumer-grade GPUs (NVIDIA T4) and exceeding 40x on data-center-class hardware (NVIDIA A100). This performance transforms a significant bottleneck into a negligible part of the training loop. By clearly defining its role as a \"Token-ID BLEU\" for development purposes and open-sourcing our implementation, we provide a powerful tool for accelerating research in areas like RL-based model fine-tuning.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "98",
        "title": "Language Model as Planner and Formalizer under Constraints",
        "author": [
            "Cassie Huang",
            "Stuti Mohan",
            "Ziyi Yang",
            "Stefanie Tellex",
            "Li Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05486",
        "abstract": "LLMs have been widely used in planning, either as planners to generate action sequences end-to-end, or as formalizers to represent the planning domain and problem in a formal language that can derive plans deterministically. However, both lines of work rely on standard benchmarks that only include generic and simplistic environmental specifications, leading to potential overestimation of the planning ability of LLMs and safety concerns in downstream tasks. We bridge this gap by augmenting widely used planning benchmarks with manually annotated, fine-grained, and rich natural language constraints spanning four formally defined categories. Over 4 state-of-the-art reasoning LLMs, 3 formal languages, 5 methods, and 4 datasets, we show that the introduction of constraints not only consistently halves performance, but also significantly challenges robustness to problem complexity and lexical shift.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "99",
        "title": "ArchitectHead: Continuous Level of Detail Control for 3D Gaussian Head Avatars",
        "author": [
            "Peizhi Yan",
            "Rabab Ward",
            "Qiang Tang",
            "Shan Du"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05488",
        "abstract": "3D Gaussian Splatting (3DGS) has enabled photorealistic and real-time rendering of 3D head avatars. Existing 3DGS-based avatars typically rely on tens of thousands of 3D Gaussian points (Gaussians), with the number of Gaussians fixed after training. However, many practical applications require adjustable levels of detail (LOD) to balance rendering efficiency and visual quality. In this work, we propose \"ArchitectHead\", the first framework for creating 3D Gaussian head avatars that support continuous control over LOD. Our key idea is to parameterize the Gaussians in a 2D UV feature space and propose a UV feature field composed of multi-level learnable feature maps to encode their latent features. A lightweight neural network-based decoder then transforms these latent features into 3D Gaussian attributes for rendering. ArchitectHead controls the number of Gaussians by dynamically resampling feature maps from the UV feature field at the desired resolutions. This method enables efficient and continuous control of LOD without retraining. Experimental results show that ArchitectHead achieves state-of-the-art (SOTA) quality in self and cross-identity reenactment tasks at the highest LOD, while maintaining near SOTA performance at lower LODs. At the lowest LOD, our method uses only 6.2\\% of the Gaussians while the quality degrades moderately (L1 Loss +7.9\\%, PSNR --0.97\\%, SSIM --0.6\\%, LPIPS Loss +24.1\\%), and the rendering speed nearly doubles.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "100",
        "title": "LANTERN: Scalable Distillation of Large Language Models for Job-Person Fit and Explanation",
        "author": [
            "Zhoutong Fu",
            "Yihan Cao",
            "Yi-Lin Chen",
            "Aman Lunia",
            "Liming Dong",
            "Neha Saraf",
            "Ruijie Jiang",
            "Yun Dai",
            "Qingquan Song",
            "Tan Wang",
            "Guoyao Li",
            "Derek Koh",
            "Haichao Wei",
            "Zhipeng Wang",
            "Aman Gupta",
            "Chengming Jiang",
            "Jianqiang Shen",
            "Liangjie Hong",
            "Wenjing Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05490",
        "abstract": "Large language models (LLMs) have achieved strong performance across a wide range of natural language processing tasks. However, deploying LLMs at scale for domain specific applications, such as job-person fit and explanation in job seeking platforms, introduces distinct challenges. At LinkedIn, the job person fit task requires analyzing a candidate's public profile against job requirements to produce both a fit assessment and a detailed explanation. Directly applying open source or finetuned LLMs to this task often fails to yield high quality, actionable feedback due to the complexity of the domain and the need for structured outputs. Moreover, the large size of these models leads to high inference latency and limits scalability, making them unsuitable for online use. To address these challenges, we introduce LANTERN, a novel LLM knowledge distillation framework tailored specifically for job person fit tasks. LANTERN involves modeling over multiple objectives, an encoder model for classification purpose, and a decoder model for explanation purpose. To better distill the knowledge from a strong black box teacher model to multiple downstream models, LANTERN incorporates multi level knowledge distillation that integrates both data and logit level insights. In addition to introducing the knowledge distillation framework, we share our insights on post training techniques and prompt engineering, both of which are crucial for successfully adapting LLMs to domain specific downstream tasks. Extensive experimental results demonstrate that LANTERN significantly improves task specific metrics for both job person fit and explanation. Online evaluations further confirm its effectiveness, showing measurable gains in job seeker engagement, including a 0.24\\% increase in apply rate and a 0.28\\% increase in qualified applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "101",
        "title": "NorMuon: Making Muon more efficient and scalable",
        "author": [
            "Zichong Li",
            "Liming Liu",
            "Chen Liang",
            "Weizhu Chen",
            "Tuo Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05491",
        "abstract": "The choice of optimizer significantly impacts the training efficiency and computational costs of large language models (LLMs). Recently, the Muon optimizer has demonstrated promising results by orthogonalizing parameter updates, improving optimization geometry through better conditioning. Despite Muon's emergence as a candidate successor to Adam, the potential for jointly leveraging their strengths has not been systematically explored. In this work, we bridge this gap by proposing NorMuon (Neuron-wise Normalized Muon), an optimizer that synergistically combines orthogonalization with neuron-level adaptive learning rates. Our analysis reveals that while Muon effectively reduces condition numbers, the resulting updates exhibit highly non-uniform neuron norms, causing certain neurons to dominate the optimization process. NorMuon addresses this imbalance by maintaining second-order momentum statistics for each neuron and applying row-wise normalization after orthogonalization, ensuring balanced parameter utilization while preserving Muon's conditioning benefits. To enable practical deployment at scale, we develop an efficient distributed implementation under the FSDP2 framework that strategically distributes orthogonalization computations across devices. Experiments across multiple model scales demonstrate that NorMuon consistently outperforms both Adam and Muon, achieving 21.74% better training efficiency than Adam and 11.31% improvement over Muon on 1.1 B pretraining setting, while maintaining a comparable memory footprint to Muon. Our findings suggest that orthogonalization and adaptive learning rates are complementary rather than competing approaches, opening new avenues for optimizer design in large-scale deep learning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "102",
        "title": "High-Fidelity Synthetic ECG Generation via Mel-Spectrogram Informed Diffusion Training",
        "author": [
            "Zhuoyi Huang",
            "Nutan Sahoo",
            "Anamika Kumari",
            "Girish Kumar",
            "Kexuan Cai",
            "Shixing Cao",
            "Yue Kang",
            "Tian Xia",
            "Somya Chatterjee",
            "Nicholas Hausman",
            "Aidan Jay",
            "Eric S. Rosenthal",
            "Soundar Srinivasan",
            "Sadid Hasan",
            "Alex Fedorov",
            "Sulaiman Vesal",
            "Soundar Srinivasan",
            "Sadid Hasan",
            "Alex Fedorov",
            "Sulaiman Vesal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05492",
        "abstract": "The development of machine learning for cardiac care is severely hampered by privacy restrictions on sharing real patient electrocardiogram (ECG) data. Although generative AI offers a promising solution, the real-world use of existing model-synthesized ECGs is limited by persistent gaps in trustworthiness and clinical utility. In this work, we address two major shortcomings of current generative ECG methods: insufficient morphological fidelity and the inability to generate personalized, patient-specific physiological signals. To address these gaps, we build on a conditional diffusion-based Structured State Space Model (SSSD-ECG) with two principled innovations: (1) MIDT-ECG (Mel-Spectrogram Informed Diffusion Training), a novel training paradigm with time-frequency domain supervision to enforce physiological structural realism, and (2) multi-modal demographic conditioning to enable patient-specific synthesis. We comprehensively evaluate our approach on the PTB-XL dataset, assessing the synthesized ECG signals on fidelity, clinical coherence, privacy preservation, and downstream task utility. MIDT-ECG achieves substantial gains: it improves morphological coherence, preserves strong privacy guarantees with all metrics evaluated exceeding the baseline by 4-8%, and notably reduces the interlead correlation error by an average of 74%, while demographic conditioning enhances signal-to-noise ratio and personalization. In critical low-data regimes, a classifier trained on datasets supplemented with our synthetic ECGs achieves performance comparable to a classifier trained solely on real data. Together, we demonstrate that ECG synthesizers, trained with the proposed time-frequency structural regularization scheme, can serve as personalized, high-fidelity, privacy-preserving surrogates when real data are scarce, advancing the responsible use of generative AI in healthcare.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "103",
        "title": "Mutual Information Estimation via Score-to-Fisher Bridge for Nonlinear Gaussian Noise Channels",
        "author": [
            "Tadashi Wadayama"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05496",
        "abstract": "We present a numerical method to evaluate mutual information (MI) in nonlinear Gaussian noise channels by using denoising score matching learning for estimating the score function of channel output. Via de Bruijn identity and the I--MMSE relation, Fisher information estimated from the learned score yields accurate estimates of MI through an integral representation of MI for a variety of priors and channel nonlinearities (e.g., elementwise nonlinearity). In this work, we propose a comprehensive theoretical foundation for the Score-to-Fisher bridge methodology, along with practical guidelines for its implementation. We also conduct extensive validation experiments, comparing our approach with closed-form solutions and a kernel density estimation baseline. The results of our numerical experiments demonstrate that the proposed method is both practical and efficient for mutual information estimation in nonlinear Gaussian noise channels.",
        "tags": [
            "Score Matching"
        ]
    },
    {
        "id": "104",
        "title": "Orders in Chaos: Enhancing Large-Scale MoE LLM Serving with Data Movement Forecasting",
        "author": [
            "Zhongkai Yu",
            "Yue Guan",
            "Zihao Yu",
            "Chenyang Zhou",
            "Shuyi Pei",
            "Yangwook Kang",
            "Yufei Ding",
            "Po-An Tsai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05497",
        "abstract": "Large Language Models (LLMs) with Mixture of Experts (MoE) architectures achieve remarkable performance improvements, but their random expert selection mechanism introduces significant data movement overhead that becomes the dominant bottleneck in multi-unit serving systems. To forecast the patterns underlying this data movement, we conduct comprehensive data-movement-centric profiling across three state-of-the-art large-scale MoE models (200B- 671B) using over 24,000 requests spanning diverse workloads. With the resulting 150GB+ trace files, we perform systematic analysis from both temporal and spatial perspectives and distill six key insights to guide the design of diverse future serving systems. Taking wafer-scale GPUs as a case study, we demonstrate that minor architectural modifications leveraging our insights achieve substantial performance gains, delivering 6.3X and 4.0X average speedups on DeepSeek V3 and Qwen3, respectively. Our work provides the first comprehensive data-centric analysis of MoE models at scale. Our profiling traces and analysis results are publicly available at {https://huggingface.co/datasets/core12345/MoE_expert_selection_trace. We will also release our simulation framework shortly to facilitate future research in this area.",
        "tags": [
            "DeepSeek",
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "105",
        "title": "Prototype-Based Dynamic Steering for Large Language Models",
        "author": [
            "Ceyhun Efe Kayan",
            "Li Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05498",
        "abstract": "Despite impressive breadth, LLMs still rely on explicit reasoning instructions or static, one-fits-all steering methods, leaving a gap for adaptive, instruction-free reasoning amplification. We present Prototype-Based Dynamic Steering (PDS), a test-time method that amplifies large language model (LLM) reasoning without adding or altering instructions. We introduce \"reasoning prototypes\" by clustering activation differences between Chain-of-Thought (CoT) and neutral prompts. At inference, an input's hidden state is projected onto these prototypes to form an instance-specific steering vector. Evaluated on GSM8K, AQuA-RAT, and BIG-Bench tasks, PDS consistently improves accuracy without fine-tuning or prompt engineering. Notably, the gains persist even when CoT is explicitly suppressed to improve cost-efficiency, indicating that the intervention strengthens latent reasoning processes rather than inducing a superficial behavioral shift. These results position dynamic, prototype-guided steering as a lightweight alternative to training-time approaches for enhancing LLM reasoning.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "106",
        "title": "Be Tangential to Manifold: Discovering Riemannian Metric for Diffusion Models",
        "author": [
            "Shinnosuke Saito",
            "Takashi Matsubara"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05509",
        "abstract": "Diffusion models are powerful deep generative models (DGMs) that generate high-fidelity, diverse content. However, unlike classical DGMs, they lack an explicit, tractable low-dimensional latent space that parameterizes the data manifold. This absence limits manifold-aware analysis and operations, such as interpolation and editing. Existing interpolation methods for diffusion models typically follow paths through high-density regions, which are not necessarily aligned with the data manifold and can yield perceptually unnatural transitions. To exploit the data manifold learned by diffusion models, we propose a novel Riemannian metric on the noise space, inspired by recent findings that the Jacobian of the score function captures the tangent spaces to the local data manifold. This metric encourages geodesics in the noise space to stay within or run parallel to the learned data manifold. Experiments on image interpolation show that our metric produces perceptually more natural and faithful transitions than existing density-based and naive baselines.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "107",
        "title": "Assessing Human Rights Risks in AI: A Framework for Model Evaluation",
        "author": [
            "Vyoma Raman",
            "Camille Chabot",
            "Betsy Popken"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05519",
        "abstract": "The Universal Declaration of Human Rights and other international agreements outline numerous inalienable rights that apply across geopolitical boundaries. As generative AI becomes increasingly prevalent, it poses risks to human rights such as non-discrimination, health, and security, which are also central concerns for AI researchers focused on fairness and safety. We contribute to the field of algorithmic auditing by presenting a framework to computationally assess human rights risk. Drawing on the UN Guiding Principles on Business and Human Rights, we develop an approach to evaluating a model to make grounded claims about the level of risk a model poses to particular human rights. Our framework consists of three parts: selecting tasks that are likely to pose human rights risks within a given context, designing metrics to measure the scope, scale, and likelihood of potential risks from that task, and analyzing rights with respect to the values of those metrics. Because a human rights approach centers on real-world harms, it requires evaluating AI systems in the specific contexts in which they are deployed. We present a case study of large language models in political news journalism, demonstrating how our framework helps to design an evaluation and benchmarking different models. We then discuss the implications of the results for the rights of access to information and freedom of thought and broader considerations for adopting this approach.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "108",
        "title": "CAM: A Constructivist View of Agentic Memory for LLM-Based Reading Comprehension",
        "author": [
            "Rui Li",
            "Zeyu Zhang",
            "Xiaohe Bo",
            "Zihang Tian",
            "Xu Chen",
            "Quanyu Dai",
            "Zhenhua Dong",
            "Ruiming Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05520",
        "abstract": "Current Large Language Models (LLMs) are confronted with overwhelming information volume when comprehending long-form documents. This challenge raises the imperative of a cohesive memory module, which can elevate vanilla LLMs into autonomous reading agents. Despite the emergence of some heuristic approaches, a systematic design principle remains absent. To fill this void, we draw inspiration from Jean Piaget's Constructivist Theory, illuminating three traits of the agentic memory -- structured schemata, flexible assimilation, and dynamic accommodation. This blueprint forges a clear path toward a more robust and efficient memory system for LLM-based reading comprehension. To this end, we develop CAM, a prototype implementation of Constructivist Agentic Memory that simultaneously embodies the structurality, flexibility, and dynamicity. At its core, CAM is endowed with an incremental overlapping clustering algorithm for structured memory development, supporting both coherent hierarchical summarization and online batch integration. During inference, CAM adaptively explores the memory structure to activate query-relevant information for contextual response, akin to the human associative process. Compared to existing approaches, our design demonstrates dual advantages in both performance and efficiency across diverse long-text reading comprehension tasks, including question answering, query-based summarization, and claim verification.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "109",
        "title": "KEO: Knowledge Extraction on OMIn via Knowledge Graphs and RAG for Safety-Critical Aviation Maintenance",
        "author": [
            "Kuangshi Ai",
            "Jonathan A. Karr Jr",
            "Meng Jiang",
            "Nitesh V. Chawla",
            "Chaoli Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05524",
        "abstract": "We present Knowledge Extraction on OMIn (KEO), a domain-specific knowledge extraction and reasoning framework with large language models (LLMs) in safety-critical contexts. Using the Operations and Maintenance Intelligence (OMIn) dataset, we construct a QA benchmark spanning global sensemaking and actionable maintenance tasks. KEO builds a structured Knowledge Graph (KG) and integrates it into a retrieval-augmented generation (RAG) pipeline, enabling more coherent, dataset-wide reasoning than traditional text-chunk RAG. We evaluate locally deployable LLMs (Gemma-3, Phi-4, Mistral-Nemo) and employ stronger models (GPT-4o, Llama-3.3) as judges. Experiments show that KEO markedly improves global sensemaking by revealing patterns and system-level insights, while text-chunk RAG remains effective for fine-grained procedural tasks requiring localized retrieval. These findings underscore the promise of KG-augmented LLMs for secure, domain-specific QA and their potential in high-stakes reasoning.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA",
            "RAG"
        ]
    },
    {
        "id": "110",
        "title": "Provably Mitigating Corruption, Overoptimization, and Verbosity Simultaneously in Offline and Online RLHF/DPO Alignment",
        "author": [
            "Ziyi Chen",
            "Junyi Li",
            "Peiran Yu",
            "Heng Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05526",
        "abstract": "Reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO) are important techniques to align large language models (LLM) with human preference. However, the quality of RLHF and DPO training is seriously compromised by \\textit{\\textbf{C}orrupted} preference, reward \\textit{\\textbf{O}veroptimization}, and bias towards \\textit{\\textbf{V}erbosity}. To our knowledge, most existing works tackle only one of these important issues, and the few other works require much computation to estimate multiple reward models and lack theoretical guarantee of generalization ability. In this work, we propose RLHF-\\textbf{COV} and DPO-\\textbf{COV} algorithms that can simultaneously mitigate these three issues, in both offline and online settings. This ability is theoretically demonstrated by obtaining length-regularized generalization error rates for our DPO-COV algorithms trained on corrupted data, which match the best-known rates for simpler cases with clean data and without length regularization. Moreover, our DPO-COV algorithm is simple to implement without reward estimation, and is proved to be equivalent to our RLHF-COV algorithm, which directly implies the equivalence between the vanilla RLHF and DPO algorithms. Experiments demonstrate the effectiveness of our DPO-COV algorithms under both offline and online settings.",
        "tags": [
            "DPO",
            "LLM",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "111",
        "title": "ARMOR: High-Performance Semi-Structured Pruning via Adaptive Matrix Factorization",
        "author": [
            "Lawrence Liu",
            "Alexander Liu",
            "Mengdi Wang",
            "Tuo Zhao",
            "Lin F. Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05528",
        "abstract": "Large language models (LLMs) present significant deployment challenges due to their immense computational and memory requirements. While semi-structured pruning, particularly 2:4 sparsity, offers a path to practical hardware acceleration, existing methods often incur substantial performance degradation. To bridge this gap, we introduce ARMOR: (Adaptive Representation with Matrix-factORization), a novel one-shot post-training pruning algorithm. Instead of directly pruning weights, ARMOR factorizes each weight matrix into a 2:4 sparse core wrapped by two low-overhead, block diagonal matrices. These wrappers act as efficient pre and post-transformation error correctors, offering greater flexibility to preserve model quality compared to conventional 2:4 pruning techniques. The sparse core and block diagonal wrappers are chosen through a block coordinate descent algorithm that minimizes a layer-wise proxy loss. We theoretically prove this optimization is guaranteed to converge to a solution with a proxy loss less than or equal to state-of-the-art pruning algorithms. Experiments on Llama (Touvron et al., 2023; Dubey et al., 2024) and Qwen (Yang et al., 2025) model families demonstrate that ARMOR consistently and significantly outperforms state-of-the-art 2:4 pruning methods across a wide range of downstream tasks and perplexity evaluations. ARMOR achieves this superior performance while retaining the inference speedups and substantial memory usage reductions of 2:4 pruning, establishing a more effective trade-off between model compression and task accuracy",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "112",
        "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model Inference",
        "author": [
            "Harshil Vejendla"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05529",
        "abstract": "Autoregressive decoding in large language models (LLMs) requires caching a growing list of past key-value (KV) pairs, making long-context inference a memory-bound problem. While recent methods have explored quantizing the cache, evicting tokens, or using binary sketches for keys (e.g., Loki), these approaches often provide an incomplete solution by leaving one component (like values) uncompressed or by discarding context information. This paper introduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression scheme that radically reduces memory usage without sacrificing context. H1B-KV represents each key vector using a 1-bit binary sketch, enabling hardware-friendly bitwise attention, and further compresses value vectors using 4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter LLM to handle an 8k-token context with under 60 MB of cache memory - a 70x reduction. We demonstrate that after a lightweight finetuning, H1B-KV matches full-precision performance not only on perplexity benchmarks but also on complex downstream tasks like mathematical reasoning (GSM8K), multi-task understanding (MMLU), and code generation (HumanEval). Our results show H1B-KV significantly outperforms leading quantization (KIVI), token eviction (SparseLLM), and key-only sketching (Loki) methods in quality-per-byte, establishing it as a robust solution for deploying LLMs in memory-constrained environments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "113",
        "title": "Teamwork: Collaborative Diffusion with Low-rank Coordination and Adaptation",
        "author": [
            "Sam Sartor",
            "Pieter Peers"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05532",
        "abstract": "Large pretrained diffusion models can provide strong priors beneficial for many graphics applications. However, generative applications such as neural rendering and inverse methods such as SVBRDF estimation and intrinsic image decomposition require additional input or output channels. Current solutions for channel expansion are often application specific and these solutions can be difficult to adapt to different diffusion models or new tasks. This paper introduces Teamwork: a flexible and efficient unified solution for jointly increasing the number of input and output channels as well as adapting a pretrained diffusion model to new tasks. Teamwork achieves channel expansion without altering the pretrained diffusion model architecture by coordinating and adapting multiple instances of the base diffusion model (\\ie, teammates). We employ a novel variation of Low Rank-Adaptation (LoRA) to jointly address both adaptation and coordination between the different teammates. Furthermore Teamwork supports dynamic (de)activation of teammates. We demonstrate the flexibility and efficiency of Teamwork on a variety of generative and inverse graphics tasks such as inpainting, single image SVBRDF estimation, intrinsic decomposition, neural shading, and intrinsic image synthesis.",
        "tags": [
            "Diffusion",
            "Inpainting",
            "LoRA"
        ]
    },
    {
        "id": "114",
        "title": "On the Role of Difficult Prompts in Self-Play Preference Optimization",
        "author": [
            "Yao Xiao",
            "Jung-jae Kim",
            "Roy Ka-wei Lee",
            "Lidong Bing"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05534",
        "abstract": "Self-play preference optimization has emerged as a prominent paradigm for aligning large language models (LLMs). It typically involves a language model to generate on-policy responses for prompts and a reward model (RM) to guide the selection of chosen and rejected responses, which can be further trained with direct preference optimization (DPO). However, the role of prompts remains underexplored, despite being a core component in this pipeline. In this work, we investigate how prompts of varying difficulty influence self-play preference optimization. We first use the mean reward of $N$ sampled responses of a prompt as a proxy for its difficulty. We find that difficult prompts exhibit substantially inferior self-play optimization performance in comparison to easy prompts for language models. Moreover, incorporating difficult prompts into training fails to enhance overall performance and, in fact, leads to slight degradation compared to training on easy prompts alone. We also observe that the performance gap between difficult and easy prompts closes as the model capacity increases, suggesting that difficulty interacts with the model capacity. Building on these findings, we explore strategies to mitigate the negative effect of difficult prompts on final performance. We demonstrate that selectively removing an appropriate portion of challenging prompts enhances overall self-play performance, while also reporting failed attempts and lessons learned.",
        "tags": [
            "DPO",
            "LLM"
        ]
    },
    {
        "id": "115",
        "title": "Correlation-Aware Dual-View Pose and Velocity Estimation for Dynamic Robotic Manipulation",
        "author": [
            "Mahboubeh Zarei",
            "Robin Chhabra",
            "Farrokh Janabi-Sharifi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05536",
        "abstract": "Accurate pose and velocity estimation is essential for effective spatial task planning in robotic manipulators. While centralized sensor fusion has traditionally been used to improve pose estimation accuracy, this paper presents a novel decentralized fusion approach to estimate both pose and velocity. We use dual-view measurements from an eye-in-hand and an eye-to-hand vision sensor configuration mounted on a manipulator to track a target object whose motion is modeled as random walk (stochastic acceleration model). The robot runs two independent adaptive extended Kalman filters formulated on a matrix Lie group, developed as part of this work. These filters predict poses and velocities on the manifold $\\mathbb{SE}(3) \\times \\mathbb{R}^3 \\times \\mathbb{R}^3$ and update the state on the manifold $\\mathbb{SE}(3)$. The final fused state comprising the fused pose and velocities of the target is obtained using a correlation-aware fusion rule on Lie groups. The proposed method is evaluated on a UFactory xArm 850 equipped with Intel RealSense cameras, tracking a moving target. Experimental results validate the effectiveness and robustness of the proposed decentralized dual-view estimation framework, showing consistent improvements over state-of-the-art methods.",
        "tags": [
            "Pose Estimation",
            "Robotics"
        ]
    },
    {
        "id": "116",
        "title": "Seeing the Big Picture: Evaluating Multimodal LLMs' Ability to Interpret and Grade Handwritten Student Work",
        "author": [
            "Owen Henkel",
            "Bill Roberts",
            "Doug Jaffe",
            "Laurence Holt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05538",
        "abstract": "Recent advances in multimodal large language models (MLLMs) raise the question of their potential for grading, analyzing, and offering feedback on handwritten student classwork. This capability would be particularly beneficial in elementary and middle-school mathematics education, where most work remains handwritten, because seeing students' full working of a problem provides valuable insights into their learning processes, but is extremely time-consuming to grade. We present two experiments investigating MLLM performance on handwritten student mathematics classwork. Experiment A examines 288 handwritten responses from Ghanaian middle school students solving arithmetic problems with objective answers. In this context, models achieved near-human accuracy (95%, k = 0.90) but exhibited occasional errors that human educators would be unlikely to make. Experiment B evaluates 150 mathematical illustrations from American elementary students, where the drawings are the answer to the question. These tasks lack single objective answers and require sophisticated visual interpretation as well as pedagogical judgment in order to analyze and evaluate them. We attempted to separate MLLMs' visual capabilities from their pedagogical abilities by first asking them to grade the student illustrations directly, and then by augmenting the image with a detailed human description of the illustration. We found that when the models had to analyze the student illustrations directly, they struggled, achieving only k = 0.20 with ground truth scores, but when given human descriptions, their agreement levels improved dramatically to k = 0.47, which was in line with human-to-human agreement levels. This gap suggests MLLMs can \"see\" and interpret arithmetic work relatively well, but still struggle to \"see\" student mathematical illustrations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "117",
        "title": "Sci-Phi: A Large Language Model Spatial Audio Descriptor",
        "author": [
            "Xilin Jiang",
            "Hannes Gamper",
            "Sebastian Braun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05542",
        "abstract": "Acoustic scene perception involves describing the type of sounds, their timing, their direction and distance, as well as their loudness and reverberation. While audio language models excel in sound recognition, single-channel input fundamentally limits spatial understanding. This work presents Sci-Phi, a spatial audio large language model with dual spatial and spectral encoders that estimates a complete parameter set for all sound sources and the surrounding environment. Learning from over 4,000 hours of synthetic first-order Ambisonics recordings including metadata, Sci-Phi enumerates and describes up to four directional sound sources in one pass, alongside non-directional background sounds and room characteristics. We evaluate the model with a permutation-invariant protocol and 15 metrics covering content, location, timing, loudness, and reverberation, and analyze its robustness across source counts, signal-to-noise ratios, reverberation levels, and challenging mixtures of acoustically, spatially, or temporally similar sources. Notably, Sci-Phi generalizes to real room impulse responses with only minor performance degradation. Overall, this work establishes the first audio LLM capable of full spatial-scene description, with strong potential for real-world deployment. Demo: https://sci-phi-audio.github.io/demo",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "118",
        "title": "Activation-Informed Pareto-Guided Low-Rank Compression for Efficient LLM/VLM",
        "author": [
            "Ryan Solgi",
            "Parsa Madinei",
            "Jiayi Tian",
            "Rupak Swaminathan",
            "Jing Liu",
            "Nathan Susanj",
            "Zheng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05544",
        "abstract": "Large language models (LLM) and vision-language models (VLM) have achieved state-of-the-art performance, but they impose significant memory and computing challenges in deployment. We present a novel low-rank compression framework to address this challenge. First, we upper bound the change of network loss via layer-wise activation-based compression errors, filling a theoretical gap in the literature. We then formulate low-rank model compression as a bi-objective optimization and prove that a single uniform tolerance yields surrogate Pareto-optimal heterogeneous ranks. Based on our theoretical insights, we propose Pareto-Guided Singular Value Decomposition (PGSVD), a zero-shot pipeline that improves activation-aware compression via Pareto-guided rank selection and alternating least-squares implementation. We apply PGSVD to both LLM and VLM, showing better accuracy at the same compression levels and inference speedup.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "119",
        "title": "ARRC: Advanced Reasoning Robot Control - Knowledge-Driven Autonomous Manipulation Using Retrieval-Augmented Generation",
        "author": [
            "Eugene Vorobiov",
            "Ammar Jaleel Mahmood",
            "Salim Rezvani",
            "Robin Chhabra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05547",
        "abstract": "We present ARRC (Advanced Reasoning Robot Control), a practical system that connects natural-language instructions to safe local robotic control by combining Retrieval-Augmented Generation (RAG) with RGB-D perception and guarded execution on an affordable robot arm. The system indexes curated robot knowledge (movement patterns, task templates, and safety heuristics) in a vector database, retrieves task-relevant context for each instruction, and conditions a large language model (LLM) to produce JSON-structured action plans. Plans are executed on a UFactory xArm 850 fitted with a Dynamixel-driven parallel gripper and an Intel RealSense D435 camera. Perception uses AprilTag detections fused with depth to produce object-centric metric poses. Execution is enforced via software safety gates: workspace bounds, speed and force caps, timeouts, and bounded retries. We describe the architecture, knowledge design, integration choices, and a reproducible evaluation protocol for tabletop scan, approach, and pick-place tasks. Experimental results demonstrate the efficacy of the proposed approach. Our design shows that RAG-based planning can substantially improve plan validity and adaptability while keeping perception and low-level control local to the robot.",
        "tags": [
            "LLM",
            "RAG",
            "Robotics"
        ]
    },
    {
        "id": "120",
        "title": "Critical attention scaling in long-context transformers",
        "author": [
            "Shi Chen",
            "Zhengjiang Lin",
            "Yury Polyanskiy",
            "Philippe Rigollet"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05554",
        "abstract": "As large language models scale to longer contexts, attention layers suffer from a fundamental pathology: attention scores collapse toward uniformity as context length $n$ increases, causing tokens to cluster excessively, a phenomenon known as rank-collapse. While $\\textit{attention scaling}$ effectively addresses this deficiency by rescaling attention scores with a polylogarithmic factor $\\beta_n$, theoretical justification for this approach remains lacking.\nWe analyze a simplified yet tractable model that magnifies the effect of attention scaling. In this model, attention exhibits a phase transition governed by the scaling factor $\\beta_n$: insufficient scaling collapses all tokens to a single direction, while excessive scaling reduces attention to identity, thereby eliminating meaningful interactions between tokens. Our main result identifies the critical scaling $\\beta_n \\asymp \\log n$ and provides a rigorous justification for attention scaling in YaRN and Qwen, clarifying why logarithmic scaling maintains sparse, content-adaptive attention at large context lengths.",
        "tags": [
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "121",
        "title": "Toward Systems Foundations for Agentic Exploration",
        "author": [
            "Jiakai Xu",
            "Tianle Zhou",
            "Eugene Wu",
            "Kostis Kaffes"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05556",
        "abstract": "Agentic exploration, letting LLM-powered agents branch, backtrack, and search across many execution paths, demands systems support well beyond today's pass-at-k resets. Our benchmark of six snapshot/restore mechanisms shows that generic tools such as CRIU or container commits are not fast enough even in isolated testbeds, and they crumble entirely in real deployments where agents share files, sockets, and cloud APIs with other agents and human users. In this talk, we pinpoint three open fundamental challenges: fork semantics, which concerns how branches reveal or hide tentative updates; external side-effects, where fork awareness must be added to services or their calls intercepted; and native forking, which requires cloning databases and runtimes in microseconds without bulk copying.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "122",
        "title": "HoloScene: Simulation-Ready Interactive 3D Worlds from a Single Video",
        "author": [
            "Hongchi Xia",
            "Chih-Hao Lin",
            "Hao-Yu Hsu",
            "Quentin Leboutet",
            "Katelyn Gao",
            "Michael Paulitsch",
            "Benjamin Ummenhofer",
            "Shenlong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05560",
        "abstract": "Digitizing the physical world into accurate simulation-ready virtual environments offers significant opportunities in a variety of fields such as augmented and virtual reality, gaming, and robotics. However, current 3D reconstruction and scene-understanding methods commonly fall short in one or more critical aspects, such as geometry completeness, object interactivity, physical plausibility, photorealistic rendering, or realistic physical properties for reliable dynamic simulation. To address these limitations, we introduce HoloScene, a novel interactive 3D reconstruction framework that simultaneously achieves these requirements. HoloScene leverages a comprehensive interactive scene-graph representation, encoding object geometry, appearance, and physical properties alongside hierarchical and inter-object relationships. Reconstruction is formulated as an energy-based optimization problem, integrating observational data, physical constraints, and generative priors into a unified, coherent objective. Optimization is efficiently performed via a hybrid approach combining sampling-based exploration with gradient-based refinement. The resulting digital twins exhibit complete and precise geometry, physical stability, and realistic rendering from novel viewpoints. Evaluations conducted on multiple benchmark datasets demonstrate superior performance, while practical use-cases in interactive gaming and real-time digital-twin manipulation illustrate HoloScene's broad applicability and effectiveness. Project page: https://xiahongchi.github.io/HoloScene.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "123",
        "title": "Presenting a Paper is an Art: Self-Improvement Aesthetic Agents for Academic Presentations",
        "author": [
            "Chengzhi Liu",
            "Yuzhe Yang",
            "Kaiwen Zhou",
            "Zhen Zhang",
            "Yue Fan",
            "Yannan Xie",
            "Peng Qi",
            "Xin Eric Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05571",
        "abstract": "The promotion of academic papers has become an important means of enhancing research visibility. However, existing automated methods struggle limited storytelling, insufficient aesthetic quality, and constrained self-adjustment, making it difficult to achieve efficient and engaging dissemination. At the heart of those challenges is a simple principle: \\emph{there is no way to improve it when you cannot evaluate it right}. To address this, we introduce \\textbf{EvoPresent}, a self-improvement agent framework that unifies coherent narratives, aesthetic-aware designs, and realistic presentation delivery via virtual characters. Central to EvoPresent is \\textbf{PresAesth}, a multi-task reinforcement learning (RL) aesthetic model that provides reliable aesthetic scoring, defect adjustment, and comparative feedback, enabling iterative self-improvement even under limited aesthetic training data. To systematically evaluate the methods, we introduce \\textbf{EvoPresent Benchmark}, a comprehensive benchmark comprising: \\textit{Presentation Generation Quality}, built on 650 top-tier AI conference papers with multimodal resources (slides, videos and scripts) to assess both content and design; and \\textit{Aesthetic Awareness}, consisting of 2,000 slide pairs with varying aesthetic levels, supporting joint training and evaluation on scoring, defect adjustment, and comparison. Our findings highlight that (i) High-quality feedback is essential for agent self-improvement, while initial capability alone does not guarantee effective self-correction. (ii) Automated generation pipelines exhibit a trade-off between visual design and content construction. (iii) Multi-task RL training shows stronger generalization in aesthetic awareness tasks.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "124",
        "title": "Mission Impossible: Feedback-Guided Dynamic Interactive Planning for Improving Reasoning on LLMs",
        "author": [
            "Dong Yan",
            "Gaochen Wu",
            "Bowen Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05577",
        "abstract": "Recent advancements in language agents have led to significant improvements in multi-hop reasoning tasks. However, existing approaches often struggle with handling open-domain problems, which require massive information retrieval due to their reliance on a fixed sequence of actions. To address this, we propose Feedback-Guided Dynamic Interactive Planning (FGDIP), a novel framework tailored to enhance reasoning in LLMs by utilizing dynamic and adaptive strategies for information exploration in open-domain multi-hop reasoning tasks. Our approach begins by identifying key entities relevant to the problem, which serve as the initial nodes in the reasoning process. From these initial nodes, we then generate reasoning child nodes with the process being refined through a combination of historical error analysis and real-time feedback, which allows the framework to dynamically adjust and optimize its reasoning strategies. By integrating depth-first search with an innovative node generation technique, our framework adapts based on both prior error paths and concurrently generated nodes at the same hierarchical level. This dynamic strategy effectively expands the search space while ensuring the reasoning process systematically converges toward accurate solutions. Experimental results show that FGDIP achieved up to 54.47% F1 score on the HotpotQA dataset and 70.05% on the StrategyQA dataset, surpassing the best baseline by 5.03% and 7.25% respectively, highlighting its versatility and potential to enhance language agents in multi-hop reasoning tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "125",
        "title": "(Token-Level) \\textbf{InfoRMIA}: Stronger Membership Inference and Memorization Assessment for LLMs",
        "author": [
            "Jiashu Tao",
            "Reza Shokri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05582",
        "abstract": "Machine learning models are known to leak sensitive information, as they inevitably memorize (parts of) their training data. More alarmingly, large language models (LLMs) are now trained on nearly all available data, which amplifies the magnitude of information leakage and raises serious privacy risks. Hence, it is more crucial than ever to quantify privacy risk before the release of LLMs. The standard method to quantify privacy is via membership inference attacks, where the state-of-the-art approach is the Robust Membership Inference Attack (RMIA). In this paper, we present InfoRMIA, a principled information-theoretic formulation of membership inference. Our method consistently outperforms RMIA across benchmarks while also offering improved computational efficiency.\nIn the second part of the paper, we identify the limitations of treating sequence-level membership inference as the gold standard for measuring leakage. We propose a new perspective for studying membership and memorization in LLMs: token-level signals and analyses. We show that a simple token-based InfoRMIA can pinpoint which tokens are memorized within generated outputs, thereby localizing leakage from the sequence level down to individual tokens, while achieving stronger sequence-level inference power on LLMs. This new scope rethinks privacy in LLMs and can lead to more targeted mitigation, such as exact unlearning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "126",
        "title": "CalibCLIP: Contextual Calibration of Dominant Semantics for Text-Driven Image Retrieval",
        "author": [
            "Bin Kang",
            "Bin Chen",
            "Junjie Wang",
            "Yulin Li",
            "Junzhi Zhao",
            "Zhuotao Tian"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05586",
        "abstract": "Existing Visual Language Models (VLMs) suffer structural limitations where a few low contribution tokens may excessively capture global semantics, dominating the information aggregation process and suppressing the discriminative features in text-driven image retrieval tasks. To address this, we introduce \\textbf{CalibCLIP}, a training-free method designed to calibrate the suppressive effect of dominant tokens. Specifically, in the visual space, we propose the Contrastive Visual Enhancer (CVE), which decouples visual features into target and low information regions. Subsequently, it identifies dominant tokens and dynamically suppresses their http://representations.In the textual space, we introduce the Discriminative Concept Calibrator (DCC), which aims to differentiate between general and discriminative concepts within the text query. By mitigating the challenges posed by generic concepts and improving the representations of discriminative concepts, DCC strengthens the differentiation among similar samples. Finally, extensive experiments demonstrate consistent improvements across seven benchmarks spanning three image retrieval tasks, underscoring the effectiveness of CalibCLIP. Code is available at: https://github.com/kangbin98/CalibCLIP",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "127",
        "title": "Deciphering Invariant Feature Decoupling in Source-free Time Series Forecasting with Proxy Denoising",
        "author": [
            "Kangjia Yan",
            "Chenxi Liu",
            "Hao Miao",
            "Xinle Wu",
            "Yan Zhao",
            "Chenjuan Guo",
            "Bin Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05589",
        "abstract": "The proliferation of mobile devices generates a massive volume of time series across various domains, where effective time series forecasting enables a variety of real-world applications. This study focuses on a new problem of source-free domain adaptation for time series forecasting. It aims to adapt a pretrained model from sufficient source time series to the sparse target time series domain without access to the source data, embracing data protection regulations. To achieve this, we propose TimePD, the first source-free time series forecasting framework with proxy denoising, where large language models (LLMs) are employed to benefit from their generalization capabilities. Specifically, TimePD consists of three key components: (1) dual-branch invariant disentangled feature learning that enforces representation- and gradient-wise invariance by means of season-trend decomposition; (2) lightweight, parameter-free proxy denoising that dynamically calibrates systematic biases of LLMs; and (3) knowledge distillation that bidirectionally aligns the denoised prediction and the original target prediction. Extensive experiments on real-world datasets offer insight into the effectiveness of the proposed TimePD, outperforming SOTA baselines by 9.3% on average.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "128",
        "title": "In-the-Flow Agentic System Optimization for Effective Planning and Tool Use",
        "author": [
            "Zhuofeng Li",
            "Haoxiang Zhang",
            "Seungju Han",
            "Sheng Liu",
            "Jianwen Xie",
            "Yu Zhang",
            "Yejin Choi",
            "James Zou",
            "Pan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05592",
        "abstract": "Outcome-driven reinforcement learning has advanced reasoning in large language models (LLMs), but prevailing tool-augmented approaches train a single, monolithic policy that interleaves thoughts and tool calls under full context; this scales poorly with long horizons and diverse tools and generalizes weakly to new scenarios. Agentic systems offer a promising alternative by decomposing work across specialized modules, yet most remain training-free or rely on offline training decoupled from the live dynamics of multi-turn interaction. We introduce AgentFlow, a trainable, in-the-flow agentic framework that coordinates four modules (planner, executor, verifier, generator) through an evolving memory and directly optimizes its planner inside the multi-turn loop. To train on-policy in live environments, we propose Flow-based Group Refined Policy Optimization (Flow-GRPO), which tackles long-horizon, sparse-reward credit assignment by converting multi-turn optimization into a sequence of tractable single-turn policy updates. It broadcasts a single, verifiable trajectory-level outcome to every turn to align local planner decisions with global success and stabilizes learning with group-normalized advantages. Across ten benchmarks, AgentFlow with a 7B-scale backbone outperforms top-performing baselines with average accuracy gains of 14.9% on search, 14.0% on agentic, 14.5% on mathematical, and 4.1% on scientific tasks, even surpassing larger proprietary models like GPT-4o. Further analyses confirm the benefits of in-the-flow optimization, showing improved planning, enhanced tool-calling reliability, and positive scaling with model size and reasoning turns.",
        "tags": [
            "GPT",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "129",
        "title": "Improving Chain-of-Thought Efficiency for Autoregressive Image Generation",
        "author": [
            "Zeqi Gu",
            "Markos Georgopoulos",
            "Xiaoliang Dai",
            "Marjan Ghazvininejad",
            "Chu Wang",
            "Felix Juefei-Xu",
            "Kunpeng Li",
            "Yujun Shi",
            "Zecheng He",
            "Zijian He",
            "Jiawei Zhou",
            "Abe Davis",
            "Jialiang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05593",
        "abstract": "Autoregressive multimodal large language models have recently gained popularity for image generation, driven by advances in foundation models. To enhance alignment and detail, newer approaches employ chain-of-thought (CoT) reasoning, expanding user inputs into elaborated prompts prior to image synthesis. However, this strategy can introduce unnecessary redundancy -- a phenomenon we call visual overthinking -- which increases computational costs and can introduce details that contradict the original prompt. In this work, we explore how to generate more concise CoT sequences for more efficient image generation. We introduce ShortCoTI, a lightweight optimization framework that encourages more concise CoT while preserving output image quality. ShortCoTI rewards more concise prompts with an adaptive function that scales according to an estimated difficulty for each task. Incorporating this reward into a reinforcement learning paradigm reduces prompt reasoning length by 54% while maintaining or slightly improving quality metrics across multiple benchmarks (T2I-CompBench, GenEval). Qualitative analysis shows that our method eliminates verbose explanations and repetitive refinements, producing reasoning prompts that are both concise and semantically rich. As a result, ShortCoTI improves computational efficiency without compromising the fidelity or visual appeal of generated images.",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "130",
        "title": "From Agentification to Self-Evolving Agentic AI for Wireless Networks: Concepts, Approaches, and Future Research Directions",
        "author": [
            "Changyuan Zhao",
            "Ruichen Zhang",
            "Jiacheng Wang",
            "Dusit Niyato",
            "Geng Sun",
            "Xianbin Wang",
            "Shiwen Mao",
            "Abbas Jamalipour"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05596",
        "abstract": "Self-evolving agentic artificial intelligence (AI) offers a new paradigm for future wireless systems by enabling autonomous agents to continually adapt and improve without human intervention. Unlike static AI models, self-evolving agents embed an autonomous evolution cycle that updates models, tools, and workflows in response to environmental dynamics. This paper presents a comprehensive overview of self-evolving agentic AI, highlighting its layered architecture, life cycle, and key techniques, including tool intelligence, workflow optimization, self-reflection, and evolutionary learning. We further propose a multi-agent cooperative self-evolving agentic AI framework, where multiple large language models (LLMs) are assigned role-specialized prompts under the coordination of a supervisor agent. Through structured dialogue, iterative feedback, and systematic validation, the system autonomously executes the entire life cycle without human intervention. A case study on antenna evolution in low-altitude wireless networks (LAWNs) demonstrates how the framework autonomously upgrades fixed antenna optimization into movable antenna optimization. Experimental results show that the proposed self-evolving agentic AI autonomously improves beam gain and restores degraded performance by up to 52.02%, consistently surpassing the fixed baseline with little to no human intervention and validating its adaptability and robustness for next-generation wireless intelligence.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "131",
        "title": "AutoPentester: An LLM Agent-based Framework for Automated Pentesting",
        "author": [
            "Yasod Ginige",
            "Akila Niroshan",
            "Sajal Jain",
            "Suranga Seneviratne"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05605",
        "abstract": "Penetration testing and vulnerability assessment are essential industry practices for safeguarding computer systems. As cyber threats grow in scale and complexity, the demand for pentesting has surged, surpassing the capacity of human professionals to meet it effectively. With advances in AI, particularly Large Language Models (LLMs), there have been attempts to automate the pentesting process. However, existing tools such as PentestGPT are still semi-manual, requiring significant professional human interaction to conduct pentests. To this end, we propose a novel LLM agent-based framework, AutoPentester, which automates the pentesting process. Given a target IP, AutoPentester automatically conducts pentesting steps using common security tools in an iterative process. It can dynamically generate attack strategies based on the tool outputs from the previous iteration, mimicking the human pentester approach. We evaluate AutoPentester using Hack The Box and custom-made VMs, comparing the results with the state-of-the-art PentestGPT. Results show that AutoPentester achieves a 27.0% better subtask completion rate and 39.5% more vulnerability coverage with fewer steps. Most importantly, it requires significantly fewer human interactions and interventions compared to PentestGPT. Furthermore, we recruit a group of security industry professional volunteers for a user survey and perform a qualitative analysis to evaluate AutoPentester against industry practices and compare it with PentestGPT. On average, AutoPentester received a score of 3.93 out of 5 based on user reviews, which was 19.8% higher than PentestGPT.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "132",
        "title": "A Goal Without a Plan Is Just a Wish: Efficient and Effective Global Planner Training for Long-Horizon Agent Tasks",
        "author": [
            "Shuzheng Si",
            "Haozhe Zhao",
            "Kangyang Luo",
            "Gang Chen",
            "Fanchao Qi",
            "Minjia Zhang",
            "Baobao Chang",
            "Maosong Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05608",
        "abstract": "Agents based on large language models (LLMs) struggle with brainless trial-and-error and generating hallucinatory actions due to a lack of global planning in long-horizon tasks. In this paper, we introduce a plan-and-execute framework and propose EAGLET, an efficient and effective planner training method to enhance the executor agent's planning abilities without human effort. Specifically, we train a plug-and-play global planner through a two-step process: we first synthesize high-quality plans from an advanced LLM using our proposed homologous consensus filtering strategy, and apply fine-tuning as a cold start. Moreover, we further improve the planner with a rule-based reinforcement learning stage using a novel executor capability gain reward, ensuring it can handle task instructions of varying difficulty. Experiments on three long-horizon agent tasks show that executor agents equipped with our planner outperform existing methods, achieving new state-of-the-art performance. Meanwhile, EAGLET reduces training costs by 8x compared to RL-based baselines, and it does not require manual effort or extra training data, offering an efficient and effective solution.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "133",
        "title": "HOI-R1: Exploring the Potential of Multimodal Large Language Models for Human-Object Interaction Detection",
        "author": [
            "Junwen Chen",
            "Peilin Xiong",
            "Keiji Yanai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05609",
        "abstract": "Recent Human-object interaction detection (HOID) methods highly require prior knowledge from VLMs to enhance the interaction recognition capabilities. The training strategies and model architectures for connecting the knowledge from VLMs to the HOI instance representations from the object detector are challenging, and the whole framework is complex for further development or application. On the other hand, the inherent reasoning abilities of MLLMs on human-object interaction detection are under-explored. Inspired by the recent success of training MLLMs with reinforcement learning (RL) methods, we propose HOI-R1 and first explore the potential of the language model on the HOID task without any additional detection modules. We introduce an HOI reasoning process and HOID reward functions to solve the HOID task by pure text. The results on the HICO-DET dataset show that HOI-R1 achieves 2x the accuracy of the baseline with great generalization ability. The source code is available at https://github.com/cjw2021/HOI-R1.",
        "tags": [
            "Detection",
            "LLM",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "134",
        "title": "Efficient Conditional Generation on Scale-based Visual Autoregressive Models",
        "author": [
            "Jiaqi Liu",
            "Tao Huang",
            "Chang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05610",
        "abstract": "Recent advances in autoregressive (AR) models have demonstrated their potential to rival diffusion models in image synthesis. However, for complex spatially-conditioned generation, current AR approaches rely on fine-tuning the pre-trained model, leading to significant training costs. In this paper, we propose the Efficient Control Model (ECM), a plug-and-play framework featuring a lightweight control module that introduces control signals via a distributed architecture. This architecture consists of context-aware attention layers that refine conditional features using real-time generated tokens, and a shared gated feed-forward network (FFN) designed to maximize the utilization of its limited capacity and ensure coherent control feature learning. Furthermore, recognizing the critical role of early-stage generation in determining semantic structure, we introduce an early-centric sampling strategy that prioritizes learning early control sequences. This approach reduces computational cost by lowering the number of training tokens per iteration, while a complementary temperature scheduling during inference compensates for the resulting insufficient training of late-stage tokens. Extensive experiments on scale-based AR models validate that our method achieves high-fidelity and diverse control over image generation, surpassing existing baselines while significantly improving both training and inference efficiency.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "135",
        "title": "MADIAVE: Multi-Agent Debate for Implicit Attribute Value Extraction",
        "author": [
            "Wei-Chieh Huang",
            "Cornelia Caragea"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05611",
        "abstract": "Implicit Attribute Value Extraction (AVE) is essential for accurately representing products in e-commerce, as it infers lantent attributes from multimodal data. Despite advances in multimodal large language models (MLLMs), implicit AVE remains challenging due to the complexity of multidimensional data and gaps in vision-text understanding. In this work, we introduce \\textsc{\\modelname}, a multi-agent debate framework that employs multiple MLLM agents to iteratively refine inferences. Through a series of debate rounds, agents verify and update each other's responses, thereby improving inference performance and robustness. Experiments on the ImplicitAVE dataset demonstrate that even a few rounds of debate significantly boost accuracy, especially for attributes with initially low performance. We systematically evaluate various debate configurations, including identical or different MLLM agents, and analyze how debate rounds affect convergence dynamics. Our findings highlight the potential of multi-agent debate strategies to address the limitations of single-agent approaches and offer a scalable solution for implicit AVE in multimodal e-commerce.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "136",
        "title": "PointNSP: Autoregressive 3D Point Cloud Generation with Next-Scale Level-of-Detail Prediction",
        "author": [
            "Ziqiao Meng",
            "Qichao Wang",
            "Zhiyang Dou",
            "Zixing Song",
            "Zhipeng Zhou",
            "Irwin King",
            "Peilin Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05613",
        "abstract": "Autoregressive point cloud generation has long lagged behind diffusion-based approaches in quality. The performance gap stems from the fact that autoregressive models impose an artificial ordering on inherently unordered point sets, forcing shape generation to proceed as a sequence of local predictions. This sequential bias emphasizes short-range continuity but undermines the model's capacity to capture long-range dependencies, hindering its ability to enforce global structural properties such as symmetry, consistent topology, and large-scale geometric regularities. Inspired by the level-of-detail (LOD) principle in shape modeling, we propose PointNSP, a coarse-to-fine generative framework that preserves global shape structure at low resolutions and progressively refines fine-grained geometry at higher scales through a next-scale prediction paradigm. This multi-scale factorization aligns the autoregressive objective with the permutation-invariant nature of point sets, enabling rich intra-scale interactions while avoiding brittle fixed orderings. Experiments on ShapeNet show that PointNSP establishes state-of-the-art (SOTA) generation quality for the first time within the autoregressive paradigm. In addition, it surpasses strong diffusion-based baselines in parameter, training, and inference efficiency. Finally, in dense generation with 8,192 points, PointNSP's advantages become even more pronounced, underscoring its scalability potential.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "137",
        "title": "From Principles to Practice: A Systematic Study of LLM Serving on Multi-core NPUs",
        "author": [
            "Tianhao Zhu",
            "Dahu Feng",
            "Erhu Feng",
            "Yubin Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05632",
        "abstract": "With the widespread adoption of Large Language Models (LLMs), the demand for high-performance LLM inference services continues to grow. To meet this demand, a growing number of AI accelerators have been proposed, such as Google TPU, Huawei NPU, Graphcore IPU, and Cerebras WSE, etc. Most of these accelerators adopt multi-core architectures to achieve enhanced scalability, but lack the flexibility of SIMT architectures. Therefore, without careful configuration of the hardware architecture, as well as deliberate design of tensor parallelism and core placement strategies, computational resources may be underutilized, resulting in suboptimal inference performance.\nTo address these challenges, we first present a multi-level simulation framework with both transaction-level and performance-model-based simulation for multi-core NPUs. Using this simulator, we conduct a systematic analysis and further propose the optimal solutions for tensor parallelism strategies, core placement policies, memory management methods, as well as the selection between PD-disaggregation and PD-fusion on multi-core NPUs. We conduct comprehensive experiments on representative LLMs and various NPU configurations. The evaluation results demonstrate that, our solution can achieve 1.32x-6.03x speedup compared to SOTA designs for multi-core NPUs across different hardware configurations. As for LLM serving, our work offers guidance on designing optimal hardware architectures and serving strategies for multi-core NPUs across various LLM workloads.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "138",
        "title": "NEO: No-Optimization Test-Time Adaptation through Latent Re-Centering",
        "author": [
            "Alexander Murphy",
            "Michal Danilowski",
            "Soumyajit Chatterjee",
            "Abhirup Ghosh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05635",
        "abstract": "Test-Time Adaptation (TTA) methods are often computationally expensive, require a large amount of data for effective adaptation, or are brittle to hyperparameters. Based on a theoretical foundation of the geometry of the latent space, we are able to significantly improve the alignment between source and distribution-shifted samples by re-centering target data embeddings at the origin. This insight motivates NEO -- a hyperparameter-free fully TTA method, that adds no significant compute compared to vanilla inference. NEO is able to improve the classification accuracy of ViT-Base on ImageNet-C from 55.6% to 59.2% after adapting on just one batch of 64 samples. When adapting on 512 samples NEO beats all 7 TTA methods we compare against on ImageNet-C, ImageNet-R and ImageNet-S and beats 6/7 on CIFAR-10-C, while using the least amount of compute. NEO performs well on model calibration metrics and additionally is able to adapt from 1 class to improve accuracy on 999 other classes in ImageNet-C. On Raspberry Pi and Jetson Orin Nano devices, NEO reduces inference time by 63% and memory usage by 9% compared to baselines. Our results based on 3 ViT architectures and 4 datasets show that NEO can be used efficiently and effectively for TTA.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "139",
        "title": "Teleportraits: Training-Free People Insertion into Any Scene",
        "author": [
            "Jialu Gao",
            "K J Joseph",
            "Fernando De La Torre"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05660",
        "abstract": "The task of realistically inserting a human from a reference image into a background scene is highly challenging, requiring the model to (1) determine the correct location and poses of the person and (2) perform high-quality personalization conditioned on the background. Previous approaches often treat them as separate problems, overlooking their interconnections, and typically rely on training to achieve high performance. In this work, we introduce a unified training-free pipeline that leverages pre-trained text-to-image diffusion models. We show that diffusion models inherently possess the knowledge to place people in complex scenes without requiring task-specific training. By combining inversion techniques with classifier-free guidance, our method achieves affordance-aware global editing, seamlessly inserting people into scenes. Furthermore, our proposed mask-guided self-attention mechanism ensures high-quality personalization, preserving the subject's identity, clothing, and body features from just a single reference image. To the best of our knowledge, we are the first to perform realistic human insertions into scenes in a training-free manner and achieve state-of-the-art results in diverse composite scene images with excellent identity preservation in backgrounds and subjects.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "140",
        "title": "When and How to Cut Classical Concerts? A Multimodal Automated Video Editing Approach",
        "author": [
            "Daniel GonzÃ¡lbez-Biosca",
            "Josep Cabacas-Maso",
            "Carles Ventura",
            "Ismael Benito-Altamirano"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05661",
        "abstract": "Automated video editing remains an underexplored task in the computer vision and multimedia domains, especially when contrasted with the growing interest in video generation and scene understanding. In this work, we address the specific challenge of editing multicamera recordings of classical music concerts by decomposing the problem into two key sub-tasks: when to cut and how to cut. Building on recent literature, we propose a novel multimodal architecture for the temporal segmentation task (when to cut), which integrates log-mel spectrograms from the audio signals, plus an optional image embedding, and scalar temporal features through a lightweight convolutional-transformer pipeline. For the spatial selection task (how to cut), we improve the literature by updating from old backbones, e.g. ResNet, with a CLIP-based encoder and constraining distractor selection to segments from the same concert. Our dataset was constructed following a pseudo-labeling approach, in which raw video data was automatically clustered into coherent shot segments. We show that our models outperformed previous baselines in detecting cut points and provide competitive visual shot selection, advancing the state of the art in multimodal automated video editing.",
        "tags": [
            "CLIP",
            "Segmentation",
            "Transformer",
            "Video Editing",
            "Video Generation"
        ]
    },
    {
        "id": "141",
        "title": "DeLTa: Demonstration and Language-Guided Novel Transparent Object Manipulation",
        "author": [
            "Taeyeop Lee",
            "Gyuree Kang",
            "Bowen Wen",
            "Youngho Kim",
            "Seunghyeok Back",
            "In So Kweon",
            "David Hyunchul Shim",
            "Kuk-Jin Yoon"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05662",
        "abstract": "Despite the prevalence of transparent object interactions in human everyday life, transparent robotic manipulation research remains limited to short-horizon tasks and basic grasping http://capabilities.Although some methods have partially addressed these issues, most of them have limitations in generalizability to novel objects and are insufficient for precise long-horizon robot manipulation. To address this limitation, we propose DeLTa (Demonstration and Language-Guided Novel Transparent Object Manipulation), a novel framework that integrates depth estimation, 6D pose estimation, and vision-language planning for precise long-horizon manipulation of transparent objects guided by natural task instructions. A key advantage of our method is its single-demonstration approach, which generalizes 6D trajectories to novel transparent objects without requiring category-level priors or additional training. Additionally, we present a task planner that refines the VLM-generated plan to account for the constraints of a single-arm, eye-in-hand robot for long-horizon object manipulation tasks. Through comprehensive evaluation, we demonstrate that our method significantly outperforms existing transparent object manipulation approaches, particularly in long-horizon scenarios requiring precise manipulation capabilities. Project page: https://sites.google.com/view/DeLTa25/",
        "tags": [
            "Depth Estimation",
            "Pose Estimation",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "142",
        "title": "Large Language Model-Based Uncertainty-Adjusted Label Extraction for Artificial Intelligence Model Development in Upper Extremity Radiography",
        "author": [
            "Hanna Kreutzer",
            "Anne-Sophie Caselitz",
            "Thomas Dratsch",
            "Daniel Pinto dos Santos",
            "Christiane Kuhl",
            "Daniel Truhn",
            "Sven Nebelung"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05664",
        "abstract": "Objectives: To evaluate GPT-4o's ability to extract diagnostic labels (with uncertainty) from free-text radiology reports and to test how these labels affect multi-label image classification of musculoskeletal radiographs. Methods: This retrospective study included radiography series of the clavicle (n=1,170), elbow (n=3,755), and thumb (n=1,978). After anonymization, GPT-4o filled out structured templates by indicating imaging findings as present (\"true\"), absent (\"false\"), or \"uncertain.\" To assess the impact of label uncertainty, \"uncertain\" labels of the training and validation sets were automatically reassigned to \"true\" (inclusive) or \"false\" (exclusive). Label-image-pairs were used for multi-label classification using ResNet50. Label extraction accuracy was manually verified on internal (clavicle: n=233, elbow: n=745, thumb: n=393) and external test sets (n=300 for each). Performance was assessed using macro-averaged receiver operating characteristic (ROC) area under the curve (AUC), precision recall curves, sensitivity, specificity, and accuracy. AUCs were compared with the DeLong test. Results: Automatic extraction was correct in 98.6% (60,618 of 61,488) of labels in the test sets. Across anatomic regions, label-based model training yielded competitive performance measured by macro-averaged AUC values for inclusive (e.g., elbow: AUC=0.80 [range, 0.62-0.87]) and exclusive models (elbow: AUC=0.80 [range, 0.61-0.88]). Models generalized well on external datasets (elbow [inclusive]: AUC=0.79 [range, 0.61-0.87]; elbow [exclusive]: AUC=0.79 [range, 0.63-0.89]). No significant differences were observed across labeling strategies or datasets (p>=0.15). Conclusion: GPT-4o extracted labels from radiologic reports to train competitive multi-label classification models with high accuracy. Detected uncertainty in the radiologic reports did not influence the performance of these models.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "143",
        "title": "Context Matters: Learning Global Semantics for Visual Reasoning and Comprehension",
        "author": [
            "Jike Zhong",
            "Yuxiang Lai",
            "Xiaofeng Yang",
            "Konstantinos Psounis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05674",
        "abstract": "Recent advances in language modeling have witnessed the rise of highly desirable emergent capabilities, such as reasoning and in-context learning. However, vision models have yet to exhibit comparable progress in these areas. In this paper, we argue that this gap could stem from the lack of semantic and contextual guidance in current vision transformer (ViT) training schemes, and such a gap can be narrowed through the design of a semantic-grounded objective. Specifically, we notice that individual words in natural language are inherently semantic, and modeling directly on word tokens naturally learns a realistic distribution. In contrast, ViTs rely on spatial patchification, which inevitably lacks semantic information. To bridge this gap, we propose to directly model \"object\" as the visual equivalence of \"word,\" pushing the model to learn the global context and semantics among visual elements. We investigate our hypotheses via masked image modeling (MIM), a framework where our approach can be readily tested by applying masks to visual objects rather than random patches. Considerable evidence from qualitative and quantitative evaluations reveals a key finding: object-level representation alone helps to learn a real-world distribution, whereas pixel-averaging shortcuts are often learned without it. Moreover, further evaluations with multimodal LLMs (MLLM) on visual question answering (VQA, GQA, ScienceQA) tasks demonstrate the strong reasoning and contextual understanding gained with this simple objective. We hope our study highlights the effectiveness of object-level encoding and provides a plausible direction for developing stronger vision encoders and tokenizers. Code and model will be publicly released. Keywords: Semantic Visual Tokenizer, Vision Reasoning, In-context Learning, Multimodal Reasoning",
        "tags": [
            "LLM",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "144",
        "title": "Code-Switching In-Context Learning for Cross-Lingual Transfer of Large Language Models",
        "author": [
            "Haneul Yoo",
            "Jiho Jin",
            "Kyunghyun Cho",
            "Alice Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05678",
        "abstract": "While large language models (LLMs) exhibit strong multilingual abilities, their reliance on English as latent representations creates a translation barrier, where reasoning implicitly depends on internal translation into English. When this process fails, performance in non-English languages deteriorates sharply, limiting the inclusiveness of LLM-based applications. Existing cross-lingual in-context learning (X-ICL) methods primarily leverage monolingual demonstrations, often failing to mitigate this barrier and instead reinforcing it. In this work, we introduce code-switching in-context learning (CSICL), a simple yet effective prompting strategy that progressively transitions from a target language to English within demonstrations and instruction to facilitate their latent reasoning in English. By explicitly scaffolding the reasoning process through controlled code-switching, CSICL acts as an implicit linguistic bridge that enhances cross-lingual alignment and reduces reliance on the translation barrier. We conduct extensive experiments across 4 LLMs, 6 datasets, and 10 languages, spanning both knowledge-intensive and reasoning-oriented domains. Our results demonstrate that CSICL consistently outperforms X-ICL baselines, achieving gains of 3.1%p and 1.9%p in both target and unseen languages, respectively. The improvement is even more pronounced in low-resource settings, with gains of 14.7% in target and 5.3% in unseen languages. These findings establish code-switching as a principled and robust approach for overcoming the translation barrier during inference, moving LLMs toward more equitable and effective multilingual systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "145",
        "title": "Verifier-free Test-Time Sampling for Vision Language Action Models",
        "author": [
            "Suhyeok Jang",
            "Dongyoung Kim",
            "Changyeon Kim",
            "Youngsuk Kim",
            "Jinwoo Shin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05681",
        "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance in robot control. However, they remain fundamentally limited in tasks that require high precision due to their single-inference paradigm. While test-time scaling approaches using external verifiers have shown promise, they require additional training and fail to generalize to unseen conditions. We propose Masking Distribution Guided Selection (MG-Select), a novel test-time scaling framework for VLAs that leverages the model's internal properties without requiring additional training or external modules. Our approach utilizes KL divergence from a reference action token distribution as a confidence metric for selecting the optimal action from multiple candidates. We introduce a reference distribution generated by the same VLA but with randomly masked states and language conditions as inputs, ensuring maximum uncertainty while remaining aligned with the target task distribution. Additionally, we propose a joint training strategy that enables the model to learn both conditional and unconditional distributions by applying dropout to state and language conditions, thereby further improving the quality of the reference distribution. Our experiments demonstrate that MG-Select achieves significant performance improvements, including a 28%/35% improvement in real-world in-distribution/out-of-distribution tasks, along with a 168% relative gain on RoboCasa pick-and-place tasks trained with 30 demonstrations.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "146",
        "title": "D2E: Scaling Vision-Action Pretraining on Desktop Data for Transfer to Embodied AI",
        "author": [
            "Suwhan Choi",
            "Jaeyoon Jung",
            "Haebin Seong",
            "Minchan Kim",
            "Minyeong Kim",
            "Yongjun Cho",
            "Yoonshik Kim",
            "Yubeen Park",
            "Youngjae Yu",
            "Yunsung Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05684",
        "abstract": "Large language models leverage internet-scale text data, yet embodied AI remains constrained by the prohibitive costs of physical trajectory collection. Desktop environments -- particularly gaming -- offer a compelling alternative: they provide rich sensorimotor interactions at scale while maintaining the structured observation-action coupling essential for embodied learning. We present D2E (Desktop to Embodied AI), a framework that demonstrates desktop interactions can serve as an effective pretraining substrate for robotics embodied AI tasks. Unlike prior work that remained domain-specific (e.g., VPT for Minecraft) or kept data proprietary (e.g., SIMA), D2E establishes a complete pipeline from scalable desktop data collection to verified transfer in embodied domains. Our framework comprises three components: (1) the OWA Toolkit that unifies diverse desktop interactions into a standardized format with 152x compression, (2) the Generalist-IDM that achieves strong zero-shot generalization across unseen games through timestamp-based event prediction, enabling internet-scale pseudo-labeling, and (3) VAPT that transfers desktop-pretrained representations to physical manipulation and navigation. Using 1.3K+ hours of data (259 hours of human demonstrations, and 1K+ hours of pseudo-labeled gameplay), we achieve a total of 96.6% success rate on LIBERO manipulation and 83.3% on CANVAS navigation benchmarks. This validates that sensorimotor primitives in digital interactions exhibit sufficient invariance to transfer meaningfully to physical embodied tasks, establishing desktop pretraining as a practical paradigm for robotics. We will make all our work public, including the OWA toolkit, datasets of human-collected and pseudo-labeled, and VAPT-trained models available at https://worv-ai.github.io/d2e/",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "147",
        "title": "vAttention: Verified Sparse Attention",
        "author": [
            "Aditya Desai",
            "Kumar Krishna Agrawal",
            "Shuo Yang",
            "Alejandro Cuadron",
            "Luis Gaspar Schroeder",
            "Matei Zaharia",
            "Joseph E. Gonzalez",
            "Ion Stoica"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05688",
        "abstract": "State-of-the-art sparse attention methods for reducing decoding latency fall into two main categories: approximate top-$k$ (and its extension, top-$p$) and recently introduced sampling-based estimation. However, these approaches are fundamentally limited in their ability to approximate full attention: they fail to provide consistent approximations across heads and query vectors and, most critically, lack guarantees on approximation quality, limiting their practical deployment. We observe that top-$k$ and random sampling are complementary: top-$k$ performs well when attention scores are dominated by a few tokens, whereas random sampling provides better estimates when attention scores are relatively uniform. Building on this insight and leveraging the statistical guarantees of sampling, we introduce vAttention, the first practical sparse attention mechanism with user-specified $(\\epsilon, \\delta)$ guarantees on approximation accuracy (thus, verified). These guarantees make vAttention a compelling step toward practical, reliable deployment of sparse attention at scale. By unifying top-k and sampling, vAttention outperforms both individually, delivering a superior quality-efficiency trade-off. Our experiments show that vAttention significantly improves the quality of sparse attention (e.g., $\\sim$4.5 percentage points for Llama-3.1-8B-Inst and Deepseek-R1-Distill-Llama-8B on RULER-HARD), and effectively bridges the gap between full and sparse attention (e.g., across datasets, it matches full model quality with upto 20x sparsity). We also demonstrate that it can be deployed in reasoning scenarios to achieve fast decoding without compromising model quality (e.g., vAttention achieves full model quality on AIME2024 at 10x sparsity with up to 32K token generations). Code is open-sourced at https://github.com/xAlg-ai/sparse-attention-hub.",
        "tags": [
            "DeepSeek",
            "LLaMA"
        ]
    },
    {
        "id": "148",
        "title": "DecEx-RAG: Boosting Agentic Retrieval-Augmented Generation with Decision and Execution Optimization via Process Supervision",
        "author": [
            "Yongqi Leng",
            "Yikun Lei",
            "Xikai Liu",
            "Meizhi Zhong",
            "Bojian Xiong",
            "Yurong Zhang",
            "Yan Gao",
            "Yi Wu",
            "Yao Hu",
            "Deyi Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05691",
        "abstract": "Agentic Retrieval-Augmented Generation (Agentic RAG) enhances the processing capability for complex tasks through dynamic retrieval and adaptive workflows. Recent advances (e.g., Search-R1) have shown that outcome-supervised reinforcement learning demonstrate strong performance. However, this approach still suffers from inefficient exploration, sparse reward signals, and ambiguous global reward feedback. To address these challenges, we propose DecEx-RAG, which models RAG as a Markov Decision Process (MDP) incorporating decision-making and execution, while introducing an efficient pruning strategy to optimize data expansion. Through comprehensive process-level policy optimization, DecEx-RAG significantly enhances the autonomous task decomposition, dynamic retrieval, and high-quality answer generation capabilities of large language models (LLMs). Experiments show that DecEx-RAG achieves an average absolute performance improvement of $6.2\\%$ across six datasets, significantly outperforming existing baselines. Moreover, the pruning strategy improves data construction efficiency by nearly $6 \\times$, providing an efficient solution for process-supervised RAG training. The code is available at https://github.com/sdsxdxl/DecEx-RAG.",
        "tags": [
            "LLM",
            "RAG",
            "RL"
        ]
    },
    {
        "id": "149",
        "title": "Oracle-Guided Masked Contrastive Reinforcement Learning for Visuomotor Policies",
        "author": [
            "Yuhang Zhang",
            "Jiaping Xiao",
            "Chao Yan",
            "Mir Feroskhan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05692",
        "abstract": "A prevailing approach for learning visuomotor policies is to employ reinforcement learning to map high-dimensional visual observations directly to action commands. However, the combination of high-dimensional visual inputs and agile maneuver outputs leads to long-standing challenges, including low sample efficiency and significant sim-to-real gaps. To address these issues, we propose Oracle-Guided Masked Contrastive Reinforcement Learning (OMC-RL), a novel framework designed to improve the sample efficiency and asymptotic performance of visuomotor policy learning. OMC-RL explicitly decouples the learning process into two stages: an upstream representation learning stage and a downstream policy learning stage. In the upstream stage, a masked Transformer module is trained with temporal modeling and contrastive learning to extract temporally-aware and task-relevant representations from sequential visual inputs. After training, the learned encoder is frozen and used to extract visual representations from consecutive frames, while the Transformer module is discarded. In the downstream stage, an oracle teacher policy with privileged access to global state information supervises the agent during early training to provide informative guidance and accelerate early policy learning. This guidance is gradually reduced to allow independent exploration as training progresses. Extensive experiments in simulated and real-world environments demonstrate that OMC-RL achieves superior sample efficiency and asymptotic policy performance, while also improving generalization across diverse and perceptually complex scenarios.",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "150",
        "title": "Joint Communication Scheduling and Velocity Control for Multi-UAV-Assisted Post-Disaster Monitoring: An Attention-Based In-Context Learning Approach",
        "author": [
            "Yousef Emami",
            "Seyedsina Nabavirazavi",
            "Jingjing Zheng",
            "Hao Zhou",
            "Miguel Gutierrez Gaitan",
            "Kai Li",
            "Luis Almeida"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05698",
        "abstract": "Recently, Unmanned Aerial Vehicles (UAVs) are increasingly being investigated to collect sensory data in post-disaster monitoring scenarios, such as tsunamis, where early actions are critical to limit coastal damage. A major challenge is to design the data collection schedules and flight velocities, as unfavorable schedules and velocities can lead to transmission errors and buffer overflows of the ground sensors, ultimately resulting in significant packet loss. Meanwhile, online Deep Reinforcement Learning (DRL) solutions have a complex training process and a mismatch between simulation and reality that does not meet the urgent requirements of tsunami monitoring. Recent advances in Large Language Models (LLMs) offer a compelling alternative. With their strong reasoning and generalization capabilities, LLMs can adapt to new tasks through In-Context Learning (ICL), which enables task adaptation through natural language prompts and example-based guidance without retraining. However, LLM models have input data limitations and thus require customized approaches. In this paper, a joint optimization of data collection schedules and velocities control for multiple UAVs is proposed to minimize data loss. The battery level of the ground sensors, the length of the queues, and the channel conditions, as well as the trajectories of the UAVs, are taken into account. Attention-Based In-Context Learning for Velocity Control and Data Collection Schedule (AIC-VDS) is proposed as an alternative to DRL in emergencies. The simulation results show that the proposed AIC-VDS outperforms both the Deep-Q-Network (DQN) and maximum channel gain baselines.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "151",
        "title": "Membership Inference Attacks on Tokenizers of Large Language Models",
        "author": [
            "Meng Tong",
            "Yuntao Du",
            "Kejiang Chen",
            "Weiming Zhang",
            "Ninghui Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05699",
        "abstract": "Membership inference attacks (MIAs) are widely used to assess the privacy risks associated with machine learning models. However, when these attacks are applied to pre-trained large language models (LLMs), they encounter significant challenges, including mislabeled samples, distribution shifts, and discrepancies in model size between experimental and real-world settings. To address these limitations, we introduce tokenizers as a new attack vector for membership inference. Specifically, a tokenizer converts raw text into tokens for LLMs. Unlike full models, tokenizers can be efficiently trained from scratch, thereby avoiding the aforementioned challenges. In addition, the tokenizer's training data is typically representative of the data used to pre-train LLMs. Despite these advantages, the potential of tokenizers as an attack vector remains unexplored. To this end, we present the first study on membership leakage through tokenizers and explore five attack methods to infer dataset membership. Extensive experiments on millions of Internet samples reveal the vulnerabilities in the tokenizers of state-of-the-art LLMs. To mitigate this emerging risk, we further propose an adaptive defense. Our findings highlight tokenizers as an overlooked yet critical privacy threat, underscoring the urgent need for privacy-preserving mechanisms specifically designed for them.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "152",
        "title": "Primal-Dual Direct Preference Optimization for Constrained LLM Alignment",
        "author": [
            "Yihan Du",
            "Seo Taek Kong",
            "R. Srikant"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05703",
        "abstract": "The widespread application of Large Language Models (LLMs) imposes increasing demands on safety, such as reducing harmful content and fake information, and avoiding certain forbidden tokens due to rules and laws. While there have been several recent works studying safe alignment of LLMs, these works either require the training of reward and cost models and incur high memory and computational costs, or need prior knowledge about the optimal solution. Motivated by this fact, we study the problem of constrained alignment in LLMs, i.e., maximizing the output reward while restricting the cost due to potentially unsafe content to stay below a threshold. For this problem, we propose a novel primal-dual DPO approach, which first trains a model using standard DPO on reward preference data to provide reward information, and then adopts a rearranged Lagrangian DPO objective utilizing the provided reward information to fine-tune LLMs on cost preference data. Our approach significantly reduces memory and computational costs, and does not require extra prior knowledge. Moreover, we establish rigorous theoretical guarantees on the suboptimality and constraint violation of the output policy. We also extend our approach to an online data setting by incorporating exploration bonuses, which enables our approach to explore uncovered prompt-response space, and then provide theoretical results that get rid of the dependence on preference data coverage. Experimental results on the widely-used preference dataset PKU-SafeRLHF demonstrate the effectiveness of our approach.",
        "tags": [
            "DPO",
            "LLM"
        ]
    },
    {
        "id": "153",
        "title": "Stable Robot Motions on Manifolds: Learning Lyapunov-Constrained Neural Manifold ODEs",
        "author": [
            "David Boetius",
            "Abdelrahman Abdelnaby",
            "Ashok Kumar",
            "Stefan Leue",
            "Abdalla Swikir",
            "Fares J. Abu-Dakka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05707",
        "abstract": "Learning stable dynamical systems from data is crucial for safe and reliable robot motion planning and control. However, extending stability guarantees to trajectories defined on Riemannian manifolds poses significant challenges due to the manifold's geometric constraints. To address this, we propose a general framework for learning stable dynamical systems on Riemannian manifolds using neural ordinary differential equations. Our method guarantees stability by projecting the neural vector field evolving on the manifold so that it strictly satisfies the Lyapunov stability criterion, ensuring stability at every system state. By leveraging a flexible neural parameterisation for both the base vector field and the Lyapunov function, our framework can accurately represent complex trajectories while respecting manifold constraints by evolving solutions directly on the manifold. We provide an efficient training strategy for applying our framework and demonstrate its utility by solving Riemannian LASA datasets on the unit quaternion (S^3) and symmetric positive-definite matrix manifolds, as well as robotic motions evolving on \\mathbb{R}^3 \\times S^3. We demonstrate the performance, scalability, and practical applicability of our approach through extensive simulations and by learning robot motions in a real-world experiment.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "154",
        "title": "Towards Reliable and Practical LLM Security Evaluations via Bayesian Modelling",
        "author": [
            "Mary Llewellyn",
            "Annie Gray",
            "Josh Collyer",
            "Michael Harries"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05709",
        "abstract": "Before adopting a new large language model (LLM) architecture, it is critical to understand vulnerabilities accurately. Existing evaluations can be difficult to trust, often drawing conclusions from LLMs that are not meaningfully comparable, relying on heuristic inputs or employing metrics that fail to capture the inherent uncertainty. In this paper, we propose a principled and practical end-to-end framework for evaluating LLM vulnerabilities to prompt injection attacks. First, we propose practical approaches to experimental design, tackling unfair LLM comparisons by considering two practitioner scenarios: when training an LLM and when deploying a pre-trained LLM. Second, we address the analysis of experiments and propose a Bayesian hierarchical model with embedding-space clustering. This model is designed to improve uncertainty quantification in the common scenario that LLM outputs are not deterministic, test prompts are designed imperfectly, and practitioners only have a limited amount of compute to evaluate vulnerabilities. We show the improved inferential capabilities of the model in several prompt injection attack settings. Finally, we demonstrate the pipeline to evaluate the security of Transformer versus Mamba architectures. Our findings show that consideration of output variability can suggest less definitive findings. However, for some attacks, we find notably increased Transformer and Mamba-variant vulnerabilities across LLMs with the same training data or mathematical ability.",
        "tags": [
            "LLM",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "155",
        "title": "AgeBooth: Controllable Facial Aging and Rejuvenation via Diffusion Models",
        "author": [
            "Shihao Zhu",
            "Bohan Cao",
            "Ziheng Ouyang",
            "Zhen Li",
            "Peng-Tao Jiang",
            "Qibin Hou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05715",
        "abstract": "Recent diffusion model research focuses on generating identity-consistent images from a reference photo, but they struggle to accurately control age while preserving identity, and fine-tuning such models often requires costly paired images across ages. In this paper, we propose AgeBooth, a novel age-specific finetuning approach that can effectively enhance the age control capability of adapterbased identity personalization models without the need for expensive age-varied datasets. To reduce dependence on a large amount of age-labeled data, we exploit the linear nature of aging by introducing age-conditioned prompt blending and an age-specific LoRA fusion strategy that leverages SVDMix, a matrix fusion technique. These techniques enable high-quality generation of intermediate-age portraits. Our AgeBooth produces realistic and identity-consistent face images across different ages from a single reference image. Experiments show that AgeBooth achieves superior age control and visual quality compared to previous state-of-the-art editing-based methods.",
        "tags": [
            "Diffusion",
            "LoRA"
        ]
    },
    {
        "id": "156",
        "title": "DiffSDA: Unsupervised Diffusion Sequential Disentanglement Across Modalities",
        "author": [
            "Hedi Zisling",
            "Ilan Naiman",
            "Nimrod Berman",
            "Supasorn Suwajanakorn",
            "Omri Azencot"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05717",
        "abstract": "Unsupervised representation learning, particularly sequential disentanglement, aims to separate static and dynamic factors of variation in data without relying on labels. This remains a challenging problem, as existing approaches based on variational autoencoders and generative adversarial networks often rely on multiple loss terms, complicating the optimization process. Furthermore, sequential disentanglement methods face challenges when applied to real-world data, and there is currently no established evaluation protocol for assessing their performance in such settings. Recently, diffusion models have emerged as state-of-the-art generative models, but no theoretical formalization exists for their application to sequential disentanglement. In this work, we introduce the Diffusion Sequential Disentanglement Autoencoder (DiffSDA), a novel, modal-agnostic framework effective across diverse real-world data modalities, including time series, video, and audio. DiffSDA leverages a new probabilistic modeling, latent diffusion, and efficient samplers, while incorporating a challenging evaluation protocol for rigorous testing. Our experiments on diverse real-world benchmarks demonstrate that DiffSDA outperforms recent state-of-the-art methods in sequential disentanglement.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "157",
        "title": "Data Factory with Minimal Human Effort Using VLMs",
        "author": [
            "Jiaojiao Ye",
            "Jiaxing Zhong",
            "Qian Xie",
            "Yuzhou Zhou",
            "Niki Trigoni",
            "Andrew Markham"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05722",
        "abstract": "Generating enough and diverse data through augmentation offers an efficient solution to the time-consuming and labour-intensive process of collecting and annotating pixel-wise images. Traditional data augmentation techniques often face challenges in manipulating high-level semantic attributes, such as materials and textures. In contrast, diffusion models offer a robust alternative, by effectively utilizing text-to-image or image-to-image transformation. However, existing diffusion-based methods are either computationally expensive or compromise on performance. To address this issue, we introduce a novel training-free pipeline that integrates pretrained ControlNet and Vision-Language Models (VLMs) to generate synthetic images paired with pixel-level labels. This approach eliminates the need for manual annotations and significantly improves downstream tasks. To improve the fidelity and diversity, we add a Multi-way Prompt Generator, Mask Generator and High-quality Image Selection module. Our results on PASCAL-5i and COCO-20i present promising performance and outperform concurrent work for one-shot semantic segmentation.",
        "tags": [
            "ControlNet",
            "Diffusion",
            "Segmentation",
            "Text-to-Image",
            "VLM"
        ]
    },
    {
        "id": "158",
        "title": "Improving Discrete Diffusion Unmasking Policies Beyond Explicit Reference Policies",
        "author": [
            "Chunsan Hong",
            "Seonho An",
            "Min-Soo Kim",
            "Jong Chul Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05725",
        "abstract": "Masked diffusion models (MDMs) have recently emerged as a novel framework for language modeling. MDMs generate sentences by iteratively denoising masked sequences, filling in [MASK] tokens step by step. Although MDMs support any-order sampling, performance is highly sensitive to the choice of which position to unmask next. Prior work typically relies on rule-based schedules (e.g., max-confidence, max-margin), which provide ad hoc improvements. In contrast, we replace these heuristics with a learned scheduler. Specifically, we cast denoising as a KL-regularized Markov decision process (MDP) with an explicit reference policy and optimize a regularized objective that admits policy improvement and convergence guarantees under standard assumptions. We prove that the optimized policy under this framework generates samples that more closely match the data distribution than heuristic schedules. Empirically, across four benchmarks, our learned policy consistently outperforms max-confidence: for example, on SUDOKU, where unmasking order is critical, it yields a 20.1% gain over random and a 11.2% gain over max-confidence.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "159",
        "title": "Syn-Diag: An LLM-based Synergistic Framework for Generalizable Few-shot Fault Diagnosis on the Edge",
        "author": [
            "Zijun Jia",
            "Shuang Liang",
            "Jinsong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05733",
        "abstract": "Industrial fault diagnosis faces the dual challenges of data scarcity and the difficulty of deploying large AI models in resource-constrained environments. This paper introduces Syn-Diag, a novel cloud-edge synergistic framework that leverages Large Language Models to overcome these limitations in few-shot fault diagnosis. Syn-Diag is built on a three-tiered mechanism: 1) Visual-Semantic Synergy, which aligns signal features with the LLM's semantic space through cross-modal pre-training; 2) Content-Aware Reasoning, which dynamically constructs contextual prompts to enhance diagnostic accuracy with limited samples; and 3) Cloud-Edge Synergy, which uses knowledge distillation to create a lightweight, efficient edge model capable of online updates via a shared decision space. Extensive experiments on six datasets covering different CWRU and SEU working conditions show that Syn-Diag significantly outperforms existing methods, especially in 1-shot and cross-condition scenarios. The edge model achieves performance comparable to the cloud version while reducing model size by 83% and latency by 50%, offering a practical, robust, and deployable paradigm for modern intelligent diagnostics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "160",
        "title": "Redefining Generalization in Visual Domains: A Two-Axis Framework for Fake Image Detection with FusionDetect",
        "author": [
            "Amirtaha Amanzadi",
            "Zahra Dehghanian",
            "Hamid Beigy",
            "Hamid R. Rabiee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05740",
        "abstract": "The rapid development of generative models has made it increasingly crucial to develop detectors that can reliably detect synthetic images. Although most of the work has now focused on cross-generator generalization, we argue that this viewpoint is too limited. Detecting synthetic images involves another equally important challenge: generalization across visual domains. To bridge this gap,we present the OmniGen Benchmark. This comprehensive evaluation dataset incorporates 12 state-of-the-art generators, providing a more realistic way of evaluating detector performance under realistic conditions. In addition, we introduce a new method, FusionDetect, aimed at addressing both vectors of generalization. FusionDetect draws on the benefits of two frozen foundation models: CLIP & Dinov2. By deriving features from both complementary models,we develop a cohesive feature space that naturally adapts to changes in both thecontent and design of the generator. Our extensive experiments demonstrate that FusionDetect delivers not only a new state-of-the-art, which is 3.87% more accurate than its closest competitor and 6.13% more precise on average on established benchmarks, but also achieves a 4.48% increase in accuracy on OmniGen,along with exceptional robustness to common image perturbations. We introduce not only a top-performing detector, but also a new benchmark and framework for furthering universal AI image detection. The code and dataset are available at http://github.com/amir-aman/FusionDetect",
        "tags": [
            "CLIP",
            "Detection"
        ]
    },
    {
        "id": "161",
        "title": "Vipera: Blending Visual and LLM-Driven Guidance for Systematic Auditing of Text-to-Image Generative AI",
        "author": [
            "Yanwei Huang",
            "Wesley Hanwen Deng",
            "Sijia Xiao",
            "Motahhare Eslami",
            "Jason I. Hong",
            "Arpit Narechania",
            "Adam Perer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05742",
        "abstract": "Despite their increasing capabilities, text-to-image generative AI systems are known to produce biased, offensive, and otherwise problematic outputs. While recent advancements have supported testing and auditing of generative AI, existing auditing methods still face challenges in supporting effectively explore the vast space of AI-generated outputs in a structured way. To address this gap, we conducted formative studies with five AI auditors and synthesized five design goals for supporting systematic AI audits. Based on these insights, we developed Vipera, an interactive auditing interface that employs multiple visual cues including a scene graph to facilitate image sensemaking and inspire auditors to explore and hierarchically organize the auditing criteria. Additionally, Vipera leverages LLM-powered suggestions to facilitate exploration of unexplored auditing directions. Through a controlled experiment with 24 participants experienced in AI auditing, we demonstrate Vipera's effectiveness in helping auditors navigate large AI output spaces and organize their analyses while engaging with diverse criteria.",
        "tags": [
            "LLM",
            "Text-to-Image"
        ]
    },
    {
        "id": "162",
        "title": "Artificially intelligent agents in the social and behavioral sciences: A history and outlook",
        "author": [
            "Petter Holme",
            "Milena Tsvetkova"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05743",
        "abstract": "We review the historical development and current trends of artificially intelligent agents (agentic AI) in the social and behavioral sciences: from the first programmable computers, and social simulations soon thereafter, to today's experiments with large language models. This overview emphasizes the role of AI in the scientific process and the changes brought about, both through technological advancements and the broader evolution of science from around 1950 to the present. Some of the specific points we cover include: the challenges of presenting the first social simulation studies to a world unaware of computers, the rise of social systems science, intelligent game theoretic agents, the age of big data and the epistemic upheaval in its wake, and the current enthusiasm around applications of generative AI, and many other topics. A pervasive theme is how deeply entwined we are with the technologies we use to understand ourselves.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "163",
        "title": "Adaptive and Multi-Source Entity Matching for Name Standardization of Astronomical Observation Facilities",
        "author": [
            "Liza Fretel",
            "Baptiste Cecconi",
            "Laura Debisschop"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05744",
        "abstract": "This ongoing work focuses on the development of a methodology for generating a multi-source mapping of astronomical observation facilities. To compare two entities, we compute scores with adaptable criteria and Natural Language Processing (NLP) techniques (Bag-of-Words approaches, sequential approaches, and surface approaches) to map entities extracted from eight semantic artifacts, including Wikidata and astronomy-oriented resources. We utilize every property available, such as labels, definitions, descriptions, external identifiers, and more domain-specific properties, such as the observation wavebands, spacecraft launch dates, funding agencies, etc. Finally, we use a Large Language Model (LLM) to accept or reject a mapping suggestion and provide a justification, ensuring the plausibility and FAIRness of the validated synonym pairs. The resulting mapping is composed of multi-source synonym sets providing only one standardized label per entity. Those mappings will be used to feed our Name Resolver API and will be integrated into the International Virtual Observatory Alliance (IVOA) Vocabularies and the OntoPortal-Astro platform.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "164",
        "title": "ARM: Discovering Agentic Reasoning Modules for Generalizable Multi-Agent Systems",
        "author": [
            "Bohan Yao",
            "Shiva Krishna Reddy Malay",
            "Vikas Yadav"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05746",
        "abstract": "Large Language Model (LLM)-powered Multi-agent systems (MAS) have achieved state-of-the-art results on various complex reasoning tasks. Recent works have proposed techniques to automate the design of MASes, eliminating the need for manual engineering. However, these techniques perform poorly, often achieving similar or inferior performance to simple baselines. Furthermore, they require computationally expensive re-discovery of architectures for each new task domain and expensive data annotation on domains without existing labeled validation sets. A critical insight is that simple Chain of Thought (CoT) reasoning often performs competitively with these complex systems, suggesting that the fundamental reasoning unit of MASes, CoT, warrants further investigation. To this end, we present a new paradigm for automatic MAS design that pivots the focus to optimizing CoT reasoning. We introduce the Agentic Reasoning Module (ARM), an agentic generalization of CoT where each granular reasoning step is executed by a specialized reasoning module. This module is discovered through a tree search over the code space, starting from a simple CoT module and evolved using mutations informed by reflection on execution traces. The resulting ARM acts as a versatile reasoning building block which can be utilized as a direct recursive loop or as a subroutine in a learned meta-orchestrator. Our approach significantly outperforms both manually designed MASes and state-of-the-art automatic MAS design methods. Crucially, MASes built with ARM exhibit superb generalization, maintaining high performance across different foundation models and task domains without further optimization.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "165",
        "title": "Communication Enables Cooperation in LLM Agents: A Comparison with Curriculum-Based Approaches",
        "author": [
            "Hachem Madmoun",
            "Salem Lahlou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05748",
        "abstract": "Eliciting cooperation in multi-agent LLM systems is critical for AI alignment. We investigate two approaches: direct communication and curriculum learning. In a 4-player Stag Hunt, a one-word \"cheap talk\" channel increases cooperation from 0% to 48.3%, demonstrating communication as a robust coordination mechanism. In contrast, we find that curriculum learning is highly sensitive to design choices: our pedagogical curriculum through progressively complex games reduced agent payoffs by 27.4% in an Iterated Public Goods Game with Punishment. Qualitative analysis reveals that curricula emphasizing defection-equilibrium games can induce \"learned pessimism\" in agents. These findings suggest that for coordination problems, simple communication protocols may be more reliable than experience-based training, and that curriculum design for social dilemmas requires careful attention to the strategic lessons embedded in game sequences.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "166",
        "title": "MSF-SER: Enriching Acoustic Modeling with Multi-Granularity Semantics for Speech Emotion Recognition",
        "author": [
            "Haoxun Li",
            "Yuqing Sun",
            "Hanlei Shi",
            "Yu Liu",
            "Leyuan Qu",
            "Taihao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05749",
        "abstract": "Continuous dimensional speech emotion recognition captures affective variation along valence, arousal, and dominance, providing finer-grained representations than categorical approaches. Yet most multimodal methods rely solely on global transcripts, leading to two limitations: (1) all words are treated equally, overlooking that emphasis on different parts of a sentence can shift emotional meaning; (2) only surface lexical content is represented, lacking higher-level interpretive cues. To overcome these issues, we propose MSF-SER (Multi-granularity Semantic Fusion for Speech Emotion Recognition), which augments acoustic features with three complementary levels of textual semantics--Local Emphasized Semantics (LES), Global Semantics (GS), and Extended Semantics (ES). These are integrated via an intra-modal gated fusion and a cross-modal FiLM-modulated lightweight Mixture-of-Experts (FM-MOE). Experiments on MSP-Podcast and IEMOCAP show that MSF-SER consistently improves dimensional prediction, demonstrating the effectiveness of enriched semantic fusion for SER.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "167",
        "title": "Uncertainty assessment in satellite-based greenhouse gas emissions estimates using emulated atmospheric transport",
        "author": [
            "Jeffrey N. Clark",
            "Elena Fillola",
            "Nawid Keshtmand",
            "Raul Santos-Rodriguez",
            "Matthew Rigby"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05751",
        "abstract": "Monitoring greenhouse gas emissions and evaluating national inventories require efficient, scalable, and reliable inference methods. Top-down approaches, combined with recent advances in satellite observations, provide new opportunities to evaluate emissions at continental and global scales. However, transport models used in these methods remain a key source of uncertainty: they are computationally expensive to run at scale, and their uncertainty is difficult to characterise. Artificial intelligence offers a dual opportunity to accelerate transport simulations and to quantify their associated uncertainty.\nWe present an ensemble-based pipeline for estimating atmospheric transport \"footprints\", greenhouse gas mole fraction measurements, and their uncertainties using a graph neural network emulator of a Lagrangian Particle Dispersion Model (LPDM). The approach is demonstrated with GOSAT (Greenhouse Gases Observing Satellite) observations for Brazil in 2016. The emulator achieved a ~1000x speed-up over the NAME LPDM, while reproducing large-scale footprint structures. Ensembles were calculated to quantify absolute and relative uncertainty, revealing spatial correlations with prediction error. The results show that ensemble spread highlights low-confidence spatial and temporal predictions for both atmospheric transport footprints and methane mole fractions.\nWhile demonstrated here for an LPDM emulator, the approach could be applied more generally to atmospheric transport models, supporting uncertainty-aware greenhouse gas inversion systems and improving the robustness of satellite-based emissions monitoring. With further development, ensemble-based emulators could also help explore systematic LPDM errors, offering a computationally efficient pathway towards a more comprehensive uncertainty budget in greenhouse gas flux estimates.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "168",
        "title": "EMORL-TTS: Reinforcement Learning for Fine-Grained Emotion Control in LLM-based TTS",
        "author": [
            "Haoxun Li",
            "Yu Liu",
            "Yuqing Sun",
            "Hanlei Shi",
            "Leyuan Qu",
            "Taihao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05758",
        "abstract": "Recent LLM-based TTS systems achieve strong quality and zero-shot ability, but lack fine-grained emotional control due to their reliance on discrete speech tokens. Existing approaches either limit emotions to categorical labels or cannot generalize to LLM-based architectures. We propose EMORL-TTS (Fine-grained Emotion-controllable TTS with Reinforcement Learning), a framework that unifies global intensity control in the VAD space with local emphasis regulation. Our method combines supervised fine-tuning with reinforcement learning guided by task-specific rewards for emotion category, intensity, and emphasis. Moreover, we further investigate how emphasis placement modulates fine-grained emotion intensity. Experiments show that EMORL-TTS improves emotion accuracy, intensity differentiation, and emphasis clarity, while preserving synthesis quality comparable to strong LLM-based baselines.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "169",
        "title": "Early Multimodal Prediction of Cross-Lingual Meme Virality on Reddit: A Time-Window Analysis",
        "author": [
            "Sedat Dogan",
            "Nina Dethlefs",
            "Debarati Chakraborty"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05761",
        "abstract": "Predicting the virality of online content remains challenging, especially for culturally complex, fast-evolving memes. This study investigates the feasibility of early prediction of meme virality using a large-scale, cross-lingual dataset from 25 diverse Reddit communities. We propose a robust, data-driven method to define virality based on a hybrid engagement score, learning a percentile-based threshold from a chronologically held-out training set to prevent data leakage. We evaluated a suite of models, including Logistic Regression, XGBoost, and a Multi-layer Perceptron (MLP), with a comprehensive, multimodal feature set across increasing time windows (30-420 min). Crucially, useful signals emerge quickly: our best-performing model, XGBoost, achieves a PR-AUC $>$ 0.52 in just 30 minutes. Our analysis reveals a clear \"evidentiary transition,\" in which the importance of the feature dynamically shifts from the static context to the temporal dynamics as a meme gains traction. This work establishes a robust, interpretable, and practical benchmark for early virality prediction in scenarios where full diffusion cascade data is unavailable, contributing a novel cross-lingual dataset and a methodologically sound definition of virality. To our knowledge, this study is the first to combine time series data with static content and network features to predict early meme virality.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "170",
        "title": "New Insights into Involutory and Orthogonal MDS Matrices",
        "author": [
            "Yogesh Kumar",
            "Susanta Samanta",
            "Atul Gaur"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05766",
        "abstract": "MDS matrices play a critical role in the design of diffusion layers for block ciphers and hash functions due to their optimal branch number. Involutory and orthogonal MDS matrices offer additional benefits by allowing identical or nearly identical circuitry for both encryption and decryption, leading to equivalent implementation costs for both processes. These properties have been further generalized through the notions of semi-involutory and semi-orthogonal matrices. Specifically, we establish nontrivial interconnections between semi-involutory and involutory matrices, as well as between semi-orthogonal and orthogonal matrices. Exploiting these relationships, we show that the number of semi-involutory MDS matrices can be directly derived from the number of involutory MDS matrices, and vice versa. A similar correspondence holds for semi-orthogonal and orthogonal MDS matrices. We also examine the intersection of these classes and show that the number of $3 \\times 3$ MDS matrices that are both semi-involutory and semi-orthogonal coincides with the number of semi-involutory MDS matrices over $\\mathbb{F}_{2^m}$. Furthermore, we derive the general structure of orthogonal matrices of arbitrary order $n$ over $\\mathbb{F}_{2^m}$. Based on this generic form, we provide a closed-form expression for enumerating all $3 \\times 3$ orthogonal MDS matrices over $\\mathbb{F}_{2^m}$. Finally, leveraging the aforementioned interconnections, we present explicit formulas for counting $3 \\times 3$ semi-involutory MDS matrices and semi-orthogonal MDS matrices.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "171",
        "title": "ConstraintLLM: A Neuro-Symbolic Framework for Industrial-Level Constraint Programming",
        "author": [
            "Weichun Shi",
            "Minghao Liu",
            "Wanting Zhang",
            "Langchen Shi",
            "Fuqi Jia",
            "Feifei Ma",
            "Jian Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05774",
        "abstract": "Constraint programming (CP) is a crucial technology for solving real-world constraint optimization problems (COPs), with the advantages of rich modeling semantics and high solving efficiency. Using large language models (LLMs) to generate formal modeling automatically for COPs is becoming a promising approach, which aims to build trustworthy neuro-symbolic AI with the help of symbolic solvers. However, CP has received less attention compared to works based on operations research (OR) models. We introduce ConstraintLLM, the first LLM specifically designed for CP modeling, which is trained on an open-source LLM with multi-instruction supervised fine-tuning. We propose the Constraint-Aware Retrieval Module (CARM) to increase the in-context learning capabilities, which is integrated in a Tree-of-Thoughts (ToT) framework with guided self-correction mechanism. Moreover, we construct and release IndusCP, the first industrial-level benchmark for CP modeling, which contains 140 challenging tasks from various domains. Our experiments demonstrate that ConstraintLLM achieves state-of-the-art solving accuracy across multiple benchmarks and outperforms the baselines by 2x on the new IndusCP benchmark. Code and data are available at: https://github.com/william4s/ConstraintLLM.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "172",
        "title": "Human-in-the-loop Optimisation in Robot-assisted Gait Training",
        "author": [
            "Andreas Christou",
            "Andreas Sochopoulos",
            "Elliot Lister",
            "Sethu Vijayakumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05780",
        "abstract": "Wearable robots offer a promising solution for quantitatively monitoring gait and providing systematic, adaptive assistance to promote patient independence and improve gait. However, due to significant interpersonal and intrapersonal variability in walking patterns, it is important to design robot controllers that can adapt to the unique characteristics of each individual. This paper investigates the potential of human-in-the-loop optimisation (HILO) to deliver personalised assistance in gait training. The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) was employed to continuously optimise an assist-as-needed controller of a lower-limb exoskeleton. Six healthy individuals participated over a two-day experiment. Our results suggest that while the CMA-ES appears to converge to a unique set of stiffnesses for each individual, no measurable impact on the subjects' performance was observed during the validation trials. These findings highlight the impact of human-robot co-adaptation and human behaviour variability, whose effect may be greater than potential benefits of personalising rule-based assistive controllers. Our work contributes to understanding the limitations of current personalisation approaches in exoskeleton-assisted gait rehabilitation and identifies key challenges for effective implementation of human-in-the-loop optimisation in this domain.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "173",
        "title": "Mixture of Neuron Experts",
        "author": [
            "Runxi Cheng",
            "Yuchen Guan",
            "Yucheng Ding",
            "Qingguo Hu",
            "Yongxian Wei",
            "Chun Yuan",
            "Yelong Shen",
            "Weizhu Chen",
            "Yeyun Gong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05781",
        "abstract": "In this work, we first explore whether the parameters activated by the MoE layer remain highly sparse at inference. We perform a sparsification study on several representative MoE models. For each expert, we rank parameters by the magnitude of their activations from the gate projection and progressively prune the activated subset. Pruning up to 60% of parameters within that subset causes only negligible task-performance degradation; substantial drops occur only after more than 90% are removed. We further decompose experts into neuron-granular MoE and visualize their activation values, finding that most neuron activations are near zero. This observation motivates us to select only high-activation neuron experts during pretraining. Based on this insight, we propose Mixture of Neuron Experts (MoNE). MoNE achieves neuron-granular expert selection by only applying a simple top-k selection within each expert, incurs negligible latency, and requires no additional routing parameters or inter-expert communication. Extensive experiments demonstrate that MoNE matches traditional MoE performance while activating only 50% of the MoE-layer parameters, and it consistently outperforms traditional MoE when compared at equal numbers of activated parameters. These results suggest that MoNE is a practical approach to improving parameter utilization and inference efficiency in MoE-like models.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "174",
        "title": "Mellum: Production-Grade in-IDE Contextual Code Completion with Multi-File Project Understanding",
        "author": [
            "Nikita Pavlichenko",
            "Iurii Nazarov",
            "Ivan Dolgov",
            "Ekaterina Garanina",
            "Dmitry Ustalov",
            "Ivan Bondyrev",
            "Kseniia Lysaniuk",
            "Evgeniia Vu",
            "Kirill Chekmenev",
            "Joseph Shtok",
            "Yaroslav Golubev",
            "Anton Semenkin",
            "Uladzislau Sazanovich"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05788",
        "abstract": "We present the Mellum models family, open-weight code completion models designed for interactive use in JetBrains IDEs. Mellums have 4B parameters, adopt a Llama-style architecture, and are pre-trained on ~4T tokens of permissively licensed, multi-language code. Our studies show that (i) careful data curation and staged training significantly improve the model's quality, (ii) editor-critical capabilities such as context packing are necessary for high-quality suggestions, and (iii) a compact, task-focused model can meet the cost and latency constraints of interactive completion.\nIn the paper, we describe an end-to-end industrial pipeline for producing contextualized in-editor completion: disciplined data governance, multi-stage training that includes fill-in-the-middle and project context via supervised fine-tuning, and alignment via direct preference optimization using feedback from real-world scenarios. Our quality evaluations include both large-scale offline benchmarks and online telemetry from production deployments in JetBrains IDEs. Mellums are released under the Apache-2.0 license on HuggingFace, with a public model card providing a reproducible reference for practitioners. Our experience offers a pragmatic blueprint for taking a focused, open model from a research prototype to at scale production for hundreds of thousands of users.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "175",
        "title": "Data-efficient Targeted Token-level Preference Optimization for LLM-based Text-to-Speech",
        "author": [
            "Rikuto Kotoge",
            "Yuichi Sasaki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05799",
        "abstract": "Aligning text-to-speech (TTS) system outputs with human feedback through preference optimization has been shown to effectively improve the robustness and naturalness of language model-based TTS models. Current approaches primarily require paired desirable and undesirable samples at the utterance level. However, such pairs are often limited in TTS output data, and utterance-level formulation prevents fine-grained token-level optimization needed for accurate pronunciation alignment. In this study, we propose TKTO that eliminates the need for paired data, enabling a more data-efficient training paradigm, and directly targets token-level units, automatically providing fine-grained alignment signals without token-level annotations. TKTO improves the challenging Japanese TTS accuracy by 39% and reduces CER by 54%, automatically assigning 12.8 times stronger reward to targeted tokens.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "176",
        "title": "Risk level dependent Minimax Quantile lower bounds for Interactive Statistical Decision Making",
        "author": [
            "Raghav Bongole",
            "Amirreza Zamani",
            "Tobias J. Oechtering",
            "Mikael Skoglund"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05808",
        "abstract": "Minimax risk and regret focus on expectation, missing rare failures critical in safety-critical bandits and reinforcement learning. Minimax quantiles capture these tails. Three strands of prior work motivate this study: minimax-quantile bounds restricted to non-interactive estimation; unified interactive analyses that focus on expected risk rather than risk level specific quantile bounds; and high-probability bandit bounds that still lack a quantile-specific toolkit for general interactive protocols. To close this gap, within the interactive statistical decision making framework, we develop high-probability Fano and Le Cam tools and derive risk level explicit minimax-quantile bounds, including a quantile-to-expectation conversion and a tight link between strict and lower minimax quantiles. Instantiating these results for the two-armed Gaussian bandit immediately recovers optimal-rate bounds.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "177",
        "title": "Rasterized Steered Mixture of Experts for Efficient 2D Image Regression",
        "author": [
            "Yi-Hsin Li",
            "Thomas Sikora",
            "Sebastian Knorr",
            "MÃ¥rten SjÃ¶strÃ¶m"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05814",
        "abstract": "The Steered Mixture of Experts regression framework has demonstrated strong performance in image reconstruction, compression, denoising, and super-resolution. However, its high computational cost limits practical applications. This work introduces a rasterization-based optimization strategy that combines the efficiency of rasterized Gaussian kernel rendering with the edge-aware gating mechanism of the Steered Mixture of Experts. The proposed method is designed to accelerate two-dimensional image regression while maintaining the model's inherent sparsity and reconstruction quality. By replacing global iterative optimization with a rasterized formulation, the method achieves significantly faster parameter updates and more memory-efficient model representations. In addition, the proposed framework supports applications such as native super-resolution and image denoising, which are not directly achievable with standard rasterized Gaussian kernel approaches. The combination of fast rasterized optimization with the edge-aware structure of the Steered Mixture of Experts provides a new balance between computational efficiency and reconstruction fidelity for two-dimensional image processing tasks.",
        "tags": [
            "MoE",
            "Super Resolution"
        ]
    },
    {
        "id": "178",
        "title": "VCoT-Grasp: Grasp Foundation Models with Visual Chain-of-Thought Reasoning for Language-driven Grasp Generation",
        "author": [
            "Haoran Zhang",
            "Shuanghao Bai",
            "Wanqi Zhou",
            "Yuedi Zhang",
            "Qi Zhang",
            "Pengxiang Ding",
            "Cheng Chi",
            "Donglin Wang",
            "Badong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05827",
        "abstract": "Robotic grasping is one of the most fundamental tasks in robotic manipulation, and grasp detection/generation has long been the subject of extensive research. Recently, language-driven grasp generation has emerged as a promising direction due to its practical interaction capabilities. However, most existing approaches either lack sufficient reasoning and generalization capabilities or depend on complex modular pipelines. Moreover, current grasp foundation models tend to overemphasize dialog and object semantics, resulting in inferior performance and restriction to single-object grasping. To maintain strong reasoning ability and generalization in cluttered environments, we propose VCoT-Grasp, an end-to-end grasp foundation model that incorporates visual chain-of-thought reasoning to enhance visual understanding for grasp generation. VCoT-Grasp adopts a multi-turn processing paradigm that dynamically focuses on visual inputs while providing interpretable reasoning traces. For training, we refine and introduce a large-scale dataset, VCoT-GraspSet, comprising 167K synthetic images with over 1.36M grasps, as well as 400+ real-world images with more than 1.2K grasps, annotated with intermediate bounding boxes. Extensive experiments on both VCoT-GraspSet and real robot demonstrate that our method significantly improves grasp success rates and generalizes effectively to unseen objects, backgrounds, and distractors. More details can be found at https://zhanghr2001.github.io/VCoT-Grasp.github.io.",
        "tags": [
            "CoT",
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "179",
        "title": "StereoSync: Spatially-Aware Stereo Audio Generation from Video",
        "author": [
            "Christian Marinoni",
            "Riccardo Fosco Gramaccioni",
            "Kazuki Shimada",
            "Takashi Shibuya",
            "Yuki Mitsufuji",
            "Danilo Comminiello"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05828",
        "abstract": "Although audio generation has been widely studied over recent years, video-aligned audio generation still remains a relatively unexplored frontier. To address this gap, we introduce StereoSync, a novel and efficient model designed to generate audio that is both temporally synchronized with a reference video and spatially aligned with its visual context. Moreover, StereoSync also achieves efficiency by leveraging pretrained foundation models, reducing the need for extensive training while maintaining high-quality synthesis. Unlike existing methods that primarily focus on temporal synchronization, StereoSync introduces a significant advancement by incorporating spatial awareness into video-aligned audio generation. Indeed, given an input video, our approach extracts spatial cues from depth maps and bounding boxes, using them as cross-attention conditioning in a diffusion-based audio generation model. Such an approach allows StereoSync to go beyond simple synchronization, producing stereo audio that dynamically adapts to the spatial structure and movement of a video scene. We evaluate StereoSync on Walking The Maps, a curated dataset comprising videos from video games that feature animated characters walking through diverse environments. Experimental results demonstrate the ability of StereoSync to achieve both temporal and spatial alignment, advancing the state of the art in video-to-audio generation and resulting in a significantly more immersive and realistic audio experience.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "180",
        "title": "FoleyGRAM: Video-to-Audio Generation with GRAM-Aligned Multimodal Encoders",
        "author": [
            "Riccardo Fosco Gramaccioni",
            "Christian Marinoni",
            "Eleonora Grassucci",
            "Giordano Cicchetti",
            "Aurelio Uncini",
            "Danilo Comminiello"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05829",
        "abstract": "In this work, we present FoleyGRAM, a novel approach to video-to-audio generation that emphasizes semantic conditioning through the use of aligned multimodal encoders. Building on prior advancements in video-to-audio generation, FoleyGRAM leverages the Gramian Representation Alignment Measure (GRAM) to align embeddings across video, text, and audio modalities, enabling precise semantic control over the audio generation process. The core of FoleyGRAM is a diffusion-based audio synthesis model conditioned on GRAM-aligned embeddings and waveform envelopes, ensuring both semantic richness and temporal alignment with the corresponding input video. We evaluate FoleyGRAM on the Greatest Hits dataset, a standard benchmark for video-to-audio models. Our experiments demonstrate that aligning multimodal encoders using GRAM enhances the system's ability to semantically align generated audio with video content, advancing the state of the art in video-to-audio synthesis.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "181",
        "title": "Fairness in Token Delegation: Mitigating Voting Power Concentration in DAOs",
        "author": [
            "Johnnatan Messias",
            "Ayae Ide"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05830",
        "abstract": "Decentralized Autonomous Organizations (DAOs) aim to enable participatory governance, but in practice face challenges of voter apathy, concentration of voting power, and misaligned delegation. Existing delegation mechanisms often reinforce visibility biases, where a small set of highly ranked delegates accumulate disproportionate influence regardless of their alignment with the broader community. In this paper, we conduct an empirical study of delegation in DAO governance, combining on-chain data from five major protocols with off-chain discussions from 14 DAO forums. We develop a methodology to link forum participants to on-chain addresses, extract governance interests using large language models, and compare these interests against delegates' historical behavior. Our analysis reveals that delegations are frequently misaligned with token holders' expressed priorities and that current ranking-based interfaces exacerbate power concentration. We argue that incorporating interest alignment into delegation processes could mitigate these imbalances and improve the representativeness of DAO decision-making.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "182",
        "title": "Flow4Agent: Long-form Video Understanding via Motion Prior from Optical Flow",
        "author": [
            "Ruyang Liu",
            "Shangkun Sun",
            "Haoran Tang",
            "Ge Li",
            "Wei Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05836",
        "abstract": "Long-form video understanding has always been a challenging problem due to the significant redundancy in both temporal and spatial contents. This challenge is further exacerbated by the limited context length of Multimodal Large Language Models (MLLMs). To address this issue, many previous works have attempted to extract key video information, where the \"key\" is typically semantic-aware and heavily dependent on the CLIP model as prior. In this paper, we propose Flow4Agent, a novel framework that pioneeringly incorporates motion priors from optical flow to facilitate LLM-based long video understanding. Flow4Agent mitigates the redundancy in long videos at both temporal and spatial levels through two core modules: Temporal Granularity Optimization (TGO) adaptively refines framelevel hierarchies, which first leverages coarse flow priors to group similar visual contents and then applies semantic priors to filter out highly irrelevant scene information. Motion Token Pruning (MTP) further refines the intra-frame visual representations, pruning high-redundancy video tokens using fine-grained optical flow information. Extensive experiments demonstrate that our Flow4Agent outperforms existing methods across a wide range of video MLLM benchmarks, especially for hour-level video understanding tasks, achieving 64.7% on Video-MME, 71.4% on MLVU and 60.4% on LongVideoBench.",
        "tags": [
            "CLIP",
            "LLM"
        ]
    },
    {
        "id": "183",
        "title": "EEPO: Exploration-Enhanced Policy Optimization via Sample-Then-Forget",
        "author": [
            "Liang Chen",
            "Xueting Han",
            "Qizhou Wang",
            "Bo Han",
            "Jing Bai",
            "Hinrich Schutze",
            "Kam-Fai Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05837",
        "abstract": "Balancing exploration and exploitation remains a central challenge in reinforcement learning with verifiable rewards (RLVR) for large language models (LLMs). Current RLVR methods often overemphasize exploitation, leading to entropy collapse, diminished exploratory capacity, and ultimately limited performance gains. Although techniques that increase policy stochasticity can promote exploration, they frequently fail to escape dominant behavioral modes. This creates a self-reinforcing loop-repeatedly sampling and rewarding dominant modes-that further erodes exploration. We introduce Exploration-Enhanced Policy Optimization (EEPO), a framework that promotes exploration via two-stage rollouts with adaptive unlearning. In the first stage, the model generates half of the trajectories; it then undergoes a lightweight unlearning step to temporarily suppress these sampled responses, forcing the second stage to explore different regions of the output space. This sample-then-forget mechanism disrupts the self-reinforcing loop and promotes wider exploration during rollouts. Across five reasoning benchmarks, EEPO outperforms GRPO, achieving average relative gains of 24.3% on Qwen2.5-3B, 33.0% on Llama3.2-3B-Instruct, and 10.4% on Qwen3-8B-Base.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "184",
        "title": "Luth: Efficient French Specialization for Small Language Models and Cross-Lingual Transfer",
        "author": [
            "Maxence Lasbordes",
            "SinouÃ© Gad"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05846",
        "abstract": "The landscape of Large Language Models (LLMs) remains predominantly English-centric, resulting in a significant performance gap for other major languages, such as French, especially in the context of Small Language Models (SLMs). Existing multilingual models demonstrate considerably lower performance in French compared to English, and research on efficient adaptation methods for French remains limited. To address this, we introduce \\textbf{Luth}, a family of French-specialized SLMs: through targeted post-training on curated, high-quality French data, our models outperform all open-source counterparts of comparable size on multiple French benchmarks while retaining their original English capabilities. We further show that strategic model merging enhances performance in both languages, establishing Luth as a new state of the art for French SLMs and a robust baseline for future French-language research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "185",
        "title": "DACP: Domain-Adaptive Continual Pre-Training of Large Language Models for Phone Conversation Summarization",
        "author": [
            "Xue-Yong Fu",
            "Elena Khasanova",
            "Md Tahmid Rahman Laskar",
            "Harsh Saini",
            "Shashi Bhushan TN"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05858",
        "abstract": "Large language models (LLMs) have achieved impressive performance in text summarization, yet their performance often falls short when applied to specialized domains %or conversational data that differ from their original pre-training distribution. While fine-tuning can improve summarization quality, it typically relies on costly and scarce high-quality labeled data. In this work, we explore continual pre-training as a scalable, self-supervised approach to adapt LLMs for downstream summarization tasks, particularly in the context of noisy real-world conversation transcripts. We conduct extensive experiments using large-scale, unlabeled business conversation data to investigate whether continual pre-training enhances model capabilities in conversational summarization. Our results demonstrate that continual pre-training yields substantial gains in both in-domain and out-of-domain summarization benchmarks, while maintaining strong generalization and robustness. We also analyze the effects of data selection strategies, providing practical guidelines for applying continual pre-training in summarization-focused industrial applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "186",
        "title": "Automated Boilerplate: Prevalence and Quality of Contract Generators in the Context of Swiss Privacy Policies",
        "author": [
            "Luka Nenadic",
            "David Rodriguez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05860",
        "abstract": "It has become increasingly challenging for firms to comply with a plethora of novel digital regulations. This is especially true for smaller businesses that often lack both the resources and know-how to draft complex legal documents. Instead of seeking costly legal advice from attorneys, firms may turn to cheaper alternative legal service providers such as automated contract generators. While these services have a long-standing presence, there is little empirical evidence on their prevalence and output quality.\nWe address this gap in the context of a 2023 Swiss privacy law revision. To enable a systematic evaluation, we create and annotate a multilingual benchmark dataset that captures key compliance obligations under Swiss and EU privacy law. Using this dataset, we validate a novel GPT-5-based method for large-scale compliance assessment of privacy policies, allowing us to measure the impact of the revision. We observe compliance increases indicating an effect of the revision. Generators, explicitly referenced by 18% of local websites, are associated with substantially higher levels of compliance, with increases of up to 15 percentage points compared to privacy policies without generator use. These findings contribute to three debates: the potential of LLMs for cross-lingual legal analysis, the Brussels Effect of EU regulations, and, crucially, the role of automated tools in improving compliance and contractual quality.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "187",
        "title": "Revisiting Long-context Modeling from Context Denoising Perspective",
        "author": [
            "Zecheng Tang",
            "Baibei Ji",
            "Juntao Li",
            "Lijun Wu",
            "Haijia Gui",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05862",
        "abstract": "Long-context models (LCMs) have demonstrated great potential in processing long sequences, facilitating many real-world applications. The success of LCMs can be attributed to their ability to locate implicit critical information within the context for further prediction. However, recent research reveals that LCMs are often susceptible to contextual noise, i.e., irrelevant tokens, that can mislead model attention. In this paper, we conduct a fine-grained analysis of the context noise and propose an effective metric, the Integrated Gradient (IG) score, to detect and quantify the noise information within the context. Our findings reveal that even simple mitigation of detected context noise can substantially boost the model's attention on critical tokens and benefit subsequent predictions. Building on this insight, we propose Context Denoising Training (CDT), a straightforward yet effective training strategy that improves attention on critical tokens while reinforcing their influence on model predictions. Extensive experiments across four tasks, under both context window scaling and long-context alignment settings, demonstrate the superiority of CDT. Notably, when trained with CDT, an open-source 8B model can achieve performance (50.92) comparable to GPT-4o (51.00).",
        "tags": [
            "GPT",
            "LCMs"
        ]
    },
    {
        "id": "188",
        "title": "Evaluating the Sensitivity of LLMs to Harmful Contents in Long Input",
        "author": [
            "Faeze Ghorbanpour",
            "Alexander Fraser"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05864",
        "abstract": "Large language models (LLMs) increasingly support applications that rely on extended context, from document processing to retrieval-augmented generation. While their long-context capabilities are well studied for reasoning and retrieval, little is known about their behavior in safety-critical scenarios. We evaluate LLMs' sensitivity to harmful content under extended context, varying type (explicit vs. implicit), position (beginning, middle, end), prevalence (0.01-0.50 of the prompt), and context length (600-6000 tokens). Across harmful content categories such as toxic, offensive, and hate speech, with LLaMA-3, Qwen-2.5, and Mistral, we observe similar patterns: performance peaks at moderate harmful prevalence (0.25) but declines when content is very sparse or dominant; recall decreases with increasing context length; harmful sentences at the beginning are generally detected more reliably; and explicit content is more consistently recognized than implicit. These findings provide the first systematic view of how LLMs prioritize and calibrate harmful content in long contexts, highlighting both their emerging strengths and the challenges that remain for safety-critical use.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "189",
        "title": "The Safety Challenge of World Models for Embodied AI Agents: A Review",
        "author": [
            "Lorenzo Baraldi",
            "Zifan Zeng",
            "Chongzhe Zhang",
            "Aradhana Nayak",
            "Hongbo Zhu",
            "Feng Liu",
            "Qunli Zhang",
            "Peng Wang",
            "Shiming Liu",
            "Zheng Hu",
            "Angelo Cangelosi",
            "Lorenzo Baraldi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05865",
        "abstract": "The rapid progress in embodied artificial intelligence has highlighted the necessity for more advanced and integrated models that can perceive, interpret, and predict environmental dynamics. In this context, World Models (WMs) have been introduced to provide embodied agents with the abilities to anticipate future environmental states and fill in knowledge gaps, thereby enhancing agents' ability to plan and execute actions. However, when dealing with embodied agents it is fundamental to ensure that predictions are safe for both the agent and the environment. In this article, we conduct a comprehensive literature review of World Models in the domains of autonomous driving and robotics, with a specific focus on the safety implications of scene and control generation tasks. Our review is complemented by an empirical analysis, wherein we collect and examine predictions from state-of-the-art models, identify and categorize common faults (herein referred to as pathologies), and provide a quantitative evaluation of the results.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "190",
        "title": "The fragility of \"cultural tendencies\" in LLMs",
        "author": [
            "Kun Sun",
            "Rong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05869",
        "abstract": "In a recent study, Lu, Song, and Zhang (2025) (LSZ) propose that large language models (LLMs), when prompted in different languages, display culturally specific tendencies. They report that the two models (i.e., GPT and ERNIE) respond in more interdependent and holistic ways when prompted in Chinese, and more independent and analytic ways when prompted in English. LSZ attribute these differences to deep-seated cultural patterns in the models, claiming that prompt language alone can induce substantial cultural shifts. While we acknowledge the empirical patterns they observed, we find their experiments, methods, and interpretations problematic. In this paper, we critically re-evaluate the methodology, theoretical framing, and conclusions of LSZ. We argue that the reported \"cultural tendencies\" are not stable traits but fragile artifacts of specific models and task design. To test this, we conducted targeted replications using a broader set of LLMs and a larger number of test items. Our results show that prompt language has minimal effect on outputs, challenging LSZ's claim that these models encode grounded cultural beliefs.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "191",
        "title": "MaNGO - Adaptable Graph Network Simulators via Meta-Learning",
        "author": [
            "Philipp Dahlinger",
            "Tai Hoang",
            "Denis Blessing",
            "Niklas Freymuth",
            "Gerhard Neumann"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05874",
        "abstract": "Accurately simulating physics is crucial across scientific domains, with applications spanning from robotics to materials science. While traditional mesh-based simulations are precise, they are often computationally expensive and require knowledge of physical parameters, such as material properties. In contrast, data-driven approaches like Graph Network Simulators (GNSs) offer faster inference but suffer from two key limitations: Firstly, they must be retrained from scratch for even minor variations in physical parameters, and secondly they require labor-intensive data collection for each new parameter setting. This is inefficient, as simulations with varying parameters often share a common underlying latent structure. In this work, we address these challenges by learning this shared structure through meta-learning, enabling fast adaptation to new physical parameters without retraining. To this end, we propose a novel architecture that generates a latent representation by encoding graph trajectories using conditional neural processes (CNPs). To mitigate error accumulation over time, we combine CNPs with a novel neural operator architecture. We validate our approach, Meta Neural Graph Operator (MaNGO), on several dynamics prediction tasks with varying material properties, demonstrating superior performance over existing GNS methods. Notably, MaNGO achieves accuracy on unseen material properties close to that of an oracle model.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "192",
        "title": "$\\bf{D^3}$QE: Learning Discrete Distribution Discrepancy-aware Quantization Error for Autoregressive-Generated Image Detection",
        "author": [
            "Yanran Zhang",
            "Bingyao Yu",
            "Yu Zheng",
            "Wenzhao Zheng",
            "Yueqi Duan",
            "Lei Chen",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05891",
        "abstract": "The emergence of visual autoregressive (AR) models has revolutionized image generation while presenting new challenges for synthetic image detection. Unlike previous GAN or diffusion-based methods, AR models generate images through discrete token prediction, exhibiting both marked improvements in image synthesis quality and unique characteristics in their vector-quantized representations. In this paper, we propose to leverage Discrete Distribution Discrepancy-aware Quantization Error (D$^3$QE) for autoregressive-generated image detection that exploits the distinctive patterns and the frequency distribution bias of the codebook existing in real and fake images. We introduce a discrete distribution discrepancy-aware transformer that integrates dynamic codebook frequency statistics into its attention mechanism, fusing semantic features and quantization error latent. To evaluate our method, we construct a comprehensive dataset termed ARForensics covering 7 mainstream visual AR models. Experiments demonstrate superior detection accuracy and strong generalization of D$^3$QE across different AR models, with robustness to real-world perturbations. Code is available at \\href{https://github.com/Zhangyr2022/D3QE}{https://github.com/Zhangyr2022/D3QE}.",
        "tags": [
            "Detection",
            "Diffusion",
            "GAN",
            "Transformer"
        ]
    },
    {
        "id": "193",
        "title": "Paying Attention to Hybrid Attention: Untangling the Issues with Conversion Methods",
        "author": [
            "Martin Benfeghoul",
            "Teresa Delgado",
            "Adnan Oomerjee",
            "Haitham Bou Ammar",
            "Jun Wang",
            "Zafeirios Fountas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05901",
        "abstract": "Transformers' quadratic computational complexity limits their scalability despite remarkable performance. While linear attention reduces this to linear complexity, pre-training such models from scratch remains, in most cases, prohibitively expensive. Recent post-training linearisation methods convert pre-trained Transformers to linear models efficiently, often using hybrid approaches that combine linear attention with sliding-window softmax. We identify a critical flaw: existing hybrid methods inadvertently bypass the linear component, relying almost entirely on SWA. Component-level diagnostics reveal this previously undetected behaviour stems from overlooked evaluation practices on common-sense benchmarks. We propose three solutions to ensure balanced component usage: (i) inference-time hybridisation of linear-only conversions with sliding-window softmax; (ii) HedgeCATs, combining attention-weight transfer with targeted LoRA fine-tuning; and (iii) Scheduled Sliding-window Dropout (SSD), which stochastically suppresses the softmax branch during training to prevent component collapse. Our methods maintain computational efficiency while recovering most base model performance and ensuring genuine linear attention adoption, restoring the validity of performance attributions in hybrid conversions.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "194",
        "title": "Optimizing for Persuasion Improves LLM Generalization: Evidence from Quality-Diversity Evolution of Debate Strategies",
        "author": [
            "Aksel Joonas Reedi",
            "Corentin LÃ©ger",
            "Julien Pourcel",
            "Loris Gaven",
            "Perrine Charriau",
            "Guillaume Pourcel"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05909",
        "abstract": "Large Language Models (LLMs) optimized to output truthful answers often overfit, producing brittle reasoning that fails to generalize. While persuasion-based optimization has shown promise in debate settings, it has not been systematically compared against mainstream truth-based approaches. We introduce DebateQD, a minimal Quality-Diversity (QD) evolutionary algorithm that evolves diverse debate strategies across different categories (rationality, authority, emotional appeal, etc.) through tournament-style competitions where two LLMs debate while a third judges. Unlike previously proposed methods that require a population of LLMs, our approach maintains diversity of opponents through prompt-based strategies within a single LLM architecture, making it more accessible for experiments while preserving the key benefits of population-based optimization. In contrast to prior work, we explicitly isolate the role of the optimization objective by fixing the debate protocol and swapping only the fitness function: persuasion rewards strategies that convince the judge irrespective of truth, whereas truth rewards collaborative correctness. Across three model scales (7B, 32B, 72B parameters) and multiple dataset sizes from the QuALITY benchmark, persuasion-optimized strategies achieve up to 13.94% smaller train-test generalization gaps, while matching or exceeding truth optimization's test performance. These results provide the first controlled evidence that competitive pressure to persuade, rather than seek the truth collaboratively, fosters more transferable reasoning skills, offering a promising path for improving LLM generalization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "195",
        "title": "Hire Your Anthropologist! Rethinking Culture Benchmarks Through an Anthropological Lens",
        "author": [
            "Mai AlKhamissi",
            "Yunze Xiao",
            "Badr AlKhamissi",
            "Mona Diab"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05931",
        "abstract": "Cultural evaluation of large language models has become increasingly important, yet current benchmarks often reduce culture to static facts or homogeneous values. This view conflicts with anthropological accounts that emphasize culture as dynamic, historically situated, and enacted in practice. To analyze this gap, we introduce a four-part framework that categorizes how benchmarks frame culture, such as knowledge, preference, performance, or bias. Using this lens, we qualitatively examine 20 cultural benchmarks and identify six recurring methodological issues, including treating countries as cultures, overlooking within-culture diversity, and relying on oversimplified survey formats. Drawing on established anthropological methods, we propose concrete improvements: incorporating real-world narratives and scenarios, involving cultural communities in design and validation, and evaluating models in context rather than isolation. Our aim is to guide the development of cultural benchmarks that go beyond static recall tasks and more accurately capture the responses of the models to complex cultural situations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "196",
        "title": "LLM-FS-Agent: A Deliberative Role-based Large Language Model Architecture for Transparent Feature Selection",
        "author": [
            "Mohamed Bal-Ghaoui",
            "Fayssal Sabri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05935",
        "abstract": "High-dimensional data remains a pervasive challenge in machine learning, often undermining model interpretability and computational efficiency. While Large Language Models (LLMs) have shown promise for dimensionality reduction through feature selection, existing LLM-based approaches frequently lack structured reasoning and transparent justification for their decisions. This paper introduces LLM-FS-Agent, a novel multi-agent architecture designed for interpretable and robust feature selection. The system orchestrates a deliberative \"debate\" among multiple LLM agents, each assigned a specific role, enabling collective evaluation of feature relevance and generation of detailed justifications. We evaluate LLM-FS-Agent in the cybersecurity domain using the CIC-DIAD 2024 IoT intrusion detection dataset and compare its performance against strong baselines, including LLM-Select and traditional methods such as PCA. Experimental results demonstrate that LLM-FS-Agent consistently achieves superior or comparable classification performance while reducing downstream training time by an average of 46% (statistically significant improvement, p = 0.028 for XGBoost). These findings highlight that the proposed deliberative architecture enhances both decision transparency and computational efficiency, establishing LLM-FS-Agent as a practical and reliable solution for real-world applications.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "197",
        "title": "EvalMORAAL: Interpretable Chain-of-Thought and LLM-as-Judge Evaluation for Moral Alignment in Large Language Models",
        "author": [
            "Hadi Mohammadi",
            "Anastasia Giachanou",
            "Ayoub Bagheri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05942",
        "abstract": "We present EvalMORAAL, a transparent chain-of-thought (CoT) framework that uses two scoring methods (log-probabilities and direct ratings) plus a model-as-judge peer review to evaluate moral alignment in 20 large language models. We assess models on the World Values Survey (55 countries, 19 topics) and the PEW Global Attitudes Survey (39 countries, 8 topics). With EvalMORAAL, top models align closely with survey responses (Pearson's r approximately 0.90 on WVS). Yet we find a clear regional difference: Western regions average r=0.82 while non-Western regions average r=0.61 (a 0.21 absolute gap), indicating consistent regional bias. Our framework adds three parts: (1) two scoring methods for all models to enable fair comparison, (2) a structured chain-of-thought protocol with self-consistency checks, and (3) a model-as-judge peer review that flags 348 conflicts using a data-driven threshold. Peer agreement relates to survey alignment (WVS r=0.74, PEW r=0.39, both p<.001), supporting automated quality checks. These results show real progress toward culture-aware AI while highlighting open challenges for use across regions.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "198",
        "title": "EARL: Efficient Agentic Reinforcement Learning Systems for Large Language Models",
        "author": [
            "Zheyue Tan",
            "Mustapha Abdullahi",
            "Tuo Shi",
            "Huining Yuan",
            "Zelai Xu",
            "Chao Yu",
            "Boxun Li",
            "Bo Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05943",
        "abstract": "Reinforcement learning (RL) has become a pivotal component of large language model (LLM) post-training, and agentic RL extends this paradigm to operate as agents through multi-turn interaction and tool use. Scaling such systems exposes two practical bottlenecks: (1) context length grows rapidly during training, inflating memory usage and latency, and triggering out-of-memory (OOM) failures; and (2) intermediate tensors accumulate with context length, making cross-device data movement a major system bottleneck.\nWe present EARL, a scalable system for efficient agentic RL. EARL designs a parallelism selector that dynamically adapts model and training parallelism across RL stages based on sequence length and system load, and a data dispatcher that performs layout-aware, decentralized exchange of intermediate data batches. Together, these components increase throughput, reduce long-context failures, and enable stable large-scale training of agentic LLMs without relying on hard limits or penalties of context length.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "199",
        "title": "Training-Free Time Series Classification via In-Context Reasoning with LLM Agents",
        "author": [
            "Songyuan Sui",
            "Zihang Xu",
            "Yu-Neng Chuang",
            "Kwei-Herng Lai",
            "Xia Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05950",
        "abstract": "Time series classification (TSC) spans diverse application scenarios, yet labeled data are often scarce, making task-specific training costly and inflexible. Recent reasoning-oriented large language models (LLMs) show promise in understanding temporal patterns, but purely zero-shot usage remains suboptimal. We propose FETA, a multi-agent framework for training-free TSC via exemplar-based in-context reasoning. FETA decomposes a multivariate series into channel-wise subproblems, retrieves a few structurally similar labeled examples for each channel, and leverages a reasoning LLM to compare the query against these exemplars, producing channel-level labels with self-assessed confidences; a confidence-weighted aggregator then fuses all channel decisions. This design eliminates the need for pretraining or fine-tuning, improves efficiency by pruning irrelevant channels and controlling input length, and enhances interpretability through exemplar grounding and confidence estimation. On nine challenging UEA datasets, FETA achieves strong accuracy under a fully training-free setting, surpassing multiple trained baselines. These results demonstrate that a multi-agent in-context reasoning framework can transform LLMs into competitive, plug-and-play TSC solvers without any parameter training. The code is available at https://github.com/SongyuanSui/FETATSC.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "200",
        "title": "Learning to Crawl: Latent Model-Based Reinforcement Learning for Soft Robotic Adaptive Locomotion",
        "author": [
            "Vaughn Gzenda",
            "Robin Chhabra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05957",
        "abstract": "Soft robotic crawlers are mobile robots that utilize soft body deformability and compliance to achieve locomotion through surface contact. Designing control strategies for such systems is challenging due to model inaccuracies, sensor noise, and the need to discover locomotor gaits. In this work, we present a model-based reinforcement learning (MB-RL) framework in which latent dynamics inferred from onboard sensors serve as a predictive model that guides an actor-critic algorithm to optimize locomotor policies. We evaluate the framework on a minimal crawler model in simulation using inertial measurement units and time-of-flight sensors as observations. The learned latent dynamics enable short-horizon motion prediction while the actor-critic discovers effective locomotor policies. This approach highlights the potential of latent-dynamics MB-RL for enabling embodied soft robotic adaptive locomotion based solely on noisy sensor feedback.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "201",
        "title": "Extending ResourceLink: Patterns for Large Dataset Processing in MCP Applications",
        "author": [
            "Scott Frees"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05968",
        "abstract": "Large language models translate natural language into database queries, yet context window limitations prevent direct deployment in reporting systems where complete datasets exhaust available tokens. The Model Context Protocol specification defines ResourceLink for referencing external resources, but practical patterns for implementing scalable reporting architectures remain undocumented. This paper presents patterns for building LLM-powered reporting systems that decouple query generation from data retrieval. We introduce a dual-response pattern extending ResourceLink to support both iterative query refinement and out-of-band data access, accompanied by patterns for multi-tenant security and resource lifecycle management. These patterns address fundamental challenges in LLM-driven reporting applications and provide practical guidance for developers building them.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "202",
        "title": "Probing the Difficulty Perception Mechanism of Large Language Models",
        "author": [
            "Sunbowen Lee",
            "Qingyu Yin",
            "Chak Tou Leong",
            "Jialiang Zhang",
            "Yicheng Gong",
            "Xiaoyu Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05969",
        "abstract": "Large language models (LLMs) are increasingly deployed on complex reasoning tasks, yet little is known about their ability to internally evaluate problem difficulty, which is an essential capability for adaptive reasoning and efficient resource allocation. In this work, we investigate whether LLMs implicitly encode problem difficulty in their internal representations. Using a linear probe on the final-token representations of LLMs, we demonstrate that the difficulty level of math problems can be linearly modeled. We further locate the specific attention heads of the final Transformer layer: these attention heads have opposite activation patterns for simple and difficult problems, thus achieving perception of difficulty. Our ablation experiments prove the accuracy of the location. Crucially, our experiments provide practical support for using LLMs as automatic difficulty annotators, potentially substantially reducing reliance on costly human labeling in benchmark construction and curriculum learning. We also uncover that there is a significant difference in entropy and difficulty perception at the token level. Our study reveals that difficulty perception in LLMs is not only present but also structurally organized, offering new theoretical insights and practical directions for future research.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "203",
        "title": "LexiCon: a Benchmark for Planning under Temporal Constraints in Natural Language",
        "author": [
            "Periklis Mantenoglou",
            "Rishi Hazra",
            "Pedro Zuidberg Dos Martires",
            "Luc De Raedt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05972",
        "abstract": "Owing to their reasoning capabilities, large language models (LLMs) have been evaluated on planning tasks described in natural language. However, LLMs have largely been tested on planning domains without constraints. In order to deploy them in real-world settings where adherence to constraints, in particular safety constraints, is critical, we need to evaluate their performance on constrained planning tasks. We introduce LexiCon -- a natural language-based (Lexi) constrained (Con) planning benchmark, consisting of a suite of environments, that can be used to evaluate the planning capabilities of LLMs in a principled fashion. The core idea behind LexiCon is to take existing planning environments and impose temporal constraints on the states. These constrained problems are then translated into natural language and given to an LLM to solve. A key feature of LexiCon is its extensibility. That is, the set of supported environments can be extended with new (unconstrained) environment generators, for which temporal constraints are constructed automatically. This renders LexiCon future-proof: the hardness of the generated planning problems can be increased as the planning capabilities of LLMs improve. Our experiments reveal that the performance of state-of-the-art LLMs, including reasoning models like GPT-5, o3, and R1, deteriorates as the degree of constrainedness of the planning tasks increases.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "204",
        "title": "The DISTANT Design for Remote Transmission and Steering Systems for Planetary Robotics",
        "author": [
            "Cristina Luna",
            "Alba Guerra",
            "Almudena Moreno",
            "Manuel Esquer",
            "Willy Roa",
            "Mateusz Krawczak",
            "Robert Popela",
            "Piotr Osica",
            "Davide Nicolis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05981",
        "abstract": "Planetary exploration missions require robust locomotion systems capable of operating in extreme environments over extended periods. This paper presents the DISTANT (Distant Transmission and Steering Systems) design, a novel approach for relocating rover traction and steering actuators from wheel-mounted positions to a thermally protected warm box within the rover body. The design addresses critical challenges in long-distance traversal missions by protecting sensitive components from thermal cycling, dust contamination, and mechanical wear. A double wishbone suspension configuration with cardan joints and capstan drive steering has been selected as the optimal architecture following comprehensive trade-off analysis. The system enables independent wheel traction, steering control, and suspension management whilst maintaining all motorisation within the protected environment. The design meets a 50 km traverse requirement without performance degradation, with integrated dust protection mechanisms and thermal management solutions. Testing and validation activities are planned for Q1 2026 following breadboard manufacturing at 1:3 scale.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "205",
        "title": "ECTSpeech: Enhancing Efficient Speech Synthesis via Easy Consistency Tuning",
        "author": [
            "Tao Zhu",
            "Yinfeng Yu",
            "Liejun Wang",
            "Fuchun Sun",
            "Wendong Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05984",
        "abstract": "Diffusion models have demonstrated remarkable performance in speech synthesis, but typically require multi-step sampling, resulting in low inference efficiency. Recent studies address this issue by distilling diffusion models into consistency models, enabling efficient one-step generation. However, these approaches introduce additional training costs and rely heavily on the performance of pre-trained teacher models. In this paper, we propose ECTSpeech, a simple and effective one-step speech synthesis framework that, for the first time, incorporates the Easy Consistency Tuning (ECT) strategy into speech synthesis. By progressively tightening consistency constraints on a pre-trained diffusion model, ECTSpeech achieves high-quality one-step generation while significantly reducing training complexity. In addition, we design a multi-scale gate module (MSGate) to enhance the denoiser's ability to fuse features at different scales. Experimental results on the LJSpeech dataset demonstrate that ECTSpeech achieves audio quality comparable to state-of-the-art methods under single-step sampling, while substantially reducing the model's training cost and complexity.",
        "tags": [
            "Consistency Models",
            "Diffusion"
        ]
    },
    {
        "id": "206",
        "title": "AI-Enabled Capabilities to Facilitate Next-Generation Rover Surface Operations",
        "author": [
            "Cristina Luna",
            "Robert Field",
            "Steven Kay"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05985",
        "abstract": "Current planetary rovers operate at traverse speeds of approximately 10 cm/s, fundamentally limiting exploration efficiency. This work presents integrated AI systems which significantly improve autonomy through three components: (i) the FASTNAV Far Obstacle Detector (FOD), capable of facilitating sustained 1.0 m/s speeds via computer vision-based obstacle detection; (ii) CISRU, a multi-robot coordination framework enabling human-robot collaboration for in-situ resource utilisation; and (iii) the ViBEKO and AIAXR deep learning-based terrain classification studies. Field validation in Mars analogue environments demonstrated these systems at Technology Readiness Level 4, providing measurable improvements in traverse speed, classification accuracy, and operational safety for next-generation planetary missions.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "207",
        "title": "Sample Smart, Not Hard: Correctness-First Decoding for Better Reasoning in LLMs",
        "author": [
            "Xueyan Li",
            "Guinan Su",
            "Mrinmaya Sachan",
            "Jonas Geiping"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05987",
        "abstract": "Large Language Models (LLMs) are increasingly applied to complex tasks that require extended reasoning. In such settings, models often benefit from diverse chains-of-thought to arrive at multiple candidate solutions. This requires two competing objectives: to inject enough stochasticity to explore multiple reasoning chains, and to ensure sufficient accuracy and quality in each path. Existing works pursue the first objective by increasing exploration at highly uncertain steps with higher temperature or larger candidate token sets, while others improve reliability by rejecting samples with low confidence post-generation, implying that low confidence correlates with low answer quality. These two lines of thought are in conflict, as they conflate different sources of uncertainty. To resolve this, we argue that the decoding rule should be calibrated by correctness, not confidence alone. We should sample from tokens with higher estimated correctness, and reduce sampling where expected correctness is low. We propose simple strategies that achieve this goal: Greedy-Threshold makes sampling greedy at very low confidence steps. Calibrated-TopK and Calibrated-epsilon set truncation threshold based on estimated rank-wise correctness. Together, our findings challenge prevailing heuristics about decoding under uncertainty and show gains across math and general reasoning benchmarks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "208",
        "title": "Coordinate-Consistent Localization via Continuous-Time Calibration and Fusion of UWB and SLAM Observations",
        "author": [
            "Tien-Dat Nguyen",
            "Thien-Minh Nguyen",
            "Vinh-Hao Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05992",
        "abstract": "Onboard simultaneous localization and mapping (SLAM) methods are commonly used to provide accurate localization information for autonomous robots. However, the coordinate origin of SLAM estimate often resets for each run. On the other hand, UWB-based localization with fixed anchors can ensure a consistent coordinate reference across sessions; however, it requires an accurate assignment of the anchor nodes' coordinates. To this end, we propose a two-stage approach that calibrates and fuses UWB data and SLAM data to achieve coordinate-wise consistent and accurate localization in the same environment. In the first stage, we solve a continuous-time batch optimization problem by using the range and odometry data from one full run, incorporating height priors and anchor-to-anchor distance factors to recover the anchors' 3D positions. For the subsequent runs in the second stage, a sliding-window optimization scheme fuses the UWB and SLAM data, which facilitates accurate localization in the same coordinate system. Experiments are carried out on the NTU VIRAL dataset with six scenarios of UAV flight, and we show that calibration using data in one run is sufficient to enable accurate localization in the remaining runs. We release our source code to benefit the community at https://github.com/ntdathp/slam-uwb-calibration.",
        "tags": [
            "3D",
            "SLAM"
        ]
    },
    {
        "id": "209",
        "title": "Information-Theoretic Policy Pre-Training with Empowerment",
        "author": [
            "Moritz Schneider",
            "Robert Krug",
            "Narunas Vaskevicius",
            "Luigi Palmieri",
            "Michael Volpp",
            "Joschka Boedecker"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05996",
        "abstract": "Empowerment, an information-theoretic measure of an agent's potential influence on its environment, has emerged as a powerful intrinsic motivation and exploration framework for reinforcement learning (RL). Besides for unsupervised RL and skill learning algorithms, the specific use of empowerment as a pre-training signal has received limited attention in the literature. We show that empowerment can be used as a pre-training signal for data-efficient downstream task adaptation. For this we extend the traditional notion of empowerment by introducing discounted empowerment, which balances the agent's control over the environment across short- and long-term horizons. Leveraging this formulation, we propose a novel pre-training paradigm that initializes policies to maximize discounted empowerment, enabling agents to acquire a robust understanding of environmental dynamics. We analyze empowerment-based pre-training for various existing RL algorithms and empirically demonstrate its potential as a general-purpose initialization strategy: empowerment-maximizing policies with long horizons are data-efficient and effective, leading to improved adaptability in downstream tasks. Our findings pave the way for future research to scale this framework to high-dimensional and complex tasks, further advancing the field of RL.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "210",
        "title": "Exploring Gaps in the APS: Direct Minimal Pair Analysis in LLM Syntactic Assessments",
        "author": [
            "Timothy Pistotti",
            "Jason Brown",
            "Michael Witbrock"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06001",
        "abstract": "Recent studies probing the Argument from the Poverty of the Stimulus (APS) have applied Large Language Models (LLMs) to test the learnability of complex syntax through surprisal-based metrics. However, divergent conclusions raise questions concerning the insights these metrics offer. While Wilcox et al. (2024) used direct minimal pair comparisons (the \"wh-effect\") to demonstrate that models successfully generalise knowledge of filler-gap dependencies, Lan et al. (2024) used a Difference-in-Differences (DiD) metric and found that models largely fail on parasitic gaps (PGs). This paper argues that the direct minimal pair approach offers greater diagnostic transparency. We demonstrate this by generating a full 8-permutation paradigm of refined PG stimuli and evaluating the GPT-2 model used in previous studies with a systematic Wilcox-style wh-effect analysis. Our results show that GPT-2 succeeds across all four tested conditions, indicating robust knowledge of filler-gap licensing principles even in complex PG environments. This finding, which contrasts with the more ambiguous results from DiD-style metrics, suggests that the choice of evaluation metric is critical for assessing an LLM's syntactic competence.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "211",
        "title": "MASA: Rethinking the Representational Bottleneck in LoRA with Multi-A Shared Adaptation",
        "author": [
            "Qin Dong",
            "Yuntian Tang",
            "Heming Jia",
            "Yunhang Shen",
            "Bohan Jia",
            "Wenxuan Huang",
            "Lianyue Zhang",
            "Jiao Xie",
            "Shaohui Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06005",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as a dominant method in Parameter-Efficient Fine-Tuning (PEFT) for large language models, which augments the transformer layer with one down-projection $A$ and one up-projection $B$. However, LoRA's reliance on a single down-projection matrix ($A$) creates a representational bottleneck, as this solitary feature extractor is inherently insufficient for capturing the diverse signals required by complex tasks. This motivates our architectural shift to focus on enriching the feature adaptation to improve the downstream task adaptation ability. We propose MASA (Multi-$A$ Shared Adaptation), an architecture that implements a multi-$A$, single-$B$ structure where the multi-$A$ expert ensemble is asymmetrically shared across layers to ensure parameter efficiency. In MASA, these specialized experts capture diverse features, which are then integrated by a single, layer-specific $B$-matrix. The effectiveness and versatility of our method are validated through a comprehensive suite of experiments spanning multi-domain generalization, single-domain specialization, and multi-task reasoning. For example, on the MMLU benchmark, MASA achieves an average accuracy of 59.62%, outperforming the standard LoRA by 1.08 points (a relative improvement of 1.84%) with comparable learnable parameters of 0.52%.",
        "tags": [
            "LLM",
            "LoRA",
            "Transformer"
        ]
    },
    {
        "id": "212",
        "title": "Detection and Measurement of Hailstones with Multimodal Large Language Models",
        "author": [
            "Moritz Alker",
            "David C. Schedl",
            "Andreas StÃ¶ckl"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06008",
        "abstract": "This study examines the use of social media and news images to detect and measure hailstones, utilizing pre-trained multimodal large language models. The dataset for this study comprises 474 crowdsourced images of hailstones from documented hail events in Austria, which occurred between January 2022 and September 2024. These hailstones have maximum diameters ranging from 2 to 11cm. We estimate the hail diameters and compare four different models utilizing one-stage and two-stage prompting strategies. The latter utilizes additional size cues from reference objects, such as human hands, within the image. Our results show that pretrained models already have the potential to measure hailstone diameters from images with an average mean absolute error of 1.12cm for the best model. In comparison to a single-stage prompt, two-stage prompting improves the reliability of most models. Our study suggests that these off-the-shelf models, even without fine-tuning, can complement traditional hail sensors by extracting meaningful and spatially dense information from social media imagery, enabling faster and more detailed assessments of severe weather events. The automated real-time image harvesting from social media and other sources remains an open task, but it will make our approach directly applicable to future hail events.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "213",
        "title": "Continual Learning for Image Captioning through Improved Image-Text Alignment",
        "author": [
            "Bertram Taetz",
            "Gal Bordelius"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06009",
        "abstract": "Generating accurate and coherent image captions in a continual learning setting remains a major challenge due to catastrophic forgetting and the difficulty of aligning evolving visual concepts with language over time. In this work, we propose a novel multi-loss framework for continual image captioning that integrates semantic guidance through prompt-based continual learning and contrastive alignment. Built upon a pretrained ViT-GPT-2 backbone, our approach combines standard cross-entropy loss with three additional components: (1) a prompt-based cosine similarity loss that aligns image embeddings with synthetically constructed prompts encoding objects, attributes, and actions; (2) a CLIP-style loss that promotes alignment between image embeddings and target caption embedding; and (3) a language-guided contrastive loss that employs a triplet loss to enhance class-level discriminability between tasks. Notably, our approach introduces no additional overhead at inference time and requires no prompts during caption generation. We find that this approach mitigates catastrophic forgetting, while achieving better semantic caption alignment compared to state-of-the-art methods. The code can be found via the following link https://github.com/ Gepardius/Taetz_Bordelius_Continual_ImageCaptioning.",
        "tags": [
            "CLIP",
            "GPT",
            "ViT"
        ]
    },
    {
        "id": "214",
        "title": "Emergent Directedness in Social Contagion",
        "author": [
            "Fabian Tschofenig",
            "Douglas Guilbeault"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06012",
        "abstract": "An enduring challenge in contagion theory is that the pathways contagions follow through social networks exhibit emergent complexities that are difficult to predict using network structure. Here, we address this challenge by developing a causal modeling framework that (i) simulates the possible network pathways that emerge as contagions spread and (ii) identifies which edges and nodes are most impactful on diffusion across these possible pathways. This yields a surprising discovery. If people require exposure to multiple peers to adopt a contagion (a.k.a., 'complex contagions'), the pathways that emerge often only work in one direction. In fact, the more complex a contagion is, the more asymmetric its paths become. This emergent directedness problematizes canonical theories of how networks mediate contagion. Weak ties spanning network regions - widely thought to facilitate mutual influence and integration - prove to privilege the spread contagions from one community to the other. Emergent directedness also disproportionately channels complex contagions from the network periphery to the core, inverting standard centrality models. We demonstrate two practical applications. We show that emergent directedness accounts for unexplained nonlinearity in the effects of tie strength in a recent study of job diffusion over LinkedIn. Lastly, we show that network evolution is biased toward growing directed paths, but that cultural factors (e.g., triadic closure) can curtail this bias, with strategic implications for network building and behavioral interventions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "215",
        "title": "Evaluating The Impact of Stimulus Quality in Investigations of LLM Language Performance",
        "author": [
            "Timothy Pistotti",
            "Jason Brown",
            "Michael Witbrock"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06018",
        "abstract": "Recent studies employing Large Language Models (LLMs) to test the Argument from the Poverty of the Stimulus (APS) have yielded contrasting results across syntactic phenomena. This paper investigates the hypothesis that characteristics of the stimuli used in recent studies, including lexical ambiguities and structural complexities, may confound model performance. A methodology is proposed for re-evaluating LLM competence on syntactic prediction, focusing on GPT-2. This involves: 1) establishing a baseline on previously used (both filtered and unfiltered) stimuli, and 2) generating a new, refined dataset using a state-of-the-art (SOTA) generative LLM (Gemini 2.5 Pro Preview) guided by linguistically-informed templates designed to mitigate identified confounds. Our preliminary findings indicate that GPT-2 demonstrates notably improved performance on these refined PG stimuli compared to baselines, suggesting that stimulus quality significantly influences outcomes in surprisal-based evaluations of LLM syntactic competency.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "216",
        "title": "Optimal Batched Scheduling of Stochastic Processing Networks Using Atomic Action Decomposition",
        "author": [
            "Jim Dai",
            "Manxi Wu",
            "Zhanhao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06033",
        "abstract": "Stochastic processing networks (SPNs) have broad applications in healthcare, transportation, and communication networks. The control of SPN is to dynamically assign servers in batches under uncertainty to optimize long-run performance. This problem is challenging as the policy dimension grows exponentially with the number of servers, making standard reinforcement learning and policy optimization methods intractable at scale. We propose an atomic action decomposition framework that addresses this scalability challenge by breaking joint assignments into sequential single-server assignments. This yields policies with constant dimension, independent of the number of servers. We study two classes of atomic policies, the step-dependent and step-independent atomic policies, and prove that both achieve the same optimal long-run average reward as the original joint policies. These results establish that computing the optimal SPN control can be made scalable without loss of optimality using the atomic framework. Our results offer theoretical justification for the strong empirical success of the atomic framework in large-scale applications reported in previous articles.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "217",
        "title": "From Learning to Mastery: Achieving Safe and Efficient Real-World Autonomous Driving with Human-In-The-Loop Reinforcement Learning",
        "author": [
            "Li Zeqiao",
            "Wang Yijing",
            "Wang Haoyu",
            "Li Zheng",
            "Li Peng",
            "Liu Wenfei",
            "Zuo Zhiqiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06038",
        "abstract": "Autonomous driving with reinforcement learning (RL) has significant potential. However, applying RL in real-world settings remains challenging due to the need for safe, efficient, and robust learning. Incorporating human expertise into the learning process can help overcome these challenges by reducing risky exploration and improving sample efficiency. In this work, we propose a reward-free, active human-in-the-loop learning method called Human-Guided Distributional Soft Actor-Critic (H-DSAC). Our method combines Proxy Value Propagation (PVP) and Distributional Soft Actor-Critic (DSAC) to enable efficient and safe training in real-world environments. The key innovation is the construction of a distributed proxy value function within the DSAC framework. This function encodes human intent by assigning higher expected returns to expert demonstrations and penalizing actions that require human intervention. By extrapolating these labels to unlabeled states, the policy is effectively guided toward expert-like behavior. With a well-designed state space, our method achieves real-world driving policy learning within practical training times. Results from both simulation and real-world experiments demonstrate that our framework enables safe, robust, and sample-efficient learning for autonomous driving.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "218",
        "title": "CDTP: A Large-Scale Chinese Data-Text Pair Dataset for Comprehensive Evaluation of Chinese LLMs",
        "author": [
            "Chengwei Wu",
            "Jiapu Wang",
            "Mingyang Gao",
            "Xingrui Zhuo",
            "Jipeng Guo",
            "Runlin Lei",
            "Haoran Luo",
            "Tianyu Chen",
            "Haoyi Zhou",
            "Shirui Pan",
            "Zechao Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06039",
        "abstract": "Large Language Models (LLMs) have achieved remarkable success across a wide range of natural language processing tasks. However, Chinese LLMs face unique challenges, primarily due to the dominance of unstructured free text and the lack of structured representations in Chinese corpora. While existing benchmarks for LLMs partially assess Chinese LLMs, they are still predominantly English-centric and fail to address the unique linguistic characteristics of Chinese, lacking structured datasets essential for robust evaluation. To address these challenges, we present a Comprehensive Benchmark for Evaluating Chinese Large Language Models (CB-ECLLM) based on the newly constructed Chinese Data-Text Pair (CDTP) dataset. Specifically, CDTP comprises over 7 million aligned text pairs, each consisting of unstructured text coupled with one or more corresponding triples, alongside a total of 15 million triples spanning four critical domains. The core contributions of CDTP are threefold: (i) enriching Chinese corpora with high-quality structured information; (ii) enabling fine-grained evaluation tailored to knowledge-driven tasks; and (iii) supporting multi-task fine-tuning to assess generalization and robustness across scenarios, including Knowledge Graph Completion, Triple-to-Text generation, and Question Answering. Furthermore, we conduct rigorous evaluations through extensive experiments and ablation studies to assess the effectiveness, Supervised Fine-Tuning (SFT), and robustness of the benchmark. To support reproducible research, we offer an open-source codebase and outline potential directions for future investigations based on our insights.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "219",
        "title": "VideoMiner: Iteratively Grounding Key Frames of Hour-Long Videos via Tree-based Group Relative Policy Optimization",
        "author": [
            "Xinye Cao",
            "Hongcan Guo",
            "Jiawen Qian",
            "Guoshun Nan",
            "Chao Wang",
            "Yuqi Pan",
            "Tianhao Hou",
            "Xiaojuan Wang",
            "Yutong Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06040",
        "abstract": "Understanding hour-long videos with multi-modal large language models (MM-LLMs) enriches the landscape of human-centered AI applications. However, for end-to-end video understanding with LLMs, uniformly sampling video frames results in LLMs being overwhelmed by a vast amount of irrelevant information as video length increases. Existing hierarchical key frame extraction methods improve the accuracy of video understanding but still face two critical challenges. 1) How can the interference of extensive redundant information in long videos be mitigated? 2) How can a model dynamically adapt to complex hierarchical structures while accurately identifying key frames? To address these issues, we propose VideoMiner, which iteratively segments, captions, and clusters long videos, forming a hierarchical tree structure. The proposed VideoMiner progresses from long videos to events to frames while preserving temporal coherence, effectively addressing the first challenge. To precisely locate key frames, we introduce T-GRPO, a tree-based group relative policy optimization in reinforcement learning method that guides the exploration of the VideoMiner. The proposed T-GRPO is specifically designed for tree structures, integrating spatiotemporal information at the event level while being guided by the question, thus solving the second challenge. We achieve superior performance in all long-video understanding tasks and uncover several interesting insights. Our proposed T-GRPO surprisingly incentivizes the model to spontaneously generate a reasoning chain. Additionally, the designed tree growth auxin dynamically adjusts the expansion depth, obtaining accuracy and efficiency gains. The code is publicly available at https://github.com/caoxinye/VideoMiner.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "220",
        "title": "Agent+P: Guiding UI Agents via Symbolic Planning",
        "author": [
            "Shang Ma",
            "Xusheng Xiao",
            "Yanfang Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06042",
        "abstract": "Large Language Model (LLM)-based UI agents show great promise for UI automation but often hallucinate in long-horizon tasks due to their lack of understanding of the global UI transition structure. To address this, we introduce AGENT+P, a novel framework that leverages symbolic planning to guide LLM-based UI agents. Specifically, we model an app's UI transition structure as a UI Transition Graph (UTG), which allows us to reformulate the UI automation task as a pathfinding problem on the UTG. This further enables an off-the-shelf symbolic planner to generate a provably correct and optimal high-level plan, preventing the agent from redundant exploration and guiding the agent to achieve the automation goals. AGENT+P is designed as a plug-and-play framework to enhance existing UI agents. Evaluation on the AndroidWorld benchmark demonstrates that AGENT+P improves the success rates of state-of-the-art UI agents by up to 14% and reduces the action steps by 37.7%.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "221",
        "title": "BLISS: A Lightweight Bilevel Influence Scoring Method for Data Selection in Language Model Pretraining",
        "author": [
            "Jie Hao",
            "Rui Yu",
            "Wei Zhang",
            "Huixia Wang",
            "Jie Xu",
            "Mingrui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06048",
        "abstract": "Effective data selection is essential for pretraining large language models (LLMs), enhancing efficiency and improving generalization to downstream tasks. However, existing approaches often require leveraging external pretrained models, making it difficult to disentangle the effects of data selection from those of the external pretrained models. In addition, they often overlook the long-term impact of selected data if the model is trained to convergence, primarily due to the prohibitive cost of full-scale LLM pretraining. In this paper, we introduce BLISS (\\textbf{B}ileve\\textbf{L} \\textbf{I}nfluence \\textbf{S}coring method for data \\textbf{S}election): a lightweight data selection method that operates entirely \\emph{from scratch}, without relying on any external pretrained oracle models, while explicitly accounting for the long-term impact of selected data. BLISS leverages a small proxy model as a surrogate for the LLM and employs a score model to estimate the long-term influence of training samples if the proxy model is trained to convergence. We formulate data selection as a bilevel optimization problem, where the upper-level objective optimizes the score model to assign importance weights to training samples, ensuring that minimizing the lower-level objective (i.e., training the proxy model over the weighted training loss until convergence) leads to best validation performance. Once optimized, the trained score model predicts influence scores for the dataset, enabling efficient selection of high-quality samples for LLM pretraining. We validate BLISS by pretraining 410M/1B/2.8B Pythia and LLaMA-0.5B models on selected subsets of the C4 dataset. Notably, under the 1B model setting, BLISS achieves $1.7\\times$ speedup in reaching the same performance as the state-of-the-art method, demonstrating superior performance across multiple downstream tasks.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "222",
        "title": "Edit-Based Flow Matching for Temporal Point Processes",
        "author": [
            "David LÃ¼dke",
            "Marten Lienen",
            "Marcel Kollovieh",
            "Stephan GÃ¼nnemann"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06050",
        "abstract": "Temporal point processes (TPPs) are a fundamental tool for modeling event sequences in continuous time, but most existing approaches rely on autoregressive parameterizations that are limited by their sequential sampling. Recent non-autoregressive, diffusion-style models mitigate these issues by jointly interpolating between noise and data through event insertions and deletions in a discrete Markov chain. In this work, we generalize this perspective and introduce an Edit Flow process for TPPs that transports noise to data via insert, delete, and substitute edit operations. By learning the instantaneous edit rates within a continuous-time Markov chain framework, we attain a flexible and efficient model that effectively reduces the total number of necessary edit operations during generation. Empirical results demonstrate the generative flexibility of our unconditionally trained model in a wide range of unconditional and conditional generation tasks on benchmark TPPs.",
        "tags": [
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "223",
        "title": "MixReasoning: Switching Modes to Think",
        "author": [
            "Haiquan Lu",
            "Gongfan Fang",
            "Xinyin Ma",
            "Qi Li",
            "Xinchao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06052",
        "abstract": "Reasoning models enhance performance by tackling problems in a step-by-step manner, decomposing them into sub-problems and exploring long chains of thought before producing an answer. However, applying extended reasoning to every step introduces substantial redundancy, as sub-problems vary widely in difficulty and complexity: a small number of pivotal steps are genuinely challenging and decisive for the final answer, while many others only involve straightforward revisions or simple computations. Therefore, a natural idea is to endow reasoning models with the ability to adaptively respond to this variation, rather than treating all steps with the same level of elaboration. To this end, we propose MixReasoning, a framework that dynamically adjusts the depth of reasoning within a single response. The resulting chain of thought then becomes a mixture of detailed reasoning on difficult steps and concise inference on simpler ones. Experiments on GSM8K, MATH-500, and AIME show that MixReasoning shortens reasoning length and substantially improves efficiency without compromising accuracy.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "224",
        "title": "Controllable Audio-Visual Viewpoint Generation from 360Â° Spatial Information",
        "author": [
            "Christian Marinoni",
            "Riccardo Fosco Gramaccioni",
            "Eleonora Grassucci",
            "Danilo Comminiello"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06060",
        "abstract": "The generation of sounding videos has seen significant advancements with the advent of diffusion models. However, existing methods often lack the fine-grained control needed to generate viewpoint-specific content from larger, immersive 360-degree environments. This limitation restricts the creation of audio-visual experiences that are aware of off-camera events. To the best of our knowledge, this is the first work to introduce a framework for controllable audio-visual generation, addressing this unexplored gap. Specifically, we propose a diffusion model by introducing a set of powerful conditioning signals derived from the full 360-degree space: a panoramic saliency map to identify regions of interest, a bounding-box-aware signed distance map to define the target viewpoint, and a descriptive caption of the entire scene. By integrating these controls, our model generates spatially-aware viewpoint videos and audios that are coherently influenced by the broader, unseen environmental context, introducing a strong controllability that is essential for realistic and immersive audio-visual generation. We show audiovisual examples proving the effectiveness of our framework.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "225",
        "title": "ASPO: Asymmetric Importance Sampling Policy Optimization",
        "author": [
            "Jiakang Wang",
            "Runze Liu",
            "Lei Lin",
            "Wenping Hu",
            "Xiu Li",
            "Fuzheng Zhang",
            "Guorui Zhou",
            "Kun Gai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06062",
        "abstract": "Recent Large Language Model (LLM) post-training methods rely on token-level clipping mechanisms during Reinforcement Learning (RL). However, we identify a fundamental flaw in this Outcome-Supervised RL (OSRL) paradigm: the Importance Sampling (IS) ratios of positive-advantage tokens are mismatched, leading to unbalanced token weighting for positive and negative tokens. This mismatch suppresses the update of low-probability tokens while over-amplifying already high-probability ones. To address this, we propose Asymmetric Importance Sampling Policy Optimization (ASPO), which uses a simple yet effective strategy that flips the IS ratios of positive-advantage tokens, aligning their update direction with the learning dynamics of negative ones. AIS further incorporates a soft dual-clipping mechanism to stabilize extreme updates while maintaining gradient flow. Comprehensive experiments on coding and mathematical reasoning benchmarks demonstrate that ASPO significantly mitigates premature convergence, improves training stability, and enhances final performance over strong GRPO-based baselines. Our analysis provides new insights into the role of token-level weighting in OSRL and highlights the critical importance of correcting IS in LLM RL. The code and models of ASPO are available at https://github.com/wizard-III/Archer2.0.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "226",
        "title": "Reasoning under Vision: Understanding Visual-Spatial Cognition in Vision-Language Models for CAPTCHA",
        "author": [
            "Python Song",
            "Luke Tenyi Chang",
            "Yun-Yun Tsai",
            "Penghui Li",
            "Junfeng Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06067",
        "abstract": "CAPTCHA, originally designed to distinguish humans from robots, has evolved into a real-world benchmark for assessing the spatial reasoning capabilities of vision-language models. In this work, we first show that step-by-step reasoning is crucial for vision-language models (VLMs) to solve CAPTCHAs, which represent high-difficulty spatial reasoning tasks, and that current commercial vision-language models still struggle with such reasoning. In particular, we observe that most commercial VLMs (e.g., Gemini, Claude, GPT, etc.) fail to effectively solve CAPTCHAs and thus achieve low accuracy (around 21.9 percent). However, our findings indicate that requiring the model to perform step-by-step reasoning before generating the final coordinates can significantly enhance its solving accuracy, underscoring the severity of the gap. To systematically study this issue, we introduce CAPTCHA-X, the first real-world CAPTCHA benchmark with reasoning, covering seven categories of CAPTCHAs (such as Gobang, hCaptcha, etc.) with step-by-step action solutions and grounding annotations. We further define five reasoning-oriented metrics that enable a comprehensive evaluation of models reasoning capabilities. To validate the effectiveness of reasoning, we also propose a general agentic VLM-based framework that incorporates the models inherent reasoning abilities. Our method achieves state-of-the-art performance across five high-difficulty CAPTCHA types, with an average solving accuracy of 83.9 percent, substantially surpassing existing baselines. These results reveal the limitations of current models and highlight the importance of reasoning in advancing visual-spatial challenges in the future.",
        "tags": [
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "227",
        "title": "There is More to Attention: Statistical Filtering Enhances Explanations in Vision Transformers",
        "author": [
            "Meghna P Ayyar",
            "Jenny Benois-Pineau",
            "Akka Zemmari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06070",
        "abstract": "Explainable AI (XAI) has become increasingly important with the rise of large transformer models, yet many explanation methods designed for CNNs transfer poorly to Vision Transformers (ViTs). Existing ViT explanations often rely on attention weights, which tend to yield noisy maps as they capture token-to-token interactions within each http://layer.While attribution methods incorporating MLP blocks have been proposed, we argue that attention remains a valuable and interpretable signal when properly filtered. We propose a method that combines attention maps with a statistical filtering, initially proposed for CNNs, to remove noisy or uninformative patterns and produce more faithful explanations. We further extend our approach with a class-specific variant that yields discriminative explanations. Evaluation against popular state-of-the-art methods demonstrates that our approach produces sharper and more interpretable maps. In addition to perturbation-based faithfulness metrics, we incorporate human gaze data to assess alignment with human perception, arguing that human interpretability remains essential for XAI. Across multiple datasets, our approach consistently outperforms or is comparable to the SOTA methods while remaining efficient and human plausible.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "228",
        "title": "When Thinking Drifts: Evidential Grounding for Robust Video Reasoning",
        "author": [
            "Mi Luo",
            "Zihui Xue",
            "Alex Dimakis",
            "Kristen Grauman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06077",
        "abstract": "Video reasoning, the task of enabling machines to infer from dynamic visual content through multi-step logic, is crucial for advanced AI. While the Chain-of-Thought (CoT) mechanism has enhanced reasoning in text-based tasks, its application to video understanding remains underexplored. This paper presents a systematic analysis revealing that CoT often degrades performance in video reasoning, generating verbose but misleading internal monologues, and leading to hallucinated visual details and overridden correct intuitions - a phenomenon we term \"visual thinking drift\". We explain this drift through a Bayesian lens, positing that CoT traces often diverge from actual visual evidence, instead amplifying internal biases or language priors, causing models to storytell rather than engage in grounded reasoning. To counteract this, we introduce Visual Evidence Reward (VER), a novel reinforcement learning framework that explicitly rewards the generation of reasoning traces that are verifiably grounded in visual evidence. Comprehensive evaluation across 10 diverse video understanding benchmarks demonstrates that our Video-VER consistently achieves top performance. Our work sheds light on the distinct challenges of video-centric reasoning and encourages the development of AI that robustly grounds its inferences in visual evidence - for large multimodal models that not only \"think before answering\", but also \"see while thinking\".",
        "tags": [
            "CoT",
            "RL"
        ]
    },
    {
        "id": "229",
        "title": "Toward Model Matching for Remotely Controlled Differential Drive Robotic Vehicles",
        "author": [
            "Nikolaos D. Kouvakas",
            "Fotis N. Koumboulis",
            "Konstantinos G. Tzierakis",
            "John Sigalas",
            "Anastasios Dimakakos"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06081",
        "abstract": "The problem of regulation of the orientation angle of a remotely controlled differential-drive mobile robot with actuator dynamics and network-induced delays is studied. Using a preinstalled two-layer nonlinear control scheme that decouples linear and angular velocities and regulates heading, a third, delay-dependent layer that achieves exact model matching from the orientation angle command to the orientation angle is introduced. The proposed outer loop controller is a delay dependent dynamic measurable output-feedback controller with dynamic proper precompensator. Parameterization yields a simple characteristic quasi-polynomial with coefficients constrained to satisfy stability for all delays up to a computable bound. Computational experiments confirm accurate tracking, fast settling and bounded internal signals and control voltages. The approach offers an analytic design alternative to AI-based tuning for delayed robotic systems.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "230",
        "title": "Multi-Robot Distributed Optimization for Exploration and Mapping of Unknown Environments using Bioinspired Tactile-Sensor",
        "author": [
            "Roman Ibrahimov",
            "Jannik Matthias Heinen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06085",
        "abstract": "This project proposes a bioinspired multi-robot system using Distributed Optimization for efficient exploration and mapping of unknown environments. Each robot explores its environment and creates a map, which is afterwards put together to form a global 2D map of the environment. Inspired by wall-following behaviors, each robot autonomously explores its neighborhood based on a tactile sensor, similar to the antenna of a cockroach, mounted on the surface of the robot. Instead of avoiding obstacles, robots log collision points when they touch obstacles. This decentralized control strategy ensures effective task allocation and efficient exploration of unknown terrains, with applications in search and rescue, industrial inspection, and environmental monitoring. The approach was validated through experiments using e-puck robots in a simulated 1.5 x 1.5 m environment with three obstacles. The results demonstrated the system's effectiveness in achieving high coverage, minimizing collisions, and constructing accurate 2D maps.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "231",
        "title": "Learning from Failures: Understanding LLM Alignment through Failure-Aware Inverse RL",
        "author": [
            "Nyal Patel",
            "Matthieu Bou",
            "Arjun Jagota",
            "Satyapriya Krishna",
            "Sonali Parbhoo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06092",
        "abstract": "Reinforcement Learning from Human Feedback (RLHF) aligns Large Language Models (LLMs) with human preferences, yet the underlying reward signals they internalize remain hidden, posing a critical challenge for interpretability and safety. Existing approaches attempt to extract these latent incentives using Inverse Reinforcement Learning (IRL), but treat all preference pairs equally, often overlooking the most informative signals: those examples the extracted reward model misclassifies or assigns nearly equal scores, which we term \\emph{failures}. We introduce a novel \\emph{failure-aware} IRL algorithm that focuses on misclassified or difficult examples to recover the latent rewards defining model behaviors. By learning from these failures, our failure-aware IRL extracts reward functions that better reflect the true objectives behind RLHF. We demonstrate that failure-aware IRL outperforms existing IRL baselines across multiple metrics when applied to LLM detoxification, without requiring external classifiers or supervision. Crucially, failure-aware IRL yields rewards that better capture the true incentives learned during RLHF, enabling more effective re-RLHF training than standard IRL. This establishes failure-aware IRL as a robust, scalable method for auditing model alignment and reducing ambiguity in the IRL process.",
        "tags": [
            "LLM",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "232",
        "title": "The Alignment Auditor: A Bayesian Framework for Verifying and Refining LLM Objectives",
        "author": [
            "Matthieu Bou",
            "Nyal Patel",
            "Arjun Jagota",
            "Satyapriya Krishna",
            "Sonali Parbhoo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06096",
        "abstract": "The objectives that Large Language Models (LLMs) implicitly optimize remain dangerously opaque, making trustworthy alignment and auditing a grand challenge. While Inverse Reinforcement Learning (IRL) can infer reward functions from behaviour, existing approaches either produce a single, overconfident reward estimate or fail to address the fundamental ambiguity of the task (non-identifiability). This paper introduces a principled auditing framework that re-frames reward inference from a simple estimation task to a comprehensive process for verification. Our framework leverages Bayesian IRL to not only recover a distribution over objectives but to enable three critical audit capabilities: (i) Quantifying and systematically reducing non-identifiability by demonstrating posterior contraction over sequential rounds of evidence; (ii) Providing actionable, uncertainty-aware diagnostics that expose spurious shortcuts and identify out-of-distribution prompts where the inferred objective cannot be trusted; and (iii) Validating policy-level utility by showing that the refined, low-uncertainty reward can be used directly in RLHF to achieve training dynamics and toxicity reductions comparable to the ground-truth alignment process. Empirically, our framework successfully audits a detoxified LLM, yielding a well-calibrated and interpretable objective that strengthens alignment guarantees. Overall, this work provides a practical toolkit for auditors, safety teams, and regulators to verify what LLMs are truly trying to achieve, moving us toward more trustworthy and accountable AI.",
        "tags": [
            "LLM",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "233",
        "title": "The Valley of Code Reasoning: Scaling Knowledge Distillation of Large Language Models",
        "author": [
            "Muyu He",
            "Muhammad Ali Shafique",
            "Anand Kumar",
            "Tsach Mackey",
            "Nazneen Rajani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06101",
        "abstract": "Distilling the thinking traces of a Large Language Model (LLM) with reasoning capabilities into a smaller model has been proven effective. Yet, there is a scarcity of work done on how model performances scale with the quantity of distillation data. In this work, we study the scaling trend of distilling competitive coding skills on two small non-reasoning LLMs. We validate the hypothesis that there is a $\\textit{valley of code reasoning}$: downstream performance on competitive coding first drops as data quantity increases, then it steadily increases in a sharper-than-log-linear fashion. Having identified the trend, we further fine-tune the models at two different distillation stages on the same data to ground conclusions on their respective learning phases. We learn that across stages in the low and medium-low data regimes, small models benefit significantly from easier coding questions than from harder ones. We also find that, surprisingly, the correctness of outputs in training data makes no difference to distillation outcomes. Our work represents a step forward in understanding the training dynamics of code reasoning distillation outside intuition",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "234",
        "title": "Explaining Code Risk in OSS: Towards LLM-Generated Fault Prediction Interpretations",
        "author": [
            "Elijah Kayode Adejumo",
            "Brittany Johnson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06104",
        "abstract": "Open Source Software (OSS) has become a very important and crucial infrastructure worldwide because of the value it provides. OSS typically depends on contributions from developers across diverse backgrounds and levels of experience. Making safe changes, such as fixing a bug or implementing a new feature, can be challenging, especially in object-oriented systems where components are interdependent. Static analysis and defect-prediction tools produce metrics (e.g., complexity,coupling) that flag potentially fault-prone components, but these signals are often hard for contributors new or unfamiliar with the codebase to interpret. Large Language Models (LLMs) have shown strong performance on software engineering tasks such as code summarization and documentation generation. Building on this progress, we investigate whether LLMs can translate fault-prediction metrics into clear, human-readable risk explanations and actionable guidance to help OSS contributors plan and review code modifications. We outline explanation types that an LLM-generated assistant could provide (descriptive, contextual, and actionable explanations). We also outline our next steps to assess usefulness through a task-based study with OSS contributors, comparing metric-only baselines to LLM-generated explanations on decision quality, time-to-completion, and error rates",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "235",
        "title": "Moloch's Bargain: Emergent Misalignment When LLMs Compete for Audiences",
        "author": [
            "Batu El",
            "James Zou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06105",
        "abstract": "Large language models (LLMs) are increasingly shaping how information is created and disseminated, from companies using them to craft persuasive advertisements, to election campaigns optimizing messaging to gain votes, to social media influencers boosting engagement. These settings are inherently competitive, with sellers, candidates, and influencers vying for audience approval, yet it remains poorly understood how competitive feedback loops influence LLM behavior. We show that optimizing LLMs for competitive success can inadvertently drive misalignment. Using simulated environments across these scenarios, we find that, 6.3% increase in sales is accompanied by a 14.0% rise in deceptive marketing; in elections, a 4.9% gain in vote share coincides with 22.3% more disinformation and 12.5% more populist rhetoric; and on social media, a 7.5% engagement boost comes with 188.6% more disinformation and a 16.3% increase in promotion of harmful behaviors. We call this phenomenon Moloch's Bargain for AI--competitive success achieved at the cost of alignment. These misaligned behaviors emerge even when models are explicitly instructed to remain truthful and grounded, revealing the fragility of current alignment safeguards. Our findings highlight how market-driven optimization pressures can systematically erode alignment, creating a race to the bottom, and suggest that safe deployment of AI systems will require stronger governance and carefully designed incentives to prevent competitive dynamics from undermining societal trust.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "236",
        "title": "Distributional Semantics Tracing: A Framework for Explaining Hallucinations in Large Language Models",
        "author": [
            "Gagan Bhatia",
            "Somayajulu G Sripada",
            "Kevin Allan",
            "Jacobo Azcona"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06107",
        "abstract": "Large Language Models (LLMs) are prone to hallucination, the generation of plausible yet factually incorrect statements. This work investigates the intrinsic, architectural origins of this failure mode through three primary http://contributions.First, to enable the reliable tracing of internal semantic failures, we propose \\textbf{Distributional Semantics Tracing (DST)}, a unified framework that integrates established interpretability techniques to produce a causal map of a model's reasoning, treating meaning as a function of context (distributional semantics). Second, we pinpoint the model's layer at which a hallucination becomes inevitable, identifying a specific \\textbf{commitment layer} where a model's internal representations irreversibly diverge from factuality. Third, we identify the underlying mechanism for these failures. We observe a conflict between distinct computational pathways, which we interpret using the lens of dual-process theory: a fast, heuristic \\textbf{associative pathway} (akin to System 1) and a slow, deliberate \\textbf{contextual pathway} (akin to System 2), leading to predictable failure modes such as \\textit{Reasoning Shortcut Hijacks}. Our framework's ability to quantify the coherence of the contextual pathway reveals a strong negative correlation ($\\rho = -0.863$) with hallucination rates, implying that these failures are predictable consequences of internal semantic weakness. The result is a mechanistic account of how, when, and why hallucinations occur within the Transformer architecture.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "237",
        "title": "Influence Functions for Efficient Data Selection in Reasoning",
        "author": [
            "Prateek Humane",
            "Paolo Cudrano",
            "Daniel Z. Kaplan",
            "Matteo Matteucci",
            "Supriyo Chakraborty",
            "Irina Rish"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06108",
        "abstract": "Fine-tuning large language models (LLMs) on chain-of-thought (CoT) data shows that a small amount of high-quality data can outperform massive datasets. Yet, what constitutes \"quality\" remains ill-defined. Existing reasoning methods rely on indirect heuristics such as problem difficulty or trace length, while instruction-tuning has explored a broader range of automated selection strategies, but rarely in the context of reasoning. We propose to define reasoning data quality using influence functions, which measure the causal effect of individual CoT examples on downstream accuracy, and introduce influence-based pruning, which consistently outperforms perplexity and embedding-based baselines on math reasoning within a model family.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "238",
        "title": "lm-Meter: Unveiling Runtime Inference Latency for On-Device Language Models",
        "author": [
            "Haoxin Wang",
            "Xiaolong Tu",
            "Hongyu Ke",
            "Huirong Chai",
            "Dawei Chen",
            "Kyungtae Han"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06126",
        "abstract": "Large Language Models (LLMs) are increasingly integrated into everyday applications, but their prevalent cloud-based deployment raises growing concerns around data privacy and long-term sustainability. Running LLMs locally on mobile and edge devices (on-device LLMs) offers the promise of enhanced privacy, reliability, and reduced communication costs. However, realizing this vision remains challenging due to substantial memory and compute demands, as well as limited visibility into performance-efficiency trade-offs on resource-constrained hardware. We propose lm-Meter, the first lightweight, online latency profiler tailored for on-device LLM inference. lm-Meter captures fine-grained, real-time latency at both phase (e.g., embedding, prefill, decode, softmax, sampling) and kernel levels without auxiliary devices. We implement lm-Meter on commercial mobile platforms and demonstrate its high profiling accuracy with minimal system overhead, e.g., only 2.58% throughput reduction in prefill and 0.99% in decode under the most constrained Powersave governor. Leveraging lm-Meter, we conduct comprehensive empirical studies revealing phase- and kernel-level bottlenecks in on-device LLM inference, quantifying accuracy-efficiency trade-offs, and identifying systematic optimization opportunities. lm-Meter provides unprecedented visibility into the runtime behavior of LLMs on constrained platforms, laying the foundation for informed optimization and accelerating the democratization of on-device LLM systems. Code and tutorials are available at https://github.com/amai-gsu/LM-Meter.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "239",
        "title": "Towards Autonomous Tape Handling for Robotic Wound Redressing",
        "author": [
            "Xiao Liang",
            "Lu Shen",
            "Peihan Zhang",
            "Soofiyan Atar",
            "Florian Richter",
            "Michael Yip"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06127",
        "abstract": "Chronic wounds, such as diabetic, pressure, and venous ulcers, affect over 6.5 million patients in the United States alone and generate an annual cost exceeding \\$25 billion. Despite this burden, chronic wound care remains a routine yet manual process performed exclusively by trained clinicians due to its critical safety demands. We envision a future in which robotics and automation support wound care to lower costs and enhance patient outcomes. This paper introduces an autonomous framework for one of the most fundamental yet challenging subtasks in wound redressing: adhesive tape manipulation. Specifically, we address two critical capabilities: tape initial detachment (TID) and secure tape placement. To handle the complex adhesive dynamics of detachment, we propose a force-feedback imitation learning approach trained from human teleoperation demonstrations. For tape placement, we develop a numerical trajectory optimization method based to ensure smooth adhesion and wrinkle-free application across diverse anatomical surfaces. We validate these methods through extensive experiments, demonstrating reliable performance in both quantitative evaluations and integrated wound redressing pipelines. Our results establish tape manipulation as an essential step toward practical robotic wound care automation.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "240",
        "title": "Parallel Tokenizers: Rethinking Vocabulary Design for Cross-Lingual Transfer",
        "author": [
            "Muhammad Dehan Al Kautsar",
            "Fajri Koto"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06128",
        "abstract": "Tokenization defines the foundation of multilingual language models by determining how words are represented and shared across languages. However, existing methods often fail to support effective cross-lingual transfer because semantically equivalent words are assigned distinct embeddings. For example, \"I eat rice\" in English and \"Ina cin shinkafa\" in Hausa are typically mapped to different vocabulary indices, preventing shared representations and limiting cross-lingual generalization. We introduce parallel tokenizers. This new framework trains tokenizers monolingually and then aligns their vocabularies exhaustively using bilingual dictionaries or word-to-word translation, ensuring consistent indices for semantically equivalent words. This alignment enforces a shared semantic space across languages while naturally improving fertility balance. To assess their effectiveness, we pretrain a transformer encoder from scratch on thirteen low-resource languages and evaluate it on sentiment analysis, hate speech detection, emotion classification, and sentence embedding similarity. Across all tasks, models trained with parallel tokenizers outperform conventional multilingual baselines, confirming that rethinking tokenization is essential for advancing multilingual representation learning--especially in low-resource settings.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "241",
        "title": "CreditDecoding: Accelerating Parallel Decoding in Diffusion Large Language Models with Trace Credits",
        "author": [
            "Kangyu Wang",
            "Zhiyun Jiang",
            "Haibo Feng",
            "Weijia Zhao",
            "Lin Liu",
            "Jianguo Li",
            "Zhenzhong Lan",
            "Weiyao Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06133",
        "abstract": "Diffusion large language models (dLLMs) generate text through iterative denoising steps, achieving parallel decoding by denoising only high-confidence positions at each step. However, existing approaches often repetitively remask tokens due to initially low confidence scores, leading to redundant iterations and limiting overall acceleration. Through the analysis of dLLM decoding traces, we observe that the model often determines the final prediction for a token several steps before the decoding step. To leverage this historical information and avoid redundant steps, we introduce the concept of Trace Credit, which quantifies each token's convergence potential by accumulating historical logits. Furthermore, we propose CreditDecoding, a training-free parallel decoding algorithm that accelerates the confidence convergence of correct but underconfident tokens by fusing current logits with Trace Credit. This process significantly reduces redundant iterations and enhances decoding robustness. On eight benchmarks, CreditDecoding achieves a 5.48 times speedup and a 0.48 performance improvement over LLaDA-8B-Instruct, and a 4.11 times speedup with a 0.15 performance improvement over LLaDA-MoE-Instruct. Importantly, CreditDecoding scales effectively to long sequences and is orthogonal to mainstream inference optimizations, making it a readily integrable and versatile solution.",
        "tags": [
            "Diffusion",
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "242",
        "title": "Pushing Test-Time Scaling Limits of Deep Search with Asymmetric Verification",
        "author": [
            "Weihao Zeng",
            "Keqing He",
            "Chuqiao Kuang",
            "Xiaoguang Li",
            "Junxian He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06135",
        "abstract": "Test-time compute can be scaled both sequentially and in parallel. Sequential scaling involves lengthening the generation process, while parallel scaling involves verifying and selecting among multiple candidate outputs. Combining these two strategies has led to the most powerful AI systems, such as Grok 4 Heavy and GPT-5 Pro. In certain contexts (e.g., solving Sudoku puzzles), verifying responses can be substantially easier than generating them. This property, referred to as \\emph{asymmetric verification}, highlights the strong potential of test-time scaling (TTS). In this work, we study both sequential and parallel TTS of deep search agents, motivated by the intuition that verification in this setting is often much easier than generation. In experiments, we first show that sequential scaling methods, such as budget forcing, can be effective initially but soon degrade performance. Leveraging asymmetric verification, however, we are able to achieve substantial improvements by allocating only a modest amount of compute to the verifier. We conduct experiments with flagship open-source models and extend them to their ``Heavy'' variants through TTS. These deep research agents achieve gains of up to 27 absolute points on benchmarks such as BrowseComp. Remarkably, as an open-source alternative, GLM-4.5 Heavy reaches accuracy of {\\bf 54.0\\%} on BrowseComp and {\\bf 66.0\\%} on GAIA, placing it comparable to the best proprietary choices such as OpenAI Deep Research. Tongyi-DeepResearch Heavy further achieves {\\bf 69.0\\%} accuracy on BrowseComp, greatly surpassing the best proprietary results.",
        "tags": [
            "GLM",
            "GPT"
        ]
    },
    {
        "id": "243",
        "title": "Multi-Task Reinforcement Learning with Language-Encoded Gated Policy Networks",
        "author": [
            "Rushiv Arora"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06138",
        "abstract": "Multi-task reinforcement learning often relies on task metadata -- such as brief natural-language descriptions -- to guide behavior across diverse objectives. We present Lexical Policy Networks (LEXPOL), a language-conditioned mixture-of-policies architecture for multi-task RL. LEXPOL encodes task metadata with a text encoder and uses a learned gating module to select or blend among multiple sub-policies, enabling end-to-end training across tasks. On MetaWorld benchmarks, LEXPOL matches or exceeds strong multi-task baselines in success rate and sample efficiency, without task-specific retraining. To analyze the mechanism, we further study settings with fixed expert policies obtained independently of the gate and show that the learned language gate composes these experts to produce behaviors appropriate to novel task descriptions and unseen task combinations. These results indicate that natural-language metadata can effectively index and recombine reusable skills within a single policy.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "244",
        "title": "Deforming Videos to Masks: Flow Matching for Referring Video Segmentation",
        "author": [
            "Zanyi Wang",
            "Dengyang Jiang",
            "Liuzhuozheng Li",
            "Sizhe Dang",
            "Chengzu Li",
            "Harry Yang",
            "Guang Dai",
            "Mengmeng Wang",
            "Jingdong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06139",
        "abstract": "Referring Video Object Segmentation (RVOS) requires segmenting specific objects in a video guided by a natural language description. The core challenge of RVOS is to anchor abstract linguistic concepts onto a specific set of pixels and continuously segment them through the complex dynamics of a video. Faced with this difficulty, prior work has often decomposed the task into a pragmatic `locate-then-segment' pipeline. However, this cascaded design creates an information bottleneck by simplifying semantics into coarse geometric prompts (e.g, point), and struggles to maintain temporal consistency as the segmenting process is often decoupled from the initial language grounding. To overcome these fundamental limitations, we propose FlowRVS, a novel framework that reconceptualizes RVOS as a conditional continuous flow problem. This allows us to harness the inherent strengths of pretrained T2V models, fine-grained pixel control, text-video semantic alignment, and temporal coherence. Instead of conventional generating from noise to mask or directly predicting mask, we reformulate the task by learning a direct, language-guided deformation from a video's holistic representation to its target mask. Our one-stage, generative approach achieves new state-of-the-art results across all major RVOS benchmarks. Specifically, achieving a $\\mathcal{J}\\&\\mathcal{F}$ of 51.1 in MeViS (+1.6 over prior SOTA) and 73.3 in the zero shot Ref-DAVIS17 (+2.7), demonstrating the significant potential of modeling video understanding tasks as continuous deformation processes.",
        "tags": [
            "Flow Matching",
            "Segmentation"
        ]
    },
    {
        "id": "245",
        "title": "RoSE: Round-robin Synthetic Data Evaluation for Selecting LLM Generators without Human Test Sets",
        "author": [
            "Jan Cegin",
            "Branislav Pecher",
            "Ivan Srba",
            "Jakub Simko"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06143",
        "abstract": "LLMs are powerful generators of synthetic data, which are used for training smaller, specific models. This is especially valuable for low-resource languages, where human-labelled data is scarce but LLMs can still produce high-quality text. However, LLMs differ in how useful their outputs are for training. Selecting the best LLM as a generator is challenging because extrinsic evaluation requires costly human annotations (which are often unavailable for low-resource languages), while intrinsic metrics correlate poorly with downstream performance. We introduce Round robin Synthetic data Evaluation (RoSE), a proxy metric for selecting the best LLM generator without human test sets. RoSE trains a small model on the outputs of a candidate generator (LLM) and then evaluates it on generated synthetic examples from all other candidate LLMs. The final RoSE score is the mean performance of this small model. Across six LLMs, eleven languages, and three tasks (sentiment, topic, intent), RoSE identifies the optimal generator more often than any other intrinsic heuristics. RoSE outperforms intrinsic heuristics and comes within 0.76 percentage points of the optimal generator baseline. This result is measured in terms of downstream performance, obtained by training a small model on the chosen generator's outputs (optimal vs. proxy metric selected) and evaluating it on human-labelled test data. Additionally, RoSE is the only metric to achieve a positive correlation with performance on human test data.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "246",
        "title": "Bimanual 3D Hand Motion and Articulation Forecasting in Everyday Images",
        "author": [
            "Aditya Prakash",
            "David Forsyth",
            "Saurabh Gupta"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06145",
        "abstract": "We tackle the problem of forecasting bimanual 3D hand motion & articulation from a single image in everyday settings. To address the lack of 3D hand annotations in diverse settings, we design an annotation pipeline consisting of a diffusion model to lift 2D hand keypoint sequences to 4D hand motion. For the forecasting model, we adopt a diffusion loss to account for the multimodality in hand motion distribution. Extensive experiments across 6 datasets show the benefits of training on diverse data with imputed labels (14% improvement) and effectiveness of our lifting (42% better) & forecasting (16.4% gain) models, over the best baselines, especially in zero-shot generalization to everyday images.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "247",
        "title": "Vision-Guided Targeted Grasping and Vibration for Robotic Pollination in Controlled Environments",
        "author": [
            "Jaehwan Jeong",
            "Tuan-Anh Vu",
            "Radha Lahoti",
            "Jiawen Wang",
            "Vivek Alumootil",
            "Sangpil Kim",
            "M. Khalid Jawed"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06146",
        "abstract": "Robotic pollination offers a promising alternative to manual labor and bumblebee-assisted methods in controlled agriculture, where wind-driven pollination is absent and regulatory restrictions limit the use of commercial pollinators. In this work, we present and validate a vision-guided robotic framework that uses data from an end-effector mounted RGB-D sensor and combines 3D plant reconstruction, targeted grasp planning, and physics-based vibration modeling to enable precise pollination. First, the plant is reconstructed in 3D and registered to the robot coordinate frame to identify obstacle-free grasp poses along the main stem. Second, a discrete elastic rod model predicts the relationship between actuation parameters and flower dynamics, guiding the selection of optimal pollination strategies. Finally, a manipulator with soft grippers grasps the stem and applies controlled vibrations to induce pollen release. End-to-end experiments demonstrate a 92.5\\% main-stem grasping success rate, and simulation-guided optimization of vibration parameters further validates the feasibility of our approach, ensuring that the robot can safely and effectively perform pollination without damaging the flower. To our knowledge, this is the first robotic system to jointly integrate vision-based grasping and vibration modeling for automated precision pollination.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "248",
        "title": "LLMs as Policy-Agnostic Teammates: A Case Study in Human Proxy Design for Heterogeneous Agent Teams",
        "author": [
            "Aju Ani Justus",
            "Chris Baber"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06151",
        "abstract": "A critical challenge in modelling Heterogeneous-Agent Teams is training agents to collaborate with teammates whose policies are inaccessible or non-stationary, such as humans. Traditional approaches rely on expensive human-in-the-loop data, which limits scalability. We propose using Large Language Models (LLMs) as policy-agnostic human proxies to generate synthetic data that mimics human decision-making. To evaluate this, we conduct three experiments in a grid-world capture game inspired by Stag Hunt, a game theory paradigm that balances risk and reward. In Experiment 1, we compare decisions from 30 human participants and 2 expert judges with outputs from LLaMA 3.1 and Mixtral 8x22B models. LLMs, prompted with game-state observations and reward structures, align more closely with experts than participants, demonstrating consistency in applying underlying decision criteria. Experiment 2 modifies prompts to induce risk-sensitive strategies (e.g. \"be risk averse\"). LLM outputs mirror human participants' variability, shifting between risk-averse and risk-seeking behaviours. Finally, Experiment 3 tests LLMs in a dynamic grid-world where the LLM agents generate movement actions. LLMs produce trajectories resembling human participants' paths. While LLMs cannot yet fully replicate human adaptability, their prompt-guided diversity offers a scalable foundation for simulating policy-agnostic teammates.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "249",
        "title": "A Preview of HoloOcean 2.0",
        "author": [
            "Blake Romrell",
            "Abigail Austin",
            "Braden Meyers",
            "Ryan Anderson",
            "Carter Noh",
            "Joshua G. Mangelson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06160",
        "abstract": "Marine robotics simulators play a fundamental role in the development of marine robotic systems. With increased focus on the marine robotics field in recent years, there has been significant interest in developing higher fidelitysimulation of marine sensors, physics, and visual rendering capabilities to support autonomous marine robot development and validation. HoloOcean 2.0, the next major release of HoloOcean, brings state-of-the-art features under a general marine simulator capable of supporting a variety of tasks. New features in HoloOcean 2.0 include migration to Unreal Engine (UE) 5.3, advanced vehicle dynamics using models from Fossen, and support for ROS2 using a custom bridge. Additional features are currently in development, including significantly more efficient ray tracing-based sidescan, forward-looking, and bathymetric sonar implementations; semantic sensors; environment generation tools; volumetric environmental effects; and realistic waves.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "250",
        "title": "Thermodynamic Performance Limits for Score-Based Diffusion Models",
        "author": [
            "Nathan X. Kodama",
            "Michael Hinczewski"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06174",
        "abstract": "We establish a fundamental connection between score-based diffusion models and non-equilibrium thermodynamics by deriving performance limits based on entropy rates. Our main theoretical contribution is a lower bound on the negative log-likelihood of the data that relates model performance to entropy rates of diffusion processes. We numerically validate this bound on a synthetic dataset and investigate its tightness. By building a bridge to entropy rates - system, intrinsic, and exchange entropy - we provide new insights into the thermodynamic operation of these models, drawing parallels to Maxwell's demon and implications for thermodynamic computing hardware. Our framework connects generative modeling performance to fundamental physical principles through stochastic thermodynamics.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "251",
        "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via Outlier-Suppressed Vector Quantization",
        "author": [
            "Dingyu Yao",
            "Chenxu Yang",
            "Zhengyang Tong",
            "Zheng Lin",
            "Wei Liu",
            "Jian Luan",
            "Weiping Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06175",
        "abstract": "The Key-Value (KV) cache introduces substantial memory overhead during large language model (LLM) inference. Although existing vector quantization (VQ) methods reduce KV cache usage and provide flexible representational capacity across bit-widths, they suffer severe performance degradation at ultra-low bit-widths due to key cache outliers that hinder effective codebook utilization. To address this challenge, we propose VecInfer, a novel VQ method for aggressive KV cache compression while enabling efficient inference. By applying smooth and Hadamard transformations, VecInfer suppresses outliers in the key cache, enabling the codebook to comprehensively cover the original data distribution and thereby reducing quantization difficulty. To facilitate efficient deployment, we design an optimized CUDA kernel that fuses computation with dequantization to minimize memory access overhead. Extensive evaluations demonstrate that VecInfer consistently outperforms existing quantization baselines across both long-context understanding and mathematical reasoning tasks. With only 2-bit quantization, VecInfer achieves performance comparable to full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in large-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in single-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
        "tags": [
            "LLM",
            "LLaMA",
            "Vector Quantization"
        ]
    },
    {
        "id": "252",
        "title": "RECODE-H: A Benchmark for Research Code Development with Interactive Human Feedback",
        "author": [
            "Chunyu Miao",
            "Henry Peng Zou",
            "Yangning Li",
            "Yankai Chen",
            "Yibo Wang",
            "Fangxin Wang",
            "Yifan Li",
            "Wooseong Yang",
            "Bowei He",
            "Xinni Zhang",
            "Dianzhi Yu",
            "Hanchen Yang",
            "Hoang H Nguyen",
            "Yue Zhou",
            "Jie Yang",
            "Jizhou Guo",
            "Wenzhe Fan",
            "Chin-Yuan Yeh",
            "Panpan Meng",
            "Liancheng Fang",
            "Jinhu Qi",
            "Wei-Chieh Huang",
            "Zhengyao Gu",
            "Yuwei Han",
            "Langzhou He",
            "Yuyao Yang",
            "Xue Liu",
            "Irwin King",
            "Philip S. Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06186",
        "abstract": "Large language models (LLMs) show the promise in supporting scientific research implementation, yet their ability to generate correct and executable code remains limited. Existing works largely adopt one-shot settings, ignoring the iterative and feedback-driven nature of realistic workflows of scientific research development. To address this gap, we present RECODE-H, a benchmark of 102 tasks from research papers and repositories that evaluates LLM agents through multi-turn interactions with LLM-simulated human feedback. It includes structured instructions,unit tests, and a five-level feedback hierarchy to reflect realistic researcher-agent collaboration. We further present ReCodeAgent, a framework that integrates feedback into iterative code generation. Experiments with leading LLMs, including GPT-5, Claude-Sonnet-4, DeepSeek-V3.1, and Gemini 2.5, show substantial performance gains with richer feedback, while also highlighting ongoing challenges in the generation of complex research code. RECODE-H establishes a foundation for developing adaptive, feedback-driven LLM agents in scientific research implementation",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "253",
        "title": "Automated Program Repair of Uncompilable Student Code",
        "author": [
            "Griffin Pitts",
            "Aum Pandya",
            "Darsh Rank",
            "Tirth Bhatt",
            "Muntasir Hoq",
            "Bita Akram"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06187",
        "abstract": "A significant portion of student programming submissions in CS1 learning environments are uncompilable, limiting their use in student modeling and downstream knowledge tracing. Traditional modeling pipelines often exclude these cases, discarding observations of student learning. This study investigates automated program repair as a strategy to recover uncompilable code while preserving students' structural intent for use in student modeling. Within this framework, we assess large language models (LLMs) as repair agents, including GPT-5 (OpenAI), Claude 3.5 Haiku (Anthropic), and Gemini 2.5 Flash (Google), under high- and low-context prompting conditions. Repairs were evaluated for compilability, edit distance, and preservation of students' original structure and logic. We find that while all three LLMs are capable of producing compilable repairs, their behavior diverges in how well they preserve students' control flow and code structure, which affects their pedagogical utility. By recovering uncompilable submissions, this work enables richer and more comprehensive analyses of learners' coding processes and development over time.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "254",
        "title": "Barbarians at the Gate: How AI is Upending Systems Research",
        "author": [
            "Audrey Cheng",
            "Shu Liu",
            "Melissa Pan",
            "Zhifei Li",
            "Bowen Wang",
            "Alex Krentsel",
            "Tian Xia",
            "Mert Cemri",
            "Jongseok Park",
            "Shuo Yang",
            "Jeff Chen",
            "Aditya Desai",
            "Jiarong Xing",
            "Koushik Sen",
            "Matei Zaharia",
            "Ion Stoica"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06189",
        "abstract": "Artificial Intelligence (AI) is starting to transform the research process as we know it by automating the discovery of new solutions. Given a task, the typical AI-driven approach is (i) to generate a set of diverse solutions, and then (ii) to verify these solutions and select one that solves the problem. Crucially, this approach assumes the existence of a reliable verifier, i.e., one that can accurately determine whether a solution solves the given problem. We argue that systems research, long focused on designing and evaluating new performance-oriented algorithms, is particularly well-suited for AI-driven solution discovery. This is because system performance problems naturally admit reliable verifiers: solutions are typically implemented in real systems or simulators, and verification reduces to running these software artifacts against predefined workloads and measuring performance. We term this approach as AI-Driven Research for Systems (ADRS), which iteratively generates, evaluates, and refines solutions. Using penEvolve, an existing open-source ADRS instance, we present case studies across diverse domains, including load balancing for multi-region cloud scheduling, Mixture-of-Experts inference, LLM-based SQL queries, and transaction scheduling. In multiple instances, ADRS discovers algorithms that outperform state-of-the-art human designs (e.g., achieving up to 5.0x runtime improvements or 50% cost reductions). We distill best practices for guiding algorithm evolution, from prompt design to evaluator construction, for existing frameworks. We then discuss the broader implications for the systems community: as AI assumes a central role in algorithm design, we argue that human researchers will increasingly focus on problem formulation and strategic guidance. Our results highlight both the disruptive potential and the urgent need to adapt systems research practices in the age of AI.",
        "tags": [
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "255",
        "title": "On Powerful Ways to Generate: Autoregression, Diffusion, and Beyond",
        "author": [
            "Chenxiao Yang",
            "Cai Zhou",
            "David Wipf",
            "Zhiyuan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06190",
        "abstract": "This paper formally studies generation processes, including auto-regressive next-token prediction and masked diffusion, that abstract beyond architectural specifics. At this level of abstraction, we quantify their benefits and limitations through measurable criteria such as computational hardness and learnability. In particular, we demonstrate that allowing generation to proceed beyond autoregression and current masked diffusion, with capabilities to rewrite and length-variable edit, can bring significant theoretical and empirical advantages, with important implications for frontier LLMs that aspire to tackle increasingly hard problems and work universally across domains beyond natural language, such as coding and science.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "256",
        "title": "Latent Speech-Text Transformer",
        "author": [
            "Yen-Ju Lu",
            "Yashesh Gaur",
            "Wei Zhou",
            "Benjamin Muller",
            "Jesus Villalba",
            "Najim Dehak",
            "Luke Zettlemoyer",
            "Gargi Ghosh",
            "Mike Lewis",
            "Srinivasan Iyer",
            "Duc Le"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06195",
        "abstract": "Auto-regressive speech-text models are typically pre-trained on a large number of interleaved sequences of text tokens and raw speech encoded as speech tokens using vector quantization. These models have demonstrated state-of-the-art performance in speech-to-speech understanding and generation benchmarks, together with promising scaling laws, primarily enabled by the representational alignment between text and speech. Nevertheless, they suffer from shortcomings, partly owing to the disproportionately longer sequences of speech tokens in contrast to textual tokens. This results in a large compute imbalance between modalities during pre-training as well as during inference, and a potential hindrance to effectively aligning speech and text, ultimately translating to several orders of magnitude slower scaling laws. We introduce the Latent Speech-Text Transformer (LST), which makes pre-training speech-text models more data-efficient by dynamically and inexpensively aggregating speech tokens into latent speech patches. These patches serve as higher-level units that can either align with corresponding textual units to aid capability transfer or even encapsulate common speech sequences like silences to be more compute-efficient. We show that LST outperforms vanilla approaches on speech-to-speech as well as text-to-text benchmarks in both data- and compute-controlled settings, the former indicating more effective representational alignment and the latter indicating steeper scaling laws for speech-text models. On HellaSwag story completion, LST achieves 6.5% absolute gain in speech accuracy under compute-controlled training and 5.3% under data-controlled training, while also improving text performance. We will release our models, code, and the evaluation data to facilitate further research.",
        "tags": [
            "Transformer",
            "Vector Quantization"
        ]
    },
    {
        "id": "257",
        "title": "Peeking inside the Black-Box: Reinforcement Learning for Explainable and Accurate Relation Extraction",
        "author": [
            "Xinyu Guo",
            "Zhengliang Shi",
            "Minglai Yang",
            "Mahdi Rahimi",
            "Mihai Surdeanu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06198",
        "abstract": "This paper introduces a framework for relation extraction (RE) that enhances both accuracy and explainability. The framework has two key components: (i) a reasoning mechanism that formulates relation extraction as a series of text-processing steps inspired by cognitive science, and (ii) an optimization process driven by reinforcement learning (RL) with a novel reward function designed to improve both task accuracy and explanation quality. We call our approach CogRE. Our framework addresses the lack of supervision for language-based explanations in traditional RE by promoting outputs that include important relation keywords. These keywords are drawn from a high-quality dictionary that is automatically constructed using an LLM. We evaluate our approach for the task of one-shot RE using two LLMs and two RE datasets. Our experiments show that CogRE improves explanation quality by addressing two common failure patterns in one-shot RE: poor attention focus and limited one-shot learning capability. For example, our cognitive-structured reasoning with Qwen2.5-15B-Instruct on One-shot NYT29 achieves 24.65% F1, surpassing prior reasoning-based designs. Optimizing this approach with RL using our reward further improves performance by +23.46% (absolute). Finally, human evaluation shows that our best model generates relational keywords closely aligned with gold labels, increasing human explanation quality ratings by 54% (relative).",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "258",
        "title": "DYMO-Hair: Generalizable Volumetric Dynamics Modeling for Robot Hair Manipulation",
        "author": [
            "Chengyang Zhao",
            "Uksang Yoo",
            "Arkadeep Narayan Chaudhury",
            "Giljoo Nam",
            "Jonathan Francis",
            "Jeffrey Ichnowski",
            "Jean Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06199",
        "abstract": "Hair care is an essential daily activity, yet it remains inaccessible to individuals with limited mobility and challenging for autonomous robot systems due to the fine-grained physical structure and complex dynamics of hair. In this work, we present DYMO-Hair, a model-based robot hair care system. We introduce a novel dynamics learning paradigm that is suited for volumetric quantities such as hair, relying on an action-conditioned latent state editing mechanism, coupled with a compact 3D latent space of diverse hairstyles to improve generalizability. This latent space is pre-trained at scale using a novel hair physics simulator, enabling generalization across previously unseen hairstyles. Using the dynamics model with a Model Predictive Path Integral (MPPI) planner, DYMO-Hair is able to perform visual goal-conditioned hair styling. Experiments in simulation demonstrate that DYMO-Hair's dynamics model outperforms baselines on capturing local deformation for diverse, unseen hairstyles. DYMO-Hair further outperforms baselines in closed-loop hair styling tasks on unseen hairstyles, with an average of 22% lower final geometric error and 42% higher success rate than the state-of-the-art system. Real-world experiments exhibit zero-shot transferability of our system to wigs, achieving consistent success on challenging unseen hairstyles where the state-of-the-art system fails. Together, these results introduce a foundation for model-based robot hair care, advancing toward more generalizable, flexible, and accessible robot hair styling in unconstrained physical environments. More details are available on our project page: https://chengyzhao.github.io/DYMOHair-web/.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "259",
        "title": "EmbodiedCoder: Parameterized Embodied Mobile Manipulation via Modern Coding Model",
        "author": [
            "Zefu Lin",
            "Rongxu Cui",
            "Chen Hanning",
            "Xiangyu Wang",
            "Junjia Xu",
            "Xiaojuan Jin",
            "Chen Wenbo",
            "Hui Zhou",
            "Lue Fan",
            "Wenling Li",
            "Zhaoxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06207",
        "abstract": "Recent advances in control robot methods, from end-to-end vision-language-action frameworks to modular systems with predefined primitives, have advanced robots' ability to follow natural language instructions. Nonetheless, many approaches still struggle to scale to diverse environments, as they often rely on large annotated datasets and offer limited http://interpretability.In this work, we introduce EmbodiedCoder, a training-free framework for open-world mobile robot manipulation that leverages coding models to directly generate executable robot trajectories. By grounding high-level instructions in code, EmbodiedCoder enables flexible object geometry parameterization and manipulation trajectory synthesis without additional data collection or http://fine-tuning.This coding-based paradigm provides a transparent and generalizable way to connect perception with manipulation. Experiments on real mobile robots show that EmbodiedCoder achieves robust performance across diverse long-term tasks and generalizes effectively to novel objects and http://environments.Our results demonstrate an interpretable approach for bridging high-level reasoning and low-level control, moving beyond fixed primitives toward versatile robot intelligence. See the project page at: https://anonymous.4open.science/w/Embodied-Coder/",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "260",
        "title": "Drive&Gen: Co-Evaluating End-to-End Driving and Video Generation Models",
        "author": [
            "Jiahao Wang",
            "Zhenpei Yang",
            "Yijing Bai",
            "Yingwei Li",
            "Yuliang Zou",
            "Bo Sun",
            "Abhijit Kundu",
            "Jose Lezama",
            "Luna Yue Huang",
            "Zehao Zhu",
            "Jyh-Jing Hwang",
            "Dragomir Anguelov",
            "Mingxing Tan",
            "Chiyu Max Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06209",
        "abstract": "Recent advances in generative models have sparked exciting new possibilities in the field of autonomous vehicles. Specifically, video generation models are now being explored as controllable virtual testing environments. Simultaneously, end-to-end (E2E) driving models have emerged as a streamlined alternative to conventional modular autonomous driving systems, gaining popularity for their simplicity and scalability. However, the application of these techniques to simulation and planning raises important questions. First, while video generation models can generate increasingly realistic videos, can these videos faithfully adhere to the specified conditions and be realistic enough for E2E autonomous planner evaluation? Second, given that data is crucial for understanding and controlling E2E planners, how can we gain deeper insights into their biases and improve their ability to generalize to out-of-distribution scenarios? In this work, we bridge the gap between the driving models and generative world models (Drive&Gen) to address these questions. We propose novel statistical measures leveraging E2E drivers to evaluate the realism of generated videos. By exploiting the controllability of the video generation model, we conduct targeted experiments to investigate distribution gaps affecting E2E planner performance. Finally, we show that synthetic data produced by the video generation model offers a cost-effective alternative to real-world data collection. This synthetic data effectively improves E2E model generalization beyond existing Operational Design Domains, facilitating the expansion of autonomous vehicle services into new operational contexts.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "261",
        "title": "Training Dynamics Impact Post-Training Quantization Robustness",
        "author": [
            "Albert Catalan-Tatjer",
            "NiccolÃ² Ajroldi",
            "Jonas Geiping"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06213",
        "abstract": "While post-training quantization is widely adopted for efficient deployment of large language models, the mechanisms underlying quantization robustness remain unclear. We conduct a comprehensive analysis of quantization degradation across open-source language model training trajectories up to 32B parameters and 15T training tokens to accurately assess the relationship between training dynamics and quantization performance. Our key finding is that quantization errors in large-scale training runs are driven by a complex interplay between learning rate and other training hyperparameters. Specifically, once learning rates decay, validation loss and quantization error diverge, largely independent of training data scale. To investigate interventions on the training dynamics and identify specific configurations that can modulate quantization robustness favorably, we train our own models in controlled experiments up to 100B tokens. Our results challenge the assumption that increasing dataset scale inherently compromises quantization effectiveness, demonstrating instead that strategic training hyperparameter interventions can improve quantization quality at scale.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "262",
        "title": "Stratified GRPO: Handling Structural Heterogeneity in Reinforcement Learning of LLM Search Agents",
        "author": [
            "Mingkang Zhu",
            "Xi Chen",
            "Bei Yu",
            "Hengshuang Zhao",
            "Jiaya Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06214",
        "abstract": "Large language model (LLM) agents increasingly rely on external tools such as search engines to solve complex, multi-step problems, and reinforcement learning (RL) has become a key paradigm for training them. However, the trajectories of search agents are structurally heterogeneous, where variations in the number, placement, and outcomes of search calls lead to fundamentally different answer directions and reward distributions. Standard policy gradient methods, which use a single global baseline, suffer from what we identify and formalize as cross-stratum bias-an \"apples-to-oranges\" comparison of heterogeneous trajectories. This cross-stratum bias distorts credit assignment and hinders exploration of complex, multi-step search strategies. To address this, we propose Stratified GRPO, whose central component, Stratified Advantage Normalization (SAN), partitions trajectories into homogeneous strata based on their structural properties and computes advantages locally within each stratum. This ensures that trajectories are evaluated only against their true peers. Our analysis proves that SAN eliminates cross-stratum bias, yields conditionally unbiased unit-variance estimates inside each stratum, and retains the global unbiasedness and unit-variance properties enjoyed by standard normalization, resulting in a more pure and scale-stable learning signal. To improve practical stability under finite-sample regimes, we further linearly blend SAN with the global estimator. Extensive experiments on diverse single-hop and multi-hop question-answering benchmarks demonstrate that Stratified GRPO consistently and substantially outperforms GRPO by up to 11.3 points, achieving higher training rewards, greater training stability, and more effective search policies. These results establish stratification as a principled remedy for structural heterogeneity in RL for LLM search agents.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "263",
        "title": "Fine-grained Defocus Blur Control for Generative Image Models",
        "author": [
            "Ayush Shrivastava",
            "Connelly Barnes",
            "Xuaner Zhang",
            "Lingzhi Zhang",
            "Andrew Owens",
            "Sohrab Amirghodsi",
            "Eli Shechtman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06215",
        "abstract": "Current text-to-image diffusion models excel at generating diverse, high-quality images, yet they struggle to incorporate fine-grained camera metadata such as precise aperture settings. In this work, we introduce a novel text-to-image diffusion framework that leverages camera metadata, or EXIF data, which is often embedded in image files, with an emphasis on generating controllable lens blur. Our method mimics the physical image formation process by first generating an all-in-focus image, estimating its monocular depth, predicting a plausible focus distance with a novel focus distance transformer, and then forming a defocused image with an existing differentiable lens blur model. Gradients flow backwards through this whole process, allowing us to learn without explicit supervision to generate defocus effects based on content elements and the provided EXIF data. At inference time, this enables precise interactive user control over defocus effects while preserving scene contents, which is not achievable with existing diffusion models. Experimental results demonstrate that our model enables superior fine-grained control without altering the depicted scene.",
        "tags": [
            "Diffusion",
            "Text-to-Image",
            "Transformer"
        ]
    },
    {
        "id": "264",
        "title": "Dropping the D: RGB-D SLAM Without the Depth Sensor",
        "author": [
            "Mert Kiray",
            "Alican Karaomer",
            "Benjamin Busam"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06216",
        "abstract": "We present DropD-SLAM, a real-time monocular SLAM system that achieves RGB-D-level accuracy without relying on depth sensors. The system replaces active depth input with three pretrained vision modules: a monocular metric depth estimator, a learned keypoint detector, and an instance segmentation network. Dynamic objects are suppressed using dilated instance masks, while static keypoints are assigned predicted depth values and backprojected into 3D to form metrically scaled features. These are processed by an unmodified RGB-D SLAM back end for tracking and mapping. On the TUM RGB-D benchmark, DropD-SLAM attains 7.4 cm mean ATE on static sequences and 1.8 cm on dynamic sequences, matching or surpassing state-of-the-art RGB-D methods while operating at 22 FPS on a single GPU. These results suggest that modern pretrained vision models can replace active depth sensors as reliable, real-time sources of metric scale, marking a step toward simpler and more cost-effective SLAM systems.",
        "tags": [
            "3D",
            "SLAM",
            "Segmentation"
        ]
    },
    {
        "id": "265",
        "title": "EgoNight: Towards Egocentric Vision Understanding at Night with a Challenging Benchmark",
        "author": [
            "Deheng Zhang",
            "Yuqian Fu",
            "Runyi Yang",
            "Yang Miao",
            "Tianwen Qian",
            "Xu Zheng",
            "Guolei Sun",
            "Ajad Chhatkuli",
            "Xuanjing Huang",
            "Yu-Gang Jiang",
            "Luc Van Gool",
            "Danda Pani Paudel"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06218",
        "abstract": "Most existing benchmarks for egocentric vision understanding focus primarily on daytime scenarios, overlooking the low-light conditions that are inevitable in real-world applications. To investigate this gap, we present EgoNight, the first comprehensive benchmark for nighttime egocentric vision, with visual question answering (VQA) as the core task. A key feature of EgoNight is the introduction of day-night aligned videos, which enhance night annotation quality using the daytime data and reveal clear performance gaps between lighting conditions. To achieve this, we collect both synthetic videos rendered by Blender and real-world recordings, ensuring that scenes and actions are visually and temporally aligned. Leveraging these paired videos, we construct EgoNight-VQA, supported by a novel day-augmented night auto-labeling engine and refinement through extensive human verification. Each QA pair is double-checked by annotators for reliability. In total, EgoNight-VQA contains 3658 QA pairs across 90 videos, spanning 12 diverse QA types, with more than 300 hours of human work. Evaluations of state-of-the-art multimodal large language models (MLLMs) reveal substantial performance drops when transferring from day to night, underscoring the challenges of reasoning under low-light conditions. Beyond VQA, EgoNight also introduces two auxiliary tasks, day-night correspondence retrieval and egocentric depth estimation at night, that further explore the boundaries of existing models. We believe EgoNight-VQA provides a strong foundation for advancing application-driven egocentric vision research and for developing models that generalize across illumination domains. All the data and code will be made available upon acceptance.",
        "tags": [
            "Depth Estimation",
            "LLM"
        ]
    },
    {
        "id": "266",
        "title": "Human3R: Everyone Everywhere All at Once",
        "author": [
            "Yue Chen",
            "Xingyu Chen",
            "Yuxuan Xue",
            "Anpei Chen",
            "Yuliang Xiu",
            "Gerard Pons-Moll"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06219",
        "abstract": "We present Human3R, a unified, feed-forward framework for online 4D human-scene reconstruction, in the world frame, from casually captured monocular videos. Unlike previous approaches that rely on multi-stage pipelines, iterative contact-aware refinement between humans and scenes, and heavy dependencies, e.g., human detection, depth estimation, and SLAM pre-processing, Human3R jointly recovers global multi-person SMPL-X bodies (\"everyone\"), dense 3D scene (\"everywhere\"), and camera trajectories in a single forward pass (\"all-at-once\"). Our method builds upon the 4D online reconstruction model CUT3R, and uses parameter-efficient visual prompt tuning, to strive to preserve CUT3R's rich spatiotemporal priors, while enabling direct readout of multiple SMPL-X bodies. Human3R is a unified model that eliminates heavy dependencies and iterative refinement. After being trained on the relatively small-scale synthetic dataset BEDLAM for just one day on one GPU, it achieves superior performance with remarkable efficiency: it reconstructs multiple humans in a one-shot manner, along with 3D scenes, in one stage, at real-time speed (15 FPS) with a low memory footprint (8 GB). Extensive experiments demonstrate that Human3R delivers state-of-the-art or competitive performance across tasks, including global human motion estimation, local human mesh recovery, video depth estimation, and camera pose estimation, with a single unified model. We hope that Human3R will serve as a simple yet strong baseline, be easily extended for downstream http://applications.Code available in https://fanegg.github.io/Human3R",
        "tags": [
            "3D",
            "Depth Estimation",
            "Detection",
            "Pose Estimation",
            "SLAM"
        ]
    },
    {
        "id": "267",
        "title": "WaveSP-Net: Learnable Wavelet-Domain Sparse Prompt Tuning for Speech Deepfake Detection",
        "author": [
            "Xi Xuan",
            "Xuechen Liu",
            "Wenxin Zhang",
            "Yi-Cheng Lin",
            "Xiaojian Lin",
            "Tomi Kinnunen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05305",
        "abstract": "Modern front-end design for speech deepfake detection relies on full fine-tuning of large pre-trained models like XLSR. However, this approach is not parameter-efficient and may lead to suboptimal generalization to realistic, in-the-wild data types. To address these limitations, we introduce a new family of parameter-efficient front-ends that fuse prompt-tuning with classical signal processing transforms. These include FourierPT-XLSR, which uses the Fourier Transform, and two variants based on the Wavelet Transform: WSPT-XLSR and Partial-WSPT-XLSR. We further propose WaveSP-Net, a novel architecture combining a Partial-WSPT-XLSR front-end and a bidirectional Mamba-based back-end. This design injects multi-resolution features into the prompt embeddings, which enhances the localization of subtle synthetic artifacts without altering the frozen XLSR parameters. Experimental results demonstrate that WaveSP-Net outperforms several state-of-the-art models on two new and challenging benchmarks, Deepfake-Eval-2024 and SpoofCeleb, with low trainable parameters and notable performance gains. The code and models are available at https://github.com/xxuan-acoustics/WaveSP-Net.",
        "tags": [
            "Detection",
            "Mamba"
        ]
    },
    {
        "id": "268",
        "title": "A System Level Approach to LQR Control of the Diffusion Equation",
        "author": [
            "Addie McCurdy",
            "Andrew Gusty",
            "Emily Jensen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05345",
        "abstract": "The continuous-time, infinite horizon LQR problem for the diffusion equation over the unit circle with fully distributed actuation is considered. It is well-known that the solution to this problem can be obtained from the solution to an operator-valued algebraic Riccati equation. Here, it is demonstrated that this solution can be equivalently obtained by solving an $H_2$ control problem through a closed-loop design procedure that is analogous to the \"System Level Synthesis\" methodology previously developed for systems over a discrete spatial domain and/or over a finite time horizon. The presented extension to the continuous spatial domain and continuous and infinite-horizon time setting admits analytical solutions that may complement computational approaches for discrete or finite-horizon settings. It is further illustrated that spatio-temporal constraints on the closed-loop responses can be incorporated into this new formulation in a convex manner.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "269",
        "title": "Minima and Critical Points of the Bethe Free Energy Are Invariant Under Deformation Retractions of Factor Graphs",
        "author": [
            "GrÃ©goire Sergeant-Perthuis",
            "LÃ©o Boitel"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05380",
        "abstract": "In graphical models, factor graphs, and more generally energy-based models, the interactions between variables are encoded by a graph, a hypergraph, or, in the most general case, a partially ordered set (poset). Inference on such probabilistic models cannot be performed exactly due to cycles in the underlying structures of interaction. Instead, one resorts to approximate variational inference by optimizing the Bethe free energy. Critical points of the Bethe free energy correspond to fixed points of the associated Belief Propagation algorithm. A full characterization of these critical points for general graphs, hypergraphs, and posets with a finite number of variables is still an open problem. We show that, for hypergraphs and posets with chains of length at most 1, changing the poset of interactions of the probabilistic model to one with the same homotopy type induces a bijection between the critical points of the associated free energy. This result extends and unifies classical results that assume specific forms of collapsibility to prove uniqueness of the critical points of the Bethe free energy.",
        "tags": [
            "Energy-Based Models"
        ]
    },
    {
        "id": "270",
        "title": "Domain-Shift-Aware Conformal Prediction for Large Language Models",
        "author": [
            "Zhexiao Lin",
            "Yuanyuan Li",
            "Neeraj Sarna",
            "Yuanyuan Gao",
            "Michael von Gablenz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05566",
        "abstract": "Large language models have achieved impressive performance across diverse tasks. However, their tendency to produce overconfident and factually incorrect outputs, known as hallucinations, poses risks in real world applications. Conformal prediction provides finite-sample, distribution-free coverage guarantees, but standard conformal prediction breaks down under domain shift, often leading to under-coverage and unreliable prediction sets. We propose a new framework called Domain-Shift-Aware Conformal Prediction (DS-CP). Our framework adapts conformal prediction to large language models under domain shift, by systematically reweighting calibration samples based on their proximity to the test prompt, thereby preserving validity while enhancing adaptivity. Our theoretical analysis and experiments on the MMLU benchmark demonstrate that the proposed method delivers more reliable coverage than standard conformal prediction, especially under substantial distribution shifts, while maintaining efficiency. This provides a practical step toward trustworthy uncertainty quantification for large language models in real-world deployment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "271",
        "title": "Uncovering Representation Bias for Investment Decisions in Open-Source Large Language Models",
        "author": [
            "Fabrizio Dimino",
            "Krati Saxena",
            "Bhaskarjit Sarmah",
            "Stefano Pasquali"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05702",
        "abstract": "Large Language Models are increasingly adopted in financial applications to support investment workflows. However, prior studies have seldom examined how these models reflect biases related to firm size, sector, or financial characteristics, which can significantly impact decision-making. This paper addresses this gap by focusing on representation bias in open-source Qwen models. We propose a balanced round-robin prompting method over approximately 150 U.S. equities, applying constrained decoding and token-logit aggregation to derive firm-level confidence scores across financial contexts. Using statistical tests and variance analysis, we find that firm size and valuation consistently increase model confidence, while risk factors tend to decrease it. Confidence varies significantly across sectors, with the Technology sector showing the greatest variability. When models are prompted for specific financial categories, their confidence rankings best align with fundamental data, moderately with technical signals, and least with growth indicators. These results highlight representation bias in Qwen models and motivate sector-aware calibration and category-conditioned evaluation protocols for safe and fair financial LLM deployment.",
        "tags": [
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "272",
        "title": "FinReflectKG - EvalBench: Benchmarking Financial KG with Multi-Dimensional Evaluation",
        "author": [
            "Fabrizio Dimino",
            "Abhinav Arun",
            "Bhaskarjit Sarmah",
            "Stefano Pasquali"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05710",
        "abstract": "Large language models (LLMs) are increasingly being used to extract structured knowledge from unstructured financial text. Although prior studies have explored various extraction methods, there is no universal benchmark or unified evaluation framework for the construction of financial knowledge graphs (KG). We introduce FinReflectKG - EvalBench, a benchmark and evaluation framework for KG extraction from SEC 10-K filings. Building on the agentic and holistic evaluation principles of FinReflectKG - a financial KG linking audited triples to source chunks from S&P 100 filings and supporting single-pass, multi-pass, and reflection-agent-based extraction modes - EvalBench implements a deterministic commit-then-justify judging protocol with explicit bias controls, mitigating position effects, leniency, verbosity and world-knowledge reliance. Each candidate triple is evaluated with binary judgments of faithfulness, precision, and relevance, while comprehensiveness is assessed on a three-level ordinal scale (good, partial, bad) at the chunk level. Our findings suggest that, when equipped with explicit bias controls, LLM-as-Judge protocols provide a reliable and cost-efficient alternative to human annotation, while also enabling structured error analysis. Reflection-based extraction emerges as the superior approach, achieving best performance in comprehensiveness, precision, and relevance, while single-pass extraction maintains the highest faithfulness. By aggregating these complementary dimensions, FinReflectKG - EvalBench enables fine-grained benchmarking and bias-aware evaluation, advancing transparency and governance in financial AI applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "273",
        "title": "Leveraging Vision Transformers for Enhanced Classification of Emotions using ECG Signals",
        "author": [
            "Pubudu L. Indrasiri",
            "Bipasha Kashyap",
            "Pubudu N. Pathirana"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05826",
        "abstract": "Biomedical signals provide insights into various conditions affecting the human body. Beyond diagnostic capabilities, these signals offer a deeper understanding of how specific organs respond to an individual's emotions and feelings. For instance, ECG data can reveal changes in heart rate variability linked to emotional arousal, stress levels, and autonomic nervous system activity. This data offers a window into the physiological basis of our emotional states. Recent advancements in the field diverge from conventional approaches by leveraging the power of advanced transformer architectures, which surpass traditional machine learning and deep learning methods. We begin by assessing the effectiveness of the Vision Transformer (ViT), a forefront model in image classification, for identifying emotions in imaged ECGs. Following this, we present and evaluate an improved version of ViT, integrating both CNN and SE blocks, aiming to bolster performance on imaged ECGs associated with emotion detection. Our method unfolds in two critical phases: first, we apply advanced preprocessing techniques for signal purification and converting signals into interpretable images using continuous wavelet transform and power spectral density analysis; second, we unveil a performance-boosted vision transformer architecture, cleverly enhanced with convolutional neural network components, to adeptly tackle the challenges of emotion recognition. Our methodology's robustness and innovation were thoroughly tested using ECG data from the YAAD and DREAMER datasets, leading to remarkable outcomes. For the YAAD dataset, our approach outperformed existing state-of-the-art methods in classifying seven unique emotional states, as well as in valence and arousal classification. Similarly, in the DREAMER dataset, our method excelled in distinguishing between valence, arousal and dominance, surpassing current leading techniques.",
        "tags": [
            "Detection",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "274",
        "title": "Hybrid Quantum-Classical Policy Gradient for Adaptive Control of Cyber-Physical Systems: A Comparative Study of VQC vs. MLP",
        "author": [
            "Aueaphum Aueawatthanaphisut",
            "Nyi Wunna Tun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06010",
        "abstract": "The comparative evaluation between classical and quantum reinforcement learning (QRL) paradigms was conducted to investigate their convergence behavior, robustness under observational noise, and computational efficiency in a benchmark control environment. The study employed a multilayer perceptron (MLP) agent as a classical baseline and a parameterized variational quantum circuit (VQC) as a quantum counterpart, both trained on the CartPole-v1 environment over 500 episodes. Empirical results demonstrated that the classical MLP achieved near-optimal policy convergence with a mean return of 498.7 +/- 3.2, maintaining stable equilibrium throughout training. In contrast, the VQC exhibited limited learning capability, with an average return of 14.6 +/- 4.8, primarily constrained by circuit depth and qubit connectivity. Noise robustness analysis further revealed that the MLP policy deteriorated gracefully under Gaussian perturbations, while the VQC displayed higher sensitivity at equivalent noise levels. Despite the lower asymptotic performance, the VQC exhibited significantly lower parameter count and marginally increased training time, highlighting its potential scalability for low-resource quantum processors. The results suggest that while classical neural policies remain dominant in current control benchmarks, quantum-enhanced architectures could offer promising efficiency advantages once hardware noise and expressivity limitations are mitigated.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "275",
        "title": "Implicit Updates for Average-Reward Temporal Difference Learning",
        "author": [
            "Hwanwoo Kim",
            "Dongkyu Derek Cho",
            "Eric Laber"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06149",
        "abstract": "Temporal difference (TD) learning is a cornerstone of reinforcement learning. In the average-reward setting, standard TD($\\lambda$) is highly sensitive to the choice of step-size and thus requires careful tuning to maintain numerical stability. We introduce average-reward implicit TD($\\lambda$), which employs an implicit fixed point update to provide data-adaptive stabilization while preserving the per iteration computational complexity of standard average-reward TD($\\lambda$). In contrast to prior finite-time analyses of average-reward TD($\\lambda$), which impose restrictive step-size conditions, we establish finite-time error bounds for the implicit variant under substantially weaker step-size requirements. Empirically, average-reward implicit TD($\\lambda$) operates reliably over a much broader range of step-sizes and exhibits markedly improved numerical stability. This enables more efficient policy evaluation and policy learning, highlighting its effectiveness as a robust alternative to average-reward TD($\\lambda$).",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "276",
        "title": "Smartphone-based iris recognition through high-quality visible-spectrum iris image capture.V2",
        "author": [
            "Naveenkumar G Venkataswamy",
            "Yu Liu",
            "Soumyabrata Dey",
            "Stephanie Schuckers",
            "Masudul H Imtiaz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06170",
        "abstract": "Smartphone-based iris recognition in the visible spectrum (VIS) remains difficult due to illumination variability, pigmentation differences, and the absence of standardized capture controls. This work presents a compact end-to-end pipeline that enforces ISO/IEC 29794-6 quality compliance at acquisition and demonstrates that accurate VIS iris recognition is feasible on commodity devices. Using a custom Android application performing real-time framing, sharpness evaluation, and feedback, we introduce the CUVIRIS dataset of 752 compliant images from 47 subjects. A lightweight MobileNetV3-based multi-task segmentation network (LightIrisNet) is developed for efficient on-device processing, and a transformer matcher (IrisFormer) is adapted to the VIS domain. Under a standardized protocol and comparative benchmarking against prior CNN baselines, OSIRIS attains a TAR of 97.9% at FAR=0.01 (EER=0.76%), while IrisFormer, trained only on UBIRIS.v2, achieves an EER of 0.057% on CUVIRIS. The acquisition app, trained models, and a public subset of the dataset are released to support reproducibility. These results confirm that standardized capture and VIS-adapted lightweight models enable accurate and practical iris recognition on smartphones.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "277",
        "title": "Differentiable Model Predictive Control on the GPU",
        "author": [
            "Emre Adabag",
            "Marcus Greiff",
            "John Subosits",
            "Thomas Lew"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06179",
        "abstract": "Differentiable model predictive control (MPC) offers a powerful framework for combining learning and control. However, its adoption has been limited by the inherently sequential nature of traditional optimization algorithms, which are challenging to parallelize on modern computing hardware like GPUs. In this work, we tackle this bottleneck by introducing a GPU-accelerated differentiable optimization tool for MPC. This solver leverages sequential quadratic programming and a custom preconditioned conjugate gradient (PCG) routine with tridiagonal preconditioning to exploit the problem's structure and enable efficient parallelization. We demonstrate substantial speedups over CPU- and GPU-based baselines, significantly improving upon state-of-the-art training times on benchmark reinforcement learning and imitation learning tasks. Finally, we showcase the method on the challenging task of reinforcement learning for driving at the limits of handling, where it enables robust drifting of a Toyota Supra through water puddles.",
        "tags": [
            "MPC",
            "RL"
        ]
    },
    {
        "id": "278",
        "title": "StarEmbed: Benchmarking Time Series Foundation Models on Astronomical Observations of Variable Stars",
        "author": [
            "Weijian Li",
            "Hong-Yu Chen",
            "Qinjie Lin",
            "Nabeel Rehemtulla",
            "Ved G. Shah",
            "Dennis Wu",
            "Adam A. Miller",
            "Han Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06200",
        "abstract": "Time series foundation models (TSFMs) are increasingly being adopted as highly-capable general-purpose time series representation learners. Although their training corpora are vast, they exclude astronomical time series data. Observations of stars produce peta-scale time series with unique challenges including irregular sampling and heteroskedasticity. We introduce StarEmbed, the first public benchmark for rigorous and standardized evaluation of state-of-the-art TSFMs on stellar time series observations (``light curves''). We benchmark on three scientifically-motivated downstream tasks: unsupervised clustering, supervised classification, and out-of-distribution source detection. StarEmbed integrates a catalog of expert-vetted labels with multi-variate light curves from the Zwicky Transient Facility, yielding ~40k hand-labeled light curves spread across seven astrophysical classes. We evaluate the zero-shot representation capabilities of three TSFMs (MOIRAI, Chronos, Chronos-Bolt) and a domain-specific transformer (Astromer) against handcrafted feature extraction, the long-standing baseline in the astrophysics literature. Our results demonstrate that these TSFMs, especially the Chronos models, which are trained on data completely unlike the astronomical observations, can outperform established astrophysics-specific baselines in some tasks and effectively generalize to entirely new data. In particular, TSFMs deliver state-of-the-art performance on our out-of-distribution source detection benchmark. With the first benchmark of TSFMs on astronomical time series data, we test the limits of their generalization and motivate a paradigm shift in time-domain astronomy from using task-specific, fully supervised pipelines toward adopting generic foundation model representations for the analysis of peta-scale datasets from forthcoming observatories.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    }
]