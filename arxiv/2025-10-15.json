[
    {
        "id": "1",
        "title": "Modeling Hypergraph Using Large Language Models",
        "author": [
            "Bingqiao Gu",
            "Jiale Zeng",
            "Xingqin Qi",
            "Dong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11728",
        "abstract": "Due to the advantages of hypergraphs in modeling high-order relationships in complex systems, they have been applied to higher-order clustering, hypergraph neural networks and computer vision. These applications rely heavily on access to high-quality, large-scale real-world hypergraph data. Yet, compared to traditional pairwise graphs, real hypergraph datasets remain scarce in both scale and diversity. This shortage significantly limits the development and evaluation of advanced hypergraph learning algorithms. Therefore, how to quickly generate large-scale hypergraphs that conform to the characteristics of real networks is a crucial task that has not received sufficient attention. Motivated by recent advances in large language models (LLMs), particularly their capabilities in semantic reasoning, structured generation, and simulating human behavior, we investigate whether LLMs can facilitate hypergraph generation from a fundamentally new perspective. We introduce HyperLLM, a novel LLM-driven hypergraph generator that simulates the formation and evolution of hypergraphs through a multi-agent collaboration. The framework integrates prompts and structural feedback mechanisms to ensure that the generated hypergraphs reflect key real-world patterns. Extensive experiments across diverse datasets demonstrate that HyperLLM achieves superior fidelity to structural and temporal hypergraph patterns, while requiring minimal statistical priors. Our findings suggest that LLM-based frameworks offer a promising new direction for hypergraph modeling.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "2",
        "title": "Scaling Law in LLM Simulated Personality: More Detailed and Realistic Persona Profile Is All You Need",
        "author": [
            "Yuqi Bai",
            "Tianyu Huang",
            "Kun Sun",
            "Yuting Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11734",
        "abstract": "This research focuses on using large language models (LLMs) to simulate social experiments, exploring their ability to emulate human personality in virtual persona role-playing. The research develops an end-to-end evaluation framework, including individual-level analysis of stability and identifiability, as well as population-level analysis called progressive personality curves to examine the veracity and consistency of LLMs in simulating human personality. Methodologically, this research proposes important modifications to traditional psychometric approaches (CFA and construct validity) which are unable to capture improvement trends in LLMs at their current low-level simulation, potentially leading to remature rejection or methodological misalignment. The main contributions of this research are: proposing a systematic framework for LLM virtual personality evaluation; empirically demonstrating the critical role of persona detail in personality simulation quality; and identifying marginal utility effects of persona profiles, especially a Scaling Law in LLM personality simulation, offering operational evaluation metrics and a theoretical foundation for applying large language models in social science experiments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "3",
        "title": "AI Agents for the Dhumbal Card Game: A Comparative Study",
        "author": [
            "Sahaj Raj Malla"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11736",
        "abstract": "This study evaluates Artificial Intelligence (AI) agents for Dhumbal, a culturally significant multiplayer card game with imperfect information, through a systematic comparison of rule-based, search-based, and learning-based strategies. We formalize Dhumbal's mechanics and implement diverse agents, including heuristic approaches (Aggressive, Conservative, Balanced, Opportunistic), search-based methods such as Monte Carlo Tree Search (MCTS) and Information Set Monte Carlo Tree Search (ISMCTS), and reinforcement learning approaches including Deep Q-Network (DQN) and Proximal Policy Optimization (PPO), and a random baseline. Evaluation involves within-category tournaments followed by a cross-category championship. Performance is measured via win rate, economic outcome, Jhyap success, cards discarded per round, risk assessment, and decision efficiency. Statistical significance is assessed using Welch's t-test with Bonferroni correction, effect sizes via Cohen's d, and 95% confidence intervals (CI). Across 1024 simulated rounds, the rule-based Aggressive agent achieves the highest win rate (88.3%, 95% CI: [86.3, 90.3]), outperforming ISMCTS (9.0%) and PPO (1.5%) through effective exploitation of Jhyap declarations. The study contributes a reproducible AI framework, insights into heuristic efficacy under partial information, and open-source code, thereby advancing AI research and supporting digital preservation of cultural games.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "4",
        "title": "SeeingSounds: Learning Audio-to-Visual Alignment via Text",
        "author": [
            "Simone Carnemolla",
            "Matteo Pennisi",
            "Chiara Russo",
            "Simone Palazzo",
            "Daniela Giordano",
            "Concetto Spampinato"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11738",
        "abstract": "We introduce SeeingSounds, a lightweight and modular framework for audio-to-image generation that leverages the interplay between audio, language, and vision-without requiring any paired audio-visual data or training on visual generative models. Rather than treating audio as a substitute for text or relying solely on audio-to-text mappings, our method performs dual alignment: audio is projected into a semantic language space via a frozen language encoder, and, contextually grounded into the visual domain using a vision-language model. This approach, inspired by cognitive neuroscience, reflects the natural cross-modal associations observed in human perception. The model operates on frozen diffusion backbones and trains only lightweight adapters, enabling efficient and scalable learning. Moreover, it supports fine-grained and interpretable control through procedural text prompt generation, where audio transformations (e.g., volume or pitch shifts) translate into descriptive prompts (e.g., \"a distant thunder\") that guide visual outputs. Extensive experiments across standard benchmarks confirm that SeeingSounds outperforms existing methods in both zero-shot and supervised settings, establishing a new state of the art in controllable audio-to-visual generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "5",
        "title": "The Ethics Engine: A Modular Pipeline for Accessible Psychometric Assessment of Large Language Models",
        "author": [
            "Jake Van Clief",
            "Constantine Kyritsopoulos"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11742",
        "abstract": "As Large Language Models increasingly mediate human communication and decision-making, understanding their value expression becomes critical for research across disciplines. This work presents the Ethics Engine, a modular Python pipeline that transforms psychometric assessment of LLMs from a technically complex endeavor into an accessible research tool. The pipeline demonstrates how thoughtful infrastructure design can expand participation in AI research, enabling investigators across cognitive science, political psychology, education, and other fields to study value expression in language models. Recent adoption by University of Edinburgh researchers studying authoritarianism validates its research utility, processing over 10,000 AI responses across multiple models and contexts. We argue that such tools fundamentally change the landscape of AI research by lowering technical barriers while maintaining scientific rigor. As LLMs increasingly serve as cognitive infrastructure, their embedded values shape millions of daily interactions. Without systematic measurement of these value expressions, we deploy systems whose moral influence remains uncharted. The Ethics Engine enables the rigorous assessment necessary for informed governance of these influential technologies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "Benefits and Limitations of Using GenAI for Political Education and Municipal Elections",
        "author": [
            "Raphael Fischer",
            "Youssef Abdelrahim",
            "Katharina Poitz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11749",
        "abstract": "Generative artificial intelligence (GenAI) presents both challenges and opportunities across all areas of education. Facing the municipal elections in North Rhine-Westphalia, the Young AI Leaders in Dortmund asked themselves: Could GenAI be used to make political programs more accessible, in order to facilitate political education? To explore respective potentials and limitations, we therefore performed an experimental study that combines different GenAI approaches. Language models were used to automatically translate and analyze the contents of each program, deriving five potential visual appearance changes to the city of Dortmund. Based on each analysis, we then generated images with diffusion models and published all results as an interactive webpage. All GenAI models were locally deployed on a Dortmund-based computing cluster, allowing us to also investigate environmental impacts. This manuscript explores the project in full depth, discussing technical details and critically reflecting on the results. As part of the global Young AI Leaders Community, our work promotes the Sustainable Development Goal Quality Education (SDG 4) by transparently discussing the pros and cons of using GenAI for education and political agendas.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "7",
        "title": "AwareCompiler: Agentic Context-Aware Compiler Optimization via a Synergistic Knowledge-Data Driven Framework",
        "author": [
            "Hongyu Lin",
            "Haolin Pan",
            "Haoran Luo",
            "Yuchen Li",
            "Kaichun Yao",
            "Libo Zhang",
            "Mingjie Xing",
            "Yanjun Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11759",
        "abstract": "Compiler optimization is crucial for enhancing program performance by transforming the sequence of optimization passes while maintaining correctness. Despite the promising potential of large language models (LLMs)-based agent for software optimization, automating compiler optimization remains challenging due to: (1) semantic misalignment between abstract program representations and concrete optimization passes, (2) inefficient interaction mechanisms between agents and compiler environments, and (3) reward sparsity from the extensive decision-making process within large optimization spaces. This paper introduces \\textbf{AwareCompiler}, an agentic framework for compiler optimization that addresses these challenges through three key innovations: structured knowledge integration and dataset construction, knowledge-driven adaptive pass generation, and data-driven hybrid training pipeline. Experimental results on standard benchmarks demonstrate that AwareCompiler significantly outperforms existing baselines in both performance and efficiency, highlighting the effectiveness of our synergistic knowledge-data-driven approach. Our code is publicly available at https://github.com/LHY-24/AwareCompiler.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "8",
        "title": "GAR: Generative Adversarial Reinforcement Learning for Formal Theorem Proving",
        "author": [
            "Ruida Wang",
            "Jiarui Yao",
            "Rui Pan",
            "Shizhe Diao",
            "Tong Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11769",
        "abstract": "Solving math problems through verifiable languages such as Lean has significantly impacted both the mathematics and computer science communities. Current state-of-the-art models are often trained with expensive online Reinforcement Learning (RL) or expert iteration. However, these approaches rely on fixed problem sets, which causes inefficient training and limits the model to tackle complex problems. To overcome these limitations, we propose GAR: Generative Adversarial Reinforcement learning, a comprehensive RL training framework that jointly trains the problem composer and solver in an adversarial loop. GAR introduces an implicit curriculum learning mechanism, which aligns task difficulty with the prover's evolving capability. It thereby improves the training efficiency and enables stronger performance of proving advanced theorems. Experiments show that with GAR training, Goedel-Prover-V2-8B and DeepSeek-Prover-V2-7B achieve an average relative improvement in pass@32 of 4.20% on MiniF2F-Test benchmark, while DeepSeek-Prover-V2's pass@32 on ProofNet-Test increases from 22.58% to 25.81%. Beyond formal proving, GAR establishes a general RL paradigm for co-evolution of problem generation and solving under verifiable environments.",
        "tags": [
            "DeepSeek",
            "RL"
        ]
    },
    {
        "id": "9",
        "title": "PHANTOM RECALL: When Familiar Puzzles Fool Smart Models",
        "author": [
            "Souradeep Mukhopadhyay",
            "Rishabh Baral",
            "Nimeesh Mahajan",
            "Samhitha Harish",
            "Aswin RRV",
            "Mihir Parmar",
            "Mutsumi Nakamura",
            "Chitta Baral"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11812",
        "abstract": "Large language models (LLMs) such as GPT, Gemini, and Claude often appear adept at solving classic logic puzzles--but how much genuine reasoning underlies their answers? Recent evidence suggests that these models frequently rely on memorized templates rather than reasoning from first principles. When puzzles are slightly modified, their performance collapses, revealing a striking fragility. In particular, we asked: Have LLMs addressed these issues? To what extent? How about perturbations to other puzzles? Is there a general way of reformulating the prompt so that the models do better? To examine these things systematically, we introduce PHANTOM RECALL, a benchmark comprising 25 well-known logic puzzles and 149 carefully designed perturbations that preserve reasoning structure but alter superficial details and solutions. We evaluate eleven leading LLMs and identify a recurring failure mode--phantom recall--where models confidently reproduce memorized solutions or spurious rationales that no longer fit the altered scenario. To probe and mitigate this issue, we contribute three tools: (i) an automated logical-equivalence judge to detect reasoning mismatches, (ii) a taxonomy of fine-grained reasoning error categories, and (iii) a prompting-based mitigation framework guided by these categories. Despite near-perfect accuracy on unmodified puzzles, models significantly underperform humans on perturbed ones, exhibiting both phantom recall and over-elaboration. Our findings reveal a crucial limitation: LLMs often fail to re-reason when contextual cues shift--highlighting the gap between linguistic fluency and logical understanding.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "10",
        "title": "Task-Aware Reduction for Scalable LLM-Database Systems",
        "author": [
            "Marcus Emmanuel Barnes",
            "Taher A. Ghaleb",
            "Safwat Hassan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11813",
        "abstract": "Large Language Models (LLMs) are increasingly applied to data-intensive workflows, from database querying to developer observability. Yet the effectiveness of these systems is constrained by the volume, verbosity, and noise of real-world text-rich data such as logs, telemetry, and monitoring streams. Feeding such data directly into LLMs is costly, environmentally unsustainable, and often misaligned with task objectives. Parallel efforts in LLM efficiency have focused on model- or architecture-level optimizations, but the challenge of reducing upstream input verbosity remains underexplored. In this paper, we argue for treating the token budget of an LLM as an attention budget and elevating task-aware text reduction as a first-class design principle for language -- data systems. We position input-side reduction not as compression, but as attention allocation: prioritizing information most relevant to downstream tasks. We outline open research challenges for building benchmarks, designing adaptive reduction pipelines, and integrating token-budget--aware preprocessing into database and retrieval systems. Our vision is to channel scarce attention resources toward meaningful signals in noisy, data-intensive workflows, enabling scalable, accurate, and sustainable LLM--data integration.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "11",
        "title": "Beyond Consensus: Mitigating the Agreeableness Bias in LLM Judge Evaluations",
        "author": [
            "Suryaansh Jain",
            "Umair Z. Ahmed",
            "Shubham Sahai",
            "Ben Leong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11822",
        "abstract": "New Large Language Models (LLMs) become available every few weeks, and modern application developers confronted with the unenviable task of having to decide if they should switch to a new model. While human evaluation remains the gold standard, it is costly and unscalable. The state-of-the-art approach is to use LLMs as evaluators ( LLM-as-a-judge), but this suffers from a critical flaw: LLMs exhibit a strong positive bias. We provide empirical evidence showing that while LLMs can identify valid outputs with high accuracy (i.e., True Positive Rate 96%), they are remarkably poor at identifying invalid ones (i.e., True Negative Rate <25%). This systematic bias, coupled with class imbalance, often leads to inflated reliability scores.\nWhile ensemble-based methods like majority voting can help, we show that they are not good enough. We introduce an optimal minority-veto strategy that is resilient to missing data and mitigates this bias to a large extent. For scenarios requiring even higher precision, we propose a novel regression-based framework that directly models the validator bias using a small set of human-annotated ground truth data. On a challenging code feedback task over 366 high-school Python programs, our regression approach reduces the maximum absolute error to just 1.2%, achieving a 2x improvement over the best-performing ensemble of 14 state-of-the-art LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "12",
        "title": "BlackIce: A Containerized Red Teaming Toolkit for AI Security Testing",
        "author": [
            "Caelin Kaplan",
            "Alexander Warnecke",
            "Neil Archibald"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11823",
        "abstract": "AI models are being increasingly integrated into real-world systems, raising significant concerns about their safety and security. Consequently, AI red teaming has become essential for organizations to proactively identify and address vulnerabilities before they can be exploited by adversaries. While numerous AI red teaming tools currently exist, practitioners face challenges in selecting the most appropriate tools from a rapidly expanding landscape, as well as managing complex and frequently conflicting software dependencies across isolated projects. Given these challenges and the relatively small number of organizations with dedicated AI red teams, there is a strong need to lower barriers to entry and establish a standardized environment that simplifies the setup and execution of comprehensive AI model assessments.\nInspired by Kali Linux's role in traditional penetration testing, we introduce BlackIce, an open-source containerized toolkit designed for red teaming Large Language Models (LLMs) and classical machine learning (ML) models. BlackIce provides a reproducible, version-pinned Docker image that bundles 14 carefully selected open-source tools for Responsible AI and Security testing, all accessible via a unified command-line interface. With this setup, initiating red team assessments is as straightforward as launching a container, either locally or using a cloud platform. Additionally, the image's modular architecture facilitates community-driven extensions, allowing users to easily adapt or expand the toolkit as new threats emerge. In this paper, we describe the architecture of the container image, the process used for selecting tools, and the types of evaluations they support.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "13",
        "title": "Empirical Study on Robustness and Resilience in Cooperative Multi-Agent Reinforcement Learning",
        "author": [
            "Simin Li",
            "Zihao Mao",
            "Hanxiao Li",
            "Zonglei Jing",
            "Zhuohang bian",
            "Jun Guo",
            "Li Wang",
            "Zhuoran Han",
            "Ruixiao Xu",
            "Xin Yu",
            "Chengdong Ma",
            "Yuqing Ma",
            "Bo An",
            "Yaodong Yang",
            "Weifeng Lv",
            "Xianglong Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11824",
        "abstract": "In cooperative Multi-Agent Reinforcement Learning (MARL), it is a common practice to tune hyperparameters in ideal simulated environments to maximize cooperative performance. However, policies tuned for cooperation often fail to maintain robustness and resilience under real-world uncertainties. Building trustworthy MARL systems requires a deep understanding of robustness, which ensures stability under uncertainties, and resilience, the ability to recover from disruptions--a concept extensively studied in control systems but largely overlooked in MARL. In this paper, we present a large-scale empirical study comprising over 82,620 experiments to evaluate cooperation, robustness, and resilience in MARL across 4 real-world environments, 13 uncertainty types, and 15 hyperparameters. Our key findings are: (1) Under mild uncertainty, optimizing cooperation improves robustness and resilience, but this link weakens as perturbations intensify. Robustness and resilience also varies by algorithm and uncertainty type. (2) Robustness and resilience do not generalize across uncertainty modalities or agent scopes: policies robust to action noise for all agents may fail under observation noise on a single agent. (3) Hyperparameter tuning is critical for trustworthy MARL: surprisingly, standard practices like parameter sharing, GAE, and PopArt can hurt robustness, while early stopping, high critic learning rates, and Leaky ReLU consistently help. By optimizing hyperparameters only, we observe substantial improvement in cooperation, robustness and resilience across all MARL backbones, with the phenomenon also generalizing to robust MARL methods across these backbones. Code and results available at https://github.com/BUAA-TrustworthyMARL/adv_marl_benchmark .",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "14",
        "title": "Z0-Inf: Zeroth Order Approximation for Data Influence",
        "author": [
            "Narine Kokhlikyan",
            "Kamalika Chaudhuri",
            "Saeed Mahloujifar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11832",
        "abstract": "A critical aspect of analyzing and improving modern machine learning systems lies in understanding how individual training examples influence a model's predictive behavior. Estimating this influence enables critical applications, including data selection and model debugging; in particular, self-influence, which quantifies the influence of a training point on itself, has found many uses in data quality assessment and outlier detection. Existing methods for measuring data influence, however, are often impractical for large models due to low accuracy or prohibitive computational costs: most approaches either provide poor approximations or rely on gradients and inverse-Hessian computations that remain challenging to scale. In this work, we introduce a highly efficient zeroth-order approximation for estimating the influence of training data that requires only a fraction of the time and memory footprint of prior methods. Notably, our method relies solely on loss values of intermediate checkpoints on the training and test data, along with the checkpoints themselves, making it broadly applicable even when the loss function of interest is non-differentiable. Beyond its computational efficiency, our approach achieves superior accuracy in estimating self-influence and comparable or improved accuracy in estimating train-test influence for fine-tuned large language models, enabling scalable and practical analysis of how training data shapes model behavior.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "15",
        "title": "Don't Walk the Line: Boundary Guidance for Filtered Generation",
        "author": [
            "Sarah Ball",
            "Andreas Haupt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11834",
        "abstract": "Generative models are increasingly paired with safety classifiers that filter harmful or undesirable outputs. A common strategy is to fine-tune the generator to reduce the probability of being filtered, but this can be suboptimal: it often pushes the model toward producing samples near the classifier's decision boundary, increasing both false positives and false negatives. We propose Boundary Guidance, a reinforcement learning fine-tuning method that explicitly steers generation away from the classifier's margin. On a benchmark of jailbreak and ambiguous prompts, Boundary Guidance improves both the safety and the utility of outputs, as judged by LLM-as-a-Judge evaluations. Comprehensive ablations across model scales and reward designs demonstrate the robustness of our approach.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "16",
        "title": "Data or Language Supervision: What Makes CLIP Better than DINO?",
        "author": [
            "Yiming Liu",
            "Yuhui Zhang",
            "Dhruba Ghosh",
            "Ludwig Schmidt",
            "Serena Yeung-Levy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11835",
        "abstract": "CLIP outperforms self-supervised models like DINO as vision encoders for vision-language models (VLMs), but it remains unclear whether this advantage stems from CLIP's language supervision or its much larger training data. To disentangle these factors, we pre-train CLIP and DINO under controlled settings -- using the same architecture, dataset, and training configuration -- achieving similar ImageNet accuracy. Embedding analysis shows that CLIP captures high-level semantics (e.g., object categories, text), while DINO is more responsive to low-level features like colors and styles. When integrated into VLMs and evaluated on 20 VQA benchmarks, CLIP excels at text-intensive tasks, while DINO slightly outperforms on vision-centric ones. Variants of language supervision (e.g., sigmoid loss, pre-trained language encoders) yield limited gains. Our findings provide scientific insights into vision encoder design and its impact on VLM performance.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "17",
        "title": "Countermind: A Multi-Layered Security Architecture for Large Language Models",
        "author": [
            "Dominik Schwarz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11837",
        "abstract": "The security of Large Language Model (LLM) applications is fundamentally challenged by \"form-first\" attacks like prompt injection and jailbreaking, where malicious instructions are embedded within user inputs. Conventional defenses, which rely on post hoc output filtering, are often brittle and fail to address the root cause: the model's inability to distinguish trusted instructions from untrusted data. This paper proposes Countermind, a multi-layered security architecture intended to shift defenses from a reactive, post hoc posture to a proactive, pre-inference, and intra-inference enforcement model. The architecture proposes a fortified perimeter designed to structurally validate and transform all inputs, and an internal governance mechanism intended to constrain the model's semantic processing pathways before an output is generated. The primary contributions of this work are conceptual designs for: (1) A Semantic Boundary Logic (SBL) with a mandatory, time-coupled Text Crypter intended to reduce the plaintext prompt injection attack surface, provided all ingestion paths are enforced. (2) A Parameter-Space Restriction (PSR) mechanism, leveraging principles from representation engineering, to dynamically control the LLM's access to internal semantic clusters, with the goal of mitigating semantic drift and dangerous emergent behaviors. (3) A Secure, Self-Regulating Core that uses an OODA loop and a learning security module to adapt its defenses based on an immutable audit log. (4) A Multimodal Input Sandbox and Context-Defense mechanisms to address threats from non-textual data and long-term semantic poisoning. This paper outlines an evaluation plan designed to quantify the proposed architecture's effectiveness in reducing the Attack Success Rate (ASR) for form-first attacks and to measure its potential latency overhead.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "18",
        "title": "Lingxi: Repository-Level Issue Resolution Framework Enhanced by Procedural Knowledge Guided Scaling",
        "author": [
            "Xu Yang",
            "Jiayuan Zhou",
            "Michael Pacheco",
            "Wenhan Zhu",
            "Pengfei He",
            "Shaowei Wang",
            "Kui Liu",
            "Ruiqi Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11838",
        "abstract": "Driven by the advancements of Large Language Models (LLMs), LLM-powered agents are making significant improvements in software engineering tasks, yet struggle with complex, repository-level issue resolution. Existing agent-based methods have two key limitations. First, they lack of procedural knowledge (i.e., how an issue is fixed step-by-step and rationales behind it) to learn and leverage for issue resolution. Second, they rely on massive computational power to blindly explore the solution space. %\nTo address those limitations, we propose Lingxi, an issue resolution framework that leverages procedural knowledge extracted from historical issue-fixing data to guide agents in solving repository-level issues. \\ourTool first constructs this knowledge offline through a hierarchical abstraction mechanism, enabling agents to learn the how and why behind a fix, not just the final solution. During online application, it employs a knowledge-driven scaling method that leverages the procedural knowledge of similar issues to intelligently analyze the target issue from multiple perspectives, in sharp contrast to undirected, brute-force exploration. %\nLingxi successfully resolves 74.6\\% of bugs on the SWE-bench Verified benchmark in Past@1 setting, outperforming five state-of-the-art techniques by a significant margin (5.4\\% to 14.9\\%). Our comprehensive ablation study confirmed that the success of Lingxi comes directly from its use of procedural knowledge. Without it, the performance gains from scaling alone is negligible. Our qualitative study further shows that the ``design patterns $\\&$ coding practices'' is the most critical knowledge aspect, and that the roles of different knowledge aspects switch across different stages (i.e., analysis, planning, and fixing).",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "19",
        "title": "WaveletDiff: Multilevel Wavelet Diffusion For Time Series Generation",
        "author": [
            "Yu-Hsiang Wang",
            "Olgica Milenkovic"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11839",
        "abstract": "Time series are ubiquitous in many applications that involve forecasting, classification and causal inference tasks, such as healthcare, finance, audio signal processing and climate sciences. Still, large, high-quality time series datasets remain scarce. Synthetic generation can address this limitation; however, current models confined either to the time or frequency domains struggle to reproduce the inherently multi-scaled structure of real-world time series. We introduce WaveletDiff, a novel framework that trains diffusion models directly on wavelet coefficients to exploit the inherent multi-resolution structure of time series data. The model combines dedicated transformers for each decomposition level with cross-level attention mechanisms that enable selective information exchange between temporal and frequency scales through adaptive gating. It also incorporates energy preservation constraints for individual levels based on Parseval's theorem to preserve spectral fidelity throughout the diffusion process. Comprehensive tests across six real-world datasets from energy, finance, and neuroscience domains demonstrate that WaveletDiff consistently outperforms state-of-the-art time-domain and frequency-domain generative methods on both short and long time series across five diverse performance metrics. For example, WaveletDiff achieves discriminative scores and Context-FID scores that are $3\\times$ smaller on average than the second-best baseline across all datasets.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "20",
        "title": "Deep Research Brings Deeper Harm",
        "author": [
            "Shuo Chen",
            "Zonggen Li",
            "Zhen Han",
            "Bailan He",
            "Tong Liu",
            "Haokun Chen",
            "Georg Groh",
            "Philip Torr",
            "Volker Tresp",
            "Jindong Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11851",
        "abstract": "Deep Research (DR) agents built on Large Language Models (LLMs) can perform complex, multi-step research by decomposing tasks, retrieving online information, and synthesizing detailed reports. However, the misuse of LLMs with such powerful capabilities can lead to even greater risks. This is especially concerning in high-stakes and knowledge-intensive domains such as biosecurity, where DR can generate a professional report containing detailed forbidden knowledge. Unfortunately, we have found such risks in practice: simply submitting a harmful query, which a standalone LLM directly rejects, can elicit a detailed and dangerous report from DR agents. This highlights the elevated risks and underscores the need for a deeper safety analysis. Yet, jailbreak methods designed for LLMs fall short in exposing such unique risks, as they do not target the research ability of DR agents. To address this gap, we propose two novel jailbreak strategies: Plan Injection, which injects malicious sub-goals into the agent's plan; and Intent Hijack, which reframes harmful queries as academic research questions. We conducted extensive experiments across different LLMs and various safety benchmarks, including general and biosecurity forbidden prompts. These experiments reveal 3 key findings: (1) Alignment of the LLMs often fail in DR agents, where harmful prompts framed in academic terms can hijack agent intent; (2) Multi-step planning and execution weaken the alignment, revealing systemic vulnerabilities that prompt-level safeguards cannot address; (3) DR agents not only bypass refusals but also produce more coherent, professional, and dangerous content, compared with standalone LLMs. These results demonstrate a fundamental misalignment in DR agents and call for better alignment techniques tailored to DR agents. Code and datasets are available at https://chenxshuo.github.io/deeper-harm.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "21",
        "title": "Evaluating Open-Source Vision-Language Models for Multimodal Sarcasm Detection",
        "author": [
            "Saroj Basnet",
            "Shafkat Farabi",
            "Tharindu Ranasinghe",
            "Diptesh Kanoji",
            "Marcos Zampieri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11852",
        "abstract": "Recent advances in open-source vision-language models (VLMs) offer new opportunities for understanding complex and subjective multimodal phenomena such as sarcasm. In this work, we evaluate seven state-of-the-art VLMs - BLIP2, InstructBLIP, OpenFlamingo, LLaVA, PaliGemma, Gemma3, and Qwen-VL - on their ability to detect multimodal sarcasm using zero-, one-, and few-shot prompting. Furthermore, we evaluate the models' capabilities in generating explanations to sarcastic instances. We evaluate the capabilities of VLMs on three benchmark sarcasm datasets (Muse, MMSD2.0, and SarcNet). Our primary objectives are twofold: (1) to quantify each model's performance in detecting sarcastic image-caption pairs, and (2) to assess their ability to generate human-quality explanations that highlight the visual-textual incongruities driving sarcasm. Our results indicate that, while current models achieve moderate success in binary sarcasm detection, they are still not able to generate high-quality explanations without task-specific finetuning.",
        "tags": [
            "Detection",
            "LLaVA",
            "Qwen",
            "VLM"
        ]
    },
    {
        "id": "22",
        "title": "Robust Adversarial Reinforcement Learning in Stochastic Games via Sequence Modeling",
        "author": [
            "Xiaohang Tang",
            "Zhuowen Cheng",
            "Satyabrat Kumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11877",
        "abstract": "The Transformer, a highly expressive architecture for sequence modeling, has recently been adapted to solve sequential decision-making, most notably through the Decision Transformer (DT), which learns policies by conditioning on desired returns. Yet, the adversarial robustness of reinforcement learning methods based on sequence modeling remains largely unexplored. Here we introduce the Conservative Adversarially Robust Decision Transformer (CART), to our knowledge the first framework designed to enhance the robustness of DT in adversarial stochastic games. We formulate the interaction between the protagonist and the adversary at each stage as a stage game, where the payoff is defined as the expected maximum value over subsequent states, thereby explicitly incorporating stochastic state transitions. By conditioning Transformer policies on the NashQ value derived from these stage games, CART generates policy that are simultaneously less exploitable (adversarially robust) and conservative to transition uncertainty. Empirically, CART achieves more accurate minimax value estimation and consistently attains superior worst-case returns across a range of adversarial stochastic games.",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "23",
        "title": "GS-Verse: Mesh-based Gaussian Splatting for Physics-aware Interaction in Virtual Reality",
        "author": [
            "Anastasiya Pechko",
            "Piotr Borycki",
            "Joanna WaczyÅska",
            "Daniel Barczyk",
            "Agata SzymaÅska",
            "SÅawomir Tadeja",
            "PrzemysÅaw Spurek"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11878",
        "abstract": "As the demand for immersive 3D content grows, the need for intuitive and efficient interaction methods becomes paramount. Current techniques for physically manipulating 3D content within Virtual Reality (VR) often face significant limitations, including reliance on engineering-intensive processes and simplified geometric representations, such as tetrahedral cages, which can compromise visual fidelity and physical accuracy. In this paper, we introduce \\our{} (\\textbf{G}aussian \\textbf{S}platting for \\textbf{V}irtual \\textbf{E}nvironment \\textbf{R}endering and \\textbf{S}cene \\textbf{E}diting), a novel method designed to overcome these challenges by directly integrating an object's mesh with a Gaussian Splatting (GS) representation. Our approach enables more precise surface approximation, leading to highly realistic deformations and interactions. By leveraging existing 3D mesh assets, \\our{} facilitates seamless content reuse and simplifies the development workflow. Moreover, our system is designed to be physics-engine-agnostic, granting developers robust deployment flexibility. This versatile architecture delivers a highly realistic, adaptable, and intuitive approach to interactive 3D manipulation. We rigorously validate our method against the current state-of-the-art technique that couples VR with GS in a comparative user study involving 18 participants. Specifically, we demonstrate that our approach is statistically significantly better for physics-aware stretching manipulation and is also more consistent in other physics-based manipulations like twisting and shaking. Further evaluation across various interactions and scenes confirms that our method consistently delivers high and reliable performance, showing its potential as a plausible alternative to existing methods.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "24",
        "title": "R-WoM: Retrieval-augmented World Model For Computer-use Agents",
        "author": [
            "Kai Mei",
            "Jiang Guo",
            "Shuaichen Chang",
            "Mingwen Dong",
            "Dongkyu Lee",
            "Xing Niu",
            "Jiarong Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11892",
        "abstract": "Large Language Models (LLMs) can serve as world models to enhance agent decision-making in digital environments by simulating future states and predicting action outcomes, potentially eliminating costly trial-and-error exploration. However, this capability is fundamentally limited by LLMs' tendency toward hallucination and their reliance on static training knowledge, which can lead to compounding errors that inhibit long-horizon simulations. To systematically investigate whether LLMs are appropriate for world modeling, we probe two core capabilities of world models--future state prediction and reward estimation--through three tasks: next-state identification, full-procedure planning alignment, and milestone transition recognition. Our analysis shows that while LLMs effectively capture immediate next states and identify meaningful state transitions, their performance rapidly degrades in full-procedure planning. This highlights LLMs' limitations in reliably modeling environment dynamics over long horizons. To address these limitations, we propose the Retrieval-augmented World Model (R-WoM), which grounds LLM simulations by incorporating factual, up-to-date knowledge retrieved from external tutorials. Experiments show that R-WoM achieves substantial improvements of up to 25.3% (OSWorld) and 18.1% (WebArena) compared to baselines, with particular advantages in longer-horizon simulations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "25",
        "title": "A Longitudinal Study on Different Annotator Feedback Loops in Complex RAG Tasks",
        "author": [
            "Sara Rosenthal",
            "Maeda Hanafi",
            "Yannis Katsis",
            "Lucian Popa",
            "Marina Danilevsky"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11897",
        "abstract": "Grounding conversations in existing passages, known as Retrieval-Augmented Generation (RAG), is an important aspect of Chat-Based Assistants powered by Large Language Models (LLMs) to ensure they are faithful and don't provide misinformation. Several benchmarks have been created to measure the performance of LLMs on this task. We present a longitudinal study comparing the feedback loop of an internal and external human annotator group for the complex annotation task of creating multi-turn RAG conversations for evaluating LLMs. We analyze the conversations produced by both groups and provide results of a survey comparing their experiences. Our study highlights the advantages of each annotator population and the impact of the different feedback loops; a closer loop creates higher quality conversations with a decrease in quantity and diversity. Further, we present guidance for how to best utilize two different population groups when performing annotation tasks, particularly when the task is complex.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "26",
        "title": "ADARL: Adaptive Low-Rank Structures for Robust Policy Learning under Uncertainty",
        "author": [
            "Chenliang Li",
            "Junyu Leng",
            "Jiaxiang Li",
            "Youbang Sun",
            "Shixiang Chen",
            "Shahin Shahrampour",
            "Alfredo Garcia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11899",
        "abstract": "Robust reinforcement learning (Robust RL) seeks to handle epistemic uncertainty in environment dynamics, but existing approaches often rely on nested min--max optimization, which is computationally expensive and yields overly conservative policies. We propose \\textbf{Adaptive Rank Representation (AdaRL)}, a bi-level optimization framework that improves robustness by aligning policy complexity with the intrinsic dimension of the task. At the lower level, AdaRL performs policy optimization under fixed-rank constraints with dynamics sampled from a Wasserstein ball around a centroid model. At the upper level, it adaptively adjusts the rank to balance the bias--variance trade-off, projecting policy parameters onto a low-rank manifold. This design avoids solving adversarial worst-case dynamics while ensuring robustness without over-parameterization. Empirical results on MuJoCo continuous control benchmarks demonstrate that AdaRL not only consistently outperforms fixed-rank baselines (e.g., SAC) and state-of-the-art robust RL methods (e.g., RNAC, Parseval), but also converges toward the intrinsic rank of the underlying tasks. These results highlight that adaptive low-rank policy representations provide an efficient and principled alternative for robust RL under model uncertainty.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "27",
        "title": "LLM Knowledge is Brittle: Truthfulness Representations Rely on Superficial Resemblance",
        "author": [
            "Patrick Haller",
            "Mark Ibrahim",
            "Polina Kirichenko",
            "Levent Sagun",
            "Samuel J. Bell"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11905",
        "abstract": "For Large Language Models (LLMs) to be reliable, they must learn robust knowledge that can be generally applied in diverse settings -- often unlike those seen during training. Yet, extensive research has shown that LLM performance can be brittle, with models exhibiting excessive sensitivity to trivial input variations. In this work, we explore whether this brittleness is a direct result of unstable internal knowledge representations. To explore this question, we build on previous work showing that LLM representations encode statement truthfulness -- i.e., true, factual statements can be easily separated from false, inaccurate ones. Specifically, we test the robustness of learned knowledge by evaluating representation separability on samples that have undergone superficial transformations to drive them out-of-distribution (OOD), such as typos or reformulations. By applying semantically-preserving perturbations, we study how separability degrades as statements become more OOD, across four LLM families, five evaluation datasets, and three knowledge probing methods. Our results reveal that internal representations of statement truthfulness collapse as the samples' presentations become less similar to those seen during pre-training. While LLMs can often distinguish between true and false statements when they closely resemble the pre-training data, this ability is highly dependent on the statement's exact surface form. These findings offer a possible explanation for brittle benchmark performance: LLMs may learn shallow, non-robust knowledge representations that allow for only limited generalizability. Our work presents a fundamental challenge for the utility of truthfulness probes, and more broadly, calls for further research on improving the robustness of learned knowledge representations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "28",
        "title": "Robust ML-based Detection of Conventional, LLM-Generated, and Adversarial Phishing Emails Using Advanced Text Preprocessing",
        "author": [
            "Deeksha Hareesha Kulal",
            "Chidozie Princewill Arannonu",
            "Afsah Anwar",
            "Nidhi Rastogi",
            "Quamar Niyaz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11915",
        "abstract": "Phishing remains a critical cybersecurity threat, especially with the advent of large language models (LLMs) capable of generating highly convincing malicious content. Unlike earlier phishing attempts which are identifiable by grammatical errors, misspellings, incorrect phrasing, and inconsistent formatting, LLM generated emails are grammatically sound, contextually relevant, and linguistically natural. These advancements make phishing emails increasingly difficult to distinguish from legitimate ones, challenging traditional detection mechanisms. Conventional phishing detection systems often fail when faced with emails crafted by LLMs or manipulated using adversarial perturbation techniques. To address this challenge, we propose a robust phishing email detection system featuring an enhanced text preprocessing pipeline. This pipeline includes spelling correction and word splitting to counteract adversarial modifications and improve detection accuracy. Our approach integrates widely adopted natural language processing (NLP) feature extraction techniques and machine learning algorithms. We evaluate our models on publicly available datasets comprising both phishing and legitimate emails, achieving a detection accuracy of 94.26% and F1-score of 84.39% in model deployment setting. To assess robustness, we further evaluate our models using adversarial phishing samples generated by four attack methods in Python TextAttack framework. Additionally, we evaluate models' performance against phishing emails generated by LLMs including ChatGPT and Llama. Results highlight the resilience of models against evolving AI-powered phishing threats.",
        "tags": [
            "Detection",
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "29",
        "title": "LLM Reasoning for Machine Translation: Synthetic Data Generation over Thinking Tokens",
        "author": [
            "Armel Zebaze",
            "Rachel Bawden",
            "BenoÃ®t Sagot"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11919",
        "abstract": "Large reasoning models (LRMs) have led to new possibilities in terms of problem-solving, through the devising of a natural language thought process prior to answering a query. While their capabilities are well known across mathematics and coding tasks, their impact on the task of machine translation (MT) remains underexplored. In this work, we explore the benefits of the generation of intermediate tokens when performing MT across multiple language pairs of different levels of resourcedness and multiple setups. We find that \"thinking tokens\" do not help LRMs better perform MT. This result generalizes to models fine-tuned to reason before translating using distilled chain of thought (CoT) inspired by human translators' practices. Specifically, fine-tuning a model with synthetic CoT explanations detailing how to translate step-by-step does not outperform standard input-output fine-tuning. However, constructing the intermediate tokens by combining the outputs of modular translation-specific prompting strategies results in improvements. Our findings underscore that the contribution of intermediate tokens during fine-tuning highly depends on the presence of translation attempts within them. More broadly, our results suggest that using a teacher to refine target translations or to expand parallel corpora is more impactful than distilling their CoT explanations into \"thinking\" MT models.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "30",
        "title": "Indoor Localization using Compact, Telemetry-Agnostic, Transfer-Learning Enabled Decoder-Only Transformer",
        "author": [
            "Nayan Sanjay Bhatia",
            "Pranay Kocheta",
            "Russell Elliott",
            "Harikrishna S. Kuttivelil",
            "Katia Obraczka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11926",
        "abstract": "Indoor Wi-Fi positioning remains a challenging problem due to the high sensitivity of radio signals to environmental dynamics, channel propagation characteristics, and hardware heterogeneity. Conventional fingerprinting and model-based approaches typically require labor-intensive calibration and suffer rapid performance degradation when devices, channel or deployment conditions change. In this paper, we introduce Locaris, a decoder-only large language model (LLM) for indoor localization. Locaris treats each access point (AP) measurement as a token, enabling the ingestion of raw Wi-Fi telemetry without pre-processing. By fine-tuning its LLM on different Wi-Fi datasets, Locaris learns a lightweight and generalizable mapping from raw signals directly to device location. Our experimental study comparing Locaris with state-of-the-art methods consistently shows that Locaris matches or surpasses existing techniques for various types of telemetry. Our results demonstrate that compact LLMs can serve as calibration-free regression models for indoor localization, offering scalable and robust cross-environment performance in heterogeneous Wi-Fi deployments. Few-shot adaptation experiments, using only a handful of calibration points per device, further show that Locaris maintains high accuracy when applied to previously unseen devices and deployment scenarios. This yields sub-meter accuracy with just a few hundred samples, robust performance under missing APs and supports any and all available telemetry. Our findings highlight the practical viability of Locaris for indoor positioning in the real-world scenarios, particularly in large-scale deployments where extensive calibration is infeasible.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "31",
        "title": "Efficient Restarts in Non-Stationary Model-Free Reinforcement Learning",
        "author": [
            "Hiroshi Nonaka",
            "Simon Ambrozak",
            "Sofia R. Miskala-Dinc",
            "Amedeo Ercole",
            "Aviva Prins"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11933",
        "abstract": "In this work, we propose three efficient restart paradigms for model-free non-stationary reinforcement learning (RL). We identify two core issues with the restart design of Mao et al. (2022)'s RestartQ-UCB algorithm: (1) complete forgetting, where all the information learned about an environment is lost after a restart, and (2) scheduled restarts, in which restarts occur only at predefined timings, regardless of the incompatibility of the policy with the current environment dynamics. We introduce three approaches, which we call partial, adaptive, and selective restarts to modify the algorithms RestartQ-UCB and RANDOMIZEDQ (Wang et al., 2025). We find near-optimal empirical performance in multiple different environments, decreasing dynamic regret by up to $91$% relative to RestartQ-UCB.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "32",
        "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline Refactoring in Fragmented Serverless Clusters",
        "author": [
            "Yanying Lin",
            "Shijie Peng",
            "Chengzhi Lu",
            "Chengzhong Xu",
            "Kejiang Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11938",
        "abstract": "Serving Large Language Models (LLMs) in production faces significant challenges from highly variable request patterns and severe resource fragmentation in serverless clusters. Current systems rely on static pipeline configurations that struggle to adapt to dynamic workload conditions, leading to substantial inefficiencies. We present FlexPipe, a novel system that dynamically reconfigures pipeline architectures during runtime to address these fundamental limitations. FlexPipe decomposes models into fine-grained stages and intelligently adjusts pipeline granularity based on real-time request pattern analysis, implementing three key innovations: fine-grained model partitioning with preserved computational graph constraints, inflight pipeline refactoring with consistent cache transitions, and topology-aware resource allocation that navigates GPU fragmentation. Comprehensive evaluation on an 82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource efficiency while maintaining 38.3% lower latency compared to state-of-the-art systems, reducing GPU reservation requirements from 75% to 30% of peak capacity.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "33",
        "title": "TopoAlign: A Framework for Aligning Code to Math via Topological Decomposition",
        "author": [
            "Yupei Li",
            "Philipp Borchert",
            "Gerasimos Lampouras"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11944",
        "abstract": "Large Language Models (LLMs) excel at both informal and formal (e.g. Lean 4) mathematical reasoning but still struggle with autoformalisation, the task of transforming informal into formal mathematical statements. Autoformalisation helps pair the informal reasoning of LLMs with formal proof assistants which enable machine-verifiable generation and mitigate hallucinations. Yet, the performance of current Math LLMs is constrained by the scarcity of large-scale corpora, particularly those containing pairs of informal and formal statements. Although current models are trained to generate code from natural language instructions, structural and syntactic differences between these and formal mathematics limit effective transfer learning. We propose TopoAlign, a framework that unlocks widely available code repositories as training resources for Math LLMs. TopoAlign decomposes code into docstrings, main functions, and dependency functions, and reassembles these components into analogues that structurally mirror formal statements. This produces structurally aligned code data that can be used for training Math LLMs without requiring additional human annotation. We train two state-of-the-art models, DeepSeek-Math and Herald, and evaluate them on the minif2f, Putnam, and ProofNet benchmarks. TopoAlign provides substantial gains for DeepSeek-Math, improving performance by 17.77% on BEq@10 and 68.82% on typecheck@10. Despite introducing no new mathematical knowledge, our framework achieves gains of 0.12% and 1.09% for Herald on BEq@10 and typecheck@10, respectively, demonstrating that training on aligned code data is beneficial even for specialized models.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "34",
        "title": "GRAVITY: A Framework for Personalized Text Generation via Profile-Grounded Synthetic Preferences",
        "author": [
            "Priyanka Dey",
            "Daniele Rosa",
            "Wenqing Zheng",
            "Daniel Barcklow",
            "Jieyu Zhao",
            "Emilio Ferrara"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11952",
        "abstract": "Personalization in LLMs often relies on costly human feedback or interaction logs, limiting scalability and neglecting deeper user attributes. To reduce the reliance on human annotations, we introduce GRAVITY (Generative Response with Aligned Values, Interests, and Traits of You), a framework for generating synthetic, profile-grounded preference data that captures users' interests, values, beliefs, and personality traits. By integrating demographic, cultural, and psychological frameworks -- including Hofstede's cultural dimensions, Schwartz's basic values, the World Values Survey, and Big Five OCEAN traits -- GRAVITY synthesizes preference pairs to guide personalized content generation. We evaluate GRAVITY on book descriptions for 400 Amazon users, comparing it to prompt-based conditioning, standard fine-tuning, and naive synthetic pair generation. Profile-grounded synthetic data consistently improves generation, especially across multiple cultures (USA, Brazil, Japan, India), achieving over 4% higher preference gains across baselines, with user studies showing that GRAVITY outputs are preferred over 86% of the time. Our results show that scenario-grounded synthetic data can capture richer user variation, reduce reliance on costly annotation, and produce more engaging, user-centered content, offering a scalable path for LLM personalization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "35",
        "title": "Sculpting Latent Spaces With MMD: Disentanglement With Programmable Priors",
        "author": [
            "Quentin Fruytier",
            "Akshay Malhotra",
            "Shahab Hamidi-Rad",
            "Aditya Sant",
            "Aryan Mokhtari",
            "Sujay Sanghavi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11953",
        "abstract": "Learning disentangled representations, where distinct factors of variation are captured by independent latent variables, is a central goal in machine learning. The dominant approach has been the Variational Autoencoder (VAE) framework, which uses a Kullback-Leibler (KL) divergence penalty to encourage the latent space to match a factorized Gaussian prior. In this work, however, we provide direct evidence that this KL-based regularizer is an unreliable mechanism, consistently failing to enforce the target distribution on the aggregate posterior. We validate this and quantify the resulting entanglement using our novel, unsupervised Latent Predictability Score (LPS). To address this failure, we introduce the Programmable Prior Framework, a method built on the Maximum Mean Discrepancy (MMD). Our framework allows practitioners to explicitly sculpt the latent space, achieving state-of-the-art mutual independence on complex datasets like CIFAR-10 and Tiny ImageNet without the common reconstruction trade-off. Furthermore, we demonstrate how this programmability can be used to engineer sophisticated priors that improve alignment with semantically meaningful features. Ultimately, our work provides a foundational tool for representation engineering, opening new avenues for model identifiability and causal reasoning.",
        "tags": [
            "VAE"
        ]
    },
    {
        "id": "36",
        "title": "Evaluating Retrieval-Augmented Generation Systems on Unanswerable, Uncheatable, Realistic, Multi-hop Queries",
        "author": [
            "Gabrielle Kaili-May Liu",
            "Bryan Li",
            "Arman Cohan",
            "William Gantt Walden",
            "Eugene Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11956",
        "abstract": "Real-world use cases often present RAG systems with complex queries for which relevant information is missing from the corpus or is incomplete. In these settings, RAG systems must be able to reject unanswerable, out-of-scope queries and identify failures of retrieval and multi-hop reasoning. Despite this, existing RAG benchmarks rarely reflect realistic task complexity for multi-hop or out-of-scope questions, which often can be cheated via disconnected reasoning (i.e., solved without genuine multi-hop inference) or require only simple factual recall. This limits the ability for such benchmarks to uncover limitations of existing RAG systems. To address this gap, we present the first pipeline for automatic, difficulty-controlled creation of un$\\underline{c}$heatable, $\\underline{r}$ealistic, $\\underline{u}$nanswerable, and $\\underline{m}$ulti-hop $\\underline{q}$uerie$\\underline{s}$ (CRUMQs), adaptable to any corpus and domain. We use our pipeline to create CRUMQs over two popular RAG datasets and demonstrate its effectiveness via benchmark experiments on leading retrieval-augmented LLMs. Results show that compared to prior RAG benchmarks, CRUMQs are highly challenging for RAG systems and achieve up to 81.0\\% reduction in cheatability scores. More broadly, our pipeline offers a simple way to enhance benchmark difficulty and realism and drive development of more capable RAG systems.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "37",
        "title": "Direct Multi-Token Decoding",
        "author": [
            "Xuan Luo",
            "Weizhi Wang",
            "Xifeng Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11958",
        "abstract": "Decoder-only transformers have become the standard architecture for large language models (LLMs) due to their strong performance. Recent studies suggest that, in pre-trained LLMs, early, middle, and late layers may serve distinct roles: Early layers focus on understanding the input context, middle layers handle task-specific processing, and late layers convert abstract representations into output tokens. We hypothesize that once representations have been processed by the early and middle layers, the resulting hidden states may encapsulate sufficient information to support the generation of multiple tokens using only the late layers, eliminating the need to repeatedly traverse the early and middle layers. We refer to this inference paradigm as Direct Multi-Token Decoding (DMTD). Unlike speculative decoding, our method introduces no additional parameters, auxiliary routines, or post-generation verification. Despite being trained on a limited dataset, a fine-tuned DMTD Qwen3-4B model has already demonstrated promising results, achieving up to a 2x speedup with only minor performance loss. Moreover, as shown in our scaling analysis, its performance is expected to further improve with larger training datasets.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "38",
        "title": "MosaicDiff: Training-free Structural Pruning for Diffusion Model Acceleration Reflecting Pretraining Dynamics",
        "author": [
            "Bowei Guo",
            "Shengkun Tang",
            "Cong Zeng",
            "Zhiqiang Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11962",
        "abstract": "Diffusion models are renowned for their generative capabilities, yet their pretraining processes exhibit distinct phases of learning speed that have been entirely overlooked in prior post-training acceleration efforts in the community. In this study, we introduce a novel framework called MosaicDiff that aligns diffusion pretraining dynamics with post-training sampling acceleration via trajectory-aware structural pruning. Our approach leverages the observation that the middle, fast-learning stage of diffusion pretraining requires more conservative pruning to preserve critical model features, while the early and later, slow-learning stages benefit from a more aggressive pruning strategy. This adaptive pruning mechanism is the first to explicitly mirror the inherent learning speed variations of diffusion pretraining, thereby harmonizing the model's inner training dynamics with its accelerated sampling process. Extensive experiments on DiT and SDXL demonstrate that our method achieves significant speed-ups in sampling without compromising output quality, outperforming previous state-of-the-art methods by large margins, also providing a new viewpoint for more efficient and robust training-free diffusion acceleration.",
        "tags": [
            "DiT",
            "Diffusion",
            "SDXL"
        ]
    },
    {
        "id": "39",
        "title": "QLENS: Towards A Quantum Perspective of Language Transformers",
        "author": [
            "Aditya Gupta",
            "Kirandeep Kaur",
            "Vinayak Gupta"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11963",
        "abstract": "In natural language processing, current methods for understanding Transformers are successful at identifying intermediate predictions during a model's inference. However, these approaches function as limited diagnostic checkpoints, lacking a mathematical framework for mechanistically modeling how each layer facilitates transitions between these evolving states. This interpretability gap and past successes of interdisciplinary outlooks inspire us to turn to physics in search of a descriptive mathematical framework for Transformers. We observe that language models are intrinsically probabilistic, an attribute that is echoed in the core postulates of quantum mechanics. This parallel inspires us to translate insights from this discipline to that of natural language processing. Towards this objective, we propose QLENS a novel attempt to develop a physics-based perspective on the Transformer generation process. Under QLENS, a Transformer is studied by converting its latent activations into a state vector in a Hilbert space derived from the model's output units. This state subsequently evolves through hidden layers - reformulated as unitary operators and analogously defined Hamiltonians - during inference. The model's final probability distribution is obtained by applying the Born rule to the end state using a specific measurement operator. To demonstrate QLENS's potential, we conduct a proof-of-concept by probing a toy Transformer to investigate the influence of individual layers in a model's prediction trajectory. We present our work as a foundation for cross-domain insights to be leveraged towards a broader understanding of Transformers.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "40",
        "title": "Scaling Long-Horizon LLM Agent via Context-Folding",
        "author": [
            "Weiwei Sun",
            "Miao Lu",
            "Zhan Ling",
            "Kang Liu",
            "Xuesong Yao",
            "Yiming Yang",
            "Jiecao Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11967",
        "abstract": "Large language model (LLM) agents are fundamentally constrained by context length on long-horizon tasks. We introduce Context-Folding, a framework that empowers agents to actively manage their working context. An agent can procedurally branch into a sub-trajectory to handle a subtask and then fold it upon completion, collapsing the intermediate steps while retaining a concise summary of the outcome. To make this behavior learnable, we develop an end-to-end reinforcement learning framework FoldGRPO with specific process rewards to encourage effective task decomposition and context management. On complex long-horizon tasks (Deep Research and SWE), our folding agent matches or outperforms the ReAct baselines while using an active context 10$\\times$ smaller and significantly outperforms models that rely on summarization-based context management.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "41",
        "title": "CTIArena: Benchmarking LLM Knowledge and Reasoning Across Heterogeneous Cyber Threat Intelligence",
        "author": [
            "Yutong Cheng",
            "Yang Liu",
            "Changze Li",
            "Dawn Song",
            "Peng Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11974",
        "abstract": "Cyber threat intelligence (CTI) is central to modern cybersecurity, providing critical insights for detecting and mitigating evolving threats. With the natural language understanding and reasoning capabilities of large language models (LLMs), there is increasing interest in applying them to CTI, which calls for benchmarks that can rigorously evaluate their performance. Several early efforts have studied LLMs on some CTI tasks but remain limited: (i) they adopt only closed-book settings, relying on parametric knowledge without leveraging CTI knowledge bases; (ii) they cover only a narrow set of tasks, lacking a systematic view of the CTI landscape; and (iii) they restrict evaluation to single-source analysis, unlike realistic scenarios that require reasoning across multiple sources. To fill these gaps, we present CTIArena, the first benchmark for evaluating LLM performance on heterogeneous, multi-source CTI under knowledge-augmented settings. CTIArena spans three categories, structured, unstructured, and hybrid, further divided into nine tasks that capture the breadth of CTI analysis in modern security operations. We evaluate ten widely used LLMs and find that most struggle in closed-book setups but show noticeable gains when augmented with security-specific knowledge through our designed retrieval-augmented techniques. These findings highlight the limitations of general-purpose LLMs and the need for domain-tailored techniques to fully unlock their potential for CTI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "42",
        "title": "Holistic Agent Leaderboard: The Missing Infrastructure for AI Agent Evaluation",
        "author": [
            "Sayash Kapoor",
            "Benedikt Stroebl",
            "Peter Kirgis",
            "Nitya Nadgir",
            "Zachary S Siegel",
            "Boyi Wei",
            "Tianci Xue",
            "Ziru Chen",
            "Felix Chen",
            "Saiteja Utpala",
            "Franck Ndzomga",
            "Dheeraj Oruganty",
            "Sophie Luskin",
            "Kangheng Liu",
            "Botao Yu",
            "Amit Arora",
            "Dongyoon Hahm",
            "Harsh Trivedi",
            "Huan Sun",
            "Juyong Lee",
            "Tengjun Jin",
            "Yifan Mai",
            "Yifei Zhou",
            "Yuxuan Zhu",
            "Rishi Bommasani",
            "Daniel Kang",
            "Dawn Song",
            "Peter Henderson",
            "Yu Su",
            "Percy Liang",
            "Arvind Narayanan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11977",
        "abstract": "AI agents have been developed for complex real-world tasks from coding to customer service. But AI agent evaluations suffer from many challenges that undermine our understanding of how well agents really work. We introduce the Holistic Agent Leaderboard (HAL) to address these challenges. We make three main contributions. First, we provide a standardized evaluation harness that orchestrates parallel evaluations across hundreds of VMs, reducing evaluation time from weeks to hours while eliminating common implementation bugs. Second, we conduct three-dimensional analysis spanning models, scaffolds, and benchmarks. We validate the harness by conducting 21,730 agent rollouts across 9 models and 9 benchmarks in coding, web navigation, science, and customer service with a total cost of about $40,000. Our analysis reveals surprising insights, such as higher reasoning effort reducing accuracy in the majority of runs. Third, we use LLM-aided log inspection to uncover previously unreported behaviors, such as searching for the benchmark on HuggingFace instead of solving a task, or misusing credit cards in flight booking tasks. We share all agent logs, comprising 2.5B tokens of language model calls, to incentivize further research into agent behavior. By standardizing how the field evaluates agents and addressing common pitfalls in agent evaluation, we hope to shift the focus from agents that ace benchmarks to agents that work reliably in the real world.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "43",
        "title": "Learning Dynamics of VLM Finetuning",
        "author": [
            "Jusheng Zhang",
            "Kaitong Cai",
            "Jing Yang",
            "Keze Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11978",
        "abstract": "Preference-based finetuning of vision--language models (VLMs) is brittle: trivially wrong negatives inject uninformative gradients that destabilize training. We recast alignment as \\textbf{learning-dynamics--aware optimization} and introduce \\textbf{Cooling-Weighted DPO (CW-DPO)}, a two-stage recipe that explicitly models and exploits the training trajectory. \\textbf{Stage 1} performs supervised finetuning with \\textbf{gentle negatives}: \\textbf{low-weight smoothed supervision} that regularizes the base policy and curbs overconfidence without explicit penalties. \\textbf{Stage 2} applies a DPO objective in which the \\textbf{negative term is scaled by a cooling weight} computed from the model's \\textbf{average token log-probability} on each negative, suppressing uninformative gradients from easy or off-distribution samples while preserving signal from hard negatives. In practice, we emphasize \\textbf{on-policy negatives} and allow \\textbf{mixed negatives} by blending a controllable fraction of dataset negatives to maintain contrast freshness. Throughout, we instrument training with $\\Delta\\!\\log p$ probes on positives and negatives as first-class signals for early stopping, curriculum design, and failure diagnosis. Across diverse VLM tasks, CW-DPO yields \\textbf{more stable optimization}, \\textbf{better calibration}, and \\textbf{higher pairwise win-rates} than SFT-only and vanilla DPO, while \\textbf{converging in fewer steps}. Ablations isolate the \\textbf{cooling-weight mechanism} as the primary driver of these gains and show complementary benefits from mixing on-policy and dataset negatives. Taken together, our results show that \\textbf{smoothing learning dynamics before cooling preferences} is a simple, general principle for robust VLM alignment.",
        "tags": [
            "DPO",
            "VLM"
        ]
    },
    {
        "id": "44",
        "title": "Conjecturing: An Overlooked Step in Formal Mathematical Reasoning",
        "author": [
            "Jasivan Alex Sivakumar",
            "Philipp Borchert",
            "Ronald Cardenas",
            "Gerasimos Lampouras"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11986",
        "abstract": "Autoformalisation, the task of expressing informal mathematical statements in formal language, is often viewed as a direct translation process. This, however, disregards a critical preceding step: conjecturing. Many mathematical problems cannot be formalised directly without first conjecturing a conclusion such as an explicit answer, or a specific bound. Since Large Language Models (LLMs) already struggle with autoformalisation, and the evaluation of their conjecturing ability is limited and often entangled within autoformalisation or proof, it is particularly challenging to understand its effect. To address this gap, we augment existing datasets to create ConjectureBench, and redesign the evaluation framework and metric specifically to measure the conjecturing capabilities of LLMs both as a distinct task and within the autoformalisation pipeline. Our evaluation of foundational models, including GPT-4.1 and DeepSeek-V3.1, reveals that their autoformalisation performance is substantially overestimated when the conjecture is accounted for during evaluation. However, the conjecture should not be assumed to be provided. We design an inference-time method, Lean-FIRe to improve conjecturing and autoformalisation, which, to the best of our knowledge, achieves the first successful end-to-end autoformalisation of 13 PutnamBench problems with GPT-4.1 and 7 with DeepSeek-V3.1. We demonstrate that while LLMs possess the requisite knowledge to generate accurate conjectures, improving autoformalisation performance requires treating conjecturing as an independent task, and investigating further how to correctly integrate it within autoformalisation. Finally, we provide forward-looking guidance to steer future research toward improving conjecturing, an overlooked step of formal mathematical reasoning.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "45",
        "title": "PanoTPS-Net: Panoramic Room Layout Estimation via Thin Plate Spline Transformation",
        "author": [
            "Hatem Ibrahem",
            "Ahmed Salem",
            "Qinmin Vivian Hu",
            "Guanghui Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11992",
        "abstract": "Accurately estimating the 3D layout of rooms is a crucial task in computer vision, with potential applications in robotics, augmented reality, and interior design. This paper proposes a novel model, PanoTPS-Net, to estimate room layout from a single panorama image. Leveraging a Convolutional Neural Network (CNN) and incorporating a Thin Plate Spline (TPS) spatial transformation, the architecture of PanoTPS-Net is divided into two stages: First, a convolutional neural network extracts the high-level features from the input images, allowing the network to learn the spatial parameters of the TPS transformation. Second, the TPS spatial transformation layer is generated to warp a reference layout to the required layout based on the predicted parameters. This unique combination empowers the model to properly predict room layouts while also generalizing effectively to both cuboid and non-cuboid layouts. Extensive experiments on publicly available datasets and comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed method. The results underscore the model's accuracy in room layout estimation and emphasize the compatibility between the TPS transformation and panorama images. The robustness of the model in handling both cuboid and non-cuboid room layout estimation is evident with a 3DIoU value of 85.49, 86.16, 81.76, and 91.98 on PanoContext, Stanford-2D3D, Matterport3DLayout, and ZInD datasets, respectively. The source code is available at: https://github.com/HatemHosam/PanoTPS_Net.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "46",
        "title": "Prompt-Guided Spatial Understanding with RGB-D Transformers for Fine-Grained Object Relation Reasoning",
        "author": [
            "Tanner Muturi",
            "Blessing Agyei Kyem",
            "Joshua Kofi Asamoah",
            "Neema Jakisa Owor",
            "Richard Dyzinela",
            "Andrews Danyo",
            "Yaw Adu-Gyamfi",
            "Armstrong Aboah"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11996",
        "abstract": "Spatial reasoning in large-scale 3D environments such as warehouses remains a significant challenge for vision-language systems due to scene clutter, occlusions, and the need for precise spatial understanding. Existing models often struggle with generalization in such settings, as they rely heavily on local appearance and lack explicit spatial grounding. In this work, we introduce a dedicated spatial reasoning framework for the Physical AI Spatial Intelligence Warehouse dataset introduced in the Track 3 2025 AI City Challenge. Our approach enhances spatial comprehension by embedding mask dimensions in the form of bounding box coordinates directly into the input prompts, enabling the model to reason over object geometry and layout. We fine-tune the framework across four question categories namely: Distance Estimation, Object Counting, Multi-choice Grounding, and Spatial Relation Inference using task-specific supervision. To further improve consistency with the evaluation system, normalized answers are appended to the GPT response within the training set. Our comprehensive pipeline achieves a final score of 73.0606, placing 4th overall on the public leaderboard. These results demonstrate the effectiveness of structured prompt enrichment and targeted optimization in advancing spatial reasoning for real-world industrial environments.",
        "tags": [
            "3D",
            "GPT"
        ]
    },
    {
        "id": "47",
        "title": "UALM: Unified Audio Language Model for Understanding, Generation and Reasoning",
        "author": [
            "Jinchuan Tian",
            "Sang-gil Lee",
            "Zhifeng Kong",
            "Sreyan Ghosh",
            "Arushi Goel",
            "Chao-Han Huck Yang",
            "Wenliang Dai",
            "Zihan Liu",
            "Hanrong Ye",
            "Shinji Watanabe",
            "Mohammad Shoeybi",
            "Bryan Catanzaro",
            "Rafael Valle",
            "Wei Ping"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12000",
        "abstract": "Recent advances in the audio language modeling (ALM) domain tackle audio understanding and text-to-audio generation as separate tasks. Very few studies attempt to unify these tasks -- an essential step toward advanced multimodal reasoning. This paper introduces U}nified Audio Language Model (UALM), which aims to unify audio understanding, text-to-audio generation, and multimodal reasoning in a single model. To achieve this goal, we first present UALM-Gen, a text-to-audio language model that directly predicts audio tokens and is comparable to state-of-the-art diffusion-based models. We then demonstrate, using proper data blending, training recipes, and inference techniques, that our single UALM model matches the quality of state-of-the-art specialized models in audio understanding, text-to-audio generation, and text reasoning. Furthermore, we present UALM-Reason, a multimodal reasoning model that utilizes both text and audio in the intermediate thinking steps to facilitate complex generation tasks. To our knowledge, this is the first demonstration in audio research of cross-modal generative reasoning, with its effectiveness confirmed by subjective evaluations.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "48",
        "title": "Generate Logical Equivalence Questions",
        "author": [
            "Xinyu Wang",
            "Haoming Yu",
            "Yicheng Yang",
            "Zhiyuan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12001",
        "abstract": "Academic dishonesty is met with zero tolerance in higher education, yet plagiarism has become increasingly prevalent in the era of online teaching and learning. Automatic Question Generation (AQG) presents a potential solution to mitigate copying by creating unique questions for each student. Additionally, AQG can provide a vast array of practice questions. Our AQG focuses on generating logical equivalence questions for Discrete Mathematics, a foundational course for first-year computer science students. A literature review reveals that existing AQGs for this type of question generate all propositions that meet user-defined constraints, resulting in inefficiencies and a lack of uniform question difficulty. To address this, we propose a new approach that defines logical equivalence questions using a formal language, translates this language into two sets of generation rules, and develops a linear-time algorithm for question generation. We evaluated our AQG through two experiments. The first involved a group of students completing questions generated by our system. Statistical analysis shows that the accuracy of these questions is comparable to that of textbook questions. The second experiment assessed the number of steps required to solve our generated questions, textbook questions, and those generated by multiple large language models. The results indicated that the difficulty of our questions was similar to that of textbook questions, confirming the quality of our AQG.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "49",
        "title": "Information Extraction from Conversation Transcripts: Neuro-Symbolic vs. LLM",
        "author": [
            "Alice Saebom Kwak",
            "Maria Alexeeva",
            "Gus Hahn-Powell",
            "Keith Alcock",
            "Kevin McLaughlin",
            "Doug McCorkle",
            "Gabe McNunn",
            "Mihai Surdeanu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12023",
        "abstract": "The current trend in information extraction (IE) is to rely extensively on large language models, effectively discarding decades of experience in building symbolic or statistical IE systems. This paper compares a neuro-symbolic (NS) and an LLM-based IE system in the agricultural domain, evaluating them on nine interviews across pork, dairy, and crop subdomains. The LLM-based system outperforms the NS one (F1 total: 69.4 vs. 52.7; core: 63.0 vs. 47.2), where total includes all extracted information and core focuses on essential details. However, each system has trade-offs: the NS approach offers faster runtime, greater control, and high accuracy in context-free tasks but lacks generalizability, struggles with contextual nuances, and requires significant resources to develop and maintain. The LLM-based system achieves higher performance, faster deployment, and easier maintenance but has slower runtime, limited control, model dependency and hallucination risks. Our findings highlight the \"hidden cost\" of deploying NLP systems in real-world applications, emphasizing the need to balance performance, efficiency, and control.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "50",
        "title": "Mamaba Can Learn Low-Dimensional Targets In-Context via Test-Time Feature Learning",
        "author": [
            "Junsoo Oh",
            "Wei Huang",
            "Taiji Suzuki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12026",
        "abstract": "Mamba, a recently proposed linear-time sequence model, has attracted significant attention for its computational efficiency and strong empirical performance. However, a rigorous theoretical understanding of its underlying mechanisms remains limited. In this work, we provide a theoretical analysis of Mamba's in-context learning (ICL) capability by focusing on tasks defined by low-dimensional nonlinear target functions. Specifically, we study in-context learning of a single-index model $y \\approx g_*(\\langle \\boldsymbol{\\beta}, \\boldsymbol{x} \\rangle)$, which depends on only a single relevant direction $\\boldsymbol{\\beta}$, referred to as feature. We prove that Mamba, pretrained by gradient-based methods, can achieve efficient ICL via test-time feature learning, extracting the relevant direction directly from context examples. Consequently, we establish a test-time sample complexity that improves upon linear Transformers -- analyzed to behave like kernel methods -- and is comparable to nonlinear Transformers, which have been shown to surpass the Correlational Statistical Query (CSQ) lower bound and achieve near information-theoretically optimal rate in previous works. Our analysis reveals the crucial role of the nonlinear gating mechanism in Mamba for feature extraction, highlighting it as the fundamental driver behind Mamba's ability to achieve both computational efficiency and high performance.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "51",
        "title": "CPR: Mitigating Large Language Model Hallucinations with Curative Prompt Refinement",
        "author": [
            "Jung-Woo Shim",
            "Yeong-Joon Ju",
            "Ji-Hoon Park",
            "Seong-Whan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12029",
        "abstract": "Recent advancements in large language models (LLMs) highlight their fluency in generating responses to diverse prompts. However, these models sometimes generate plausible yet incorrect ``hallucinated\" facts, undermining trust. A frequent but often overlooked cause of such errors is the use of poorly structured or vague prompts by users, leading LLMs to base responses on assumed rather than actual intentions. To mitigate hallucinations induced by these ill-formed prompts, we introduce Curative Prompt Refinement (CPR), a plug-and-play framework for curative prompt refinement that 1) cleans ill-formed prompts, and 2) generates additional informative task descriptions to align the intention of the user and the prompt using a fine-tuned small language model. When applied to language models, we discover that CPR significantly increases the quality of generation while also mitigating hallucination. Empirical studies show that prompts with CPR applied achieves over a 90\\% win rate over the original prompts without any external knowledge.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "52",
        "title": "Multi-stage Prompt Refinement for Mitigating Hallucinations in Large Language Models",
        "author": [
            "Jung-Woo Shim",
            "Yeong-Joon Ju",
            "Ji-Hoon Park",
            "Seong-Whan Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12032",
        "abstract": "Recent advancements in large language models (LLMs) have shown strong performance in natural language understanding and generation tasks. However, LLMs continue to encounter challenges with hallucinations, where models generate plausible but incorrect information. While several factors contribute to hallucinations, the impact of ill-formed prompts, prompts with ambiguous wording, incorrect grammar, or incomplete information, was relatively under explored. To address this, we introduce Multi-stage Prompt Refinement (MPR), a framework designed to systematically improve these ill-formed prompts across multiple stages. Each stage addresses specific errors such as punctuation, typographical mistakes, and misuse of key terms, using small language models (SLMs) fine-tuned for these tasks. MPR iteratively enhances the clarity of prompts with additional context and employs a self-reflection mechanism with ranking to prioritize the most relevant input. Experimental results on hallucination benchmarks show that prompts refined by MPR achieve over an 85~\\% win rate compared to their original forms, demonstrating its effectiveness in reducing hallucinations and improving LLM output accuracy. Interestingly, we reveal that MPR can be combined with existing post-hoc hallucination mitigation frameworks, further enhancing its versatility. MPR provides a lightweight and adaptable solution for enhancing LLM reliability across various domains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "53",
        "title": "Uncertainty Quantification for Hallucination Detection in Large Language Models: Foundations, Methodology, and Future Directions",
        "author": [
            "Sungmin Kang",
            "Yavuz Faruk Bakman",
            "Duygu Nur Yaldiz",
            "Baturalp Buyukates",
            "Salman Avestimehr"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12040",
        "abstract": "The rapid advancement of large language models (LLMs) has transformed the landscape of natural language processing, enabling breakthroughs across a wide range of areas including question answering, machine translation, and text summarization. Yet, their deployment in real-world applications has raised concerns over reliability and trustworthiness, as LLMs remain prone to hallucinations that produce plausible but factually incorrect outputs. Uncertainty quantification (UQ) has emerged as a central research direction to address this issue, offering principled measures for assessing the trustworthiness of model generations. We begin by introducing the foundations of UQ, from its formal definition to the traditional distinction between epistemic and aleatoric uncertainty, and then highlight how these concepts have been adapted to the context of LLMs. Building on this, we examine the role of UQ in hallucination detection, where quantifying uncertainty provides a mechanism for identifying unreliable generations and improving reliability. We systematically categorize a wide spectrum of existing methods along multiple dimensions and present empirical results for several representative approaches. Finally, we discuss current limitations and outline promising future research directions, providing a clearer picture of the current landscape of LLM UQ for hallucination detection.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "54",
        "title": "Improving Text-to-Image Generation with Input-Side Inference-Time Scaling",
        "author": [
            "Ruibo Chen",
            "Jiacheng Pan",
            "Heng Huang",
            "Zhenheng Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12041",
        "abstract": "Recent advances in text-to-image (T2I) generation have achieved impressive results, yet existing models often struggle with simple or underspecified prompts, leading to suboptimal image-text alignment, aesthetics, and quality. We propose a prompt rewriting framework that leverages large language models (LLMs) to refine user inputs before feeding them into T2I backbones. Our approach introduces a carefully designed reward system and an iterative direct preference optimization (DPO) training pipeline, enabling the rewriter to enhance prompts without requiring supervised fine-tuning data. We evaluate our method across diverse T2I models and benchmarks. Results show that our prompt rewriter consistently improves image-text alignment, visual quality, and aesthetics, outperforming strong baselines. Furthermore, we demonstrate strong transferability by showing that a prompt rewriter trained on one T2I backbone generalizes effectively to others without needing to be retrained. We also systematically study scalability, evaluating how performance gains scale with the capacity of the large LLM used as the rewriter. These findings highlight that prompt rewriting is an effective, scalable, and practical model-agnostic strategy for improving T2I systems. We plan to release the code and trained prompt rewriters soon.",
        "tags": [
            "DPO",
            "LLM",
            "Text-to-Image"
        ]
    },
    {
        "id": "55",
        "title": "Do Large Language Models Respect Contracts? Evaluating and Enforcing Contract-Adherence in Code Generation",
        "author": [
            "Soohan Lim",
            "Joonghyuk Hahn",
            "Hyunwoo Park",
            "Sang-Ki Ko",
            "Yo-Sub Han"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12047",
        "abstract": "Prevailing code generation benchmarks, such as HumanEval+ and MBPP+, primarily evaluate large language models (LLMs) with pass@k on functional correctness using well-formed inputs. However, they ignore a crucial aspect of real-world software: adherence to contracts-the preconditions and validity constraints that dictate how ill-formed inputs must be rejected. This critical oversight means that existing benchmarks fail to measure, and models consequently fail to generate, truly robust and reliable code snippets. We introduce PACT, a program assessment and contract-adherence evaluation framework, to bridge this gap. PACT is the first framework designed to systematically evaluate and enhance contract-adherence in LLM-generated code snippets alongside functional correctness. PACT's contributions are threefold: First, it provides a comprehensive test-suite corpus focused on contract violations, extending HumanEval+ and MBPP+. Second, it enables a systematic analysis of code generation under varied prompting conditions. This analysis demonstrates that augmenting prompts with contract-violating test cases significantly enhance a model's ability to respect contracts compared to using contract description alone. Finally, it introduces novel metrics to rigorously quantify contract adherence in both test generation and code generation. By revealing critical errors that conventional benchmarks overlook, PACT provides the rigorous and interpretable metrics to evaluate the robustness of LLM-generated code snippets in both functionality and http://contract-adherence.Our code and data are available at https://github.com/suhanmen/PACT.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "56",
        "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
        "author": [
            "Baisub Lee",
            "Sanghyun Byun",
            "Mohanad Odema",
            "Jung Guack",
            "Jacob Song",
            "Woo Seong Chung"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12051",
        "abstract": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing two key challenges: (1) A growing memory footprint due to quadratic self-attention and linear KV-cache scaling in memory as sequence length increases; (2) the ContextRot phenomena where empirical evidence suggests that transformer architecture's performance degrades with increasing context length. Given the shared dependency on the input, a natural question arises: Can we surgically select the most important input chunks for processing to synergistically (a) reduce the memory footprint, and (b) mitigate the ContextRot effects? In this paper, we answer this question in the affirmative for long-context summarization tasks. We propose APCE as a context-aware solution to select the most important input chunks through low-dimensional semantic similarity matching with the current query. By directly operating on the input, APCE decouples from strict dependency on underlying hardware or CUDA environments, promising a compatible solution scalable to different deployment systems. Our empirical evaluations have demonstrated superior or on-par summarization performance for APCE compared to the full dense baseline using a fraction (50%-70%) of the input sequence resulting in KV-cache and self-attention memory efficiency improvements. We hope our findings inspire further research on context-aware efficiency solutions for LCTMs geared towards other relevant long-context tasks.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "57",
        "title": "Your VAR Model is Secretly an Efficient and Explainable Generative Classifier",
        "author": [
            "Yi-Chung Chen",
            "David I. Inouye",
            "Jing Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12060",
        "abstract": "Generative classifiers, which leverage conditional generative models for classification, have recently demonstrated desirable properties such as robustness to distribution shifts. However, recent progress in this area has been largely driven by diffusion-based models, whose substantial computational cost severely limits scalability. This exclusive focus on diffusion-based methods has also constrained our understanding of generative classifiers. In this work, we propose a novel generative classifier built on recent advances in visual autoregressive (VAR) modeling, which offers a new perspective for studying generative classifiers. To further enhance its performance, we introduce the Adaptive VAR Classifier$^+$ (A-VARC$^+$), which achieves a superior trade-off between accuracy and inference speed, thereby significantly improving practical applicability. Moreover, we show that the VAR-based method exhibits fundamentally different properties from diffusion-based methods. In particular, due to its tractable likelihood, the VAR-based classifier enables visual explainability via token-wise mutual information and demonstrates inherent resistance to catastrophic forgetting in class-incremental learning tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "58",
        "title": "Empowering LLM Agents with Geospatial Awareness: Toward Grounded Reasoning for Wildfire Response",
        "author": [
            "Yiheng Chen",
            "Lingyao Li",
            "Zihui Ma",
            "Qikai Hu",
            "Yilun Zhu",
            "Min Deng",
            "Runlong Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12061",
        "abstract": "Effective disaster response is essential for safeguarding lives and property. Existing statistical approaches often lack semantic context, generalize poorly across events, and offer limited interpretability. While Large language models (LLMs) provide few-shot generalization, they remain text-bound and blind to geography. To bridge this gap, we introduce a Geospatial Awareness Layer (GAL) that grounds LLM agents in structured earth data. Starting from raw wildfire detections, GAL automatically retrieves and integrates infrastructure, demographic, terrain, and weather information from external geodatabases, assembling them into a concise, unit-annotated perception script. This enriched context enables agents to produce evidence-based resource-allocation recommendations (e.g., personnel assignments, budget allocations), further reinforced by historical analogs and daily change signals for incremental updates. We evaluate the framework in real wildfire scenarios across multiple LLM models, showing that geospatially grounded agents can outperform baselines. The proposed framework can generalize to other hazards such as floods and hurricanes.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "59",
        "title": "ThinkPilot: Steering Reasoning Models via Automated Think-prefixes Optimization",
        "author": [
            "Sunzhu Li",
            "Zhiyu Lin",
            "Shuling Yang",
            "Jiale Zhao",
            "Wei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12063",
        "abstract": "Large Reasoning Models (LRMs) are powerful, but they still suffer from inefficient and off-target reasoning. Currently, training-free methods are limited to either rigid heuristics or descriptive, non-actionable analyses. In this paper, we introduce ThinkPilot, a training-free framework that automatically optimizes LRMs reasoning. It uses an evolutionary process to generate think-prefixes, which are instructions that evolve driven by a taxonomy of reasoning behaviors to guide models toward superior performance. Extensive experiments demonstrate ThinkPilot's broad effectiveness: it significantly improves the accuracy-length trade-off for efficient reasoning, drastically improves safety (for example, cutting the StrongREJECT score of DeepSeek-R1-Distill-Qwen-32B from 27.0% to 0.7), and enhances instruction following. It also synergizes with existing training-based methods. Our analysis reveals that think-prefixes can reliably control LRMs' reasoning behaviors, and that different tasks have strong preferences for specific behavioral distributions. By automatically identifying and eliciting these behaviors, ThinkPilot provides a generalizable framework for aligning LRMs reasoning with task demands. Data and code are available at https://github.com/teqkilla/ThinkPilot",
        "tags": [
            "DeepSeek",
            "Qwen"
        ]
    },
    {
        "id": "60",
        "title": "GeoPipe: a Geo-distributed LLM Training Framework with enhanced Pipeline Parallelism in a Lossless RDMA-enabled Datacenter Optical Transport Network",
        "author": [
            "Jun Dai",
            "Xiaorun Wang",
            "Kexiong Fang",
            "Zheng Yang",
            "Yuefeng Ji",
            "Jiawei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12064",
        "abstract": "The proliferation of Large Language Models (LLMs) with exponentially growing parameters is making cross-data center (DC) training an inevitable trend. However, viable strategies for extending single-DC training frameworks to multi-DC environments remain underdeveloped. We experimentally demonstrate, for the first time, a high-performance geo-distributed LLMs training framework across multiple DCs interconnected by a lossless, remote direct memory access (RDMA) enabled Datacenter Optical Transport Network (DC-OTN). An enhanced pipeline parallelism scheme is implemented within the Ascend full-stack environment of Huawei, which effectively eliminates the impact of cross-DC communication overhead on training efficiency. The overlapped computation and cross-DC communication is achieved with constraint cross-DC bandwidth and High Bandwidth Memory (HBM), reducing computation bubble ratio by up to 78.91%.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "61",
        "title": "AI Agents as Universal Task Solvers",
        "author": [
            "Alessandro Achille",
            "Stefano Soatto"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12066",
        "abstract": "AI reasoning agents are already able to solve a variety of tasks by deploying tools, simulating outcomes of multiple hypotheses and reflecting on them. In doing so, they perform computation, although not in the classical sense -- there is no program being executed. Still, if they perform computation, can AI agents be universal? Can chain-of-thought reasoning solve any computable task? How does an AI Agent learn to reason? Is it a matter of model size? Or training dataset size?\nIn this work, we reinterpret the role of learning in the context of AI Agents, viewing them as compute-capable stochastic dynamical systems, and highlight the role of time in a foundational principle for learning to reason. In doing so, we propose a shift from classical inductive learning to transductive learning -- where the objective is not to approximate the distribution of past data, but to capture their algorithmic structure to reduce the time needed to find solutions to new tasks.\nTransductive learning suggests that, counter to Shannon's theory, a key role of information in learning is about reduction of time rather than reconstruction error. In particular, we show that the optimal speed-up that a universal solver can achieve using past data is tightly related to their algorithmic information. Using this, we show a theoretical derivation for the observed power-law scaling of inference time versus training time. We then show that scaling model size can lead to behaviors that, while improving accuracy on benchmarks, fail any reasonable test of intelligence, let alone super-intelligence: In the limit of infinite space and time, large models can behave as savants, able to brute-force through any task without any insight. Instead, we argue that the key quantity to optimize when scaling reasoning models is time, whose critical role in learning has so far only been indirectly considered.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "62",
        "title": "HiCoTraj:Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory",
        "author": [
            "Junyi Xie",
            "Yuankun Jiao",
            "Jina Kim",
            "Yao-Yi Chiang",
            "Lingyi Zhao",
            "Khurram Shafique"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12067",
        "abstract": "Inferring demographic attributes such as age, sex, or income level from human mobility patterns enables critical applications such as targeted public health interventions, equitable urban planning, and personalized transportation services. Existing mobility-based demographic inference studies heavily rely on large-scale trajectory data with demographic labels, leading to limited interpretability and poor generalizability across different datasets and user groups. We propose HiCoTraj (Zero-Shot Demographic Reasoning via Hierarchical Chain-of-Thought Prompting from Trajectory), a framework that leverages LLMs' zero-shot learning and semantic understanding capabilities to perform demographic inference without labeled training data. HiCoTraj transforms trajectories into semantically rich, natural language representations by creating detailed activity chronicles and multi-scale visiting summaries. Then HiCoTraj uses a novel hierarchical chain of thought reasoning to systematically guide LLMs through three cognitive stages: factual feature extraction, behavioral pattern analysis, and demographic inference with structured output. This approach addresses the scarcity challenge of labeled demographic data while providing transparent reasoning chains. Experimental evaluation on real-world trajectory data demonstrates that HiCoTraj achieves competitive performance across multiple demographic attributes in zero-shot scenarios.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "63",
        "title": "VIDMP3: Video Editing by Representing Motion with Pose and Position Priors",
        "author": [
            "Sandeep Mishra",
            "Oindrila Saha",
            "Alan C. Bovik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12069",
        "abstract": "Motion-preserved video editing is crucial for creators, particularly in scenarios that demand flexibility in both the structure and semantics of swapped objects. Despite its potential, this area remains underexplored. Existing diffusion-based editing methods excel in structure-preserving tasks, using dense guidance signals to ensure content integrity. While some recent methods attempt to address structure-variable editing, they often suffer from issues such as temporal inconsistency, subject identity drift, and the need for human intervention. To address these challenges, we introduce VidMP3, a novel approach that leverages pose and position priors to learn a generalized motion representation from source videos. Our method enables the generation of new videos that maintain the original motion while allowing for structural and semantic flexibility. Both qualitative and quantitative evaluations demonstrate the superiority of our approach over existing methods. The code will be made publicly available at https://github.com/sandeep-sm/VidMP3.",
        "tags": [
            "Diffusion",
            "Video Editing"
        ]
    },
    {
        "id": "64",
        "title": "EmboMatrix: A Scalable Training-Ground for Embodied Decision-Making",
        "author": [
            "Zixing Lei",
            "Sheng Yin",
            "Yichen Xiong",
            "Yuanzhuo Ding",
            "Wenhao Huang",
            "Yuxi Wei",
            "Qingyao Xu",
            "Yiming Li",
            "Weixin Li",
            "Yunhong Wang",
            "Siheng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12072",
        "abstract": "Embodied decision-making enables agents to translate high-level goals into executable actions through continuous interactions within the physical world, forming a cornerstone of general-purpose embodied intelligence. Large language models (LLMs), with their general decision-making capabilities, offer a promising path to realize this potential; however, LLMs trained solely on language lack exposure to physical environments, limiting their true embodied understanding. To bridge this gap, we propose the concept of a training ground: a comprehensive infrastructure that provides task and scene simulation, embodied interaction, and feedback signals, offering a one-stop solution for LLM acquire genuine embodied decision-making skills. In this work, we present EmboMatrix, the first training ground of its kind, providing massive and diverse tasks with efficient simulation and precise rewards. EmboMatrix incorporates a series of novel techniques: a multi-agent data engine for large-scale task and scene generation, a distributed heterogeneous-hardware system for scalable simulation, and a multi-level reward architecture for precise supervision. Leveraging EmboMatrix, we cultivate EmboBrain, an LLM whose embodied decision-making abilities emerge from extensive embodied interactions. Experiments show that EmboBrain-7B surpasses the 671B DeepSeek-R1 baseline by 9.5\\% on two challenging embodied decision-making benchmarks, demonstrating the power of interactive, environment-grounded learning for building truly intelligent embodied agents.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "65",
        "title": "FedLoDrop: Federated LoRA with Dropout for Generalized LLM Fine-tuning",
        "author": [
            "Sijing Xie",
            "Dingzhu Wen",
            "Changsheng You",
            "Qimei Chen",
            "Mehdi Bennis",
            "Kaibin Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12078",
        "abstract": "Fine-tuning (FT) large language models (LLMs) is crucial for adapting general-purpose models to specific tasks, enhancing accuracy and relevance with minimal resources. To further enhance generalization ability while reducing training costs, this paper proposes Federated LoRA with Dropout (FedLoDrop), a new framework that applies dropout to the rows and columns of the trainable matrix in Federated LoRA. A generalization error bound and convergence analysis under sparsity regularization are obtained, which elucidate the fundamental trade-off between underfitting and overfitting. The error bound reveals that a higher dropout rate increases model sparsity, thereby lowering the upper bound of pointwise hypothesis stability (PHS). While this reduces the gap between empirical and generalization errors, it also incurs a higher empirical error, which, together with the gap, determines the overall generalization error. On the other hand, though dropout reduces communication costs, deploying FedLoDrop at the network edge still faces challenges due to limited network resources. To address this issue, an optimization problem is formulated to minimize the upper bound of the generalization error, by jointly optimizing the dropout rate and resource allocation subject to the latency and per-device energy consumption constraints. To solve this problem, a branch-and-bound (B\\&B)-based method is proposed to obtain its globally optimal solution. Moreover, to reduce the high computational complexity of the B\\&B-based method, a penalized successive convex approximation (P-SCA)-based algorithm is proposed to efficiently obtain its high-quality suboptimal solution. Finally, numerical results demonstrate the effectiveness of the proposed approach in mitigating overfitting and improving the generalization capability.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "66",
        "title": "Evaluating the Quality of Randomness and Entropy in Tasks Supported by Large Language Models",
        "author": [
            "Rabimba Karanjai",
            "Yang Lu",
            "Ranjith Chodavarapu",
            "Lei Xu",
            "Weidong Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12080",
        "abstract": "The rapid advancement of large language model (LLM) technology has led to diverse applications, many of which inherently require randomness, such as stochastic decision-making, gaming, scheduling, AI agents, and cryptography-related tasks. However, the capabilities of LLMs in handling randomness, particularly in generating and utilizing random numbers effectively, remain unclear. This paper investigates the capacity of LLMs for handling tasks that involve randomness through a series of experiments. We designed a set of experiments that consider various factors that can influence an LLM's performance in tasks involving randomness, such as accessibility to external tools, types of tasks, model states (fresh vs. non-fresh), and prompting strategies. The experiments cover a range of tasks, including generating random numbers, generating random strings such as passwords, shuffling items, and evaluating the quality of randomness using entropy and the NIST randomness test-suite. Our findings reveal that while LLMs can generate outputs that exhibit some degree of randomness, their performance is inconsistent and often deviates significantly from the expected behavior. The analysis of the experimental results highlights key limitations and areas where improvement is needed for the LLMs to effectively handle tasks involving randomness",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "67",
        "title": "GraphShaper: Geometry-aware Alignment for Improving Transfer Learning in Text-Attributed Graphs",
        "author": [
            "Heng Zhang",
            "Tianyi Zhang",
            "Yuling Shi",
            "Xiaodong Gu",
            "Yaomin Shen",
            "Haochen You",
            "Zijian Zhang",
            "Yilei Yuan",
            "Jin Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12085",
        "abstract": "Graph foundation models represent a transformative paradigm for learning transferable representations across diverse graph domains. Recent methods leverage large language models to unify graph and text modalities into a shared representation space using contrastive learning. However, systematic evaluations reveal significant performance degradation at structural boundaries where distinct topological patterns converge, with accuracy losses exceeding 20 percentage points. This issue arises from a key limitation: current methods assume all graph structures can be encoded within a single Euclidean space. In reality, tree structures require hyperbolic geometry to preserve hierarchical branching, while cyclic patterns depend on spherical geometry for closure properties. At structural boundaries, nodes experience conflicting geometric constraints that uniform encoding spaces cannot resolve. This raises a crucial challenge: \\textbf{Can alignment frameworks be designed to respect the intrinsic geometric diversity of graph structures?} We introduce \\textbf{GraphShaper}, a geometry-aware framework that enhances graph encoding through multi-geometric specialization. Our approach employs expert networks tailored to different geometric spaces, dynamically computing fusion weights to adaptively integrate geometric properties based on local structural characteristics. This adaptive fusion preserves structural integrity before alignment with text embeddings. Extensive experiments demonstrate that GraphShaper achieves 9.47\\% accuracy improvements on citation networks and 7.63\\% on social networks in zero-shot settings.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "68",
        "title": "Playmate2: Training-Free Multi-Character Audio-Driven Animation via Diffusion Transformer with Reward Feedback",
        "author": [
            "Xingpei Ma",
            "Shenneng Huang",
            "Jiaran Cai",
            "Yuansheng Guan",
            "Shen Zheng",
            "Hanfeng Zhao",
            "Qiang Zhang",
            "Shunsi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12089",
        "abstract": "Recent advances in diffusion models have significantly improved audio-driven human video generation, surpassing traditional methods in both quality and controllability. However, existing approaches still face challenges in lip-sync accuracy, temporal coherence for long video generation, and multi-character animation. In this work, we propose a diffusion transformer (DiT)-based framework for generating lifelike talking videos of arbitrary length, and introduce a training-free method for multi-character audio-driven animation. First, we employ a LoRA-based training strategy combined with a position shift inference approach, which enables efficient long video generation while preserving the capabilities of the foundation model. Moreover, we combine partial parameter updates with reward feedback to enhance both lip synchronization and natural body motion. Finally, we propose a training-free approach, Mask Classifier-Free Guidance (Mask-CFG), for multi-character animation, which requires no specialized datasets or model modifications and supports audio-driven animation for three or more characters. Experimental results demonstrate that our method outperforms existing state-of-the-art approaches, achieving high-quality, temporally coherent, and multi-character audio-driven video generation in a simple, efficient, and cost-effective manner.",
        "tags": [
            "DiT",
            "Diffusion",
            "LoRA",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": "69",
        "title": "IL3D: A Large-Scale Indoor Layout Dataset for LLM-Driven 3D Scene Generation",
        "author": [
            "Wenxu Zhou",
            "Kaixuan Nie",
            "Hang Du",
            "Dong Yin",
            "Wei Huang",
            "Siqiang Guo",
            "Xiaobo Zhang",
            "Pengbo Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12095",
        "abstract": "In this study, we present IL3D, a large-scale dataset meticulously designed for large language model (LLM)-driven 3D scene generation, addressing the pressing demand for diverse, high-quality training data in indoor layout design. Comprising 27,816 indoor layouts across 18 prevalent room types and a library of 29,215 high-fidelity 3D object assets, IL3D is enriched with instance-level natural language annotations to support robust multimodal learning for vision-language tasks. We establish rigorous benchmarks to evaluate LLM-driven scene generation. Experimental results show that supervised fine-tuning (SFT) of LLMs on IL3D significantly improves generalization and surpasses the performance of SFT on other datasets. IL3D offers flexible multimodal data export capabilities, including point clouds, 3D bounding boxes, multiview images, depth maps, normal maps, and semantic masks, enabling seamless adaptation to various visual tasks. As a versatile and robust resource, IL3D significantly advances research in 3D scene generation and embodied intelligence, by providing high-fidelity scene data to support environment perception tasks of embodied agents.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "70",
        "title": "Rethinking the Role of Dynamic Sparse Training for Scalable Deep Reinforcement Learning",
        "author": [
            "Guozheng Ma",
            "Lu Li",
            "Zilin Wang",
            "Haoyu Wang",
            "Shengchao Hu",
            "Leszek Rutkowski",
            "Dacheng Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12096",
        "abstract": "Scaling neural networks has driven breakthrough advances in machine learning, yet this paradigm fails in deep reinforcement learning (DRL), where larger models often degrade performance due to unique optimization pathologies such as plasticity loss. While recent works show that dynamically adapting network topology during training can mitigate these issues, existing studies have three critical limitations: (1) applying uniform dynamic training strategies across all modules despite encoder, critic, and actor following distinct learning paradigms, (2) focusing evaluation on basic architectures without clarifying the relative importance and interaction between dynamic training and architectural improvements, and (3) lacking systematic comparison between different dynamic approaches including sparse-to-sparse, dense-to-sparse, and sparse-to-dense. Through comprehensive investigation across modules and architectures, we reveal that dynamic sparse training strategies provide module-specific benefits that complement the primary scalability foundation established by architectural improvements. We finally distill these insights into Module-Specific Training (MST), a practical framework that further exploits the benefits of architectural improvements and demonstrates substantial scalability gains across diverse RL algorithms without algorithmic modifications.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "71",
        "title": "An Adaptive Edge-Guided Dual-Network Framework for Fast QR Code Motion Deblurring",
        "author": [
            "Jianping Li",
            "Dongyang Guo",
            "Wenjie Li",
            "Wei Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12098",
        "abstract": "Unlike general image deblurring that prioritizes perceptual quality, QR code deblurring focuses on ensuring successful decoding. QR codes are characterized by highly structured patterns with sharp edges, a robust prior for restoration. Yet existing deep learning methods rarely exploit these priors explicitly. To address this gap, we propose the Edge-Guided Attention Block (EGAB), which embeds explicit edge priors into a Transformer architecture. Based on EGAB, we develop Edge-Guided Restormer (EG-Restormer), an effective network that significantly boosts the decoding rate of severely blurred QR codes. For mildly blurred inputs, we design the Lightweight and Efficient Network (LENet) for fast deblurring. We further integrate these two networks into an Adaptive Dual-network (ADNet), which dynamically selects the suitable network based on input blur severity, making it ideal for resource-constrained mobile devices. Extensive experiments show that our EG-Restormer and ADNet achieve state-of-the-art performance with a competitive speed. Project page: https://github.com/leejianping/ADNet",
        "tags": [
            "Deblurring",
            "Transformer"
        ]
    },
    {
        "id": "72",
        "title": "G4Splat: Geometry-Guided Gaussian Splatting with Generative Prior",
        "author": [
            "Junfeng Ni",
            "Yixin Chen",
            "Zhifei Yang",
            "Yu Liu",
            "Ruijie Lu",
            "Song-Chun Zhu",
            "Siyuan Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12099",
        "abstract": "Despite recent advances in leveraging generative prior from pre-trained diffusion models for 3D scene reconstruction, existing methods still face two critical limitations. First, due to the lack of reliable geometric supervision, they struggle to produce high-quality reconstructions even in observed regions, let alone in unobserved areas. Second, they lack effective mechanisms to mitigate multi-view inconsistencies in the generated images, leading to severe shape-appearance ambiguities and degraded scene geometry. In this paper, we identify accurate geometry as the fundamental prerequisite for effectively exploiting generative models to enhance 3D scene reconstruction. We first propose to leverage the prevalence of planar structures to derive accurate metric-scale depth maps, providing reliable supervision in both observed and unobserved regions. Furthermore, we incorporate this geometry guidance throughout the generative pipeline to improve visibility mask estimation, guide novel view selection, and enhance multi-view consistency when inpainting with video diffusion models, resulting in accurate and consistent scene completion. Extensive experiments on Replica, ScanNet++, and DeepBlending show that our method consistently outperforms existing baselines in both geometry and appearance reconstruction, particularly for unobserved regions. Moreover, our method naturally supports single-view inputs and unposed videos, with strong generalizability in both indoor and outdoor scenarios with practical real-world applicability. The project page is available at https://dali-jack.github.io/g4splat-web/.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Inpainting"
        ]
    },
    {
        "id": "73",
        "title": "SpikePool: Event-driven Spiking Transformer with Pooling Attention",
        "author": [
            "Donghyun Lee",
            "Alex Sima",
            "Yuhang Li",
            "Panos Stinis",
            "Priyadarshini Panda"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12102",
        "abstract": "Building on the success of transformers, Spiking Neural Networks (SNNs) have increasingly been integrated with transformer architectures, leading to spiking transformers that demonstrate promising performance on event-based vision tasks. However, despite these empirical successes, there remains limited understanding of how spiking transformers fundamentally process event-based data. Current approaches primarily focus on architectural modifications without analyzing the underlying signal processing characteristics. In this work, we analyze spiking transformers through the frequency spectrum domain and discover that they behave as high-pass filters, contrasting with Vision Transformers (ViTs) that act as low-pass filters. This frequency domain analysis reveals why certain designs work well for event-based data, which contains valuable high-frequency information but is also sparse and noisy. Based on this observation, we propose SpikePool, which replaces spike-based self-attention with max pooling attention, a low-pass filtering operation, to create a selective band-pass filtering effect. This design preserves meaningful high-frequency content while capturing critical features and suppressing noise, achieving a better balance for event-based data processing. Our approach demonstrates competitive results on event-based datasets for both classification and object detection tasks while significantly reducing training and inference time by up to 42.5% and 32.8%, respectively.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "74",
        "title": "Deep Associations, High Creativity: A Simple yet Effective Metric for Evaluating Large Language Models",
        "author": [
            "Ziliang Qiu",
            "Renfen Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12110",
        "abstract": "The evaluation of LLMs' creativity represents a crucial research domain, though challenges such as data contamination and costly human assessments often impede progress. Drawing inspiration from human creativity assessment, we propose PACE, asking LLMs to generate Parallel Association Chains to Evaluate their creativity. PACE minimizes the risk of data contamination and offers a straightforward, highly efficient evaluation, as evidenced by its strong correlation with Chatbot Arena Creative Writing rankings (Spearman's $\\rho = 0.739$, $p < 0.001$) across various proprietary and open-source models. A comparative analysis of associative creativity between LLMs and humans reveals that while high-performing LLMs achieve scores comparable to average human performance, professional humans consistently outperform LLMs. Furthermore, linguistic analysis reveals that both humans and LLMs exhibit a trend of decreasing concreteness in their associations, and humans demonstrating a greater diversity of associative patterns.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "75",
        "title": "Chimera: State Space Models Beyond Sequences",
        "author": [
            "Aakash Lahoti",
            "Tanya Marwah",
            "Ratish Puduppully",
            "Albert Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12111",
        "abstract": "Transformer-based deep learning methods have become the standard approach for modeling diverse data such as sequences, images, and graphs. These methods rely on self-attention, which treats data as an unordered set of elements. This ignores the neighborhood structure or graph topology of the data and requires inductive biases--such as position embeddings in sequences and images, or random walks in graphs--to incorporate topology. However, designing such task-specific biases requires significant effort and can introduce side effects that hinder generalization. We introduce Chimera, a unified model that directly incorporates data topology in a principled way, removing the need for domain-specific biases. The key idea is that state space models--which naturally do not require position embeddings--can be generalized to capture any graph topology. Our experiments show that Chimera achieves strong performance across language, vision, and graph domains, outperforming BERT on GLUE by 0.7 points, ViT on ImageNet-1k by 2.6%, and all baselines on the Long Range Graph Benchmark. We further propose algorithmic optimizations to improve Chimera's efficiency: (1) for Directed Acyclic Graphs, Chimera can be implemented as a linear-time recurrence; (2) for general graphs, a simple mathematical relaxation achieves Transformer's quadratic complexity without domain-specific heuristics. These results validate Chimera's core contribution and support the idea that data topology is a powerful inductive bias across modalities.",
        "tags": [
            "BERT",
            "SSMs",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "76",
        "title": "Self-Supervised Selective-Guided Diffusion Model for Old-Photo Face Restoration",
        "author": [
            "Wenjie Li",
            "Xiangyi Wang",
            "Heng Guo",
            "Guangwei Gao",
            "Zhanyu Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12114",
        "abstract": "Old-photo face restoration poses significant challenges due to compounded degradations such as breakage, fading, and severe blur. Existing pre-trained diffusion-guided methods either rely on explicit degradation priors or global statistical guidance, which struggle with localized artifacts or face color. We propose Self-Supervised Selective-Guided Diffusion (SSDiff), which leverages pseudo-reference faces generated by a pre-trained diffusion model under weak guidance. These pseudo-labels exhibit structurally aligned contours and natural colors, enabling region-specific restoration via staged supervision: structural guidance applied throughout the denoising process and color refinement in later steps, aligned with the coarse-to-fine nature of diffusion. By incorporating face parsing maps and scratch masks, our method selectively restores breakage regions while avoiding identity mismatch. We further construct VintageFace, a 300-image benchmark of real old face photos with varying degradation levels. SSDiff outperforms existing GAN-based and diffusion-based methods in perceptual quality, fidelity, and regional controllability. Code link: https://github.com/PRIS-CV/SSDiff.",
        "tags": [
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "77",
        "title": "Tracing Multilingual Knowledge Acquisition Dynamics in Domain Adaptation: A Case Study of English-Japanese Biomedical Adaptation",
        "author": [
            "Xin Zhao",
            "Naoki Yoshinaga",
            "Yuma Tsuta",
            "Akiko Aizawa"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12115",
        "abstract": "Multilingual domain adaptation (ML-DA) is widely used to learn new domain knowledge across languages into large language models (LLMs). Although many methods have been proposed to improve domain adaptation, the mechanisms of multilingual knowledge acquisition, how domain knowledge is learned within a language and transferred across languages, remain underexplored. This gap leads to suboptimal performance, particularly in low-resource settings. This work examines the learning dynamics of LLMs during ML-DA. Because prior ML-DA studies often train and evaluate on datasets with mismatched knowledge coverage, we propose AdaXEval, an adaptive evaluation method that builds multiple-choice QA datasets from the same bilingual domain corpus used for training, thereby directly studying multilingual knowledge acquisition. Through continual training of LLMs with diverse data recipes, we track how LLMs acquire domain facts and pinpoint the mechanism behind the transformation process from domain training data to knowledge. Our experiments on a 13B English-Japanese bilingual LLM reveal that cross-lingual transfer remains challenging despite a high-quality bilingual corpus. The code has been released.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "78",
        "title": "Locket: Robust Feature-Locking Technique for Language Models",
        "author": [
            "Lipeng He",
            "Vasisht Duddu",
            "N. Asokan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12117",
        "abstract": "Chatbot providers (e.g., OpenAI) rely on tiered subscription schemes to generate revenue, offering basic models for free users, and advanced models for paying subscribers. However, a finer-grained pay-to-unlock scheme for premium features (e.g., math, coding) is thought to be more economically viable for the providers. Such a scheme requires a feature-locking technique (FLoTE) which is (i) effective in refusing locked features, (ii) utility-preserving for unlocked features, (iii) robust against evasion or unauthorized credential sharing, and (iv) scalable to multiple features and users. However, existing FLoTEs (e.g., password-locked models) are not robust or scalable. We present Locket, the first robust and scalable FLoTE to enable pay-to-unlock schemes. Locket uses a novel merging approach to attach adapters to an LLM for refusing unauthorized features. Our comprehensive evaluation shows that Locket is effective ($100$% refusal on locked features), utility-preserving ($\\leq 7$% utility degradation in unlocked features), robust ($\\leq 5$% attack success rate), and scales to multiple features and clients.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "79",
        "title": "Towards Engineering Multi-Agent LLMs: A Protocol-Driven Approach",
        "author": [
            "Zhenyu Mao",
            "Jacky Keung",
            "Fengji Zhang",
            "Shuo Liu",
            "Yifei Wang",
            "Jialong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12120",
        "abstract": "The increasing demand for software development has driven interest in automating software engineering (SE) tasks using Large Language Models (LLMs). Recent efforts extend LLMs into multi-agent systems (MAS) that emulate collaborative development workflows, but these systems often fail due to three core deficiencies: under-specification, coordination misalignment, and inappropriate verification, arising from the absence of foundational SE structuring principles. This paper introduces Software Engineering Multi-Agent Protocol (SEMAP), a protocol-layer methodology that instantiates three core SE design principles for multi-agent LLMs: (1) explicit behavioral contract modeling, (2) structured messaging, and (3) lifecycle-guided execution with verification, and is implemented atop Google's Agent-to-Agent (A2A) infrastructure. Empirical evaluation using the Multi-Agent System Failure Taxonomy (MAST) framework demonstrates that SEMAP effectively reduces failures across different SE tasks. In code development, it achieves up to a 69.6% reduction in total failures for function-level development and 56.7% for deployment-level development. For vulnerability detection, SEMAP reduces failure counts by up to 47.4% on Python tasks and 28.2% on C/C++ tasks.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "80",
        "title": "Precise Attribute Intensity Control in Large Language Models via Targeted Representation Editing",
        "author": [
            "Rongzhi Zhang",
            "Liqin Ye",
            "Yuzhao Heng",
            "Xiang Chen",
            "Tong Yu",
            "Lingkai Kong",
            "Sudheer Chava",
            "Chao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12121",
        "abstract": "Precise attribute intensity control--generating Large Language Model (LLM) outputs with specific, user-defined attribute intensities--is crucial for AI systems adaptable to diverse user expectations. Current LLM alignment methods, however, typically provide only directional or open-ended guidance, failing to reliably achieve exact attribute intensities. We address this limitation with three key designs: (1) reformulating precise attribute intensity control as a target-reaching problem, rather than simple maximization; (2) training a lightweight value function via temporal-difference learning to predict final attribute intensity scores from partial generations, thereby steering LLM outputs; and (3) employing gradient-based interventions on hidden representations to navigate the model precisely towards specific attribute intensity targets. Our method enables fine-grained, continuous control over attribute intensities, moving beyond simple directional alignment. Experiments on LLaMA-3.2-3b and Phi-4-mini confirm our method's ability to steer text generation to user-specified attribute intensities with high accuracy. Finally, we demonstrate efficiency enhancements across three downstream tasks: preference data synthesis, Pareto frontier approximation and optimization, and distillation of aligned behaviors for intervention-free inference. Our code is available on https://github.com/Pre-Control/pre-control",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "81",
        "title": "Structure-aware Propagation Generation with Large Language Models for Fake News Detection",
        "author": [
            "Mengyang Chen",
            "Lingwei Wei",
            "Wei Zhou",
            "Songlin Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12125",
        "abstract": "The spread of fake news on social media poses a serious threat to public trust and societal stability. While propagation-based methods improve fake news detection by modeling how information spreads, they often suffer from incomplete propagation data. Recent work leverages large language models (LLMs) to generate synthetic propagation, but typically overlooks the structural patterns of real-world discussions. In this paper, we propose a novel structure-aware synthetic propagation enhanced detection (StruSP) framework to fully capture structural dynamics from real propagation. It enables LLMs to generate realistic and structurally consistent propagation for better detection. StruSP explicitly aligns synthetic propagation with real-world propagation in both semantic and structural dimensions. Besides, we also design a new bidirectional evolutionary propagation (BEP) learning strategy to better align LLMs with structural patterns of propagation in the real world via structure-aware hybrid sampling and masked propagation modeling objective. Experiments on three public datasets demonstrate that StruSP significantly improves fake news detection performance in various practical detection scenarios. Further analysis indicates that BEP enables the LLM to generate more realistic and diverse propagation semantically and structurally.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "82",
        "title": "MetaCaptioner: Towards Generalist Visual Captioning with Open-source Suites",
        "author": [
            "Zhenxin Lei",
            "Zhangwei Gao",
            "Changyao Tian",
            "Erfei Cui",
            "Guanzhou Chen",
            "Danni Yang",
            "Yuchen Duan",
            "Zhaokai Wang",
            "Wenhao Li",
            "Weiyun Wang",
            "Xiangyu Zhao",
            "Jiayi Ji",
            "Yu Qiao",
            "Wenhai Wang",
            "Gen Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12126",
        "abstract": "Generalist visual captioning goes beyond a simple appearance description task, but requires integrating a series of visual cues into a caption and handling various visual domains. In this task, current open-source models present a large performance gap with commercial ones, which limits various applications such as data synthesis. To bridge the gap, this paper proposes CapFlow, a novel multi-agent collaboration workflow. CapFlow demonstrates for the first time that, by capitalizing on open-source models, it is possible to achieve caption quality on par with GPT-4.1 in various domains with an 89.5% reduction in costs. By leveraging CapFlow as the data synthesizer, we produce high-quality visual captions from image and video domains at scale, and obtain a generalist visual captioner via fine-tuning, namely MetaCaptioner. Through extensive experiments, we show that MetaCaptioner not only achieves comparable captioning capabilities with commercial models but also reaches top-tier multimodal performance in the open-source community. We hope CapFlow and MetaCaptioner can benefit future multimodal research by providing a strong and cost-effective visual captioning solution.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "83",
        "title": "SafeMT: Multi-turn Safety for Multimodal Language Models",
        "author": [
            "Han Zhu",
            "Juntao Dai",
            "Jiaming Ji",
            "Haoran Li",
            "Chengkun Cai",
            "Pengcheng Wen",
            "Chi-Min Chan",
            "Boyuan Chen",
            "Yaodong Yang",
            "Sirui Han",
            "Yike Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12133",
        "abstract": "With the widespread use of multi-modal Large Language models (MLLMs), safety issues have become a growing concern. Multi-turn dialogues, which are more common in everyday interactions, pose a greater risk than single prompts; however, existing benchmarks do not adequately consider this situation. To encourage the community to focus on the safety issues of these models in multi-turn dialogues, we introduce SafeMT, a benchmark that features dialogues of varying lengths generated from harmful queries accompanied by images. This benchmark consists of 10,000 samples in total, encompassing 17 different scenarios and four jailbreak methods. Additionally, we propose Safety Index (SI) to evaluate the general safety of MLLMs during conversations. We assess the safety of 17 models using this benchmark and discover that the risk of successful attacks on these models increases as the number of turns in harmful dialogues rises. This observation indicates that the safety mechanisms of these models are inadequate for recognizing the hazard in dialogue interactions. We propose a dialogue safety moderator capable of detecting malicious intent concealed within conversations and providing MLLMs with relevant safety policies. Experimental results from several open-source models indicate that this moderator is more effective in reducing multi-turn ASR compared to existed guard models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "84",
        "title": "Credal Transformer: A Principled Approach for Quantifying and Mitigating Hallucinations in Large Language Models",
        "author": [
            "Shihao Ji",
            "Zihui Song",
            "Jiajie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12137",
        "abstract": "Large Language Models (LLMs) hallucinate, generating factually incorrect yet confident assertions. We argue this stems from the Transformer's Softmax function, which creates \"Artificial Certainty\" by collapsing ambiguous attention scores into a single probability distribution, discarding uncertainty information at each layer. To fix this, we introduce the Credal Transformer, which replaces standard attention with a Credal Attention Mechanism (CAM) based on evidential theory. CAM produces a \"credal set\" (a set of distributions) instead of a single attention vector, with the set's size directly measuring model uncertainty. We implement this by re-conceptualizing attention scores as evidence masses for a Dirichlet distribution: sufficient evidence recovers standard attention, while insufficient evidence yields a diffuse distribution, representing ambiguity. Empirically, the Credal Transformer identifies out-of-distribution inputs, quantifies ambiguity, and significantly reduces confident errors on unanswerable questions by abstaining. Our contribution is a new architecture to mitigate hallucinations and a design paradigm that integrates uncertainty quantification directly into the model, providing a foundation for more reliable AI.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "85",
        "title": "Self-Verifying Reflection Helps Transformers with CoT Reasoning",
        "author": [
            "Zhongwei Yu",
            "Wannian Xia",
            "Xue Yan",
            "Bo Xu",
            "Haifeng Zhang",
            "Yali Du",
            "Jun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12157",
        "abstract": "Advanced large language models (LLMs) frequently reflect in reasoning chain-of-thoughts (CoTs), where they self-verify the correctness of current solutions and explore alternatives. However, given recent findings that LLMs detect limited errors in CoTs, how reflection contributes to empirical improvements remains unclear. To analyze this issue, in this paper, we present a minimalistic reasoning framework to support basic self-verifying reflection for small transformers without natural language, which ensures analytic clarity and reduces the cost of comprehensive experiments. Theoretically, we prove that self-verifying reflection guarantees improvements if verification errors are properly bounded. Experimentally, we show that tiny transformers, with only a few million parameters, benefit from self-verification in both training and reflective execution, reaching remarkable LLM-level performance in integer multiplication and Sudoku. Similar to LLM results, we find that reinforcement learning (RL) improves in-distribution performance and incentivizes frequent reflection for tiny transformers, yet RL mainly optimizes shallow statistical patterns without faithfully reducing verification errors. In conclusion, integrating generative transformers with discriminative verification inherently facilitates CoT reasoning, regardless of scaling and natural language.",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "86",
        "title": "State Space Prompting via Gathering and Spreading Spatio-Temporal Information for Video Understanding",
        "author": [
            "Jiahuan Zhou",
            "Kai Zhu",
            "Zhenyu Cui",
            "Zichen Liu",
            "Xu Zou",
            "Gang Hua"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12160",
        "abstract": "Recently, pre-trained state space models have shown great potential for video classification, which sequentially compresses visual tokens in videos with linear complexity, thereby improving the processing efficiency of video data while maintaining high performance. To apply powerful pre-trained models to downstream tasks, prompt learning is proposed to achieve efficient downstream task adaptation with only a small number of fine-tuned parameters. However, the sequentially compressed visual prompt tokens fail to capture the spatial and temporal contextual information in the video, thus limiting the effective propagation of spatial information within a video frame and temporal information between frames in the state compression model and the extraction of discriminative information. To tackle the above issue, we proposed a State Space Prompting (SSP) method for video understanding, which combines intra-frame and inter-frame prompts to aggregate and propagate key spatiotemporal information in the video. Specifically, an Intra-Frame Gathering (IFG) module is designed to aggregate spatial key information within each frame. Besides, an Inter-Frame Spreading (IFS) module is designed to spread discriminative spatio-temporal information across different frames. By adaptively balancing and compressing key spatio-temporal information within and between frames, our SSP effectively propagates discriminative information in videos in a complementary manner. Extensive experiments on four video benchmark datasets verify that our SSP significantly outperforms existing SOTA methods by 2.76% on average while reducing the overhead of fine-tuning parameters.",
        "tags": [
            "SSMs"
        ]
    },
    {
        "id": "87",
        "title": "A Survey on Parallel Reasoning",
        "author": [
            "Ziqi Wang",
            "Boye Niu",
            "Zipeng Gao",
            "Zhi Zheng",
            "Tong Xu",
            "Linghui Meng",
            "Zhongli Li",
            "Jing Liu",
            "Yilong Chen",
            "Chen Zhu",
            "Hua Wu",
            "Haifeng Wang",
            "Enhong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12164",
        "abstract": "With the increasing capabilities of Large Language Models (LLMs), parallel reasoning has emerged as a new inference paradigm that enhances reasoning robustness by concurrently exploring multiple lines of thought before converging on a final answer. It has become a significant trend to explore parallel reasoning to overcome the fragility of standard sequential methods and improve practical performance. In this paper, we aim to survey and summarize the progress and challenges of parallel reasoning. We first present a formal definition of parallel reasoning and clarify its distinction from related concepts like Chain-of-Thought. Then, we organize and discuss advanced techniques based on a novel taxonomy, including non-interactive reasoning, interactive reasoning, and efficiency-focused decoding strategies. Additionally, we explore various application scenarios, such as solving complex problems and enhancing the reliability of LLM http://outputs.Finally, we highlight the core challenges of parallel reasoning and suggest potential directions for future research. We hope that our work can provide a useful roadmap for beginners and encourage more research on improving parallel reasoning methods. Related source can be avaliable in https://github.com/PPPP-kaqiu/Awesome-Parallel-Reasoning.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "88",
        "title": "Towards Inference-time Scaling for Continuous Space Reasoning",
        "author": [
            "Minghan Wang",
            "Thuy-Trang Vu",
            "Ehsan Shareghi",
            "Gholamreza Haffari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12167",
        "abstract": "Inference-time scaling through multiple sample generation in combination with Process- or Outcome-Reward Model (PRM or ORM) re-ranking has proven effective for text-based reasoning in large language models. This paper investigates whether such established techniques can be successfully adapted to reasoning in the continuous space, using COCONUT (Hao et al. 2024) continuous space reasoning LM as the backbone. We demonstrate the feasibility of generating diverse reasoning paths through dropout-based sampling. Our Pass@N analysis on the generated samples reveals the potential that could enable a significant gain in performance akin to observed gain in the discrete space. However, we highlight unique challenges faced for materializing this gain in the continuous thought space. In particular, working recipes for data generation and training PRM and ORM models in the discrete space unlocks only marginal improvements in the continuous space. Through probing various aspects including geometric properties and trajectory dynamics we identify the underlying reasons that prevent effective discrimination between correct and incorrect reasoning (essential for the functioning of PRM and ORM). Our findings reveal that current limitations stem from the absence of key inductive biases in continuous thought representations. We argue that the training frameworks for continuous reasoning LMs require not only to optimize for accuracy but also to explicitly incorporate inductive biases that could be utilized during inference-time for discrimination of correct and incorrect thoughts.\\footnote{Our code and data will be publicly available.}",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "89",
        "title": "MatSciBench: Benchmarking the Reasoning Ability of Large Language Models in Materials Science",
        "author": [
            "Junkai Zhang",
            "Jingru Gan",
            "Xiaoxuan Wang",
            "Zian Jia",
            "Changquan Gu",
            "Jianpeng Chen",
            "Yanqiao Zhu",
            "Mingyu Derek Ma",
            "Dawei Zhou",
            "Ling Li",
            "Wei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12171",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable abilities in scientific reasoning, yet their reasoning capabilities in materials science remain underexplored. To fill this gap, we introduce MatSciBench, a comprehensive college-level benchmark comprising 1,340 problems that span the essential subdisciplines of materials science. MatSciBench features a structured and fine-grained taxonomy that categorizes materials science questions into 6 primary fields and 31 sub-fields, and includes a three-tier difficulty classification based on the reasoning length required to solve each question. MatSciBench provides detailed reference solutions enabling precise error analysis and incorporates multimodal reasoning through visual contexts in numerous questions. Evaluations of leading models reveal that even the highest-performing model, Gemini-2.5-Pro, achieves under 80% accuracy on college-level materials science questions, highlighting the complexity of MatSciBench. Our systematic analysis of different reasoning strategie--basic chain-of-thought, tool augmentation, and self-correction--demonstrates that no single method consistently excels across all scenarios. We further analyze performance by difficulty level, examine trade-offs between efficiency and accuracy, highlight the challenges inherent in multimodal reasoning tasks, analyze failure modes across LLMs and reasoning methods, and evaluate the influence of retrieval-augmented generation. MatSciBench thus establishes a comprehensive and solid benchmark for assessing and driving improvements in the scientific reasoning capabilities of LLMs within the materials science domain.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "90",
        "title": "UniGS: Unified Geometry-Aware Gaussian Splatting for Multimodal Rendering",
        "author": [
            "Yusen Xie",
            "Zhenmin Huang",
            "Jianhao Jiao",
            "Dimitrios Kanoulas",
            "Jun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12174",
        "abstract": "In this paper, we propose UniGS, a unified map representation and differentiable framework for high-fidelity multimodal 3D reconstruction based on 3D Gaussian Splatting. Our framework integrates a CUDA-accelerated rasterization pipeline capable of rendering photo-realistic RGB images, geometrically accurate depth maps, consistent surface normals, and semantic logits simultaneously. We redesign the rasterization to render depth via differentiable ray-ellipsoid intersection rather than using Gaussian centers, enabling effective optimization of rotation and scale attribute through analytic depth gradients. Furthermore, we derive the analytic gradient formulation for surface normal rendering, ensuring geometric consistency among reconstructed 3D scenes. To improve computational and storage efficiency, we introduce a learnable attribute that enables differentiable pruning of Gaussians with minimal contribution during training. Quantitative and qualitative experiments demonstrate state-of-the-art reconstruction accuracy across all modalities, validating the efficacy of our geometry-aware paradigm. Source code and multimodal viewer will be available on GitHub.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "91",
        "title": "Audio Palette: A Diffusion Transformer with Multi-Signal Conditioning for Controllable Foley Synthesis",
        "author": [
            "Junnuo Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12175",
        "abstract": "Recent advances in diffusion-based generative models have enabled high-quality text-to-audio synthesis, but fine-grained acoustic control remains a significant challenge in open-source research. We present Audio Palette, a diffusion transformer (DiT) based model that extends the Stable Audio Open architecture to address this \"control gap\" in controllable audio generation. Unlike prior approaches that rely solely on semantic conditioning, Audio Palette introduces four time-varying control signals: loudness, pitch, spectral centroid, and timbre, for precise and interpretable manipulation of acoustic features. The model is efficiently adapted for the nuanced domain of Foley synthesis using Low-Rank Adaptation (LoRA) on a curated subset of AudioSet, requiring only 0.85 percent of the original parameters to be trained. Experiments demonstrate that Audio Palette achieves fine-grained, interpretable control of sound attributes. Crucially, it accomplishes this novel controllability while maintaining high audio quality and strong semantic alignment to text prompts, with performance on standard metrics such as Frechet Audio Distance (FAD) and LAION-CLAP scores remaining comparable to the original baseline model. We provide a scalable, modular pipeline for audio research, emphasizing sequence-based conditioning, memory efficiency, and a three-scale classifier-free guidance mechanism for nuanced inference-time control. This work establishes a robust foundation for controllable sound design and performative audio synthesis in open-source settings, enabling a more artist-centric workflow.",
        "tags": [
            "DiT",
            "Diffusion",
            "LoRA",
            "Transformer"
        ]
    },
    {
        "id": "92",
        "title": "CompoDistill: Attention Distillation for Compositional Reasoning in Multimodal LLMs",
        "author": [
            "Jiwan Kim",
            "Kibum Kim",
            "Sangwoo Seo",
            "Chanyoung Park"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12184",
        "abstract": "Recently, efficient Multimodal Large Language Models (MLLMs) have gained significant attention as a solution to their high computational complexity, making them more practical for real-world applications. In this regard, the knowledge distillation (KD) approach has emerged as a promising alternative, which transfers the rich visual and linguistic knowledge from a larger model (teacher) to a smaller model (student). However, we observe that existing KD methods struggle to effectively distill the teacher MLLM's rich visual perception abilities to the student, a challenge that has been largely overlooked in previous studies. Through a systematic analysis, we identify visual attention misalignment between student and teacher as the main cause of this issue. Based on this insight, we propose CompoDistill, a novel KD framework that explicitly aligns the student's visual attention with that of the teacher to enhance the student's visual perception abilities. Our extensive experiments show that CompoDistill significantly improves performance on compositional reasoning tasks that require visual perception abilities while maintaining strong performance on visual question answering tasks, as done in existing studies. Furthermore, CompoDistill demonstrates effectiveness with a more advanced backbone, highlighting its generalizability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "93",
        "title": "iCodeReviewer: Improving Secure Code Review with Mixture of Prompts",
        "author": [
            "Yun Peng",
            "Kisub Kim",
            "Linghan Meng",
            "Kui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12186",
        "abstract": "Code review is an essential process to ensure the quality of software that identifies potential software issues at an early stage of software development. Among all software issues, security issues are the most important to identify, as they can easily lead to severe software crashes and service disruptions. Recent research efforts have been devoted to automated approaches to reduce the manual efforts required in the secure code review process. Despite the progress, current automated approaches on secure code review, including static analysis, deep learning models, and prompting approaches, still face the challenges of limited precision and coverage, and a lack of comprehensive evaluation.\nTo mitigate these challenges, we propose iCodeReviewer, which is an automated secure code review approach based on large language models (LLMs). iCodeReviewer leverages a novel mixture-of-prompts architecture that incorporates many prompt experts to improve the coverage of security issues. Each prompt expert is a dynamic prompt pipeline to check the existence of a specific security issue. iCodeReviewer also implements an effective routing algorithm to activate only necessary prompt experts based on the code features in the input program, reducing the false positives induced by LLM hallucination. Experiment results in our internal dataset demonstrate the effectiveness of iCodeReviewer in security issue identification and localization with an F1 of 63.98%. The review comments generated by iCodeReviewer also achieve a high acceptance rate up to 84% when it is deployed in production environments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "94",
        "title": "Spatial two-grid compact difference scheme for two-dimensional nonlinear diffusion-wave equations with variable exponent",
        "author": [
            "Hao Zhang",
            "Kexin Li",
            "Wenlin Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12188",
        "abstract": "This paper presents a spatial two-grid (STG) compact difference scheme for a two-dimensional (2D) nonlinear diffusion-wave equation with variable exponent, which describes, e.g., the propagation of mechanical diffusive waves in viscoelastic media with varying material properties. Following the idea of the convolution approach, the diffusion-wave model is first transformed into an equivalent formulation. A fully discrete scheme is then developed by applying a compact difference approximation in space and combining the averaged product integration rule with linear interpolation quadrature in time. An efficient high-order two-grid algorithm is constructed by solving a small-scale nonlinear system on the coarse grid and a large-scale linearized system on the fine grid, where the bicubic spline interpolation operator is used to project coarse-grid solutions to the fine grid. Under mild assumptions on the variable exponent $\\alpha(t)$, the stability and convergence of the STG compact difference scheme are rigorously established. Numerical experiments are finally presented to verify the accuracy and efficiency of the proposed method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "95",
        "title": "Agent-Based Simulation of a Financial Market with Large Language Models",
        "author": [
            "Ryuji Hashimoto",
            "Takehiro Takayanagi",
            "Masahiro Suzuki",
            "Kiyoshi Izumi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12189",
        "abstract": "In real-world stock markets, certain chart patterns -- such as price declines near historical highs -- cannot be fully explained by fundamentals alone. These phenomena suggest the presence of path dependence in price formation, where investor decisions are influenced not only by current market conditions but also by the trajectory of prices leading up to the present. Path dependence has drawn attention in behavioral finance as a key mechanism behind such anomalies. One plausible driver of path dependence is human loss aversion, anchored to individual reference points like purchase prices or past peaks, which vary with personal context. However, capturing such subtle behavioral tendencies in traditional agent-based market simulations has remained a challenge. We propose the Fundamental-Chartist-LLM-Agent (FCLAgent), which uses large language models (LLMs) to emulate human-like trading decisions. In this framework, (1) buy/sell decisions are made by LLMs based on individual situations, while (2) order price and volume follow standard rule-based methods. Simulations show that FCLAgents reproduce path-dependent patterns that conventional agents fail to capture. Furthermore, an analysis of FCLAgents' behavior reveals that the reference points guiding loss aversion vary with market trajectories, highlighting the potential of LLM-based agents to model nuanced investor behavior.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "96",
        "title": "Hierarchical Reasoning with Vision-Language Models for Incident Reports from Dashcam Videos",
        "author": [
            "Shingo Yokoi",
            "Kento Sasaki",
            "Yu Yamaguchi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12190",
        "abstract": "Recent advances in end-to-end (E2E) autonomous driving have been enabled by training on diverse large-scale driving datasets, yet autonomous driving models still struggle in out-of-distribution (OOD) scenarios. The COOOL benchmark targets this gap by encouraging hazard understanding beyond closed taxonomies, and the 2COOOL challenge extends it to generating human-interpretable incident reports. We present a hierarchical reasoning framework for incident report generation from dashcam videos that integrates frame-level captioning, incident frame detection, and fine-grained reasoning within vision-language models (VLMs). We further improve factual accuracy and readability through model ensembling and a Blind A/B Scoring selection protocol. On the official 2COOOL open leaderboard, our method ranks 2nd among 29 teams and achieves the best CIDEr-D score, producing accurate and coherent incident narratives. These results indicate that hierarchical reasoning with VLMs is a promising direction for accident analysis and for broader understanding of safety-critical traffic events. The implementation and code are available at https://github.com/riron1206/kaggle-2COOOL-2nd-Place-Solution.",
        "tags": [
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "97",
        "title": "DPO-Tuned Large Language Models for Segmentation in Simultaneous Speech Translation",
        "author": [
            "Zeyu Yang",
            "Satoshi Nakamura"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12195",
        "abstract": "Simultaneous speech translation requires accurate segmentation to balance translation quality and latency. Recent studies such as SHAS have introduced pretrained segmentation models, achieving stronger performance than heuristic rules. However, segmentation models such as SHAS, though pretrained and more robust than heuristic methods, are still constrained by supervised learning objectives and do not incorporate human preference alignment, which is crucial for natural real-time interpretation. In this work, we propose a segmentation framework based on large language models (LLMs) trained with Direct Preference Optimization (DPO). By leveraging preference alignment, our method enables LLMs to predict natural segmentation points that better meet the demands of real-time translation. We evaluate the system on the ACL 60/60 corpus across three language pairs (English-Japanese, Chinese, German), using SeamlessM4T v2 as the translation backbone. Experimental results show that our DPO-tuned LLM achieves higher segmentation accuracy than SHAS and yields consistent improvements in translation quality (BLEU, COMET) as well as latency (Average Lagging). Furthermore, our system benefits from IWSLT baselines for direct comparison. These findings highlight the potential of preference-tuned LLMs to surpass existing pretrained segmentation models and advance adaptive, human-aligned simultaneous interpretation.",
        "tags": [
            "DPO",
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "98",
        "title": "Learning Social Navigation from Positive and Negative Demonstrations and Rule-Based Specifications",
        "author": [
            "Chanwoo Kim",
            "Jihwan Yoon",
            "Hyeonseong Kim",
            "Taemoon Jeong",
            "Changwoo Yoo",
            "Seungbeen Lee",
            "Soohwan Byeon",
            "Hoon Chung",
            "Matthew Pan",
            "Jean Oh",
            "Kyungjae Lee",
            "Sungjoon Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12215",
        "abstract": "Mobile robot navigation in dynamic human environments requires policies that balance adaptability to diverse behaviors with compliance to safety constraints. We hypothesize that integrating data-driven rewards with rule-based objectives enables navigation policies to achieve a more effective balance of adaptability and safety. To this end, we develop a framework that learns a density-based reward from positive and negative demonstrations and augments it with rule-based objectives for obstacle avoidance and goal reaching. A sampling-based lookahead controller produces supervisory actions that are both safe and adaptive, which are subsequently distilled into a compact student policy suitable for real-time operation with uncertainty estimates. Experiments in synthetic and elevator co-boarding simulations show consistent gains in success rate and time efficiency over baselines, and real-world demonstrations with human participants confirm the practicality of deployment. A video illustrating this work can be found on our project page https://chanwookim971024.github.io/PioneeR/.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "99",
        "title": "GOAT: A Training Framework for Goal-Oriented Agent with Tools",
        "author": [
            "Hyunji Min",
            "Sangwon Jung",
            "Junyoung Sung",
            "Dosung Lee",
            "Leekyeung Han",
            "Paul Hongsuck Seo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12218",
        "abstract": "Large language models (LLMs) have recently been extended beyond traditional text generation to serve as interactive agents capable of using external tools based on user intent. However, current LLM agents still show limited ability to handle goal-oriented queries, which require decomposing a high-level objective into multiple interdependent API calls with correct planning and execution. Current approaches mainly rely on zero-shot evaluation due to the absence of training data. While proprietary closed-source models such as GPT-4 demonstrate strong reasoning abilities, smaller open-source models struggle to perform complex tool use effectively. Thus, we propose a novel training framework GOAT, which enables fine-tuning of LLM agents in a human annotation-free setting. GOAT automatically constructs synthetic datasets of goal-oriented API execution tasks directly from given API documents, equipping models with the ability to reason over interdependent calls and generate coherent responses. Through extensive experiments, we show that GOAT-trained agents achieve state-of-the-art performance across multiple existing goal-oriented benchmarks. In addition, we introduce GOATBench, a new goal-oriented API execution benchmark, and demonstrate that agents trained with GOAT also excel in this setting. These results highlight GOAT as a practical path toward building robust open-source LLM agents capable of complex reasoning and tool use.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "100",
        "title": "Hierarchical Koopman Diffusion: Fast Generation with Interpretable Diffusion Trajectory",
        "author": [
            "Hanru Bai",
            "Weiyang Ding",
            "Difan Zou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12220",
        "abstract": "Diffusion models have achieved impressive success in high-fidelity image generation but suffer from slow sampling due to their inherently iterative denoising process. While recent one-step methods accelerate inference by learning direct noise-to-image mappings, they sacrifice the interpretability and fine-grained control intrinsic to diffusion dynamics, key advantages that enable applications like editable generation. To resolve this dichotomy, we introduce \\textbf{Hierarchical Koopman Diffusion}, a novel framework that achieves both one-step sampling and interpretable generative trajectories. Grounded in Koopman operator theory, our method lifts the nonlinear diffusion dynamics into a latent space where evolution is governed by globally linear operators, enabling closed-form trajectory solutions. This formulation not only eliminates iterative sampling but also provides full access to intermediate states, allowing manual intervention during generation. To model the multi-scale nature of images, we design a hierarchical architecture that disentangles generative dynamics across spatial resolutions via scale-specific Koopman subspaces, capturing coarse-to-fine details systematically. We empirically show that the Hierarchical Koopman Diffusion not only achieves competitive one-step generation performance but also provides a principled mechanism for interpreting and manipulating the generative process through spectral analysis. Our framework bridges the gap between fast sampling and interpretability in diffusion models, paving the way for explainable image synthesis in generative modeling.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "101",
        "title": "HoneyBee: Data Recipes for Vision-Language Reasoners",
        "author": [
            "Hritik Bansal",
            "Devandra Singh Sachan",
            "Kai-Wei Chang",
            "Aditya Grover",
            "Gargi Ghosh",
            "Wen-tau Yih",
            "Ramakanth Pasunuru"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12225",
        "abstract": "Recent advances in vision-language models (VLMs) have made them highly effective at reasoning tasks. However, the principles underlying the construction of performant VL reasoning training datasets remain poorly understood. In this work, we introduce several data curation approaches and study their impacts on VL reasoning capabilities by carefully controlling training and evaluation setups. We analyze the effects of context (image and question pair) sources, implement targeted data interventions, and explore scaling up images, questions, and chain-of-thought (CoT) solutions. Our findings reveal that (a) context source strategies significantly affect VLM performance, (b) interventions such as auxiliary signals from image captions and the inclusion of text-only reasoning yield substantial gains, and (c) scaling all data dimensions (e.g., unique questions per image and unique CoTs per image-question pair) consistently improves reasoning capability. Motivated by these insights, we introduce HoneyBee, a large-scale, high-quality CoT reasoning dataset with 2.5M examples consisting 350K image-question pairs. VLMs trained with HoneyBee outperform state-of-the-art models across model sizes. For instance, a HoneyBee-trained VLM with 3B parameters outperforms the SOTA model and the base model by 7.8% and 24.8%, respectively, on MathVerse. Furthermore, we propose a test-time scaling strategy that reduces decoding cost by 73% without sacrificing accuracy. Overall, this work presents improved strategies for VL reasoning dataset curation research.",
        "tags": [
            "CoT",
            "VLM"
        ]
    },
    {
        "id": "102",
        "title": "Analysing Moral Bias in Finetuned LLMs through Mechanistic Interpretability",
        "author": [
            "Bianca Raimondi",
            "Daniela Dalbagno",
            "Maurizio Gabbrielli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12229",
        "abstract": "Large language models (LLMs) have been shown to internalize human-like biases during finetuning, yet the mechanisms by which these biases manifest remain unclear. In this work, we investigated whether the well-known Knobe effect, a moral bias in intentionality judgements, emerges in finetuned LLMs and whether it can be traced back to specific components of the model. We conducted a Layer-Patching analysis across 3 open-weights LLMs and demonstrated that the bias is not only learned during finetuning but also localized in a specific set of layers. Surprisingly, we found that patching activations from the corresponding pretrained model into just a few critical layers is sufficient to eliminate the effect. Our findings offer new evidence that social biases in LLMs can be interpreted, localized, and mitigated through targeted interventions, without the need for model retraining.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "103",
        "title": "BIGFix: Bidirectional Image Generation with Token Fixing",
        "author": [
            "Victor Besnier",
            "David Hurych",
            "Andrei Bursuc",
            "Eduardo Valle"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12231",
        "abstract": "Recent advances in image and video generation have raised significant interest from both academia and industry. A key challenge in this field is improving inference efficiency, as model size and the number of inference steps directly impact the commercial viability of generative models while also posing fundamental scientific challenges. A promising direction involves combining auto-regressive sequential token modeling with multi-token prediction per step, reducing inference time by up to an order of magnitude. However, predicting multiple tokens in parallel can introduce structural inconsistencies due to token incompatibilities, as capturing complex joint dependencies during training remains challenging. Traditionally, once tokens are sampled, there is no mechanism to backtrack and refine erroneous predictions. We propose a method for self-correcting image generation by iteratively refining sampled tokens. We achieve this with a novel training scheme that injects random tokens in the context, improving robustness and enabling token fixing during sampling. Our method preserves the efficiency benefits of parallel token prediction while significantly enhancing generation quality. We evaluate our approach on image generation using the ImageNet-256 and CIFAR-10 datasets, as well as on video generation with UCF-101 and NuScenes, demonstrating substantial improvements across both modalities.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "104",
        "title": "PromptFlow: Training Prompts Like Neural Networks",
        "author": [
            "Jingyi Wang",
            "Hongyuan Zhu",
            "Ye Niu",
            "Yunhui Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12246",
        "abstract": "Large Language Models (LLMs) have demonstrated profound impact on Natural Language Processing (NLP) tasks. However, their effective deployment across diverse domains often require domain-specific adaptation strategies, as generic models may underperform when faced with specialized data distributions. Recent advances in prompt engineering (PE) offer a promising alternative to extensive retraining by refining input instructions to align LLM outputs with task objectives. This paradigm has emerged as a rapid and versatile approach for model fine-tuning. Despite its potential, manual prompt design remains labor-intensive and heavily depends on specialized expertise, often requiring iterative human effort to achieve optimal formulations. To address this limitation, automated prompt engineering methodologies have been developed to systematically generate task-specific prompts. However, current implementations predominantly employ static update rules and lack mechanisms for dynamic strategy selection, resulting in suboptimal adaptation to varying NLP task requirements. Furthermore, most methods treat and update the whole prompts at each step, without considering editing prompt sections at a finer granularity. At last, in particular, the problem of how to recycle experience in LLM is still underexplored. To this end, we propose the PromptFlow, a modular training framework inspired by TensorFlow, which integrates meta-prompts, operators, optimization, and evaluator. Our framework can be equipped with the latest optimization methods and autonomously explores optimal prompt refinement trajectories through gradient-based meta-learning, requiring minimal task-specific training data. Specifically, we devise a reinforcement learning method to recycle experience for LLM in the PE process. Finally, we conduct extensive experiments on various datasets, and demonstrate the effectiveness of PromptFlow.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "105",
        "title": "DSAS: A Universal Plug-and-Play Framework for Attention Optimization in Multi-Document Question Answering",
        "author": [
            "Jiakai Li",
            "Rongzheng Wang",
            "Yizhuo Ma",
            "Shuang Liang",
            "Guangchun Luo",
            "Ke Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12251",
        "abstract": "While large language models (LLMs) show considerable promise across various fields, they have notable limitations in handling multi-document question answering (Multi-doc QA) tasks. The first challenge is long-range dependency modeling, where LLMs struggle to focus on key information in long texts, which weakens important semantic connections. Second, most LLMs suffer from the ''lost-in-the-middle'' issue, where they have difficulty processing information in the middle of long inputs. Current solutions either truncate global dependencies or demand costly finetuning, ultimately lacking a universal and simple solution for these challenges. To resolve these limitations, we propose Dual-Stage Adaptive Sharpening (DSAS) containing two modules. (i) The Contextual Gate Weighting (CGW) module alleviates ''lost-in-the-middle'' by assessing paragraph relevance through layer-wise attention tracking and position-aware weighting. (ii) The Reciprocal Attention Suppression (RAS) module enhances focus on critical paragraphs by suppressing information exchange between key and irrelevant texts, thus mitigating the limitations in long-range dependency modeling. Notably, DSAS functions as a plug-and-play solution requiring no architectural modifications or extra training parameters. Extensive experiments on four benchmarks demonstrate DSAS's efficacy across mainstream LLMs (Llama, Qwen, Mistral, and Deepseek), with an average F1-score improvement of 4.2% in Multi-doc QA tasks on Llama-3.1-8B-Instruct and Qwen2.5-14B-Instruct. Ablation studies confirm the essential contributions of both the CGW and RAS modules. In addition, detailed discussions in the Appendix further validate the robustness and scalability of DSAS.",
        "tags": [
            "DeepSeek",
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "106",
        "title": "Diffusion Models for Reinforcement Learning: Foundations, Taxonomy, and Development",
        "author": [
            "Changfu Xu",
            "Jianxiong Guo",
            "Yuzhu Liang",
            "Haiyang Huang",
            "Haodong Zou",
            "Xi Zheng",
            "Shui Yu",
            "Xiaowen Chu",
            "Jiannong Cao",
            "Tian Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12253",
        "abstract": "Diffusion Models (DMs), as a leading class of generative models, offer key advantages for reinforcement learning (RL), including multi-modal expressiveness, stable training, and trajectory-level planning. This survey delivers a comprehensive and up-to-date synthesis of diffusion-based RL. We first provide an overview of RL, highlighting its challenges, and then introduce the fundamental concepts of DMs, investigating how they are integrated into RL frameworks to address key challenges in this research field. We establish a dual-axis taxonomy that organizes the field along two orthogonal dimensions: a function-oriented taxonomy that clarifies the roles DMs play within the RL pipeline, and a technique-oriented taxonomy that situates implementations across online versus offline learning regimes. We also provide a comprehensive examination of this progression from single-agent to multi-agent domains, thereby forming several frameworks for DM-RL integration and highlighting their practical utility. Furthermore, we outline several categories of successful applications of diffusion-based RL across diverse domains, discuss open research issues of current methodologies, and highlight key directions for future research to advance the field. Finally, we summarize the survey to identify promising future development directions. We are actively maintaining a GitHub repository (https://github.com/ChangfuXu/D4RL-FTD) for papers and other related resources to apply DMs for RL.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "107",
        "title": "FedMMKT:Co-Enhancing a Server Text-to-Image Model and Client Task Models in Multi-Modal Federated Learning",
        "author": [
            "Ningxin He",
            "Yang Liu",
            "Wei Sun",
            "Xiaozhou Ye",
            "Ye Ouyang",
            "Tiegang Gao",
            "Zehui Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12254",
        "abstract": "Text-to-Image (T2I) models have demonstrated their versatility in a wide range of applications. However, adaptation of T2I models to specialized tasks is often limited by the availability of task-specific data due to privacy concerns. On the other hand, harnessing the power of rich multimodal data from modern mobile systems and IoT infrastructures presents a great opportunity. This paper introduces Federated Multi-modal Knowledge Transfer (FedMMKT), a novel framework that enables co-enhancement of a server T2I model and client task-specific models using decentralized multimodal data without compromising data privacy.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "108",
        "title": "Vectorized Video Representation with Easy Editing via Hierarchical Spatio-Temporally Consistent Proxy Embedding",
        "author": [
            "Ye Chen",
            "Liming Tan",
            "Yupeng Zhu",
            "Yuanbin Wang",
            "Bingbing Ni"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12256",
        "abstract": "Current video representations heavily rely on unstable and over-grained priors for motion and appearance modelling, \\emph{i.e.}, pixel-level matching and tracking. A tracking error of just a few pixels would lead to the collapse of the visual object representation, not to mention occlusions and large motion frequently occurring in videos. To overcome the above mentioned vulnerability, this work proposes spatio-temporally consistent proxy nodes to represent dynamically changing objects/scenes in the video. On the one hand, the hierarchical proxy nodes have the ability to stably express the multi-scale structure of visual objects, so they are not affected by accumulated tracking error, long-term motion, occlusion, and viewpoint variation. On the other hand, the dynamic representation update mechanism of the proxy nodes adequately leverages spatio-temporal priors of the video to mitigate the impact of inaccurate trackers, thereby effectively handling drastic changes in scenes and objects. Additionally, the decoupled encoding manner of the shape and texture representations across different visual objects in the video facilitates controllable and fine-grained appearance editing capability. Extensive experiments demonstrate that the proposed representation achieves high video reconstruction accuracy with fewer parameters and supports complex video processing tasks, including video in-painting and keyframe-based temporally consistent video editing.",
        "tags": [
            "Video Editing"
        ]
    },
    {
        "id": "109",
        "title": "$\\mathbf{T^3}$: Reducing Belief Deviation in Reinforcement Learning for Active Reasoning",
        "author": [
            "Deyu Zou",
            "Yongqiang Chen",
            "Jianxiang Wang",
            "Haochen Yang",
            "Mufei Li",
            "James Cheng",
            "Pan Li",
            "Yu Gong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12264",
        "abstract": "Active reasoning requires large language models (LLMs) to interact with external sources and strategically gather information to solve problems. Central to this process is belief tracking: maintaining a coherent understanding of the problem state and the missing information toward the solution. However, due to limited reasoning capabilities, LLM-based agents often suffer from belief deviation: they struggle to correctly model beliefs, lose track of problem states, and fall into uninformative or repetitive actions. Once this happens, errors compound and reinforcement learning (RL) training fails to properly credit the crucial exploratory steps. To address this issue, we propose to track the deviation of model beliefs and develop $\\mathbf{T^3}$, a simple yet effective method that detects excessive belief deviation and truncates trajectories during training to remove uninformative tails. By preserving credit for informative prefixes, $\\mathbf{T^3}$ systematically improves policy optimization. Across 5 challenging tasks, $\\mathbf{T^3}$ consistently enhances training stability, token efficiency, and final performance, achieving up to 30% gains while cutting rollout tokens by roughly 25%. These results highlight belief control as a key principle for developing robust and generalizable LLM-based active reasoners.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "110",
        "title": "Human-in-the-Loop Bandwidth Estimation for Quality of Experience Optimization in Real-Time Video Communication",
        "author": [
            "Sami Khairy",
            "Gabriel Mittag",
            "Vishak Gopal",
            "Ross Cutler"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12265",
        "abstract": "The quality of experience (QoE) delivered by video conferencing systems is significantly influenced by accurately estimating the time-varying available bandwidth between the sender and receiver. Bandwidth estimation for real-time communications remains an open challenge due to rapidly evolving network architectures, increasingly complex protocol stacks, and the difficulty of defining QoE metrics that reliably improve user experience. In this work, we propose a deployed, human-in-the-loop, data-driven framework for bandwidth estimation to address these challenges. Our approach begins with training objective QoE reward models derived from subjective user evaluations to measure audio and video quality in real-time video conferencing systems. Subsequently, we collect roughly $1$M network traces with objective QoE rewards from real-world Microsoft Teams calls to curate a bandwidth estimation training dataset. We then introduce a novel distributional offline reinforcement learning (RL) algorithm to train a neural-network-based bandwidth estimator aimed at improving QoE for users. Our real-world A/B test demonstrates that the proposed approach reduces the subjective poor call ratio by $11.41\\%$ compared to the baseline bandwidth estimator. Furthermore, the proposed offline RL algorithm is benchmarked on D4RL tasks to demonstrate its generalization beyond bandwidth estimation.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "111",
        "title": "HiLoRA: Adaptive Hierarchical LoRA Routing for Training-Free Domain Generalization",
        "author": [
            "Ziyi Han",
            "Huanyu Wang",
            "Zeyu Zhang",
            "Xiangxiang Dai",
            "Xutong Liu",
            "John C.S. Lui"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12266",
        "abstract": "Low-Rank Adaptation (LoRA) has emerged as a widely used technique for adapting large language models (LLMs) to new domains, due to its modular design and broad availability on platforms such as HuggingFace. This availability has motivated efforts to reuse existing LoRAs for domain generalization.\nHowever, existing methods often rely on explicit task labels or additional training, which are impractical for deployment. Moreover, they typically activate a fixed number of entire LoRA modules, leading to parameter redundancy or insufficiency that degrade performance.\nIn this paper, we propose \\texttt{HiLoRA}, a training-free framework that performs adaptive hierarchical routing over LoRA pools. Drawing on structural properties of LoRA, we define rank-one components (ROCs), in which each rank parameter is regarded as an independent unit. For a given input sequence, \\texttt{HiLoRA} first adaptively selects a subset of LoRAs and determines their ROC allocation based on Gaussian likelihoods at the sequence level. At the token level, it further refines routing by activating only the most informative ROCs.\nWe further provide theoretical guarantees that \\texttt{HiLoRA} selects the most relevant LoRAs with high probability.\nExtensive experiments show that \\texttt{HiLoRA} achieves substantial improvements in domain generalization, with accuracy gains of up to {\\small $55\\%$} over state-of-the-art baselines, while maintaining comparable inference throughput.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "112",
        "title": "Heterogeneous RBCs via deep multi-agent reinforcement learning",
        "author": [
            "Federico Gabriele",
            "Aldo Glielmo",
            "Marco Taboga"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12272",
        "abstract": "Current macroeconomic models with agent heterogeneity can be broadly divided into two main groups. Heterogeneous-agent general equilibrium (GE) models, such as those based on Heterogeneous Agents New Keynesian (HANK) or Krusell-Smith (KS) approaches, rely on GE and 'rational expectations', somewhat unrealistic assumptions that make the models very computationally cumbersome, which in turn limits the amount of heterogeneity that can be modelled. In contrast, agent-based models (ABMs) can flexibly encompass a large number of arbitrarily heterogeneous agents, but typically require the specification of explicit behavioural rules, which can lead to a lengthy trial-and-error model-development process. To address these limitations, we introduce MARL-BC, a framework that integrates deep multi-agent reinforcement learning (MARL) with Real Business Cycle (RBC) models. We demonstrate that MARL-BC can: (1) recover textbook RBC results when using a single agent; (2) recover the results of the mean-field KS model using a large number of identical agents; and (3) effectively simulate rich heterogeneity among agents, a hard task for traditional GE approaches. Our framework can be thought of as an ABM if used with a variety of heterogeneous interacting agents, and can reproduce GE results in limit cases. As such, it is a step towards a synthesis of these often opposed modelling paradigms.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "113",
        "title": "TFGA-Net: Temporal-Frequency Graph Attention Network for Brain-Controlled Speaker Extraction",
        "author": [
            "Youhao Si",
            "Yuan Liao",
            "Qiushi Han",
            "Yuhang Yang",
            "Rui Dai",
            "Liya Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12275",
        "abstract": "The rapid development of auditory attention decoding (AAD) based on electroencephalography (EEG) signals offers the possibility EEG-driven target speaker extraction. However, how to effectively utilize the target-speaker common information between EEG and speech remains an unresolved problem. In this paper, we propose a model for brain-controlled speaker extraction, which utilizes the EEG recorded from the listener to extract the target speech. In order to effectively extract information from EEG signals, we derive multi-scale time--frequency features and further incorporate cortical topological structures that are selectively engaged during the task. Moreover, to effectively exploit the non-Euclidean structure of EEG signals and capture their global features, the graph convolutional networks and self-attention mechanism are used in the EEG encoder. In addition, to make full use of the fused EEG and speech feature and preserve global context and capture speech rhythm and prosody, we introduce MossFormer2 which combines MossFormer and RNN-Free Recurrent as separator. Experimental results on both the public Cocktail Party and KUL dataset in this paper show that our TFGA-Net model significantly outper-forms the state-of-the-art method in certain objective evaluation metrics. The source code is available at: https://github.com/LaoDa-X/TFGA-NET.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "114",
        "title": "Spatial Forcing: Implicit Spatial Representation Alignment for Vision-language-action Model",
        "author": [
            "Fuhao Li",
            "Wenxuan Song",
            "Han Zhao",
            "Jingbo Wang",
            "Pengxiang Ding",
            "Donglin Wang",
            "Long Zeng",
            "Haoang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12276",
        "abstract": "Vision-language-action (VLA) models have recently shown strong potential in enabling robots to follow language instructions and execute precise actions. However, most VLAs are built upon vision-language models pretrained solely on 2D data, which lack accurate spatial awareness and hinder their ability to operate in the 3D physical world. Existing solutions attempt to incorporate explicit 3D sensor inputs such as depth maps or point clouds, but these approaches face challenges due to sensor noise, hardware heterogeneity, and incomplete depth coverage in existing datasets. Alternative methods that estimate 3D cues from 2D images also suffer from the limited performance of depth http://estimators.We propose Spatial Forcing (SF), a simple yet effective alignment strategy that implicitly forces VLA models to develop spatial comprehension capabilities without relying on explicit 3D inputs or depth estimators. SF aligns intermediate visual embeddings of VLAs with geometric representations produced by pretrained 3D foundation models. By enforcing alignment at intermediate layers, SF guides VLAs to encode richer spatial representations that enhance action http://precision.Extensive experiments in simulation and real-world environments demonstrate that SF achieves state-of-the-art results, surpassing both 2D- and 3D-based VLAs. SF further accelerates training by up to 3.8x and improves data efficiency across diverse robotic tasks. Project page is at https://spatial-forcing.github.io/",
        "tags": [
            "3D",
            "VLM"
        ]
    },
    {
        "id": "115",
        "title": "PAGS: Priority-Adaptive Gaussian Splatting for Dynamic Driving Scenes",
        "author": [
            "Ying A",
            "Wenzhang Sun",
            "Chang Zeng",
            "Chunfeng Wang",
            "Hao Li",
            "Jianxun Cui"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12282",
        "abstract": "Reconstructing dynamic 3D urban scenes is crucial for autonomous driving, yet current methods face a stark trade-off between fidelity and computational cost. This inefficiency stems from their semantically agnostic design, which allocates resources uniformly, treating static backgrounds and safety-critical objects with equal importance. To address this, we introduce Priority-Adaptive Gaussian Splatting (PAGS), a framework that injects task-aware semantic priorities directly into the 3D reconstruction and rendering pipeline. PAGS introduces two core contributions: (1) Semantically-Guided Pruning and Regularization strategy, which employs a hybrid importance metric to aggressively simplify non-critical scene elements while preserving fine-grained details on objects vital for navigation. (2) Priority-Driven Rendering pipeline, which employs a priority-based depth pre-pass to aggressively cull occluded primitives and accelerate the final shading computations. Extensive experiments on the Waymo and KITTI datasets demonstrate that PAGS achieves exceptional reconstruction quality, particularly on safety-critical objects, while significantly reducing training time and boosting rendering speeds to over 350 FPS.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "116",
        "title": "Dual Learning with Dynamic Knowledge Distillation and Soft Alignment for Partially Relevant Video Retrieval",
        "author": [
            "Jianfeng Dong",
            "Lei Huang",
            "Daizong Liu",
            "Xianke Chen",
            "Xun Yang",
            "Changting Lin",
            "Xun Wang",
            "Meng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12283",
        "abstract": "Almost all previous text-to-video retrieval works ideally assume that videos are pre-trimmed with short durations containing solely text-related content. However, in practice, videos are typically untrimmed in long durations with much more complicated background content. Therefore, in this paper, we focus on the more practical yet challenging task of Partially Relevant Video Retrieval (PRVR), which aims to retrieve partially relevant untrimmed videos with the given query. To tackle this task, we propose a novel framework that distills generalization knowledge from a powerful large-scale vision-language pre-trained model and transfers it to a lightweight, task-specific PRVR network. Specifically, we introduce a Dual Learning framework with Dynamic Knowledge Distillation (DL-DKD++), where a large teacher model provides supervision to a compact dual-branch student network. The student model comprises two branches: an inheritance branch that absorbs transferable knowledge from the teacher, and an exploration branch that learns task-specific information from the PRVR dataset to address domain gaps. To further enhance learning, we incorporate a dynamic soft-target construction mechanism. By replacing rigid hard-target supervision with adaptive soft targets that evolve during training, our method enables the model to better capture the fine-grained, partial relevance between videos and queries. Experiment results demonstrate that our proposed model achieves state-of-the-art performance on TVR, ActivityNet, and Charades-STA datasets for PRVR. The code is available at https://github.com/HuiGuanLab/DL-DKD.",
        "tags": [
            "Text-to-Video"
        ]
    },
    {
        "id": "117",
        "title": "Chinese ModernBERT with Whole-Word Masking",
        "author": [
            "Zeyu Zhao",
            "Ningtao Wang",
            "Xing Fu",
            "Yu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12285",
        "abstract": "Encoder-only Transformers have advanced along three axes -- architecture, data, and systems -- yielding Pareto gains in accuracy, speed, and memory efficiency. Yet these improvements have not fully transferred to Chinese, where tokenization and morphology differ markedly from English. We introduce Chinese ModernBERT, a from-scratch Chinese encoder that couples: (i) a hardware-aware 32k BPE vocabulary tailored to frequent Chinese affixes/compounds, lowering the embedding budget; (ii) whole-word masking (WWM) with a dynamic masking curriculum (30% -> 15%) to align task difficulty with training progress; (iii) a two-stage pre-training pipeline that extends the native context from 1,024 to 8,192 tokens using RoPE and alternating local/global attention; and (iv) a damped-cosine learning-rate schedule for stable long-horizon optimization. We pre-train on ~1.2T Chinese tokens from CCI3-HQ, CCI4 (Chinese), and Cosmopedia-Chinese. On CLUE, Chinese ModernBERT is competitive with strong Chinese encoders under a unified fine-tuning protocol. Under bf16 it achieves high long-sequence throughput while maintaining strong short-sequence speed, reflecting benefits from budget allocation and attention design. To probe retrieval-oriented quality, we add a small amount of open contrastive data: fine-tuning on SimCLUE (~3M pairs) improves further when adding T2Ranking (~2M), reaching 0.505 (Pearson) / 0.537 (Spearman) on the SimCLUE test set. Under this open-data setting, Chinese ModernBERT surpasses Qwen-0.6B-embedding on SimCLUE, suggesting a clear scaling path for STS with additional curated pairs. We will release tokenizer and weights to facilitate reproducible research.",
        "tags": [
            "Qwen",
            "RoPE"
        ]
    },
    {
        "id": "118",
        "title": "Vision Language Models Map Logos to Text via Semantic Entanglement in the Visual Projector",
        "author": [
            "Sifan Li",
            "Hongkai Chen",
            "Yujun Cai",
            "Qingwen Ye",
            "Liyang Chen",
            "Junsong Yuan",
            "Yiwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12287",
        "abstract": "Vision Language Models (VLMs) have achieved impressive progress in multimodal reasoning; yet, they remain vulnerable to hallucinations, where outputs are not grounded in visual evidence. In this paper, we investigate a previously overlooked setting: logo hallucination, where models generate brand names or textual content despite logos containing no visible words. Using curated splits of pure symbols, hybrids, and text-bearing logos, as well as the challenging Hard-60 subset, we systematically measure hallucination across leading VLMs. We further probe robustness through nine structured perturbations and show that hallucinations persist even under strong distortions, with occlusion exposing the sharpest weaknesses. Embedding-level analysis with open-weight LLaVA demonstrates that hallucination is tied to a small subset of projector dimensions, and targeted ablation substantially reduces errors while preserving OCR accuracy. Together, these findings reveal that VLMs often rely on symbolic priors rather than genuine glyph perception, particularly for iconic circular logos, and that projector subspaces play a decisive role in this failure mode. Our work contributes both a novel diagnostic lens and actionable mitigation insights, highlighting projector disentanglement and OCR-guided decoding as promising directions for building more trustworthy multimodal systems.",
        "tags": [
            "LLaVA",
            "VLM"
        ]
    },
    {
        "id": "119",
        "title": "Show Your Title! A Scoping Review on Verbalization in Software Engineering with LLM-Assisted Screening",
        "author": [
            "GergÅ Balogh",
            "DÃ¡vid KÃ³szÃ³",
            "Homayoun Safarpour Motealegh Mahalegi",
            "LÃ¡szlÃ³ TÃ³th",
            "Bence SzakÃ¡cs",
            "Ãron BÃºcsÃº"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12294",
        "abstract": "Understanding how software developers think, make decisions, and behave remains a key challenge in software engineering (SE). Verbalization techniques (methods that capture spoken or written thought processes) offer a lightweight and accessible way to study these cognitive aspects. This paper presents a scoping review of research at the intersection of SE and psychology (PSY), focusing on the use of verbal data. To make large-scale interdisciplinary reviews feasible, we employed a large language model (LLM)-assisted screening pipeline using GPT to assess the relevance of over 9,000 papers based solely on titles. We addressed two questions: what themes emerge from verbalization-related work in SE, and how effective are LLMs in supporting interdisciplinary review processes? We validated GPT's outputs against human reviewers and found high consistency, with a 13\\% disagreement rate. Prominent themes mainly were tied to the craft of SE, while more human-centered topics were underrepresented. The data also suggests that SE frequently draws on PSY methods, whereas the reverse is rare.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "120",
        "title": "An Empirical Study for Representations of Videos in Video Question Answering via MLLMs",
        "author": [
            "Zhi Li",
            "Yanan Wang",
            "Hao Niu",
            "Julio Vizcarra",
            "Masato Taya"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12299",
        "abstract": "Multimodal large language models have recently achieved remarkable progress in video question answering (VideoQA) by jointly processing visual, textual, and audio information. However, it remains unclear which video representations are most effective for MLLMs, and how different modalities balance task accuracy against computational efficiency. In this work, we present a comprehensive empirical study of video representation methods for VideoQA with MLLMs. We systematically evaluate single modality inputs question only, subtitles, visual frames, and audio signals as well as multimodal combinations, on two widely used benchmarks: VideoMME and LongVideoBench. Our results show that visual frames substantially enhance accuracy but impose heavy costs in GPU memory and inference latency, while subtitles provide a lightweight yet effective alternative, particularly for long videos. These findings highlight clear trade-offs between effectiveness and efficiency and provide practical insights for designing resource-aware MLLM-based VideoQA systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "121",
        "title": "A large-scale, unsupervised pipeline for automatic corpus annotation using LLMs: variation and change in the English consider construction",
        "author": [
            "Cameron Morin",
            "Matti Marttinen Larsson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12306",
        "abstract": "As natural language corpora expand at an unprecedented rate, manual annotation remains a significant methodological bottleneck in corpus linguistic work. We address this challenge by presenting a scalable, unsupervised pipeline for automating grammatical annotation in voluminous corpora using large language models (LLMs). Unlike previous supervised and iterative approaches, our method employs a four-phase workflow: prompt engineering, pre-hoc evaluation, automated batch processing, and post-hoc validation. We demonstrate the pipeline's accessibility and effectiveness through a diachronic case study of variation in the English consider construction. Using GPT-5 through the OpenAI API, we annotate 143,933 sentences from the Corpus of Historical American English (COHA) in under 60 hours, achieving 98%+ accuracy on two sophisticated annotation procedures. Our results suggest that LLMs can perform a range of data preparation tasks at scale with minimal human intervention, opening new possibilities for corpus-based research, though implementation requires attention to costs, licensing, and other ethical considerations.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "122",
        "title": "Fully mixed virtual element schemes for steady-state poroelastic stress-assisted diffusion",
        "author": [
            "Isaac Bermudez",
            "Bryan Gomez-Vargas",
            "Andres E. Rubiano",
            "Ricardo Ruiz-Baier"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12307",
        "abstract": "We propose a fully mixed virtual element method for the numerical approximation of the coupling between stress-altered diffusion and linear elasticity equations with strong symmetry of total poroelastic stress (using the Hellinger--Reissner principle). A novelty of this work is that we introduce a less restrictive assumption on the stress-assisted diffusion coefficient, requiring an analysis of the perturbed diffusion equation using Banach spaces. The solvability of the continuous and discrete problems is established using a suitable modification of the abstract theory for perturbed saddle-point problems in Banach spaces (which is in itself a new result of independent interest). In addition, we establish optimal a priori error estimates. The method and its analysis are robust with respect to the poromechanical parameters. We also include a number of numerical examples that illustrate the properties of the proposed scheme.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "123",
        "title": "Hybrid Gaussian Splatting for Novel Urban View Synthesis",
        "author": [
            "Mohamed Omran",
            "Farhad Zanjani",
            "Davide Abati",
            "Jens Petersen",
            "Amirhossein Habibian"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12308",
        "abstract": "This paper describes the Qualcomm AI Research solution to the RealADSim-NVS challenge, hosted at the RealADSim Workshop at ICCV 2025. The challenge concerns novel view synthesis in street scenes, and participants are required to generate, starting from car-centric frames captured during some training traversals, renders of the same urban environment as viewed from a different traversal (e.g. different street lane or car direction). Our solution is inspired by hybrid methods in scene generation and generative simulators merging gaussian splatting and diffusion models, and it is composed of two stages: First, we fit a 3D reconstruction of the scene and render novel views as seen from the target cameras. Then, we enhance the resulting frames with a dedicated single-step diffusion model. We discuss specific choices made in the initialization of gaussian primitives as well as the finetuning of the enhancer model and its training data curation. We report the performance of our model design and we ablate its components in terms of novel view quality as measured by PSNR, SSIM and LPIPS. On the public leaderboard reporting test results, our proposal reaches an aggregated score of 0.432, achieving the second place overall.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "124",
        "title": "Beating Harmful Stereotypes Through Facts: RAG-based Counter-speech Generation",
        "author": [
            "Greta Damo",
            "Elena Cabrio",
            "Serena Villata"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12316",
        "abstract": "Counter-speech generation is at the core of many expert activities, such as fact-checking and hate speech, to counter harmful content. Yet, existing work treats counter-speech generation as pure text generation task, mainly based on Large Language Models or NGO experts. These approaches show severe drawbacks due to the limited reliability and coherence in the generated countering text, and in scalability, respectively. To close this gap, we introduce a novel framework to model counter-speech generation as knowledge-wise text generation process. Our framework integrates advanced Retrieval-Augmented Generation (RAG) pipelines to ensure the generation of trustworthy counter-speech for 8 main target groups identified in the hate speech literature, including women, people of colour, persons with disabilities, migrants, Muslims, Jews, LGBT persons, and other. We built a knowledge base over the United Nations Digital Library, EUR-Lex and the EU Agency for Fundamental Rights, comprising a total of 32,792 texts. We use the MultiTarget-CONAN dataset to empirically assess the quality of the generated counter-speech, both through standard metrics (i.e., JudgeLM) and a human evaluation. Results show that our framework outperforms standard LLM baselines and competitive approach, on both assessments. The resulting framework and the knowledge base pave the way for studying trustworthy and sound counter-speech generation, in hate speech and beyond.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "125",
        "title": "RAG-Anything: All-in-One RAG Framework",
        "author": [
            "Zirui Guo",
            "Xubin Ren",
            "Lingrui Xu",
            "Jiahao Zhang",
            "Chao Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12323",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a fundamental paradigm for expanding Large Language Models beyond their static training limitations. However, a critical misalignment exists between current RAG capabilities and real-world information environments. Modern knowledge repositories are inherently multimodal, containing rich combinations of textual content, visual elements, structured tables, and mathematical expressions. Yet existing RAG frameworks are limited to textual content, creating fundamental gaps when processing multimodal documents. We present RAG-Anything, a unified framework that enables comprehensive knowledge retrieval across all modalities. Our approach reconceptualizes multimodal content as interconnected knowledge entities rather than isolated data types. The framework introduces dual-graph construction to capture both cross-modal relationships and textual semantics within a unified representation. We develop cross-modal hybrid retrieval that combines structural knowledge navigation with semantic matching. This enables effective reasoning over heterogeneous content where relevant evidence spans multiple modalities. RAG-Anything demonstrates superior performance on challenging multimodal benchmarks, achieving significant improvements over state-of-the-art methods. Performance gains become particularly pronounced on long documents where traditional approaches fail. Our framework establishes a new paradigm for multimodal knowledge access, eliminating the architectural fragmentation that constrains current systems. Our framework is open-sourced at: https://github.com/HKUDS/RAG-Anything.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "126",
        "title": "Finite-time Convergence Analysis of Actor-Critic with Evolving Reward",
        "author": [
            "Rui Hu",
            "Yu Chen",
            "Longbo Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12334",
        "abstract": "Many popular practical reinforcement learning (RL) algorithms employ evolving reward functions-through techniques such as reward shaping, entropy regularization, or curriculum learning-yet their theoretical foundations remain underdeveloped. This paper provides the first finite-time convergence analysis of a single-timescale actor-critic algorithm in the presence of an evolving reward function under Markovian sampling. We consider a setting where the reward parameters may change at each time step, affecting both policy optimization and value estimation. Under standard assumptions, we derive non-asymptotic bounds for both actor and critic errors. Our result shows that an $O(1/\\sqrt{T})$ convergence rate is achievable, matching the best-known rate for static rewards, provided the reward parameters evolve slowly enough. This rate is preserved when the reward is updated via a gradient-based rule with bounded gradient and on the same timescale as the actor and critic, offering a theoretical foundation for many popular RL techniques. As a secondary contribution, we introduce a novel analysis of distribution mismatch under Markovian sampling, improving the best-known rate by a factor of $\\log^2T$ in the static-reward case.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "127",
        "title": "Physics-Informed Reinforcement Learning for Large-Scale EV Smart Charging Considering Distribution Network Voltage Constraints",
        "author": [
            "Stavros Orfanoudakis",
            "Frans Oliehoek",
            "Peter Palesnky",
            "Pedro P. Vergara"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12335",
        "abstract": "Electric Vehicles (EVs) offer substantial flexibility for grid services, yet large-scale, uncoordinated charging can threaten voltage stability in distribution networks. Existing Reinforcement Learning (RL) approaches for smart charging often disregard physical grid constraints or have limited performance for complex large-scale tasks, limiting their scalability and real-world applicability. This paper introduces a physics-informed (PI) RL algorithm that integrates a differentiable power flow model and voltage-based reward design into the Twin Delayed Deep Deterministic Policy Gradient (TD3) algorithm, enabling EVs to deliver real-time voltage support while meeting user demands. The resulting PI-TD3 algorithm achieves faster convergence, improved sample efficiency, and reliable voltage magnitude regulation under uncertain and overloaded conditions. Benchmarks on the IEEE 34-bus and 123-bus networks show that the proposed PI-TD3 outperforms both model-free RL and optimization-based baselines in grid constraint management, user satisfaction, and economic metrics, even as the system scales to hundreds of EVs. These advances enable robust, scalable, and practical EV charging strategies that enhance grid resilience and support distribution networks operation.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "128",
        "title": "Achieving Meaningful Collaboration: Worker-centered Design of a Physical Human-Robot Collaborative Blending Task",
        "author": [
            "Nicky Mol",
            "Luka Peternel",
            "Alessandro Ianniello",
            "Denis Zatyagov",
            "Auke Nachenius",
            "Stephan Balvert",
            "J. Micah Prendergast",
            "Sara Muscolo",
            "Olger Siebinga",
            "Eva Verhoef",
            "Deborah Forster",
            "David A. Abbink"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12340",
        "abstract": "The use of robots in industrial settings continues to grow, driven by the need to address complex societal challenges such as labor shortages, aging populations, and ever-increasing production demands. In this abstract, we advocate for (and demonstrate) a transdisciplinary approach when considering robotics in the workplace. Transdisciplinarity emphasizes the integration of academic research with pragmatic expertise and embodied experiential knowledge, that prioritize values such as worker wellbeing and job attractiveness. In the following, we describe an ongoing multi-pronged effort to explore the potential of collaborative robots in the context of airplane engine repair and maintenance operations.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "129",
        "title": "Traveling Salesman-Based Token Ordering Improves Stability in Homomorphically Encrypted Language Models",
        "author": [
            "Donghwan Rho",
            "Sieun Seo",
            "Hyewon Sung",
            "Chohong Min",
            "Ernest K. Ryu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12343",
        "abstract": "As users increasingly interact with large language models (LLMs) using private information, secure and encrypted communication becomes essential. Homomorphic encryption (HE) provides a principled solution by enabling computation directly on encrypted data. Although prior work has explored aspects of running LLMs under HE, the challenge of text generation, particularly next-token prediction, has received limited attention and remains a key obstacle to practical encrypted interaction. In this work, we propose a TSP-based token reordering strategy to address the difficulties of encrypted text generation, together with a post-processing step that further reduces approximation error. Theoretical analysis and experimental results demonstrate that our method prevents collapse, improves coherence in generated text, and preserves data privacy throughout. Overall, our contributions advance the feasibility of practical and privacy-preserving LLM inference.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "130",
        "title": "PolygMap: A Perceptive Locomotion Framework for Humanoid Robot Stair Climbing",
        "author": [
            "Bingquan Li",
            "Ning Wang",
            "Tianwei Zhang",
            "Zhicheng He",
            "Yucong Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12346",
        "abstract": "Recently, biped robot walking technology has been significantly developed, mainly in the context of a bland walking scheme. To emulate human walking, robots need to step on the positions they see in unknown spaces accurately. In this paper, we present PolyMap, a perception-based locomotion planning framework for humanoid robots to climb stairs. Our core idea is to build a real-time polygonal staircase plane semantic map, followed by a footstep planar using these polygonal plane segments. These plane segmentation and visual odometry are done by multi-sensor fusion(LiDAR, RGB-D camera and IMUs). The proposed framework is deployed on a NVIDIA Orin, which performs 20-30 Hz whole-body motion planning output. Both indoor and outdoor real-scene experiments indicate that our method is efficient and robust for humanoid robot stair climbing.",
        "tags": [
            "Robotics",
            "Segmentation"
        ]
    },
    {
        "id": "131",
        "title": "O-Forge: An LLM + Computer Algebra Framework for Asymptotic Analysis",
        "author": [
            "Ayush Khaitan",
            "Vijay Ganesh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12350",
        "abstract": "Large language models have recently demonstrated advanced capabilities in solving IMO and Putnam problems; yet their role in research mathematics has remained fairly limited. The key difficulty is verification: suggested proofs may look plausible, but cannot be trusted without rigorous checking. We present a framework, called LLM+CAS, and an associated tool, O-Forge, that couples frontier LLMs with a computer algebra systems (CAS) in an In-Context Symbolic Feedback loop to produce proofs that are both creative and symbolically verified. Our focus is on asymptotic inequalities, a topic that often involves difficult proofs and appropriate decomposition of the domain into the \"right\" subdomains. Many mathematicians, including Terry Tao, have suggested that using AI tools to find the right decompositions can be very useful for research-level asymptotic analysis. In this paper, we show that our framework LLM+CAS turns out to be remarkably effective at proposing such decompositions via a combination of a frontier LLM and a CAS. More precisely, we use an LLM to suggest domain decomposition, and a CAS (such as Mathematica) that provides a verification of each piece axiomatically. Using this loop, we answer a question posed by Terence Tao: whether LLMs coupled with a verifier can be used to help prove intricate asymptotic inequalities. More broadly, we show how AI can move beyond contest math towards research-level tools for professional mathematicians.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "132",
        "title": "Fine-grained Analysis of Brain-LLM Alignment through Input Attribution",
        "author": [
            "Michela Proietti",
            "Roberto Capobianco",
            "Mariya Toneva"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12355",
        "abstract": "Understanding the alignment between large language models (LLMs) and human brain activity can reveal computational principles underlying language processing. We introduce a fine-grained input attribution method to identify the specific words most important for brain-LLM alignment, and leverage it to study a contentious research question about brain-LLM alignment: the relationship between brain alignment (BA) and next-word prediction (NWP). Our findings reveal that BA and NWP rely on largely distinct word subsets: NWP exhibits recency and primacy biases with a focus on syntax, while BA prioritizes semantic and discourse-level information with a more targeted recency effect. This work advances our understanding of how LLMs relate to human language processing and highlights differences in feature reliance between BA and NWP. Beyond this study, our attribution method can be broadly applied to explore the cognitive relevance of model predictions in diverse language processing tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "133",
        "title": "MoBiLE: Efficient Mixture-of-Experts Inference on Consumer GPU with Mixture of Big Little Experts",
        "author": [
            "Yushu Zhao",
            "Yubin Qin",
            "Yang Wang",
            "Xiaolong Yang",
            "Huiming Han",
            "Shaojun Wei",
            "Yang Hu",
            "Shouyi Yin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12357",
        "abstract": "Mixture-of-Experts (MoE) models have recently demonstrated exceptional performance across a diverse range of applications. The principle of sparse activation in MoE models facilitates an offloading strategy, wherein active experts are maintained in GPU HBM, while inactive experts are stored in CPU DRAM. The efficacy of this approach, however, is fundamentally constrained by the limited bandwidth of the CPU-GPU interconnect. To mitigate this bottleneck, existing approaches have employed prefetching to accelerate MoE inference. These methods attempt to predict and prefetch the required experts using specially trained modules. Nevertheless, such techniques are often encumbered by significant training overhead and have shown diminished effectiveness on recent MoE models with fine-grained expert segmentation.\nIn this paper, we propose MoBiLE, a plug-and-play offloading-based MoE inference framework with \\textit{mixture of big-little experts}. It reduces the number of experts for unimportant tokens to half for acceleration while maintaining full experts for important tokens to guarantee model quality. Further, a dedicated fallback and prefetching mechanism is designed for switching between little and big experts to improve memory efficiency. We evaluate MoBiLE on four typical modern MoE architectures and challenging generative tasks. Our results show that MoBiLE achieves a speedup of 1.60x to 1.72x compared to the baseline on a consumer GPU system, with negligible degradation in accuracy.",
        "tags": [
            "MoE",
            "Segmentation"
        ]
    },
    {
        "id": "134",
        "title": "CurriFlow: Curriculum-Guided Depth Fusion with Optical Flow-Based Temporal Alignment for 3D Semantic Scene Completion",
        "author": [
            "Jinzhou Lin",
            "Jie Zhou",
            "Wenhao Xu",
            "Rongtao Xu",
            "Changwei Wang",
            "Shunpeng Chen",
            "Kexue Fu",
            "Yihua Shao",
            "Li Guo",
            "Shibiao Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12362",
        "abstract": "Semantic Scene Completion (SSC) aims to infer complete 3D geometry and semantics from monocular images, serving as a crucial capability for camera-based perception in autonomous driving. However, existing SSC methods relying on temporal stacking or depth projection often lack explicit motion reasoning and struggle with occlusions and noisy depth supervision. We propose CurriFlow, a novel semantic occupancy prediction framework that integrates optical flow-based temporal alignment with curriculum-guided depth fusion. CurriFlow employs a multi-level fusion strategy to align segmentation, visual, and depth features across frames using pre-trained optical flow, thereby improving temporal consistency and dynamic object understanding. To enhance geometric robustness, a curriculum learning mechanism progressively transitions from sparse yet accurate LiDAR depth to dense but noisy stereo depth during training, ensuring stable optimization and seamless adaptation to real-world deployment. Furthermore, semantic priors from the Segment Anything Model (SAM) provide category-agnostic supervision, strengthening voxel-level semantic learning and spatial consistency. Experiments on the SemanticKITTI benchmark demonstrate that CurriFlow achieves state-of-the-art performance with a mean IoU of 16.9, validating the effectiveness of our motion-guided and curriculum-aware design for camera-based 3D semantic scene completion.",
        "tags": [
            "3D",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "135",
        "title": "Pretraining in Actor-Critic Reinforcement Learning for Robot Motion Control",
        "author": [
            "Jiale Fan",
            "Andrei Cramariuc",
            "Tifanny Portela",
            "Marco Hutter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12363",
        "abstract": "The pretraining-finetuning paradigm has facilitated numerous transformative advancements in artificial intelligence research in recent years. However, in the domain of reinforcement learning (RL) for robot motion control, individual skills are often learned from scratch despite the high likelihood that some generalizable knowledge is shared across all task-specific policies belonging to a single robot embodiment. This work aims to define a paradigm for pretraining neural network models that encapsulate such knowledge and can subsequently serve as a basis for warm-starting the RL process in classic actor-critic algorithms, such as Proximal Policy Optimization (PPO). We begin with a task-agnostic exploration-based data collection algorithm to gather diverse, dynamic transition data, which is then used to train a Proprioceptive Inverse Dynamics Model (PIDM) through supervised learning. The pretrained weights are loaded into both the actor and critic networks to warm-start the policy optimization of actual tasks. We systematically validated our proposed method on seven distinct robot motion control tasks, showing significant benefits to this initialization strategy. Our proposed approach on average improves sample efficiency by 40.1% and task performance by 7.5%, compared to random initialization. We further present key ablation studies and empirical analyses that shed light on the mechanisms behind the effectiveness of our method.",
        "tags": [
            "PPO",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "136",
        "title": "(R)evolution of Programming: Vibe Coding as a Post-Coding Paradigm",
        "author": [
            "Kevin Krings",
            "Nino S. Bohn",
            "Thomas Ludwig"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12364",
        "abstract": "Recent advancements in generative artificial intelligence (GenAI), particularly large language models, have introduced new possibilities for software development practices. In our paper we investigate the emerging Vibe Coding (VC) paradigm that emphasizes intuitive, affect-driven, and improvisational interactions between developers and AI systems. Building upon the discourse of End-User Development (EUD), we explore how VC diverges from conventional programming approaches such as those supported by tools like GitHub Copilot. Through five semi-structured interview sessions with ten experienced software practitioners, we identify five thematic dimensions: creativity, sustainability, the future of programming, collaboration, and criticism. Our analysis conceptualizes VC within the metaphor of co-drifting, contrasting it with the prevalent co-piloting perspective of AI-assisted development. We argue that VC reconfigures the developers role, blurring boundaries between professional and non-developers. While VC enables novel forms of expression and rapid prototyping, it also introduces challenges regarding reproducibility, scalability, and inclusivity. We propose that VC represents a meaningful shift in programming culture, warranting further investigation within human-computer interaction (HCI) and software engineering research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "137",
        "title": "LLM-REVal: Can We Trust LLM Reviewers Yet?",
        "author": [
            "Rui Li",
            "Jia-Chen Gu",
            "Po-Nien Kung",
            "Heming Xia",
            "Junfeng liu",
            "Xiangwen Kong",
            "Zhifang Sui",
            "Nanyun Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12367",
        "abstract": "The rapid advancement of large language models (LLMs) has inspired researchers to integrate them extensively into the academic workflow, potentially reshaping how research is practiced and reviewed. While previous studies highlight the potential of LLMs in supporting research and peer review, their dual roles in the academic workflow and the complex interplay between research and review bring new risks that remain largely underexplored. In this study, we focus on how the deep integration of LLMs into both peer-review and research processes may influence scholarly fairness, examining the potential risks of using LLMs as reviewers by simulation. This simulation incorporates a research agent, which generates papers and revises, alongside a review agent, which assesses the submissions. Based on the simulation results, we conduct human annotations and identify pronounced misalignment between LLM-based reviews and human judgments: (1) LLM reviewers systematically inflate scores for LLM-authored papers, assigning them markedly higher scores than human-authored ones; (2) LLM reviewers persistently underrate human-authored papers with critical statements (e.g., risk, fairness), even after multiple revisions. Our analysis reveals that these stem from two primary biases in LLM reviewers: a linguistic feature bias favoring LLM-generated writing styles, and an aversion toward critical statements. These results highlight the risks and equity concerns posed to human authors and academic research if LLMs are deployed in the peer review cycle without adequate caution. On the other hand, revisions guided by LLM reviews yield quality gains in both LLM-based and human evaluations, illustrating the potential of the LLMs-as-reviewers for early-stage researchers and enhancing low-quality papers.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "138",
        "title": "Controlling Intent Expressiveness in Robot Motion with Diffusion Models",
        "author": [
            "Wenli Shi",
            "Clemence Grislain",
            "Olivier Sigaud",
            "Mohamed Chetouani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12370",
        "abstract": "Legibility of robot motion is critical in human-robot interaction, as it allows humans to quickly infer a robot's intended goal. Although traditional trajectory generation methods typically prioritize efficiency, they often fail to make the robot's intentions clear to humans. Meanwhile, existing approaches to legible motion usually produce only a single \"most legible\" trajectory, overlooking the need to modulate intent expressiveness in different contexts. In this work, we propose a novel motion generation framework that enables controllable legibility across the full spectrum, from highly legible to highly ambiguous motions. We introduce a modeling approach based on an Information Potential Field to assign continuous legibility scores to trajectories, and build upon it with a two-stage diffusion framework that first generates paths at specified legibility levels and then translates them into executable robot actions. Experiments in both 2D and 3D reaching tasks demonstrate that our approach produces diverse and controllable motions with varying degrees of legibility, while achieving performance comparable to SOTA. Code and project page: https://legibility-modulator.github.io.",
        "tags": [
            "3D",
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "139",
        "title": "Learning to Recognize Correctly Completed Procedure Steps in Egocentric Assembly Videos through Spatio-Temporal Modeling",
        "author": [
            "Tim J. Schoonbeek",
            "Shao-Hsuan Hung",
            "Dan Lehman",
            "Hans Onvlee",
            "Jacek Kustra",
            "Peter H.N. de With",
            "Fons van der Sommen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12385",
        "abstract": "Procedure step recognition (PSR) aims to identify all correctly completed steps and their sequential order in videos of procedural tasks. The existing state-of-the-art models rely solely on detecting assembly object states in individual video frames. By neglecting temporal features, model robustness and accuracy are limited, especially when objects are partially occluded. To overcome these limitations, we propose Spatio-Temporal Occlusion-Resilient Modeling for Procedure Step Recognition (STORM-PSR), a dual-stream framework for PSR that leverages both spatial and temporal features. The assembly state detection stream operates effectively with unobstructed views of the object, while the spatio-temporal stream captures both spatial and temporal features to recognize step completions even under partial occlusion. This stream includes a spatial encoder, pre-trained using a novel weakly supervised approach to capture meaningful spatial representations, and a transformer-based temporal encoder that learns how these spatial features relate over time. STORM-PSR is evaluated on the MECCANO and IndustReal datasets, reducing the average delay between actual and predicted assembly step completions by 11.2% and 26.1%, respectively, compared to prior methods. We demonstrate that this reduction in delay is driven by the spatio-temporal stream, which does not rely on unobstructed views of the object to infer completed steps. The code for STORM-PSR, along with the newly annotated MECCANO labels, is made publicly available at https://timschoonbeek.github.io/stormpsr .",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "140",
        "title": "Hey Dashboard!: Supporting Voice, Text, and Pointing Modalities in Dashboard Onboarding",
        "author": [
            "Vaishali Dhanoa",
            "Gabriela Molina LeÃ³n",
            "Eve Hoggan",
            "Eduard GrÃ¶ller",
            "Marc Streit",
            "Niklas Elmqvist"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12386",
        "abstract": "Visualization dashboards are regularly used for data exploration and analysis, but their complex interactions and interlinked views often require time-consuming onboarding sessions from dashboard authors. Preparing these onboarding materials is labor-intensive and requires manual updates when dashboards change. Recent advances in multimodal interaction powered by large language models (LLMs) provide ways to support self-guided onboarding. We present DIANA (Dashboard Interactive Assistant for Navigation and Analysis), a multimodal dashboard assistant that helps users for navigation and guided analysis through chat, audio, and mouse-based interactions. Users can choose any interaction modality or a combination of them to onboard themselves on the dashboard. Each modality highlights relevant dashboard features to support user orientation. Unlike typical LLM systems that rely solely on text-based chat, DIANA combines multiple modalities to provide explanations directly in the dashboard interface. We conducted a qualitative user study to understand the use of different modalities for different types of onboarding tasks and their complexities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "141",
        "title": "Scene Coordinate Reconstruction Priors",
        "author": [
            "Wenjing Bian",
            "Axel Barroso-Laguna",
            "Tommaso Cavallari",
            "Victor Adrian Prisacariu",
            "Eric Brachmann"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12387",
        "abstract": "Scene coordinate regression (SCR) models have proven to be powerful implicit scene representations for 3D vision, enabling visual relocalization and structure-from-motion. SCR models are trained specifically for one scene. If training images imply insufficient multi-view constraints SCR models degenerate. We present a probabilistic reinterpretation of training SCR models, which allows us to infuse high-level reconstruction priors. We investigate multiple such priors, ranging from simple priors over the distribution of reconstructed depth values to learned priors over plausible scene coordinate configurations. For the latter, we train a 3D point cloud diffusion model on a large corpus of indoor scans. Our priors push predicted 3D scene points towards plausible geometry at each training step to increase their likelihood. On three indoor datasets our priors help learning better scene representations, resulting in more coherent scene point clouds, higher registration rates and better camera poses, with a positive effect on down-stream tasks such as novel view synthesis and camera relocalization.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "142",
        "title": "Tokenization Disparities as Infrastructure Bias: How Subword Systems Create Inequities in LLM Access and Efficiency",
        "author": [
            "Hailay Kidu Teklehaymanot",
            "Wolfgang Nejdl"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12389",
        "abstract": "Tokenization disparities pose a significant barrier to achieving equitable access to artificial intelligence across linguistically diverse populations. This study conducts a large-scale cross-linguistic evaluation of tokenization efficiency in over 200 languages to systematically quantify computational inequities in large language models (LLMs). Using a standardized experimental framework, we applied consistent preprocessing and normalization protocols, followed by uniform tokenization through the tiktoken library across all language samples. Comprehensive tokenization statistics were collected using established evaluation metrics, including Tokens Per Sentence (TPS) and Relative Tokenization Cost (RTC), benchmarked against English baselines. Our cross-linguistic analysis reveals substantial and systematic disparities: Latin-script languages consistently exhibit higher tokenization efficiency, while non-Latin and morphologically complex languages incur significantly greater token inflation, often 3-5 times higher RTC ratios. These inefficiencies translate into increased computational costs and reduced effective context utilization for underrepresented languages. Overall, the findings highlight structural inequities in current AI systems, where speakers of low-resource and non-Latin languages face disproportionate computational disadvantages. Future research should prioritize the development of linguistically informed tokenization strategies and adaptive vocabulary construction methods that incorporate typological diversity, ensuring more inclusive and computationally equitable multilingual AI systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "143",
        "title": "Improving Generative Behavior Cloning via Self-Guidance and Adaptive Chunking",
        "author": [
            "Junhyuk So",
            "Chiwoong Lee",
            "Shinyoung Lee",
            "Jungseul Ok",
            "Eunhyeok Park"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12392",
        "abstract": "Generative Behavior Cloning (GBC) is a simple yet effective framework for robot learning, particularly in multi-task settings. Recent GBC methods often employ diffusion policies with open-loop (OL) control, where actions are generated via a diffusion process and executed in multi-step chunks without replanning. While this approach has demonstrated strong success rates and generalization, its inherent stochasticity can result in erroneous action sampling, occasionally leading to unexpected task failures. Moreover, OL control suffers from delayed responses, which can degrade performance in noisy or dynamic environments. To address these limitations, we propose two novel techniques to enhance the consistency and reactivity of diffusion policies: (1) self-guidance, which improves action fidelity by leveraging past observations and implicitly promoting future-aware behavior; and (2) adaptive chunking, which selectively updates action sequences when the benefits of reactivity outweigh the need for temporal consistency. Extensive experiments show that our approach substantially improves GBC performance across a wide range of simulated and real-world robotic manipulation tasks. Our code is available at https://github.com/junhyukso/SGAC",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "144",
        "title": "A Survey of Vibe Coding with Large Language Models",
        "author": [
            "Yuyao Ge",
            "Lingrui Mei",
            "Zenghao Duan",
            "Tianhao Li",
            "Yujia Zheng",
            "Yiwei Wang",
            "Lexin Wang",
            "Jiayu Yao",
            "Tianyu Liu",
            "Yujun Cai",
            "Baolong Bi",
            "Fangda Guo",
            "Jiafeng Guo",
            "Shenghua Liu",
            "Xueqi Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12399",
        "abstract": "The advancement of large language models (LLMs) has catalyzed a paradigm shift from code generation assistance to autonomous coding agents, enabling a novel development methodology termed \"Vibe Coding\" where developers validate AI-generated implementations through outcome observation rather than line-by-line code comprehension. Despite its transformative potential, the effectiveness of this emergent paradigm remains under-explored, with empirical evidence revealing unexpected productivity losses and fundamental challenges in human-AI collaboration. To address this gap, this survey provides the first comprehensive and systematic review of Vibe Coding with large language models, establishing both theoretical foundations and practical frameworks for this transformative development approach. Drawing from systematic analysis of over 1000 research papers, we survey the entire vibe coding ecosystem, examining critical infrastructure components including LLMs for coding, LLM-based coding agent, development environment of coding agent, and feedback mechanisms. We first introduce Vibe Coding as a formal discipline by formalizing it through a Constrained Markov Decision Process that captures the dynamic triadic relationship among human developers, software projects, and coding agents. Building upon this theoretical foundation, we then synthesize existing practices into five distinct development models: Unconstrained Automation, Iterative Conversational Collaboration, Planning-Driven, Test-Driven, and Context-Enhanced Models, thus providing the first comprehensive taxonomy in this domain. Critically, our analysis reveals that successful Vibe Coding depends not merely on agent capabilities but on systematic context engineering, well-established development environments, and human-agent collaborative development models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "145",
        "title": "Towards General Urban Monitoring with Vision-Language Models: A Review, Evaluation, and a Research Agenda",
        "author": [
            "AndrÃ© Torneiro",
            "Diogo Monteiro",
            "Paulo Novais",
            "Pedro Rangel Henriques",
            "Nuno F. Rodrigues"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12400",
        "abstract": "Urban monitoring of public infrastructure (such as waste bins, road signs, vegetation, sidewalks, and construction sites) poses significant challenges due to the diversity of objects, environments, and contextual conditions involved. Current state-of-the-art approaches typically rely on a combination of IoT sensors and manual inspections, which are costly, difficult to scale, and often misaligned with citizens' perception formed through direct visual observation. This raises a critical question: Can machines now \"see\" like citizens and infer informed opinions about the condition of urban infrastructure? Vision-Language Models (VLMs), which integrate visual understanding with natural language reasoning, have recently demonstrated impressive capabilities in processing complex visual information, turning them into a promising technology to address this challenge. This systematic review investigates the role of VLMs in urban monitoring, with particular emphasis on zero-shot applications. Following the PRISMA methodology, we analyzed 32 peer-reviewed studies published between 2021 and 2025 to address four core research questions: (1) What urban monitoring tasks have been effectively addressed using VLMs? (2) Which VLM architectures and frameworks are most commonly used and demonstrate superior performance? (3) What datasets and resources support this emerging field? (4) How are VLM-based applications evaluated, and what performance levels have been reported?",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "146",
        "title": "Robot Learning: A Tutorial",
        "author": [
            "Francesco Capuano",
            "Caroline Pascal",
            "Adil Zouitine",
            "Thomas Wolf",
            "Michel Aractingi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12403",
        "abstract": "Robot learning is at an inflection point, driven by rapid advancements in machine learning and the growing availability of large-scale robotics data. This shift from classical, model-based methods to data-driven, learning-based paradigms is unlocking unprecedented capabilities in autonomous systems. This tutorial navigates the landscape of modern robot learning, charting a course from the foundational principles of Reinforcement Learning and Behavioral Cloning to generalist, language-conditioned models capable of operating across diverse tasks and even robot embodiments. This work is intended as a guide for researchers and practitioners, and our goal is to equip the reader with the conceptual understanding and practical tools necessary to contribute to developments in robot learning, with ready-to-use examples implemented in $\\texttt{lerobot}$.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "147",
        "title": "PricingLogic: Evaluating LLMs Reasoning on Complex Tourism Pricing Tasks",
        "author": [
            "Yunuo Liu",
            "Dawei Zhu",
            "Zena Al-Khalili",
            "Dai Cheng",
            "Yanjun Chen",
            "Dietrich Klakow",
            "Wei Zhang",
            "Xiaoyu Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12409",
        "abstract": "We present PricingLogic, the first benchmark that probes whether Large Language Models(LLMs) can reliably automate tourism-related prices when multiple, overlapping fare rules apply. Travel agencies are eager to offload this error-prone task onto AI systems; however, deploying LLMs without verified reliability could result in significant financial losses and erode customer trust. PricingLogic comprises 300 natural-language questions based on booking requests derived from 42 real-world pricing policies, spanning two levels of difficulty: (i) basic customer-type pricing and (ii)bundled-tour calculations involving interacting discounts. Evaluations of a line of LLMs reveal a steep performance drop on the harder tier,exposing systematic failures in rule interpretation and arithmetic http://reasoning.These results highlight that, despite their general capabilities, today's LLMs remain unreliable in revenue-critical applications without further safeguards or domain adaptation. Our code and dataset are available at https://github.com/EIT-NLP/PricingLogic.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "148",
        "title": "Targeted Pooled Latent-Space Steganalysis Applied to Generative Steganography, with a Fix",
        "author": [
            "Etienne Levecque",
            "AurÃ©lien Noirault",
            "TomÃ¡Å¡ PevnÃ½",
            "Jan Butora",
            "Patrick Bas",
            "RÃ©mi Cogranne"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12414",
        "abstract": "Steganographic schemes dedicated to generated images modify the seed vector in the latent space to embed a message, whereas most steganalysis methods attempt to detect the embedding in the image space. This paper proposes to perform steganalysis in the latent space by modeling the statistical distribution of the norm of the latent vector. Specifically, we analyze the practical security of a scheme proposed by Hu et. al. for latent diffusion models, which is both robust and practically undetectable when steganalysis is performed on generated images. We show that after embedding, the Stego (latent) vector is distributed on a hypersphere while the Cover vector is i.i.d. Gaussian. By going from the image space to the latent space, we show that it is possible to model the norm of the vector in the latent space under the Cover or Stego hypothesis as Gaussian distributions with different variances. A Likelihood Ratio Test is then derived to perform pooled steganalysis. The impact of the potential knowledge of the prompt and the number of diffusion steps, is also studied. Additionally, we also show how, by randomly sampling the norm of the latent vector before generation, the initial Stego scheme becomes undetectable in the latent space.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "149",
        "title": "VideoLucy: Deep Memory Backtracking for Long Video Understanding",
        "author": [
            "Jialong Zuo",
            "Yongtai Deng",
            "Lingdong Kong",
            "Jingkang Yang",
            "Rui Jin",
            "Yiwei Zhang",
            "Nong Sang",
            "Liang Pan",
            "Ziwei Liu",
            "Changxin Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12422",
        "abstract": "Recent studies have shown that agent-based systems leveraging large language models (LLMs) for key information retrieval and integration have emerged as a promising approach for long video understanding. However, these systems face two major challenges. First, they typically perform modeling and reasoning on individual frames, struggling to capture the temporal context of consecutive frames. Second, to reduce the cost of dense frame-level captioning, they adopt sparse frame sampling, which risks discarding crucial information. To overcome these limitations, we propose VideoLucy, a deep memory backtracking framework for long video understanding. Inspired by the human recollection process from coarse to fine, VideoLucy employs a hierarchical memory structure with progressive granularity. This structure explicitly defines the detail level and temporal scope of memory at different hierarchical depths. Through an agent-based iterative backtracking mechanism, VideoLucy systematically mines video-wide, question-relevant deep memories until sufficient information is gathered to provide a confident answer. This design enables effective temporal understanding of consecutive frames while preserving critical details. In addition, we introduce EgoMem, a new benchmark for long video understanding. EgoMem is designed to comprehensively evaluate a model's ability to understand complex events that unfold over time and capture fine-grained details in extremely long videos. Extensive experiments demonstrate the superiority of VideoLucy. Built on open-source models, VideoLucy significantly outperforms state-of-the-art methods on multiple long video understanding benchmarks, achieving performance even surpassing the latest proprietary models such as GPT-4o. Our code and dataset will be made publicly at https://videolucy.github.io",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "150",
        "title": "MTOS: A LLM-Driven Multi-topic Opinion Simulation Framework for Exploring Echo Chamber Dynamics",
        "author": [
            "Dingyi Zuo",
            "Hongjie Zhang",
            "Jie Ou",
            "Chaosheng Feng",
            "Shuwan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12423",
        "abstract": "The polarization of opinions, information segregation, and cognitive biases on social media have attracted significant academic attention. In real-world networks, information often spans multiple interrelated topics, posing challenges for opinion evolution and highlighting the need for frameworks that simulate interactions among topics. Existing studies based on large language models (LLMs) focus largely on single topics, limiting the capture of cognitive transfer in multi-topic, cross-domain contexts. Traditional numerical models, meanwhile, simplify complex linguistic attitudes into discrete values, lacking interpretability, behavioral consistency, and the ability to integrate multiple topics. To address these issues, we propose Multi-topic Opinion Simulation (MTOS), a social simulation framework integrating multi-topic contexts with LLMs. MTOS leverages LLMs alongside short-term and long-term memory, incorporates multiple user-selection interaction mechanisms and dynamic topic-selection strategies, and employs a belief decay mechanism to enable perspective updates across topics. We conduct extensive experiments on MTOS, varying topic numbers, correlation types, and performing ablation studies to assess features such as group polarization and local consistency. Results show that multi-topic settings significantly alter polarization trends: positively correlated topics amplify echo chambers, negatively correlated topics inhibit them, and irrelevant topics also mitigate echo chamber effects through resource competition. Compared with numerical models, LLM-based agents realistically simulate dynamic opinion changes, reproduce linguistic features of news texts, and capture complex human reasoning, improving simulation interpretability and system stability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "151",
        "title": "Biased-Attention Guided Risk Prediction for Safe Decision-Making at Unsignalized Intersections",
        "author": [
            "Chengyang Dong",
            "Nan Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12428",
        "abstract": "Autonomous driving decision-making at unsignalized intersections is highly challenging due to complex dynamic interactions and high conflict risks. To achieve proactive safety control, this paper proposes a deep reinforcement learning (DRL) decision-making framework integrated with a biased attention mechanism. The framework is built upon the Soft Actor-Critic (SAC) algorithm. Its core innovation lies in the use of biased attention to construct a traffic risk predictor. This predictor assesses the long-term risk of collision for a vehicle entering the intersection and transforms this risk into a dense reward signal to guide the SAC agent in making safe and efficient driving decisions. Finally, the simulation results demonstrate that the proposed method effectively improves both traffic efficiency and vehicle safety at the intersection, thereby proving the effectiveness of the intelligent decision-making framework in complex scenarios. The code of our work is available at https://github.com/hank111525/SAC-RWB.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "152",
        "title": "A Review of Longitudinal Radiology Report Generation: Dataset Composition, Methods, and Performance Evaluation",
        "author": [
            "Shaoyang Zhou",
            "Yingshu Li",
            "Yunyi Liu",
            "Lingqiao Liu",
            "Lei Wang",
            "Luping Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12444",
        "abstract": "Chest Xray imaging is a widely used diagnostic tool in modern medicine, and its high utilization creates substantial workloads for radiologists. To alleviate this burden, vision language models are increasingly applied to automate Chest Xray radiology report generation (CXRRRG), aiming for clinically accurate descriptions while reducing manual effort. Conventional approaches, however, typically rely on single images, failing to capture the longitudinal context necessary for producing clinically faithful comparison statements. Recently, growing attention has been directed toward incorporating longitudinal data into CXR RRG, enabling models to leverage historical studies in ways that mirror radiologists diagnostic workflows. Nevertheless, existing surveys primarily address single image CXRRRG and offer limited guidance for longitudinal settings, leaving researchers without a systematic framework for model design. To address this gap, this survey provides the first comprehensive review of longitudinal radiology report generation (LRRG). Specifically, we examine dataset construction strategies, report generation architectures alongside longitudinally tailored designs, and evaluation protocols encompassing both longitudinal specific measures and widely used benchmarks. We further summarize LRRG methods performance, alongside analyses of different ablation studies, which collectively highlight the critical role of longitudinal information and architectural design choices in improving model performance. Finally, we summarize five major limitations of current research and outline promising directions for future development, aiming to lay a foundation for advancing this emerging field.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "153",
        "title": "Bayesian Optimization for Dynamic Pricing and Learning",
        "author": [
            "Anush Anand",
            "Pranav Agrawal",
            "Tejas Bodas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12447",
        "abstract": "Dynamic pricing is the practice of adjusting the selling price of a product to maximize a firm's revenue by responding to market demand. The literature typically distinguishes between two settings: infinite inventory, where the firm has unlimited stock and time to sell, and finite inventory, where both inventory and selling horizon are limited. In both cases, the central challenge lies in the fact that the demand function -- how sales respond to price -- is unknown and must be learned from data. Traditional approaches often assume a specific parametric form for the demand function, enabling the use of reinforcement learning (RL) to identify near-optimal pricing strategies. However, such assumptions may not hold in real-world scenarios, limiting the applicability of these methods. In this work, we propose a Gaussian Process (GP) based nonparametric approach to dynamic pricing that avoids restrictive modeling assumptions. We treat the demand function as a black-box function of the price and develop pricing algorithms based on Bayesian Optimization (BO) -- a sample-efficient method for optimizing unknown functions. We present BO-based algorithms tailored for both infinite and finite inventory settings and provide regret guarantees for both regimes, thereby quantifying the learning efficiency of our methods. Through extensive experiments, we demonstrate that our BO-based methods outperform several state-of-the-art RL algorithms in terms of revenue, while requiring fewer assumptions and offering greater robustness. This highlights Bayesian Optimization as a powerful and practical tool for dynamic pricing in complex, uncertain environments.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "154",
        "title": "A Function Centric Perspective On Flat and Sharp Minima",
        "author": [
            "Israel Mason-Williams",
            "Gabryel Mason-Williams",
            "Helen Yannakoudakis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12451",
        "abstract": "Flat minima are widely believed to correlate with improved generalisation in deep neural networks. However, this connection has proven more nuanced in recent studies, with both theoretical counterexamples and empirical exceptions emerging in the literature. In this paper, we revisit the role of sharpness in model performance, proposing that sharpness is better understood as a function-dependent property rather than a reliable indicator of poor generalisation. We conduct extensive empirical studies, from single-objective optimisation to modern image classification tasks, showing that sharper minima often emerge when models are regularised (e.g., via SAM, weight decay, or data augmentation), and that these sharp minima can coincide with better generalisation, calibration, robustness, and functional consistency. Across a range of models and datasets, we find that baselines without regularisation tend to converge to flatter minima yet often perform worse across all safety metrics. Our findings demonstrate that function complexity, rather than flatness alone, governs the geometry of solutions, and that sharper minima can reflect more appropriate inductive biases (especially under regularisation), calling for a function-centric reappraisal of loss landscape geometry.",
        "tags": [
            "SAM"
        ]
    },
    {
        "id": "155",
        "title": "Time-Correlated Video Bridge Matching",
        "author": [
            "Viacheslav Vasilev",
            "Arseny Ivanov",
            "Nikita Gushchin",
            "Maria Kovaleva",
            "Alexander Korotin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12453",
        "abstract": "Diffusion models excel in noise-to-data generation tasks, providing a mapping from a Gaussian distribution to a more complex data distribution. However they struggle to model translations between complex distributions, limiting their effectiveness in data-to-data tasks. While Bridge Matching (BM) models address this by finding the translation between data distributions, their application to time-correlated data sequences remains unexplored. This is a critical limitation for video generation and manipulation tasks, where maintaining temporal coherence is particularly important. To address this gap, we propose Time-Correlated Video Bridge Matching (TCVBM), a framework that extends BM to time-correlated data sequences in the video domain. TCVBM explicitly models inter-sequence dependencies within the diffusion bridge, directly incorporating temporal correlations into the sampling process. We compare our approach to classical methods based on bridge matching and diffusion models for three video-related tasks: frame interpolation, image-to-video generation, and video super-resolution. TCVBM achieves superior performance across multiple quantitative metrics, demonstrating enhanced generation quality and reconstruction fidelity.",
        "tags": [
            "Diffusion",
            "Super Resolution",
            "Video Generation"
        ]
    },
    {
        "id": "156",
        "title": "Probing Latent Knowledge Conflict for Faithful Retrieval-Augmented Generation",
        "author": [
            "Linfeng Gao",
            "Baolong Bi",
            "Zheng Yuan",
            "Le Wang",
            "Zerui Chen",
            "Zhimin Wei",
            "Shenghua Liu",
            "Qinggang Zhang",
            "Jinsong Su"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12460",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm to enhance the factuality of Large Language Models (LLMs). However, existing RAG systems often suffer from an unfaithfulness issue, where the model's response contradicts evidence from the retrieved context. Existing approaches to improving contextual faithfulness largely rely on external interventions, such as prompt engineering, decoding constraints, or reward-based fine-tuning. These works treat the LLM as a black box and overlook a crucial question: how does the LLM internally integrate retrieved evidence with its parametric memory, particularly under knowledge conflicts? To address this gap, we conduct a probing-based analysis of hidden-state representations in LLMs and observe three findings: knowledge integration occurs hierarchically, conflicts manifest as latent signals at the sentence level, and irrelevant context is often amplified when aligned with parametric knowledge. Building on these findings, we propose CLEAR (Conflict-Localized and Enhanced Attention for RAG), a framework that (i) decomposes context into fine-grained sentence-level knowledge, (ii) employs hidden-state probing to localize conflicting knowledge, and (iii) introduces conflict-aware fine-tuning to guide the model to accurately integrate retrieved evidence. Extensive experiments across three benchmarks demonstrate that CLEAR substantially improves both accuracy and contextual faithfulness, consistently outperforming strong baselines under diverse conflict conditions. The related resources are available at https://github.com/LinfengGao/CLEAR.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "157",
        "title": "Evaluating and Mitigating LLM-as-a-judge Bias in Communication Systems",
        "author": [
            "Jiaxin Gao",
            "Chen Chen",
            "Yanwen Jia",
            "Xueluan Gong",
            "Kwok-Yan Lam",
            "Qian Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12462",
        "abstract": "Large Language Models (LLMs) are increasingly being used to autonomously evaluate the quality of content in communication systems, e.g., to assess responses in telecom customer support chatbots. However, the impartiality of these AI \"judges\" is not guaranteed, and any biases in their evaluation criteria could skew outcomes and undermine user trust. In this paper, we systematically investigate judgment biases in two LLM-as-a-judge models (i.e., GPT-Judge and JudgeLM) under the point-wise scoring setting, encompassing 11 types of biases that cover both implicit and explicit forms. We observed that state-of-the-art LLM judges demonstrate robustness to biased inputs, generally assigning them lower scores than the corresponding clean samples. Providing a detailed scoring rubric further enhances this robustness. We further found that fine-tuning an LLM on high-scoring yet biased responses can significantly degrade its performance, highlighting the risk of training on biased data. We also discovered that the judged scores correlate with task difficulty: a challenging dataset like GPQA yields lower average scores, whereas an open-ended reasoning dataset (e.g., JudgeLM-val) sees higher average scores. Finally, we proposed four potential mitigation strategies to ensure fair and reliable AI judging in practical communication scenarios.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "158",
        "title": "Resource-sensitive but language-blind: Community size and not grammatical complexity better predicts the accuracy of Large Language Models in a novel Wug Test",
        "author": [
            "Nikoleta Pantelidou",
            "Evelina Leivada",
            "Paolo Morosi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12463",
        "abstract": "The linguistic abilities of Large Language Models are a matter of ongoing debate. This study contributes to this discussion by investigating model performance in a morphological generalization task that involves novel words. Using a multilingual adaptation of the Wug Test, six models were tested across four partially unrelated languages (Catalan, English, Greek, and Spanish) and compared with human speakers. The aim is to determine whether model accuracy approximates human competence and whether it is shaped primarily by linguistic complexity or by the quantity of available training data. Consistent with previous research, the results show that the models are able to generalize morphological processes to unseen words with human-like accuracy. However, accuracy patterns align more closely with community size and data availability than with structural complexity, refining earlier claims in the literature. In particular, languages with larger speaker communities and stronger digital representation, such as Spanish and English, revealed higher accuracy than less-resourced ones like Catalan and Greek. Overall, our findings suggest that model behavior is mainly driven by the richness of linguistic resources rather than by sensitivity to grammatical complexity, reflecting a form of performance that resembles human linguistic competence only superficially.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "159",
        "title": "SMEC: Rethinking Matryoshka Representation Learning for Retrieval Embedding Compression",
        "author": [
            "Biao Zhang",
            "Lixin Chen",
            "Tong Liu",
            "Bo Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12474",
        "abstract": "Large language models (LLMs) generate high-dimensional embeddings that capture rich semantic and syntactic information. However, high-dimensional embeddings exacerbate computational complexity and storage requirements, thereby hindering practical deployment. To address these challenges, we propose a novel training framework named Sequential Matryoshka Embedding Compression (SMEC). This framework introduces the Sequential Matryoshka Representation Learning(SMRL) method to mitigate gradient variance during training, the Adaptive Dimension Selection (ADS) module to reduce information degradation during dimension pruning, and the Selectable Cross-batch Memory (S-XBM) module to enhance unsupervised learning between high- and low-dimensional embeddings. Experiments on image, text, and multimodal datasets demonstrate that SMEC achieves significant dimensionality reduction while maintaining performance. For instance, on the BEIR dataset, our approach improves the performance of compressed LLM2Vec embeddings (256 dimensions) by 1.1 points and 2.7 points compared to the Matryoshka-Adaptor and Search-Adaptor models, respectively.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "160",
        "title": "When Personalization Tricks Detectors: The Feature-Inversion Trap in Machine-Generated Text Detection",
        "author": [
            "Lang Gao",
            "Xuhui Li",
            "Chenxi Wang",
            "Mingzhe Li",
            "Wei Liu",
            "Zirui Song",
            "Jinghui Zhang",
            "Rui Yan",
            "Preslav Nakov",
            "Xiuying Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12476",
        "abstract": "Large language models (LLMs) have grown more powerful in language generation, producing fluent text and even imitating personal style. Yet, this ability also heightens the risk of identity impersonation. To the best of our knowledge, no prior work has examined personalized machine-generated text (MGT) detection. In this paper, we introduce \\dataset, the first benchmark for evaluating detector robustness in personalized settings, built from literary and blog texts paired with their LLM-generated imitations. Our experimental results demonstrate large performance gaps across detectors in personalized settings: some state-of-the-art models suffer significant drops. We attribute this limitation to the \\textit{feature-inversion trap}, where features that are discriminative in general domains become inverted and misleading when applied to personalized text. Based on this finding, we propose \\method, a simple and reliable way to predict detector performance changes in personalized settings. \\method identifies latent directions corresponding to inverted features and constructs probe datasets that differ primarily along these features to evaluate detector dependence. Our experiments show that \\method can accurately predict both the direction and the magnitude of post-transfer changes, showing 85\\% correlation with the actual performance gaps. We hope that this work will encourage further research on personalized text detection.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "161",
        "title": "A Task-Efficient Reinforcement Learning Task-Motion Planner for Safe Human-Robot Cooperation",
        "author": [
            "Gaoyuan Liu",
            "Joris de Winter",
            "Kelly Merckaert",
            "Denis Steckelmacher",
            "Ann Nowe",
            "Bram Vanderborght"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12477",
        "abstract": "In a Human-Robot Cooperation (HRC) environment, safety and efficiency are the two core properties to evaluate robot performance. However, safety mechanisms usually hinder task efficiency since human intervention will cause backup motions and goal failures of the robot. Frequent motion replanning will increase the computational load and the chance of failure. In this paper, we present a hybrid Reinforcement Learning (RL) planning framework which is comprised of an interactive motion planner and a RL task planner. The RL task planner attempts to choose statistically safe and efficient task sequences based on the feedback from the motion planner, while the motion planner keeps the task execution process collision-free by detecting human arm motions and deploying new paths when the previous path is not valid anymore. Intuitively, the RL agent will learn to avoid dangerous tasks, while the motion planner ensures that the chosen tasks are safe. The proposed framework is validated on the cobot in both simulation and the real world, we compare the planner with hard-coded task motion planning methods. The results show that our planning framework can 1) react to uncertain human motions at both joint and task levels; 2) reduce the times of repeating failed goal commands; 3) reduce the total number of replanning requests.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "162",
        "title": "Diff-XYZ: A Benchmark for Evaluating Diff Understanding",
        "author": [
            "Evgeniy Glukhov",
            "Michele Conti",
            "Egor Bogomolov",
            "Yaroslav Golubev",
            "Alexander Bezzubov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12487",
        "abstract": "Reliable handling of code diffs is central to agents that edit and refactor repositories at scale. We introduce Diff-XYZ, a compact benchmark for code-diff understanding with three supervised tasks: apply (old code $+$ diff $\\rightarrow$ new code), anti-apply (new code $-$ diff $\\rightarrow$ old code), and diff generation (new code $-$ old code $\\rightarrow$ diff). Instances in the benchmark are triples $\\langle \\textit{old code}, \\textit{new code}, \\textit{diff} \\rangle$ drawn from real commits in CommitPackFT, paired with automatic metrics and a clear evaluation protocol. We use the benchmark to do a focused empirical study of the unified diff format and run a cross-format comparison of different diff representations. Our findings reveal that different formats should be used depending on the use case and model size. For example, representing diffs in search-replace format is good for larger models in the diff generation scenario, yet not suited well for diff analysis and smaller models. The Diff-XYZ benchmark is a reusable foundation for assessing and improving diff handling in LLMs that can aid future development of diff formats and models editing code. The dataset is published on HuggingFace Hub: https://huggingface.co/datasets/JetBrains-Research/diff-xyz.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "163",
        "title": "BSGS: Bi-stage 3D Gaussian Splatting for Camera Motion Deblurring",
        "author": [
            "An Zhao",
            "Piaopiao Yu",
            "Zhe Zhu",
            "Mingqiang Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12493",
        "abstract": "3D Gaussian Splatting has exhibited remarkable capabilities in 3D scene http://reconstruction.However, reconstructing high-quality 3D scenes from motion-blurred images caused by camera motion poses a significant http://challenge.The performance of existing 3DGS-based deblurring methods are limited due to their inherent mechanisms, such as extreme dependence on the accuracy of camera poses and inability to effectively control erroneous Gaussian primitives densification caused by motion http://blur.To solve these problems, we introduce a novel framework, Bi-Stage 3D Gaussian Splatting, to accurately reconstruct 3D scenes from motion-blurred http://images.BSGS contains two stages. First, Camera Pose Refinement roughly optimizes camera poses to reduce motion-induced distortions. Second, with fixed rough camera poses, Global RigidTransformation further corrects motion-induced blur http://distortions.To alleviate multi-subframe gradient conflicts, we propose a subframe gradient aggregation strategy to optimize both http://stages.Furthermore, a space-time bi-stage optimization strategy is introduced to dynamically adjust primitive densification thresholds and prevent premature noisy Gaussian generation in blurred regions. Comprehensive experiments verify the effectiveness of our proposed deblurring method and show its superiority over the state of the arts.",
        "tags": [
            "3D",
            "Deblurring",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "164",
        "title": "Mitigating the Noise Shift for Denoising Generative Models via Noise Awareness Guidance",
        "author": [
            "Jincheng Zhong",
            "Boyuan Jiang",
            "Xin Tao",
            "Pengfei Wan",
            "Kun Gai",
            "Mingsheng Long"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12497",
        "abstract": "Existing denoising generative models rely on solving discretized reverse-time SDEs or ODEs. In this paper, we identify a long-overlooked yet pervasive issue in this family of models: a misalignment between the pre-defined noise level and the actual noise level encoded in intermediate states during sampling. We refer to this misalignment as noise shift. Through empirical analysis, we demonstrate that noise shift is widespread in modern diffusion models and exhibits a systematic bias, leading to sub-optimal generation due to both out-of-distribution generalization and inaccurate denoising updates. To address this problem, we propose Noise Awareness Guidance (NAG), a simple yet effective correction method that explicitly steers sampling trajectories to remain consistent with the pre-defined noise schedule. We further introduce a classifier-free variant of NAG, which jointly trains a noise-conditional and a noise-unconditional model via noise-condition dropout, thereby eliminating the need for external classifiers. Extensive experiments, including ImageNet generation and various supervised fine-tuning tasks, show that NAG consistently mitigates noise shift and substantially improves the generation quality of mainstream diffusion models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "165",
        "title": "Automated Behavior Planning for Fruit Tree Pruning via Redundant Robot Manipulators: Addressing the Behavior Planning Challenge",
        "author": [
            "Gaoyuan Liu",
            "Bas Boom",
            "Naftali Slob",
            "Yuri DurodiÃ©",
            "Ann NowÃ©",
            "Bram Vanderborght"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12509",
        "abstract": "Pruning is an essential agricultural practice for orchards. Proper pruning can promote healthier growth and optimize fruit production throughout the orchard's lifespan. Robot manipulators have been developed as an automated solution for this repetitive task, which typically requires seasonal labor with specialized skills. While previous research has primarily focused on the challenges of perception, the complexities of manipulation are often overlooked. These challenges involve planning and control in both joint and Cartesian spaces to guide the end-effector through intricate, obstructive branches. Our work addresses the behavior planning challenge for a robotic pruning system, which entails a multi-level planning problem in environments with complex collisions. In this paper, we formulate the planning problem for a high-dimensional robotic arm in a pruning scenario, investigate the system's intrinsic redundancies, and propose a comprehensive pruning workflow that integrates perception, modeling, and holistic planning. In our experiments, we demonstrate that more comprehensive planning methods can significantly enhance the performance of the robotic manipulator. Finally, we implement the proposed workflow on a real-world robot. As a result, this work complements previous efforts on robotic pruning and motivates future research and development in planning for pruning applications.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "166",
        "title": "BoN Appetit Team at LeWiDi-2025: Best-of-N Test-time Scaling Can Not Stomach Annotation Disagreements (Yet)",
        "author": [
            "Tomas Ruiz",
            "Siyao Peng",
            "Barbara Plank",
            "Carsten Schwemmer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12516",
        "abstract": "Test-time scaling is a family of techniques to improve LLM outputs at inference time by performing extra computation. To the best of our knowledge, test-time scaling has been limited to domains with verifiably correct answers, like mathematics and coding. We transfer test-time scaling to the LeWiDi-2025 tasks to evaluate annotation disagreements. We experiment with three test-time scaling methods: two benchmark algorithms (Model Averaging and Majority Voting), and a Best-of-N sampling method. The two benchmark methods improve LLM performance consistently on the LeWiDi tasks, but the Best-of-N method does not. Our experiments suggest that the Best-of-N method does not currently transfer from mathematics to LeWiDi tasks, and we analyze potential reasons for this gap.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "167",
        "title": "Voronoi-Assisted Diffusion for Computing Unsigned Distance Fields from Unoriented Points",
        "author": [
            "Jiayi Kong",
            "Chen Zong",
            "Junkai Deng",
            "Xuhui Chen",
            "Fei Hou",
            "Shiqing Xin",
            "Junhui Hou",
            "Chen Qian",
            "Ying He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12524",
        "abstract": "Unsigned Distance Fields (UDFs) provide a flexible representation for 3D shapes with arbitrary topology, including open and closed surfaces, orientable and non-orientable geometries, and non-manifold structures. While recent neural approaches have shown promise in learning UDFs, they often suffer from numerical instability, high computational cost, and limited controllability. We present a lightweight, network-free method, Voronoi-Assisted Diffusion (VAD), for computing UDFs directly from unoriented point clouds. Our approach begins by assigning bi-directional normals to input points, guided by two Voronoi-based geometric criteria encoded in an energy function for optimal alignment. The aligned normals are then diffused to form an approximate UDF gradient field, which is subsequently integrated to recover the final UDF. Experiments demonstrate that VAD robustly handles watertight and open surfaces, as well as complex non-manifold and non-orientable geometries, while remaining computationally efficient and stable.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "168",
        "title": "Unconditional Human Motion and Shape Generation via Balanced Score-Based Diffusion",
        "author": [
            "David BjÃ¶rkstrand",
            "Tiesheng Wang",
            "Lars Bretzner",
            "Josephine Sullivan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12537",
        "abstract": "Recent work has explored a range of model families for human motion generation, including Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and diffusion-based models. Despite their differences, many methods rely on over-parameterized input features and auxiliary losses to improve empirical results. These strategies should not be strictly necessary for diffusion models to match the human motion distribution. We show that on par with state-of-the-art results in unconditional human motion generation are achievable with a score-based diffusion model using only careful feature-space normalization and analytically derived weightings for the standard L2 score-matching loss, while generating both motion and shape directly, thereby avoiding slow post hoc shape recovery from joints. We build the method step by step, with a clear theoretical motivation for each component, and provide targeted ablations demonstrating the effectiveness of each proposed addition in isolation.",
        "tags": [
            "Diffusion",
            "Score Matching"
        ]
    },
    {
        "id": "169",
        "title": "VISaGE: Understanding Visual Generics and Exceptions",
        "author": [
            "Stella Frank",
            "Emily Allaway"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12548",
        "abstract": "While Vision Language Models (VLMs) learn conceptual representations, in the form of generalized knowledge, during training, they are typically used to analyze individual instances. When evaluation instances are atypical, this paradigm results in tension between two priors in the model. The first is a pragmatic prior that the textual and visual input are both relevant, arising from VLM finetuning on congruent inputs; the second is a semantic prior that the conceptual representation is generally true for instances of the category. In order to understand how VLMs trade off these priors, we introduce a new evaluation dataset, VISaGE, consisting of both typical and exceptional images. In carefully balanced experiments, we show that conceptual understanding degrades when the assumption of congruency underlying the pragmatic prior is violated with incongruent images. This effect is stronger than the effect of the semantic prior when querying about individual instances.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "170",
        "title": "CoIRL-AD: Collaborative-Competitive Imitation-Reinforcement Learning in Latent World Models for Autonomous Driving",
        "author": [
            "Xiaoji Zheng",
            "Ziyuan Yang",
            "Yanhao Chen",
            "Yuhang Peng",
            "Yuanrong Tang",
            "Gengyuan Liu",
            "Bokui Chen",
            "Jiangtao Gong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12560",
        "abstract": "End-to-end autonomous driving models trained solely with imitation learning (IL) often suffer from poor generalization. In contrast, reinforcement learning (RL) promotes exploration through reward maximization but faces challenges such as sample inefficiency and unstable convergence. A natural solution is to combine IL and RL. Moving beyond the conventional two-stage paradigm (IL pretraining followed by RL fine-tuning), we propose CoIRL-AD, a competitive dual-policy framework that enables IL and RL agents to interact during training. CoIRL-AD introduces a competition-based mechanism that facilitates knowledge exchange while preventing gradient conflicts. Experiments on the nuScenes dataset show an 18% reduction in collision rate compared to baselines, along with stronger generalization and improved performance on long-tail scenarios. Code is available at: https://github.com/SEU-zxj/CoIRL-AD.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "171",
        "title": "MMOT: The First Challenging Benchmark for Drone-based Multispectral Multi-Object Tracking",
        "author": [
            "Tianhao Li",
            "Tingfa Xu",
            "Ying Wang",
            "Haolin Qin",
            "Xu Lin",
            "Jianan Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12565",
        "abstract": "Drone-based multi-object tracking is essential yet highly challenging due to small targets, severe occlusions, and cluttered backgrounds. Existing RGB-based tracking algorithms heavily depend on spatial appearance cues such as color and texture, which often degrade in aerial views, compromising reliability. Multispectral imagery, capturing pixel-level spectral reflectance, provides crucial cues that enhance object discriminability under degraded spatial conditions. However, the lack of dedicated multispectral UAV datasets has hindered progress in this domain. To bridge this gap, we introduce MMOT, the first challenging benchmark for drone-based multispectral multi-object tracking. It features three key characteristics: (i) Large Scale - 125 video sequences with over 488.8K annotations across eight categories; (ii) Comprehensive Challenges - covering diverse conditions such as extreme small targets, high-density scenarios, severe occlusions, and complex motion; and (iii) Precise Oriented Annotations - enabling accurate localization and reduced ambiguity under aerial perspectives. To better extract spectral features and leverage oriented annotations, we further present a multispectral and orientation-aware MOT scheme adapting existing methods, featuring: (i) a lightweight Spectral 3D-Stem integrating spectral features while preserving compatibility with RGB pretraining; (ii) an orientation-aware Kalman filter for precise state estimation; and (iii) an end-to-end orientation-adaptive transformer. Extensive experiments across representative trackers consistently show that multispectral input markedly improves tracking performance over RGB baselines, particularly for small and densely packed objects. We believe our work will advance drone-based multispectral multi-object tracking research. Our MMOT, code, and benchmarks are publicly available at https://github.com/Annzstbl/MMOT.",
        "tags": [
            "3D",
            "Transformer"
        ]
    },
    {
        "id": "172",
        "title": "Learning Human Motion with Temporally Conditional Mamba",
        "author": [
            "Quang Nguyen",
            "Tri Le",
            "Baoru Huang",
            "Minh Nhat Vu",
            "Ngan Le",
            "Thieu Vo",
            "Anh Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12573",
        "abstract": "Learning human motion based on a time-dependent input signal presents a challenging yet impactful task with various applications. The goal of this task is to generate or estimate human movement that consistently reflects the temporal patterns of conditioning inputs. Existing methods typically rely on cross-attention mechanisms to fuse the condition with motion. However, this approach primarily captures global interactions and struggles to maintain step-by-step temporal alignment. To address this limitation, we introduce Temporally Conditional Mamba, a new mamba-based model for human motion generation. Our approach integrates conditional information into the recurrent dynamics of the Mamba block, enabling better temporally aligned motion. To validate the effectiveness of our method, we evaluate it on a variety of human motion tasks. Extensive experiments demonstrate that our model significantly improves temporal alignment, motion realism, and condition consistency over state-of-the-art approaches. Our project page is available at https://zquang2202.github.io/TCM.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "173",
        "title": "Unlocking Zero-Shot Plant Segmentation with Pl@ntNet Intelligence",
        "author": [
            "Simon RavÃ©",
            "Jean-Christophe Lombardo",
            "Pejman Rasti",
            "Alexis Joly",
            "David Rousseau"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12579",
        "abstract": "We present a zero-shot segmentation approach for agricultural imagery that leverages Plantnet, a large-scale plant classification model, in conjunction with its DinoV2 backbone and the Segment Anything Model (SAM). Rather than collecting and annotating new datasets, our method exploits Plantnet's specialized plant representations to identify plant regions and produce coarse segmentation masks. These masks are then refined by SAM to yield detailed segmentations. We evaluate on four publicly available datasets of various complexity in terms of contrast including some where the limited size of the training data and complex field conditions often hinder purely supervised methods. Our results show consistent performance gains when using Plantnet-fine-tuned DinoV2 over the base DinoV2 model, as measured by the Jaccard Index (IoU). These findings highlight the potential of combining foundation models with specialized plant-centric models to alleviate the annotation bottleneck and enable effective segmentation in diverse agricultural scenarios.",
        "tags": [
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "174",
        "title": "LayerSync: Self-aligning Intermediate Layers",
        "author": [
            "Yasaman Haghighi",
            "Bastien van Delft",
            "Mariam Hassan",
            "Alexandre Alahi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12581",
        "abstract": "We propose LayerSync, a domain-agnostic approach for improving the generation quality and the training efficiency of diffusion models. Prior studies have highlighted the connection between the quality of generation and the representations learned by diffusion models, showing that external guidance on model intermediate representations accelerates training. We reconceptualize this paradigm by regularizing diffusion models with their own intermediate representations. Building on the observation that representation quality varies across diffusion model layers, we show that the most semantically rich representations can act as an intrinsic guidance for weaker ones, reducing the need for external supervision. Our approach, LayerSync, is a self-sufficient, plug-and-play regularizer term with no overhead on diffusion model training and generalizes beyond the visual domain to other modalities. LayerSync requires no pretrained models nor additional data. We extensively evaluate the method on image generation and demonstrate its applicability to other domains such as audio, video, and motion generation. We show that it consistently improves the generation quality and the training efficiency. For example, we speed up the training of flow-based transformer by over 8.75x on ImageNet dataset and improved the generation quality by 23.6%. The code is available at https://github.com/vita-epfl/LayerSync.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "175",
        "title": "Advancing End-to-End Pixel Space Generative Modeling via Self-supervised Pre-training",
        "author": [
            "Jiachen Lei",
            "Keli Liu",
            "Julius Berner",
            "Haiming Yu",
            "Hongkai Zheng",
            "Jiahong Wu",
            "Xiangxiang Chu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12586",
        "abstract": "Pixel-space generative models are often more difficult to train and generally underperform compared to their latent-space counterparts, leaving a persistent performance and efficiency gap. In this paper, we introduce a novel two-stage training framework that closes this gap for pixel-space diffusion and consistency models. In the first stage, we pre-train encoders to capture meaningful semantics from clean images while aligning them with points along the same deterministic sampling trajectory, which evolves points from the prior to the data distribution. In the second stage, we integrate the encoder with a randomly initialized decoder and fine-tune the complete model end-to-end for both diffusion and consistency models. Our training framework demonstrates strong empirical performance on ImageNet dataset. Specifically, our diffusion model reaches an FID of 2.04 on ImageNet-256 and 2.35 on ImageNet-512 with 75 number of function evaluations (NFE), surpassing prior pixel-space methods by a large margin in both generation quality and efficiency while rivaling leading VAE-based models at comparable training cost. Furthermore, on ImageNet-256, our consistency model achieves an impressive FID of 8.82 in a single sampling step, significantly surpassing its latent-space counterpart. To the best of our knowledge, this marks the first successful training of a consistency model directly on high-resolution images without relying on pre-trained VAEs or diffusion models.",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "176",
        "title": "Teaching Language Models to Faithfully Express their Uncertainty",
        "author": [
            "Bryan Eikema",
            "Evgenia Ilia",
            "JosÃ© G. C. de Souza",
            "Chrysoula Zerva",
            "Wilker Aziz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12587",
        "abstract": "Large language models (LLMs) often miscommunicate their uncertainty: repeated queries can produce divergent answers, yet generated responses are typically unhedged or hedged in ways that do not reflect this variability. This conveys unfaithful information about the uncertain state of the LLMs' knowledge, creating a faithfulness gap that affects even strong LLMs. We introduce Faithful Uncertainty Tuning (FUT): a fine-tuning approach that teaches instruction-tuned LLMs to express uncertainty faithfully without altering their underlying answer distribution. We construct training data by augmenting model samples with uncertainty hedges (i.e. verbal cues such as 'possibly' or 'likely') aligned with sample consistency, requiring no supervision beyond the model and a set of prompts. We evaluate FUT on open-domain question answering (QA) across multiple models and datasets. Our results show that FUT substantially reduces the faithfulness gap, while preserving QA accuracy and introducing minimal semantic distribution shift. Further analyses demonstrate robustness across decoding strategies, choice of hedgers, and other forms of uncertainty expression (i.e. numerical). These findings establish FUT as a simple and effective way to teach LLMs to communicate uncertainty faithfully.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "177",
        "title": "StyleDecipher: Robust and Explainable Detection of LLM-Generated Texts with Stylistic Analysis",
        "author": [
            "Siyuan Li",
            "Aodu Wulianghai",
            "Xi Lin",
            "Guangyan Li",
            "Xiang Chen",
            "Jun Wu",
            "Jianhua Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12608",
        "abstract": "With the increasing integration of large language models (LLMs) into open-domain writing, detecting machine-generated text has become a critical task for ensuring content authenticity and trust. Existing approaches rely on statistical discrepancies or model-specific heuristics to distinguish between LLM-generated and human-written text. However, these methods struggle in real-world scenarios due to limited generalization, vulnerability to paraphrasing, and lack of explainability, particularly when facing stylistic diversity or hybrid human-AI authorship. In this work, we propose StyleDecipher, a robust and explainable detection framework that revisits LLM-generated text detection using combined feature extractors to quantify stylistic differences. By jointly modeling discrete stylistic indicators and continuous stylistic representations derived from semantic embeddings, StyleDecipher captures distinctive style-level divergences between human and LLM outputs within a unified representation space. This framework enables accurate, explainable, and domain-agnostic detection without requiring access to model internals or labeled segments. Extensive experiments across five diverse domains, including news, code, essays, reviews, and academic abstracts, demonstrate that StyleDecipher consistently achieves state-of-the-art in-domain accuracy. Moreover, in cross-domain evaluations, it surpasses existing baselines by up to 36.30%, while maintaining robustness against adversarial perturbations and mixed human-AI content. Further qualitative and quantitative analysis confirms that stylistic signals provide explainable evidence for distinguishing machine-generated text. Our source code can be accessed at https://github.com/SiyuanLi00/StyleDecipher.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "178",
        "title": "Towards Fast Coarse-graining and Equation Discovery with Foundation Inference Models",
        "author": [
            "Manuel Hinz",
            "Maximilian Mauel",
            "Patrick Seifner",
            "David Berghaus",
            "Kostadin Cvejoski",
            "Ramses J. Sanchez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12618",
        "abstract": "High-dimensional recordings of dynamical processes are often characterized by a much smaller set of effective variables, evolving on low-dimensional manifolds. Identifying these latent dynamics requires solving two intertwined problems: discovering appropriate coarse-grained variables and simultaneously fitting the governing equations. Most machine learning approaches tackle these tasks jointly by training autoencoders together with models that enforce dynamical consistency. We propose to decouple the two problems by leveraging the recently introduced Foundation Inference Models (FIMs). FIMs are pretrained models that estimate the infinitesimal generators of dynamical systems (e.g., the drift and diffusion of a stochastic differential equation) in zero-shot mode. By amortizing the inference of the dynamics through a FIM with frozen weights, and training only the encoder-decoder map, we define a simple, simulation-consistent loss that stabilizes representation learning. A proof of concept on a stochastic double-well system with semicircle diffusion, embedded into synthetic video data, illustrates the potential of this approach for fast and reusable coarse-graining pipelines.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "179",
        "title": "ACADATA: Parallel Dataset of Academic Data for Machine Translation",
        "author": [
            "IÃ±aki Lacunza",
            "Javier Garcia Gilabert",
            "Francesca De Luca Fornaciari",
            "Javier Aula-Blasco",
            "Aitor Gonzalez-Agirre",
            "Maite Melero",
            "Marta Villegas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12621",
        "abstract": "We present ACADATA, a high-quality parallel dataset for academic translation, that consists of two subsets: ACAD-TRAIN, which contains approximately 1.5 million author-generated paragraph pairs across 96 language directions and ACAD-BENCH, a curated evaluation set of almost 6,000 translations covering 12 directions. To validate its utility, we fine-tune two Large Language Models (LLMs) on ACAD-TRAIN and benchmark them on ACAD-BENCH against specialized machine-translation systems, general-purpose, open-weight LLMs, and several large-scale proprietary models. Experimental results demonstrate that fine-tuning on ACAD-TRAIN leads to improvements in academic translation quality by +6.1 and +12.4 d-BLEU points on average for 7B and 2B models respectively, while also improving long-context translation in a general domain by up to 24.9% when translating out of English. The fine-tuned top-performing model surpasses the best propietary and open-weight models on academic translation domain. By releasing ACAD-TRAIN, ACAD-BENCH and the fine-tuned models, we provide the community with a valuable resource to advance research in academic domain and long-context translation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "180",
        "title": "Laminar: A Scalable Asynchronous RL Post-Training Framework",
        "author": [
            "Guangming Sheng",
            "Yuxuan Tong",
            "Borui Wan",
            "Wang Zhang",
            "Chaobo Jia",
            "Xibin Wu",
            "Yuqi Wu",
            "Xiang Li",
            "Chi Zhang",
            "Yanghua Peng",
            "Haibin Lin",
            "Xin Liu",
            "Chuan Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12633",
        "abstract": "Reinforcement learning (RL) post-training for Large Language Models (LLMs) is now scaling to large clusters and running for extended durations to enhance model reasoning performance. However, the scalability of existing RL frameworks is limited, as extreme long-tail skewness in RL trajectory generation causes severe GPU underutilization. Current asynchronous RL systems attempt to mitigate this, but they rely on global weight synchronization between the actor and all rollouts, which creates a rigid model update schedule. This global synchronization is ill-suited for the highly skewed and evolving distribution of trajectory generation latency in RL training, crippling training efficiency. Our key insight is that efficient scaling requires breaking this lockstep through trajectory-level asynchrony, which generates and consumes each trajectory independently. We propose Laminar, a scalable and robust RL post-training system built on a fully decoupled architecture. First, we replace global updates with a tier of relay workers acting as a distributed parameter service. This enables asynchronous and fine-grained weight synchronization, allowing rollouts to pull the latest weight anytime without stalling the actor's training loop. Second, a dynamic repack mechanism consolidates long-tail trajectories onto a few dedicated rollouts, maximizing generation throughput. The fully decoupled design also isolates failures, ensuring robustness for long-running jobs. Our evaluation on a 1024-GPU cluster shows that Laminar achieves up to 5.48$\\times$ training throughput speedup over state-of-the-art systems, while reducing model convergence time.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "181",
        "title": "Memory as Action: Autonomous Context Curation for Long-Horizon Agentic Tasks",
        "author": [
            "Yuxiang Zhang",
            "Jiangming Shu",
            "Ye Ma",
            "Xueyuan Lin",
            "Shangxi Wu",
            "Jitao Sang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12635",
        "abstract": "Large Language Models face challenges in long-horizon agentic tasks as their constrained memory is easily overwhelmed by distracting or irrelevant context. Existing working memory methods typically rely on external, heuristic mechanisms that are decoupled from the agent's core policy. In this work, we reframe working memory management as a learnable, intrinsic capability. We propose a novel framework, Memory-as-Action, where an agent actively manages its working memory by executing explicit editing operations as part of a unified policy. This formulation allows an agent, trained via reinforcement learning, to balance memory curation against long-term task objectives under given resource constraints. However, such memory editing actions break the standard assumption of a continuously growing prefix in LLM interactions, leading to what we call trajectory fractures. These non-prefix changes disrupt the causal continuity required by standard policy gradient methods, making those methods inapplicable. To address this, we propose a new algorithm, Dynamic Context Policy Optimization, which enables stable end-to-end reinforcement learning by segmenting trajectories at memory action points and applying trajectory-level advantages to the resulting action segments. Our results demonstrate that jointly optimizing for task reasoning and memory management in an end-to-end fashion not only reduces overall computational consumption but also improves task performance, driven by adaptive context curation strategies tailored to the model's intrinsic capabilities.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "182",
        "title": "COSTAR-A: A prompting framework for enhancing Large Language Model performance on Point-of-View questions",
        "author": [
            "Nzubechukwu C. Ohalete",
            "Kevin B. Gittner",
            "Lauren M. Matheny"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12637",
        "abstract": "Large Language Models (LLMs) are highly sensitive to prompt design, and making optimized prompting techniques is crucial for generating consistent, high-quality outputs. In this study, we introduce COSTAR-A, a novel prompt engineering framework that enhances the existing COSTAR method, which stands for Context, Objective, Style, Tone, Audience, and Response, by adding the 'Answer' component at the end. We demonstrate that while the original COSTAR framework improves prompt clarity and aligns outputs for larger LLMs, its performance is less consistent with smaller, locally optimized models, particularly in tasks that require more directive or constrained outputs. Through a series of controlled prompt-output assessments with smaller (at most 8 billion parameters), fine-tuned models, we found that COSTAR-A can enhance the output structure and decisiveness of localized LLMs for certain tasks, although its effectiveness varies across models and use cases. Notably, the Llama 3.1-8B model exhibited performance improvements when prompted with COSTAR-A compared to COSTAR alone. These findings emphasize the adaptability and scalability of COSTAR-A as a prompting framework, particularly in computationally efficient AI deployments on resource-constrained hardware.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "183",
        "title": "Expert or not? assessing data quality in offline reinforcement learning",
        "author": [
            "Arip Asadulaev",
            "Fakhri Karray",
            "Martin Takac"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12638",
        "abstract": "Offline reinforcement learning (RL) learns exclusively from static datasets, without further interaction with the environment. In practice, such datasets vary widely in quality, often mixing expert, suboptimal, and even random trajectories. The choice of algorithm therefore depends on dataset fidelity. Behavior cloning can suffice on high-quality data, whereas mixed- or low-quality data typically benefits from offline RL methods that stitch useful behavior across trajectories. Yet in the wild it is difficult to assess dataset quality a priori because the data's provenance and skill composition are unknown. We address the problem of estimating offline dataset quality without training an agent. We study a spectrum of proxies from simple cumulative rewards to learned value based estimators, and introduce the Bellman Wasserstein distance (BWD), a value aware optimal transport score that measures how dissimilar a dataset's behavioral policy is from a random reference policy. BWD is computed from a behavioral critic and a state conditional OT formulation, requiring no environment interaction or full policy optimization. Across D4RL MuJoCo tasks, BWD strongly correlates with an oracle performance score that aggregates multiple offline RL algorithms, enabling efficient prediction of how well standard agents will perform on a given dataset. Beyond prediction, integrating BWD as a regularizer during policy optimization explicitly pushes the learned policy away from random behavior and improves returns. These results indicate that value aware, distributional signals such as BWD are practical tools for triaging offline RL datasets and policy optimization.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "184",
        "title": "Reasoning Pattern Matters: Learning to Reason without Human Rationales",
        "author": [
            "Chaoxu Pang",
            "Yixuan Cao",
            "Ping Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12643",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable reasoning capabilities under the widely adopted SFT+RLVR paradigm, which first performs Supervised Fine-Tuning (SFT) on human-annotated reasoning trajectories (rationales) to establish initial reasoning behaviors, then applies Reinforcement Learning with Verifiable Rewards (RLVR) to optimize the model using verifiable signals without golden rationales. However, annotating high-quality rationales for the SFT stage remains prohibitively expensive. This paper investigates when and how rationale annotation costs can be substantially reduced without compromising reasoning performance. We identify a broad class of problems, termed patterned reasoning tasks, where reasoning follows a fixed, procedural strategy consistent across instances. Although instances vary in content such as domain knowledge, factual information, or numeric values, the solution derives from applying a shared reasoning pattern. We argue that the success of SFT+RLVR on such tasks primarily stems from its ability to enable models to internalize these reasoning patterns. Using numerical semantic matching as a representative task, we provide both causal and behavioral evidence showing that reasoning patterns rather than the quantity or quality of rationales are the key determinant of performance. Building on these insights, we propose Pattern-Aware LLMs as Rationale AnnOtators (PARO), a simple yet effective framework that enables LLMs to generate rationales aligned with task-specific reasoning patterns without requiring human rationale annotations. Experiments show that PARO-generated rationales achieve comparable SFT+RLVR performance to human rationales that are 10 times larger. These results suggest that large-scale human rationale annotations can be replaced with LLM-based automatic annotations requiring only limited human supervision over reasoning patterns.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "185",
        "title": "On the Use of Hierarchical Vision Foundation Models for Low-Cost Human Mesh Recovery and Pose Estimation",
        "author": [
            "Shuhei Tarashima",
            "Yushan Wang",
            "Norio Tagawa"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12660",
        "abstract": "In this work, we aim to develop simple and efficient models for human mesh recovery (HMR) and its predecessor task, human pose estimation (HPE). State-of-the-art HMR methods, such as HMR2.0 and its successors, rely on large, non-hierarchical vision transformers as encoders, which are inherited from the corresponding HPE models like ViTPose. To establish baselines across varying computational budgets, we first construct three lightweight HMR2.0 variants by adapting the corresponding ViTPose models. In addition, we propose leveraging the early stages of hierarchical vision foundation models (VFMs), including Swin Transformer, GroupMixFormer, and VMamba, as encoders. This design is motivated by the observation that intermediate stages of hierarchical VFMs produce feature maps with resolutions comparable to or higher than those of non-hierarchical counterparts. We conduct a comprehensive evaluation of 27 hierarchical-VFM-based HMR and HPE models, demonstrating that using only the first two or three stages achieves performance on par with full-stage models. Moreover, we show that the resulting truncated models exhibit better trade-offs between accuracy and computational efficiency compared to existing lightweight alternatives.",
        "tags": [
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "186",
        "title": "Maximal Adaptation, Minimal Guidance: Permissive Reactive Robot Task Planning with Humans in the Loop",
        "author": [
            "Oz Gitelson",
            "Satya Prakash Nayak",
            "Ritam Raha",
            "Anne-Kathrin Schmuck"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12662",
        "abstract": "We present a novel framework for human-robot \\emph{logical} interaction that enables robots to reliably satisfy (infinite horizon) temporal logic tasks while effectively collaborating with humans who pursue independent and unknown tasks. The framework combines two key capabilities: (i) \\emph{maximal adaptation} enables the robot to adjust its strategy \\emph{online} to exploit human behavior for cooperation whenever possible, and (ii) \\emph{minimal tunable feedback} enables the robot to request cooperation by the human online only when necessary to guarantee progress. This balance minimizes human-robot interference, preserves human autonomy, and ensures persistent robot task satisfaction even under conflicting human goals. We validate the approach in a real-world block-manipulation task with a Franka Emika Panda robotic arm and in the Overcooked-AI benchmark, demonstrating that our method produces rich, \\emph{emergent} cooperative behaviors beyond the reach of existing approaches, while maintaining strong formal guarantees.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "187",
        "title": "The Role of Parametric Injection-A Systematic Study of Parametric Retrieval-Augmented Generation",
        "author": [
            "Minghao Tang",
            "Shiyu Ni",
            "Jingtong Wu",
            "Zengxin Han",
            "Keping Bi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12668",
        "abstract": "Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving external documents. As an emerging form of RAG, parametric retrieval-augmented generation (PRAG) encodes documents as model parameters (i.e., LoRA modules) and injects these representations into the model during inference, enabling interaction between the LLM and documents at parametric level. Compared with directly placing documents in the input context, PRAG is more efficient and has the potential to offer deeper model-document interaction. Despite its growing attention, the mechanism underlying parametric injection remains poorly understood. In this work, we present a systematic study of PRAG to clarify the role of parametric injection, showing that parameterized documents capture only partial semantic information of documents, and relying on them alone yields inferior performance compared to interaction at text level. However, these parametric representations encode high-level document information that can enhance the model's understanding of documents within the input context. When combined parameterized documents with textual documents, the model can leverage relevant information more effectively and become more robust to noisy inputs, achieving better performance than either source alone. We recommend jointly using parameterized and textual documents and advocate for increasing the information content of parametric representations to advance PRAG.",
        "tags": [
            "LLM",
            "LoRA",
            "RAG"
        ]
    },
    {
        "id": "188",
        "title": "TerraCodec: Compressing Earth Observations",
        "author": [
            "Julen Costa-Watanabe",
            "Isabelle Wittmann",
            "Benedikt Blumenstiel",
            "Konrad Schindler"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12670",
        "abstract": "Earth observation (EO) satellites produce massive streams of multispectral image time series, posing pressing challenges for storage and transmission. Yet, learned EO compression remains fragmented, lacking publicly available pretrained models and misaligned with advances in compression for natural imagery. Image codecs overlook temporal redundancy, while video codecs rely on motion priors that fail to capture the radiometric evolution of largely static scenes. We introduce TerraCodec (TEC), a family of learned codecs tailored to EO. TEC includes efficient image-based variants adapted to multispectral inputs, as well as a Temporal Transformer model (TEC-TT) that leverages dependencies across time. To overcome the fixed-rate setting of today's neural codecs, we present Latent Repacking, a novel method for training flexible-rate transformer models that operate on varying rate-distortion settings. Trained on Sentinel-2 data, TerraCodec outperforms classical codecs, achieving 3-10x stronger compression at equivalent image quality. Beyond compression, TEC-TT enables zero-shot cloud inpainting, surpassing state-of-the-art methods on the AllClear benchmark. Our results establish bespoke, learned compression algorithms as a promising direction for Earth observation. Code and model weights will be released under a permissive license.",
        "tags": [
            "Inpainting",
            "Transformer"
        ]
    },
    {
        "id": "189",
        "title": "Keep Calm and Avoid Harmful Content: Concept Alignment and Latent Manipulation Towards Safer Answers",
        "author": [
            "Ruben Belo",
            "Claudia Soares",
            "Marta Guimaraes"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12672",
        "abstract": "Large Language Models are susceptible to jailbreak attacks that bypass built-in safety guardrails (e.g., by tricking the model with adversarial prompts). We propose Concept Alignment and Concept Manipulation \\textbf{CALM}, an inference-time method that suppresses harmful concepts by modifying latent representations of the last layer of the model, without retraining. Leveraging \\gls*{cw} technique from Computer Vision combined with orthogonal projection, CALM removes unwanted latent directions associated with harmful content while preserving model performance. Experiments show that CALM reduces harmful outputs and outperforms baseline methods in most metrics, offering a lightweight approach to AI safety with no additional training data or model fine-tuning, while incurring only a small computational overhead at inference.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "190",
        "title": "Demystifying Hybrid Thinking: Can LLMs Truly Switch Between Think and No-Think?",
        "author": [
            "Shouren Wang",
            "Wang Yang",
            "Xianxuan Long",
            "Qifan Wang",
            "Vipin Chaudhary",
            "Xiaotian Han"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12680",
        "abstract": "Hybrid thinking enables LLMs to switch between reasoning and direct answering, offering a balance between efficiency and reasoning capability. Yet our experiments reveal that current hybrid thinking LLMs only achieve partial mode separation: reasoning behaviors often leak into the no-think mode. To understand and mitigate this, we analyze the factors influencing controllability and identify four that matter most: (1) larger data scale, (2) using think and no-think answers from different questions rather than the same question, (3) a moderate increase in no-think data number, and (4) a two-phase strategy that first trains reasoning ability and then applies hybrid think training. Building on these findings, we propose a practical recipe that, compared to standard training, can maintain accuracy in both modes while significantly reducing no-think output length (from $1085$ to $585$ on MATH500) and occurrences of reasoning-supportive tokens such as ``\\texttt{wait}'' (from $5917$ to $522$ on MATH500). Our findings highlight the limitations of current hybrid thinking and offer directions for strengthening its controllability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "191",
        "title": "Autonomous Legged Mobile Manipulation for Lunar Surface Operations via Constrained Reinforcement Learning",
        "author": [
            "Alvaro Belmonte-Baeza",
            "Miguel Cazorla",
            "Gabriel J. GarcÃ­a",
            "Carlos J. PÃ©rez-Del-Pulgar",
            "Jorge Pomares"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12684",
        "abstract": "Robotics plays a pivotal role in planetary science and exploration, where autonomous and reliable systems are crucial due to the risks and challenges inherent to space environments. The establishment of permanent lunar bases demands robotic platforms capable of navigating and manipulating in the harsh lunar terrain. While wheeled rovers have been the mainstay for planetary exploration, their limitations in unstructured and steep terrains motivate the adoption of legged robots, which offer superior mobility and adaptability. This paper introduces a constrained reinforcement learning framework designed for autonomous quadrupedal mobile manipulators operating in lunar environments. The proposed framework integrates whole-body locomotion and manipulation capabilities while explicitly addressing critical safety constraints, including collision avoidance, dynamic stability, and power efficiency, in order to ensure robust performance under lunar-specific conditions, such as reduced gravity and irregular terrain. Experimental results demonstrate the framework's effectiveness in achieving precise 6D task-space end-effector pose tracking, achieving an average positional accuracy of 4 cm and orientation accuracy of 8.1 degrees. The system consistently respects both soft and hard constraints, exhibiting adaptive behaviors optimized for lunar gravity conditions. This work effectively bridges adaptive learning with essential mission-critical safety requirements, paving the way for advanced autonomous robotic explorers for future lunar missions.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "192",
        "title": "EReLiFM: Evidential Reliability-Aware Residual Flow Meta-Learning for Open-Set Domain Generalization under Noisy Labels",
        "author": [
            "Kunyu Peng",
            "Di Wen",
            "Kailun Yang",
            "Jia Fu",
            "Yufan Chen",
            "Ruiping Liu",
            "Jiamin Wu",
            "Junwei Zheng",
            "M. Saquib Sarfraz",
            "Luc Van Gool",
            "Danda Pani Paudel",
            "Rainer Stiefelhagen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12687",
        "abstract": "Open-Set Domain Generalization (OSDG) aims to enable deep learning models to recognize unseen categories in new domains, which is crucial for real-world applications. Label noise hinders open-set domain generalization by corrupting source-domain knowledge, making it harder to recognize known classes and reject unseen ones. While existing methods address OSDG under Noisy Labels (OSDG-NL) using hyperbolic prototype-guided meta-learning, they struggle to bridge domain gaps, especially with limited clean labeled data. In this paper, we propose Evidential Reliability-Aware Residual Flow Meta-Learning (EReLiFM). We first introduce an unsupervised two-stage evidential loss clustering method to promote label reliability awareness. Then, we propose a residual flow matching mechanism that models structured domain- and category-conditioned residuals, enabling diverse and uncertainty-aware transfer paths beyond interpolation-based augmentation. During this meta-learning process, the model is optimized such that the update direction on the clean set maximizes the loss decrease on the noisy set, using pseudo labels derived from the most confident predicted class for supervision. Experimental results show that EReLiFM outperforms existing methods on OSDG-NL, achieving state-of-the-art performance. The source code is available at https://github.com/KPeng9510/ERELIFM.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "193",
        "title": "From Delegates to Trustees: How Optimizing for Long-Term Interests Shapes Bias and Alignment in LLM",
        "author": [
            "Suyash Fulay",
            "Jocelyn Zhu",
            "Michiel Bakker"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12689",
        "abstract": "Large language models (LLMs) have shown promising accuracy in predicting survey responses and policy preferences, which has increased interest in their potential to represent human interests in various domains. Most existing research has focused on behavioral cloning, effectively evaluating how well models reproduce individuals' expressed preferences. Drawing on theories of political representation, we highlight an underexplored design trade-off: whether AI systems should act as delegates, mirroring expressed preferences, or as trustees, exercising judgment about what best serves an individual's interests. This trade-off is closely related to issues of LLM sycophancy, where models can encourage behavior or validate beliefs that may be aligned with a user's short-term preferences, but is detrimental to their long-term interests. Through a series of experiments simulating votes on various policy issues in the U.S. context, we apply a temporal utility framework that weighs short and long-term interests (simulating a trustee role) and compare voting outcomes to behavior-cloning models (simulating a delegate). We find that trustee-style predictions weighted toward long-term interests produce policy decisions that align more closely with expert consensus on well-understood issues, but also show greater bias toward models' default stances on topics lacking clear agreement. These findings reveal a fundamental trade-off in designing AI systems to represent human interests. Delegate models better preserve user autonomy but may diverge from well-supported policy positions, while trustee models can promote welfare on well-understood issues yet risk paternalism and bias on subjective topics.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "194",
        "title": "DiffEM: Learning from Corrupted Data with Diffusion Models via Expectation Maximization",
        "author": [
            "Danial Hosseintabar",
            "Fan Chen",
            "Giannis Daras",
            "Antonio Torralba",
            "Constantinos Daskalakis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12691",
        "abstract": "Diffusion models have emerged as powerful generative priors for high-dimensional inverse problems, yet learning them when only corrupted or noisy observations are available remains challenging. In this work, we propose a new method for training diffusion models with Expectation-Maximization (EM) from corrupted data. Our proposed method, DiffEM, utilizes conditional diffusion models to reconstruct clean data from observations in the E-step, and then uses the reconstructed data to refine the conditional diffusion model in the M-step. Theoretically, we provide monotonic convergence guarantees for the DiffEM iteration, assuming appropriate statistical conditions. We demonstrate the effectiveness of our approach through experiments on various image reconstruction tasks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "195",
        "title": "ERA: Transforming VLMs into Embodied Agents via Embodied Prior Learning and Online Reinforcement Learning",
        "author": [
            "Hanyang Chen",
            "Mark Zhao",
            "Rui Yang",
            "Qinwei Ma",
            "Ke Yang",
            "Jiarui Yao",
            "Kangrui Wang",
            "Hao Bai",
            "Zhenhailong Wang",
            "Rui Pan",
            "Mengchao Zhang",
            "Jose Barreiros",
            "Aykut Onol",
            "ChengXiang Zhai",
            "Heng Ji",
            "Manling Li",
            "Huan Zhang",
            "Tong Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12693",
        "abstract": "Recent advances in embodied AI highlight the potential of vision language models (VLMs) as agents capable of perception, reasoning, and interaction in complex environments. However, top-performing systems rely on large-scale models that are costly to deploy, while smaller VLMs lack the necessary knowledge and skills to succeed. To bridge this gap, we present \\textit{Embodied Reasoning Agent (ERA)}, a two-stage framework that integrates prior knowledge learning and online reinforcement learning (RL). The first stage, \\textit{Embodied Prior Learning}, distills foundational knowledge from three types of data: (1) Trajectory-Augmented Priors, which enrich existing trajectory data with structured reasoning generated by stronger models; (2) Environment-Anchored Priors, which provide in-environment knowledge and grounding supervision; and (3) External Knowledge Priors, which transfer general knowledge from out-of-environment datasets. In the second stage, we develop an online RL pipeline that builds on these priors to further enhance agent performance. To overcome the inherent challenges in agent RL, including long horizons, sparse rewards, and training instability, we introduce three key designs: self-summarization for context management, dense reward shaping, and turn-level policy optimization. Extensive experiments on both high-level planning (EB-ALFRED) and low-level control (EB-Manipulation) tasks demonstrate that ERA-3B surpasses both prompting-based large models and previous training-based baselines. Specifically, it achieves overall improvements of 8.4\\% on EB-ALFRED and 19.4\\% on EB-Manipulation over GPT-4o, and exhibits strong generalization to unseen tasks. Overall, ERA offers a practical path toward scalable embodied intelligence, providing methodological insights for future embodied AI systems.",
        "tags": [
            "GPT",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "196",
        "title": "Multi-Agent Debate for LLM Judges with Adaptive Stability Detection",
        "author": [
            "Tianyu Hu",
            "Zhen Tan",
            "Song Wang",
            "Huaizhi Qu",
            "Tianlong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12697",
        "abstract": "With advancements in reasoning capabilities, Large Language Models (LLMs) are increasingly employed for automated judgment tasks. While LLMs-as-Judges offer promise in automating evaluations, current approaches often rely on simplistic aggregation methods (e.g., majority voting), which can fail even when individual agents provide correct answers. To address this, we propose a multi-agent debate judge framework where agents collaboratively reason and iteratively refine their responses. We formalize the debate process mathematically, analyzing agent interactions and proving that debate amplifies correctness compared to static ensembles. To enhance efficiency, we introduce a stability detection mechanism that models judge consensus dynamics via a time-varying Beta-Binomial mixture, with adaptive stopping based on distributional similarity (Kolmogorov-Smirnov test). This mechanism models the judges' collective correct rate dynamics using a time-varying mixture of Beta-Binomial distributions and employs an adaptive stopping criterion based on distributional similarity (Kolmogorov-Smirnov statistic). Experiments across multiple benchmarks and models demonstrate that our framework improves judgment accuracy over majority voting while maintaining computational efficiency.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "197",
        "title": "Generation Space Size: Understanding and Calibrating Open-Endedness of LLM Generations",
        "author": [
            "Sunny Yu",
            "Ahmad Jabbar",
            "Robert Hawkins",
            "Dan Jurafsky",
            "Myra Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12699",
        "abstract": "Different open-ended generation tasks require different degrees of output diversity. However, current LLMs are often miscalibrated. They collapse to overly homogeneous outputs for creative tasks and hallucinate diverse but incorrect responses for factual tasks. We argue that these two failure modes are unified by, and can both be addressed by, the notion of effective generation space size (GSS) -- the set of semantically distinct outputs a model considers for a prompt. We present GSSBench, a task suite of prompt pairs with ground-truth GSS relationships to assess different metrics and understand where models diverge from desired behavior. We find that hallucination detection metrics, particularly EigenScore, consistently outperform standard diversity and uncertainty quantification metrics, while using only model internals, providing interpretable insights into a model's internal task representations. We demonstrate three applications of GSS: (1) detecting prompt ambiguity and predicting clarification questions for better grounding, (2) interpreting overthinking and underthinking in reasoning models, and (3) steering models to expand their generation space to yield high-quality and diverse outputs.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "198",
        "title": "Beyond Postconditions: Can Large Language Models infer Formal Contracts for Automatic Software Verification?",
        "author": [
            "Cedric Richter",
            "Heike Wehrheim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12702",
        "abstract": "Automatic software verifiers have become increasingly effective at the task of checking software against (formal) specifications. Yet, their adoption in practice has been hampered by the lack of such specifications in real world code. Large Language Models (LLMs) have shown promise in inferring formal postconditions from natural language hints embedded in code such as function names, comments or documentation. Using the generated postconditions as specifications in a subsequent verification, however, often leads verifiers to suggest invalid inputs, hinting at potential issues that ultimately turn out to be false alarms.\nTo address this, we revisit the problem of specification inference from natural language in the context of automatic software verification. In the process, we introduce NL2Contract, the task of employing LLMs to translate informal natural language into formal functional contracts, consisting of postconditions as well as preconditions. We introduce metrics to validate and compare different NL2Contract approaches, using soundness, bug discriminative power of the generated contracts and their usability in the context of automatic software verification as key metrics. We evaluate NL2Contract with different LLMs and compare it to the task of postcondition generation nl2postcond. Our evaluation shows that (1) LLMs are generally effective at generating functional contracts sound for all possible inputs, (2) the generated contracts are sufficiently expressive for discriminating buggy from correct behavior, and (3) verifiers supplied with LLM inferred functional contracts produce fewer false alarms than when provided with postconditions alone. Further investigations show that LLM inferred preconditions generally align well with developers intentions which allows us to use automatic software verifiers to catch real-world bugs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "199",
        "title": "Reflection-Based Task Adaptation for Self-Improving VLA",
        "author": [
            "Baicheng Li",
            "Dong Wu",
            "Zike Yan",
            "Xinchen Liu",
            "Zecui Zeng",
            "Lusong Li",
            "Hongbin Zha"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12710",
        "abstract": "Pre-trained Vision-Language-Action (VLA) models represent a major leap towards general-purpose robots, yet efficiently adapting them to novel, specific tasks in-situ remains a significant hurdle. While reinforcement learning (RL) is a promising avenue for such adaptation, the process often suffers from low efficiency, hindering rapid task mastery. We introduce Reflective Self-Adaptation, a framework for rapid, autonomous task adaptation without human intervention. Our framework establishes a self-improving loop where the agent learns from its own experience to enhance both strategy and execution.\nThe core of our framework is a dual-pathway architecture that addresses the full adaptation lifecycle. First, a Failure-Driven Reflective RL pathway enables rapid learning by using the VLM's causal reasoning to automatically synthesize a targeted, dense reward function from failure analysis. This provides a focused learning signal that significantly accelerates policy exploration. However, optimizing such proxy rewards introduces a potential risk of \"reward hacking,\" where the agent masters the reward function but fails the actual task. To counteract this, our second pathway, Success-Driven Quality-Guided SFT, grounds the policy in holistic success. It identifies and selectively imitates high-quality successful trajectories, ensuring the agent remains aligned with the ultimate task goal. This pathway is strengthened by a conditional curriculum mechanism to aid initial exploration.\nWe conduct experiments in challenging manipulation tasks. The results demonstrate that our framework achieves faster convergence and higher final success rates compared to representative baselines. Our work presents a robust solution for creating self-improving agents that can efficiently and reliably adapt to new environments.",
        "tags": [
            "RL",
            "VLM"
        ]
    },
    {
        "id": "200",
        "title": "Beyond Seeing: Evaluating Multimodal LLMs on Tool-Enabled Image Perception, Transformation, and Reasoning",
        "author": [
            "Xingang Guo",
            "Utkarsh Tyagi",
            "Advait Gosai",
            "Paula Vergara",
            "Ernesto Gabriel HernÃ¡ndez Montoya",
            "Chen Bo Calvin Zhang",
            "Bin Hu",
            "Yunzhong He",
            "Bing Liu",
            "Rakshith Sharma Srinivasa"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12712",
        "abstract": "Multimodal Large Language Models (MLLMs) are increasingly applied in real-world scenarios where user-provided images are often imperfect, requiring active image manipulations such as cropping, editing, or enhancement to uncover salient visual cues. Beyond static visual perception, MLLMs must also think with images: dynamically transforming visual content and integrating it with other tools to solve complex tasks. However, this shift from treating vision as passive context to a manipulable cognitive workspace remains underexplored. Most existing benchmarks still follow a think about images paradigm, where images are regarded as static inputs. To address this gap, we introduce IRIS, an Interactive Reasoning with Images and Systems that evaluates MLLMs' ability to perceive, transform, and reason across complex visual-textual tasks under the think with images paradigm. IRIS comprises 1,204 challenging, open-ended vision tasks (603 single-turn, 601 multi-turn) spanning across five diverse domains, each paired with detailed rubrics to enable systematic evaluation. Our evaluation shows that current MLLMs struggle with tasks requiring effective integration of vision and general-purpose tools. Even the strongest model (GPT-5-think) reaches only 18.68% pass rate. We further observe divergent tool-use behaviors, with OpenAI models benefiting from diverse image manipulations while Gemini-2.5-pro shows no improvement. By introducing the first benchmark centered on think with images, IRIS offers critical insights for advancing visual intelligence in MLLMs.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "201",
        "title": "Residual MPC: Blending Reinforcement Learning with GPU-Parallelized Model Predictive Control",
        "author": [
            "Se Hwan Jeon",
            "Ho Jae Lee",
            "Seungwoo Hong",
            "Sangbae Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12717",
        "abstract": "Model Predictive Control (MPC) provides interpretable, tunable locomotion controllers grounded in physical models, but its robustness depends on frequent replanning and is limited by model mismatch and real-time computational constraints. Reinforcement Learning (RL), by contrast, can produce highly robust behaviors through stochastic training but often lacks interpretability, suffers from out-of-distribution failures, and requires intensive reward engineering. This work presents a GPU-parallelized residual architecture that tightly integrates MPC and RL by blending their outputs at the torque-control level. We develop a kinodynamic whole-body MPC formulation evaluated across thousands of agents in parallel at 100 Hz for RL training. The residual policy learns to make targeted corrections to the MPC outputs, combining the interpretability and constraint handling of model-based control with the adaptability of RL. The model-based control prior acts as a strong bias, initializing and guiding the policy towards desirable behavior with a simple set of rewards. Compared to standalone MPC or end-to-end RL, our approach achieves higher sample efficiency, converges to greater asymptotic rewards, expands the range of trackable velocity commands, and enables zero-shot adaptation to unseen gaits and uneven terrain.",
        "tags": [
            "MPC",
            "RL"
        ]
    },
    {
        "id": "202",
        "title": "CARVQ: Corrective Adaptor with Group Residual Vector Quantization for LLM Embedding Compression",
        "author": [
            "Dayin Gou",
            "Sanghyun Byun",
            "Nilesh Malpeddi",
            "Gabrielle De Micheli",
            "Prathamesh Vaste",
            "Jacob Song",
            "Woo Seong Chung"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12721",
        "abstract": "Large Language Models (LLMs) typically rely on a large number of parameters for token embedding, leading to substantial storage requirements and memory footprints. In particular, LLMs deployed on edge devices are memory-bound, and reducing the memory footprint by compressing the embedding layer not only frees up the memory bandwidth but also speeds up inference. To address this, we introduce CARVQ, a post-training novel Corrective Adaptor combined with group Residual Vector Quantization. CARVQ relies on the composition of both linear and non-linear maps and mimics the original model embedding to compress to approximately 1.6 bits without requiring specialized hardware to support lower-bit storage. We test our method on pre-trained LLMs such as LLaMA-3.2-1B, LLaMA-3.2-3B, LLaMA-3.2-3B-Instruct, LLaMA-3.1-8B, Qwen2.5-7B, Qwen2.5-Math-7B and Phi-4, evaluating on common generative, discriminative, math and reasoning tasks. We show that in most cases, CARVQ can achieve lower average bitwidth-per-parameter while maintaining reasonable perplexity and accuracy compared to scalar quantization. Our contributions include a novel compression technique that is compatible with state-of-the-art transformer quantization methods and can be seamlessly integrated into any hardware supporting 4-bit memory to reduce the model's memory footprint in memory-constrained devices. This work demonstrates a crucial step toward the efficient deployment of LLMs on edge devices.",
        "tags": [
            "LLM",
            "LLaMA",
            "Transformer",
            "Vector Quantization"
        ]
    },
    {
        "id": "203",
        "title": "T(R,O) Grasp: Efficient Graph Diffusion of Robot-Object Spatial Transformation for Cross-Embodiment Dexterous Grasping",
        "author": [
            "Xin Fei",
            "Zhixuan Xu",
            "Huaicong Fang",
            "Tianrui Zhang",
            "Lin Shao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12724",
        "abstract": "Dexterous grasping remains a central challenge in robotics due to the complexity of its high-dimensional state and action space. We introduce T(R,O) Grasp, a diffusion-based framework that efficiently generates accurate and diverse grasps across multiple robotic hands. At its core is the T(R,O) Graph, a unified representation that models spatial transformations between robotic hands and objects while encoding their geometric properties. A graph diffusion model, coupled with an efficient inverse kinematics solver, supports both unconditioned and conditioned grasp synthesis. Extensive experiments on a diverse set of dexterous hands show that T(R,O) Grasp achieves average success rate of 94.83%, inference speed of 0.21s, and throughput of 41 grasps per second on an NVIDIA A100 40GB GPU, substantially outperforming existing baselines. In addition, our approach is robust and generalizable across embodiments while significantly reducing memory consumption. More importantly, the high inference speed enables closed-loop dexterous manipulation, underscoring the potential of T(R,O) Grasp to scale into a foundation model for dexterous grasping.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "204",
        "title": "Data-Model Co-Evolution: Growing Test Sets to Refine LLM Behavior",
        "author": [
            "Minjae Lee",
            "Minsuk Kahng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12728",
        "abstract": "A long-standing challenge in machine learning has been the rigid separation between data work and model refinement, enforced by slow fine-tuning cycles. The rise of Large Language Models (LLMs) overcomes this historical barrier, allowing applications developers to instantly govern model behavior by editing prompt instructions. This shift enables a new paradigm: data-model co-evolution, where a living test set and a model's instructions evolve in tandem. We operationalize this paradigm in an interactive system designed to address the critical challenge of encoding subtle, domain-specific policies into prompt instructions. The system's structured workflow guides people to discover edge cases, articulate rationales for desired behavior, and iteratively evaluate instruction revisions against a growing test set. A user study shows our workflow helps participants refine instructions systematically and specify ambiguous policies more concretely. This work points toward more robust and responsible LLM applications through human-in-the-loop development aligned with local preferences and policies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "205",
        "title": "CTRL-Rec: Controlling Recommender Systems With Natural Language",
        "author": [
            "Micah Carroll",
            "Adeline Foote",
            "Kevin Feng",
            "Marcus Williams",
            "Anca Dragan",
            "W. Bradley Knox",
            "Smitha Milli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12742",
        "abstract": "When users are dissatisfied with recommendations from a recommender system, they often lack fine-grained controls for changing them. Large language models (LLMs) offer a solution by allowing users to guide their recommendations through natural language requests (e.g., \"I want to see respectful posts with a different perspective than mine\"). We propose a method, CTRL-Rec, that allows for natural language control of traditional recommender systems in real-time with computational efficiency. Specifically, at training time, we use an LLM to simulate whether users would approve of items based on their language requests, and we train embedding models that approximate such simulated judgments. We then integrate these user-request-based predictions into the standard weighting of signals that traditional recommender systems optimize. At deployment time, we require only a single LLM embedding computation per user request, allowing for real-time control of recommendations. In experiments with the MovieLens dataset, our method consistently allows for fine-grained control across a diversity of requests. In a study with 19 Letterboxd users, we find that CTRL-Rec was positively received by users and significantly enhanced users' sense of control and satisfaction with recommendations compared to traditional controls.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "206",
        "title": "FlashVSR: Towards Real-Time Diffusion-Based Streaming Video Super-Resolution",
        "author": [
            "Junhao Zhuang",
            "Shi Guo",
            "Xin Cai",
            "Xiaohui Li",
            "Yihao Liu",
            "Chun Yuan",
            "Tianfan Xue"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12747",
        "abstract": "Diffusion models have recently advanced video restoration, but applying them to real-world video super-resolution (VSR) remains challenging due to high latency, prohibitive computation, and poor generalization to ultra-high resolutions. Our goal in this work is to make diffusion-based VSR practical by achieving efficiency, scalability, and real-time performance. To this end, we propose FlashVSR, the first diffusion-based one-step streaming framework towards real-time VSR. FlashVSR runs at approximately 17 FPS for 768x1408 videos on a single A100 GPU by combining three complementary innovations: (i) a train-friendly three-stage distillation pipeline that enables streaming super-resolution, (ii) locality-constrained sparse attention that cuts redundant computation while bridging the train-test resolution gap, and (iii) a tiny conditional decoder that accelerates reconstruction without sacrificing quality. To support large-scale training, we also construct VSR-120K, a new dataset with 120k videos and 180k images. Extensive experiments show that FlashVSR scales reliably to ultra-high resolutions and achieves state-of-the-art performance with up to 12x speedup over prior one-step diffusion VSR models. We will release the code, pretrained models, and dataset to foster future research in efficient diffusion-based VSR.",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "207",
        "title": "VQArt-Bench: A semantically rich VQA Benchmark for Art and Cultural Heritage",
        "author": [
            "A. Alfarano",
            "L. Venturoli",
            "D. Negueruela del Castillo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12750",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in joint visual and linguistic tasks. However, existing Visual Question Answering (VQA) benchmarks often fail to evaluate deep semantic understanding, particularly in complex domains like visual art analysis. Confined to simple syntactic structures and surface-level attributes, these questions fail to capture the diversity and depth of human visual inquiry. This limitation incentivizes models to exploit statistical shortcuts rather than engage in visual reasoning. To address this gap, we introduce VQArt-Bench, a new, large-scale VQA benchmark for the cultural heritage domain. This benchmark is constructed using a novel multi-agent pipeline where specialized agents collaborate to generate nuanced, validated, and linguistically diverse questions. The resulting benchmark is structured along relevant visual understanding dimensions that probe a model's ability to interpret symbolic meaning, narratives, and complex visual relationships. Our evaluation of 14 state-of-the-art MLLMs on this benchmark reveals significant limitations in current models, including a surprising weakness in simple counting tasks and a clear performance gap between proprietary and open-source models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "208",
        "title": "AnyUp: Universal Feature Upsampling",
        "author": [
            "Thomas Wimmer",
            "Prune Truong",
            "Marie-Julie Rakotosaona",
            "Michael Oechsle",
            "Federico Tombari",
            "Bernt Schiele",
            "Jan Eric Lenssen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12764",
        "abstract": "We introduce AnyUp, a method for feature upsampling that can be applied to any vision feature at any resolution, without encoder-specific training. Existing learning-based upsamplers for features like DINO or CLIP need to be re-trained for every feature extractor and thus do not generalize to different feature types at inference time. In this work, we propose an inference-time feature-agnostic upsampling architecture to alleviate this limitation and improve upsampling quality. In our experiments, AnyUp sets a new state of the art for upsampled features, generalizes to different feature types, and preserves feature semantics while being efficient and easy to apply to a wide range of downstream tasks.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "209",
        "title": "Language Models Model Language",
        "author": [
            "Åukasz Borchmann"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12766",
        "abstract": "Linguistic commentary on LLMs, heavily influenced by the theoretical frameworks of de Saussure and Chomsky, is often speculative and unproductive. Critics challenge whether LLMs can legitimately model language, citing the need for \"deep structure\" or \"grounding\" to achieve an idealized linguistic \"competence.\" We argue for a radical shift in perspective towards the empiricist principles of Witold MaÅczak, a prominent general and historical linguist. He defines language not as a \"system of signs\" or a \"computational system of the brain\" but as the totality of all that is said and written. Above all, he identifies frequency of use of particular language elements as language's primary governing principle. Using his framework, we challenge prior critiques of LLMs and provide a constructive guide for designing, evaluating, and interpreting language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "210",
        "title": "Uncertainty Matters in Dynamic Gaussian Splatting for Monocular 4D Reconstruction",
        "author": [
            "Fengzhi Guo",
            "Chih-Chuan Hsu",
            "Sihao Ding",
            "Cheng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12768",
        "abstract": "Reconstructing dynamic 3D scenes from monocular input is fundamentally under-constrained, with ambiguities arising from occlusion and extreme novel views. While dynamic Gaussian Splatting offers an efficient representation, vanilla models optimize all Gaussian primitives uniformly, ignoring whether they are well or poorly observed. This limitation leads to motion drifts under occlusion and degraded synthesis when extrapolating to unseen views. We argue that uncertainty matters: Gaussians with recurring observations across views and time act as reliable anchors to guide motion, whereas those with limited visibility are treated as less reliable. To this end, we introduce USplat4D, a novel Uncertainty-aware dynamic Gaussian Splatting framework that propagates reliable motion cues to enhance 4D reconstruction. Our key insight is to estimate time-varying per-Gaussian uncertainty and leverages it to construct a spatio-temporal graph for uncertainty-aware optimization. Experiments on diverse real and synthetic datasets show that explicitly modeling uncertainty consistently improves dynamic Gaussian Splatting models, yielding more stable geometry under occlusion and high-quality synthesis at extreme viewpoints.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "211",
        "title": "Dr.LLM: Dynamic Layer Routing in LLMs",
        "author": [
            "Ahmed Heakl",
            "Martin Gubri",
            "Salman Khan",
            "Sangdoo Yun",
            "Seong Joon Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12773",
        "abstract": "Large Language Models (LLMs) process every token through all layers of a transformer stack, causing wasted computation on simple queries and insufficient flexibility for harder ones that need deeper reasoning. Adaptive-depth methods can improve efficiency, but prior approaches rely on costly inference-time search, architectural changes, or large-scale retraining, and in practice often degrade accuracy despite efficiency gains. We introduce http://Dr.LLM, Dynamic routing of Layers for LLMs, a retrofittable framework that equips pretrained models with lightweight per-layer routers deciding to skip, execute, or repeat a block. Routers are trained with explicit supervision: using Monte Carlo Tree Search (MCTS), we derive high-quality layer configurations that preserve or improve accuracy under a compute budget. Our design, windowed pooling for stable routing, focal loss with class balancing, and bottleneck MLP routers, ensures robustness under class imbalance and long sequences. On ARC (logic) and DART (math), http://Dr.LLM improves accuracy by up to +3.4%p while saving 5 layers per example on average. Routers generalize to out-of-domain tasks (MMLU, GSM8k, AIME, TruthfulQA, SQuADv2, GPQA, PIQA, AGIEval) with only 0.85% accuracy drop while retaining efficiency, and outperform prior routing methods by up to +7.7%p. Overall, http://Dr.LLM shows that explicitly supervised routers retrofit frozen LLMs for budget-aware, accuracy-driven inference without altering base weights.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "212",
        "title": "What If : Understanding Motion Through Sparse Interactions",
        "author": [
            "Stefan Andreas Baumann",
            "Nick Stracke",
            "Timy Phan",
            "BjÃ¶rn Ommer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12777",
        "abstract": "Understanding the dynamics of a physical scene involves reasoning about the diverse ways it can potentially change, especially as a result of local interactions. We present the Flow Poke Transformer (FPT), a novel framework for directly predicting the distribution of local motion, conditioned on sparse interactions termed \"pokes\". Unlike traditional methods that typically only enable dense sampling of a single realization of scene dynamics, FPT provides an interpretable directly accessible representation of multi-modal scene motion, its dependency on physical interactions and the inherent uncertainties of scene dynamics. We also evaluate our model on several downstream tasks to enable comparisons with prior methods and highlight the flexibility of our approach. On dense face motion generation, our generic pre-trained model surpasses specialized baselines. FPT can be fine-tuned in strongly out-of-distribution tasks such as synthetic datasets to enable significant improvements over in-domain methods in articulated object motion estimation. Additionally, predicting explicit motion distributions directly enables our method to achieve competitive performance on tasks like moving part segmentation from pokes which further demonstrates the versatility of our FPT. Code and models are publicly available at https://compvis.github.io/flow-poke-transformer.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "213",
        "title": "MVP4D: Multi-View Portrait Video Diffusion for Animatable 4D Avatars",
        "author": [
            "Felix Taubner",
            "Ruihang Zhang",
            "Mathieu Tuli",
            "Sherwin Bahmani",
            "David B. Lindell"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12785",
        "abstract": "Digital human avatars aim to simulate the dynamic appearance of humans in virtual environments, enabling immersive experiences across gaming, film, virtual reality, and more. However, the conventional process for creating and animating photorealistic human avatars is expensive and time-consuming, requiring large camera capture rigs and significant manual effort from professional 3D artists. With the advent of capable image and video generation models, recent methods enable automatic rendering of realistic animated avatars from a single casually captured reference image of a target subject. While these techniques significantly lower barriers to avatar creation and offer compelling realism, they lack constraints provided by multi-view information or an explicit 3D representation. So, image quality and realism degrade when rendered from viewpoints that deviate strongly from the reference image. Here, we build a video model that generates animatable multi-view videos of digital humans based on a single reference image and target expressions. Our model, MVP4D, is based on a state-of-the-art pre-trained video diffusion model and generates hundreds of frames simultaneously from viewpoints varying by up to 360 degrees around a target subject. We show how to distill the outputs of this model into a 4D avatar that can be rendered in real-time. Our approach significantly improves the realism, temporal consistency, and 3D consistency of generated avatars compared to previous methods.",
        "tags": [
            "3D",
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "214",
        "title": "Ax-Prover: A Deep Reasoning Agentic Framework for Theorem Proving in Mathematics and Quantum Physics",
        "author": [
            "Marco Del Tredici",
            "Jacob McCarran",
            "Benjamin Breen",
            "Javier Aspuru Mijares",
            "Weichen Winston Yin",
            "Jacob M. Taylor",
            "Frank Koppens",
            "Dirk Englund"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12787",
        "abstract": "We present Ax-Prover, a multi-agent system for automated theorem proving in Lean that can solve problems across diverse scientific domains and operate either autonomously or collaboratively with human experts. To achieve this, Ax-Prover approaches scientific problem solving through formal proof generation, a process that demands both creative reasoning and strict syntactic rigor. Ax-Prover meets this challenge by equipping Large Language Models (LLMs), which provide knowledge and reasoning, with Lean tools via the Model Context Protocol (MCP), which ensure formal correctness. To evaluate its performance as an autonomous prover, we benchmark our approach against frontier LLMs and specialized prover models on two public math benchmarks and on two Lean benchmarks we introduce in the fields of abstract algebra and quantum theory. On public datasets, Ax-Prover is competitive with state-of-the-art provers, while it largely outperform them on the new benchmarks. This shows that, unlike specialized systems that struggle to generalize, our tool-based agentic theorem prover approach offers a generalizable methodology for formal verification across diverse scientific domains. Furthermore, we demonstrate Ax-Prover's assistant capabilities in a practical use case, showing how it enabled an expert mathematician to formalize the proof of a complex cryptography theorem.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "215",
        "title": "UniFusion: Vision-Language Model as Unified Encoder in Image Generation",
        "author": [
            "Kevin Li",
            "Manuel Brack",
            "Sudeep Katakol",
            "Hareesh Ravi",
            "Ajinkya Kale"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12789",
        "abstract": "Although recent advances in visual generation have been remarkable, most existing architectures still depend on distinct encoders for images and text. This separation constrains diffusion models' ability to perform cross-modal reasoning and knowledge transfer. Prior attempts to bridge this gap often use the last layer information from VLM, employ multiple visual encoders, or train large unified models jointly for text and image generation, which demands substantial computational resources and large-scale data, limiting its http://accessibility.We present UniFusion, a diffusion-based generative model conditioned on a frozen large vision-language model (VLM) that serves as a unified multimodal encoder. At the core of UniFusion is the Layerwise Attention Pooling (LAP) mechanism that extracts both high level semantics and low level details from text and visual tokens of a frozen VLM to condition a diffusion generative model. We demonstrate that LAP outperforms other shallow fusion architectures on text-image alignment for generation and faithful transfer of visual information from VLM to the diffusion model which is key for editing. We propose VLM-Enabled Rewriting Injection with Flexibile Inference (VERIFI), which conditions a diffusion transformer (DiT) only on the text tokens generated by the VLM during in-model prompt rewriting. VERIFI combines the alignment of the conditioning distribution with the VLM's reasoning capabilities for increased capabilities and flexibility at inference. In addition, finetuning on editing task not only improves text-image alignment for generation, indicative of cross-modality knowledge transfer, but also exhibits tremendous generalization capabilities. Our model when trained on single image editing, zero-shot generalizes to multiple image references further motivating the unified encoder design of UniFusion.",
        "tags": [
            "DiT",
            "Diffusion",
            "Image Editing",
            "Transformer",
            "VLM"
        ]
    },
    {
        "id": "216",
        "title": "ViCO: A Training Strategy towards Semantic Aware Dynamic High-Resolution",
        "author": [
            "Long Cui",
            "Weiyun Wang",
            "Jie Shao",
            "Zichen Wen",
            "Gen Luo",
            "Linfeng Zhang",
            "Yanting Zhang",
            "Yu Qiao",
            "Wenhai Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12793",
        "abstract": "Existing Multimodal Large Language Models (MLLMs) suffer from increased inference costs due to the additional vision tokens introduced by image inputs. In this work, we propose Visual Consistency Learning (ViCO), a novel training algorithm that enables the model to represent images of varying semantic complexities using different numbers of vision tokens. The key idea behind our method is to employ multiple MLP connectors, each with a different image compression ratio, to downsample the vision tokens based on the semantic complexity of the image. During training, we minimize the KL divergence between the responses conditioned on different MLP connectors. At inference time, we introduce an image router, termed Visual Resolution Router (ViR), that automatically selects the appropriate compression rate for each image patch. Compared with existing dynamic high-resolution strategies, which adjust the number of visual tokens based on image resolutions, our method dynamically adapts the number of visual tokens according to semantic complexity. Experimental results demonstrate that our method can reduce the number of vision tokens by up to 50% while maintaining the model's perception, reasoning, and OCR capabilities. We hope this work will contribute to the development of more efficient MLLMs. The code and models will be released to facilitate future research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "217",
        "title": "DriveVLA-W0: World Models Amplify Data Scaling Law in Autonomous Driving",
        "author": [
            "Yingyan Li",
            "Shuyao Shang",
            "Weisong Liu",
            "Bing Zhan",
            "Haochen Wang",
            "Yuqi Wang",
            "Yuntao Chen",
            "Xiaoman Wang",
            "Yasong An",
            "Chufeng Tang",
            "Lu Hou",
            "Lue Fan",
            "Zhaoxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12796",
        "abstract": "Scaling Vision-Language-Action (VLA) models on large-scale data offers a promising path to achieving a more generalized driving intelligence. However, VLA models are limited by a ``supervision deficit'': the vast model capacity is supervised by sparse, low-dimensional actions, leaving much of their representational power underutilized. To remedy this, we propose \\textbf{DriveVLA-W0}, a training paradigm that employs world modeling to predict future images. This task generates a dense, self-supervised signal that compels the model to learn the underlying dynamics of the driving environment. We showcase the paradigm's versatility by instantiating it for two dominant VLA archetypes: an autoregressive world model for VLAs that use discrete visual tokens, and a diffusion world model for those operating on continuous visual features. Building on the rich representations learned from world modeling, we introduce a lightweight action expert to address the inference latency for real-time deployment. Extensive experiments on the NAVSIM v1/v2 benchmark and a 680x larger in-house dataset demonstrate that DriveVLA-W0 significantly outperforms BEV and VLA baselines. Crucially, it amplifies the data scaling law, showing that performance gains accelerate as the training dataset size increases.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "218",
        "title": "Detect Anything via Next Point Prediction",
        "author": [
            "Qing Jiang",
            "Junan Huo",
            "Xingyu Chen",
            "Yuda Xiong",
            "Zhaoyang Zeng",
            "Yihao Chen",
            "Tianhe Ren",
            "Junzhi Yu",
            "Lei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12798",
        "abstract": "Object detection has long been dominated by traditional coordinate regression-based models, such as YOLO, DETR, and Grounding DINO. Although recent efforts have attempted to leverage MLLMs to tackle this task, they face challenges like low recall rate, duplicate predictions, coordinate misalignment, etc. In this work, we bridge this gap and propose Rex-Omni, a 3B-scale MLLM that achieves state-of-the-art object perception performance. On benchmarks like COCO and LVIS, Rex-Omni attains performance comparable to or exceeding regression-based models (e.g., DINO, Grounding DINO) in a zero-shot setting. This is enabled by three key designs: 1) Task Formulation: we use special tokens to represent quantized coordinates from 0 to 999, reducing the model's learning difficulty and improving token efficiency for coordinate prediction; 2) Data Engines: we construct multiple data engines to generate high-quality grounding, referring, and pointing data, providing semantically rich supervision for training; \\3) Training Pipelines: we employ a two-stage training process, combining supervised fine-tuning on 22 million data with GRPO-based reinforcement post-training. This RL post-training leverages geometry-aware rewards to effectively bridge the discrete-to-continuous coordinate prediction gap, improve box accuracy, and mitigate undesirable behaviors like duplicate predictions that stem from the teacher-guided nature of the initial SFT stage. Beyond conventional detection, Rex-Omni's inherent language understanding enables versatile capabilities such as object referring, pointing, visual prompting, GUI grounding, spatial referring, OCR and key-pointing, all systematically evaluated on dedicated benchmarks. We believe that Rex-Omni paves the way for more versatile and language-aware visual perception systems.",
        "tags": [
            "Detection",
            "GRPO",
            "RL"
        ]
    },
    {
        "id": "219",
        "title": "DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search",
        "author": [
            "Kartik Narayan",
            "Yang Xu",
            "Tian Cao",
            "Kavya Nerella",
            "Vishal M. Patel",
            "Navid Shiee",
            "Peter Grasch",
            "Chao Jia",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12801",
        "abstract": "Multimodal Large Language Models (MLLMs) in real-world applications require access to external knowledge sources and must remain responsive to the dynamic and ever-changing real-world information in order to address information-seeking and knowledge-intensive user queries. Existing approaches, such as retrieval augmented generation (RAG) methods, search agents, and search equipped MLLMs, often suffer from rigid pipelines, excessive search calls, and poorly constructed search queries, which result in inefficiencies and suboptimal outcomes. To address these limitations, we present DeepMMSearch-R1, the first multimodal LLM capable of performing on-demand, multi-turn web searches and dynamically crafting queries for both image and text search tools. Specifically, DeepMMSearch-R1 can initiate web searches based on relevant crops of the input image making the image search more effective, and can iteratively adapt text search queries based on retrieved information, thereby enabling self-reflection and self-correction. Our approach relies on a two-stage training pipeline: a cold start supervised finetuning phase followed by an online reinforcement learning optimization. For training, we introduce DeepMMSearchVQA, a novel multimodal VQA dataset created through an automated pipeline intermixed with real-world information from web search tools. This dataset contains diverse, multi-hop queries that integrate textual and visual information, teaching the model when to search, what to search for, which search tool to use and how to reason over the retrieved information. We conduct extensive experiments across a range of knowledge-intensive benchmarks to demonstrate the superiority of our approach. Finally, we analyze the results and provide insights that are valuable for advancing multimodal web-search.",
        "tags": [
            "LLM",
            "RAG",
            "RL"
        ]
    },
    {
        "id": "220",
        "title": "Probabilistic Super-Resolution for Urban Micrometeorology via a SchrÃ¶dinger Bridge",
        "author": [
            "Yuki Yasuda",
            "Ryo Onishi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12148",
        "abstract": "This study employs a neural network that represents the solution to a SchrÃ¶dinger bridge problem to perform super-resolution of 2-m temperature in an urban area. SchrÃ¶dinger bridges generally describe transformations between two data distributions based on diffusion processes. We use a specific SchrÃ¶dinger-bridge model (SM) that directly transforms low-resolution data into high-resolution data, unlike denoising diffusion probabilistic models (simply, diffusion models; DMs) that generate high-resolution data from Gaussian noise. Low-resolution and high-resolution data were obtained from separate numerical simulations with a physics-based model under common initial and boundary conditions. Compared with a DM, the SM attains comparable accuracy at one-fifth the computational cost, requiring 50 neural-network evaluations per datum for the DM and only 10 for the SM. Furthermore, high-resolution samples generated by the SM exhibit larger variance, implying superior uncertainty quantification relative to the DM. Owing to the reduced computational cost of the SM, our results suggest the feasibility of real-time ensemble micrometeorological prediction using SM-based super-resolution.",
        "tags": [
            "Diffusion",
            "Super Resolution"
        ]
    },
    {
        "id": "221",
        "title": "Learning Mean-Field Games through Mean-Field Actor-Critic Flow",
        "author": [
            "Mo Zhou",
            "Haosheng Zhou",
            "Ruimeng Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12180",
        "abstract": "We propose the Mean-Field Actor-Critic (MFAC) flow, a continuous-time learning dynamics for solving mean-field games (MFGs), combining techniques from reinforcement learning and optimal transport. The MFAC framework jointly evolves the control (actor), value function (critic), and distribution components through coupled gradient-based updates governed by partial differential equations (PDEs). A central innovation is the Optimal Transport Geodesic Picard (OTGP) flow, which drives the distribution toward equilibrium along Wasserstein-2 geodesics. We conduct a rigorous convergence analysis using Lyapunov functionals and establish global exponential convergence of the MFAC flow under a suitable timescale. Our results highlight the algorithmic interplay among actor, critic, and distribution components. Numerical experiments illustrate the theoretical findings and demonstrate the effectiveness of the MFAC framework in computing MFG equilibria.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "222",
        "title": "DiSTAR: Diffusion over a Scalable Token Autoregressive Representation for Speech Generation",
        "author": [
            "Yakun Song",
            "Xiaobin Zhuang",
            "Jiawei Chen",
            "Zhikang Niu",
            "Guanrou Yang",
            "Chenpeng Du",
            "Zhuo Chen",
            "Yuping Wang",
            "Yuxuan Wang",
            "Xie Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12210",
        "abstract": "Recent attempts to interleave autoregressive (AR) sketchers with diffusion-based refiners over continuous speech representations have shown promise, but they remain brittle under distribution shift and offer limited levers for controllability. We introduce DISTAR, a zero-shot text-to-speech framework that operates entirely in a discrete residual vector quantization (RVQ) code space and tightly couples an AR language model with a masked diffusion model, without forced alignment or a duration predictor. Concretely, DISTAR drafts block-level RVQ tokens with an AR language model and then performs parallel masked-diffusion infilling conditioned on the draft to complete the next block, yielding long-form synthesis with blockwise parallelism while mitigating classic AR exposure bias. The discrete code space affords explicit control at inference: DISTAR produces high-quality audio under both greedy and sample-based decoding using classifier-free guidance, supports trade-offs between robustness and diversity, and enables variable bit-rate and controllable computation via RVQ layer pruning at test time. Extensive experiments and ablations demonstrate that DISTAR surpasses state-of-the-art zero-shot TTS systems in robustness, naturalness, and speaker/style consistency, while maintaining rich output diversity. Audio samples are provided on https://anonymous.4open.science/w/DiSTAR_demo.",
        "tags": [
            "Diffusion",
            "Vector Quantization"
        ]
    },
    {
        "id": "223",
        "title": "A Gradient Guided Diffusion Framework for Chance Constrained Programming",
        "author": [
            "Boyang Zhang",
            "Zhiguo Wang",
            "Ya-Feng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12238",
        "abstract": "Chance constrained programming (CCP) is a powerful framework for addressing optimization problems under uncertainty. In this paper, we introduce a novel Gradient-Guided Diffusion-based Optimization framework, termed GGDOpt, which tackles CCP through three key innovations. First, GGDOpt accommodates a broad class of CCP problems without requiring the knowledge of the exact distribution of uncertainty-relying solely on a set of samples. Second, to address the nonconvexity of the chance constraints, it reformulates the CCP as a sampling problem over the product of two distributions: an unknown data distribution supported on a nonconvex set and a Boltzmann distribution defined by the objective function, which fully leverages both first- and second-order gradient information. Third, GGDOpt has theoretical convergence guarantees and provides practical error bounds under mild assumptions. By progressively injecting noise during the forward diffusion process to convexify the nonconvex feasible region, GGDOpt enables guided reverse sampling to generate asymptotically optimal solutions. Experimental results on synthetic datasets and a waveform design task in wireless communications demonstrate that GGDOpt outperforms existing methods in both solution quality and stability with nearly 80% overhead reduction.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "224",
        "title": "Learning Latent Energy-Based Models via Interacting Particle Langevin Dynamics",
        "author": [
            "Joanna Marks",
            "Tim Y. J. Wang",
            "O. Deniz Akyildiz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12311",
        "abstract": "We develop interacting particle algorithms for learning latent variable models with energy-based priors. To do so, we leverage recent developments in particle-based methods for solving maximum marginal likelihood estimation (MMLE) problems. Specifically, we provide a continuous-time framework for learning latent energy-based models, by defining stochastic differential equations (SDEs) that provably solve the MMLE problem. We obtain a practical algorithm as a discretisation of these SDEs and provide theoretical guarantees for the convergence of the proposed algorithm. Finally, we demonstrate the empirical effectiveness of our method on synthetic and image datasets.",
        "tags": [
            "Energy-Based Models"
        ]
    },
    {
        "id": "225",
        "title": "LiteVPNet: A Lightweight Network for Video Encoding Control in Quality-Critical Applications",
        "author": [
            "Vibhoothi Vibhoothi",
            "FranÃ§ois PitiÃ©",
            "Anil Kokaram"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12379",
        "abstract": "In the last decade, video workflows in the cinema production ecosystem have presented new use cases for video streaming technology. These new workflows, e.g. in On-set Virtual Production, present the challenge of requiring precise quality control and energy efficiency. Existing approaches to transcoding often fall short of these requirements, either due to a lack of quality control or computational overhead. To fill this gap, we present a lightweight neural network (LiteVPNet) for accurately predicting Quantisation Parameters for NVENC AV1 encoders that achieve a specified VMAF score. We use low-complexity features, including bitstream characteristics, video complexity measures, and CLIP-based semantic embeddings. Our results demonstrate that LiteVPNet achieves mean VMAF errors below 1.2 points across a wide range of quality targets. Notably, LiteVPNet achieves VMAF errors within 2 points for over 87% of our test corpus, c.f. approx 61% with state-of-the-art methods. LiteVPNet's performance across various quality regions highlights its applicability for enhancing high-value content transport and streaming for more energy-efficient, high-quality media experiences.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "226",
        "title": "Same model, better performance: the impact of shuffling on DNA Language Models benchmarking",
        "author": [
            "Davide Greco",
            "Konrad Rawlik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12617",
        "abstract": "Large Language Models are increasingly popular in genomics due to their potential to decode complex biological sequences. Hence, researchers require a standardized benchmark to evaluate DNA Language Models (DNA LMs) capabilities. However, evaluating DNA LMs is a complex task that intersects genomic's domain-specific challenges and machine learning methodologies, where seemingly minor implementation details can significantly compromise benchmark validity. We demonstrate this through BEND (Benchmarking DNA Language Models), where hardware-dependent hyperparameters -- number of data loading workers and buffer sizes -- create spurious performance variations of up to 4% for identical models. The problem stems from inadequate data shuffling interacting with domain specific data characteristics. Experiments with three DNA language models (HyenaDNA, DNABERT-2, ResNet-LM) show these artifacts affect both absolute performance and relative model rankings. We propose a simple solution: pre-shuffling data before storage eliminates hardware dependencies while maintaining efficiency. This work highlights how standard ML practices can interact unexpectedly with domain-specific data characteristics, with broader implications for benchmark design in specialized domains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "227",
        "title": "Adapting Noise to Data: Generative Flows from 1D Processes",
        "author": [
            "Jannis Chemseddine",
            "Gregor Kornhardt",
            "Richard Duong",
            "Gabriele Steidl"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12636",
        "abstract": "We introduce a general framework for constructing generative models using one-dimensional noising processes. Beyond diffusion processes, we outline examples that demonstrate the flexibility of our approach. Motivated by this, we propose a novel framework in which the 1D processes themselves are learnable, achieved by parameterizing the noise distribution through quantile functions that adapt to the data. Our construction integrates seamlessly with standard objectives, including Flow Matching and consistency models. Learning quantile-based noise naturally captures heavy tails and compact supports when present. Numerical experiments highlight both the flexibility and the effectiveness of our method.",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "Flow Matching"
        ]
    },
    {
        "id": "228",
        "title": "Contraction and entropy production in continuous-time Sinkhorn dynamics",
        "author": [
            "Anand Srinivasan",
            "Jean-Jacques Slotine"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12639",
        "abstract": "Recently, the vanishing-step-size limit of the Sinkhorn algorithm at finite regularization parameter $\\varepsilon$ was shown to be a mirror descent in the space of probability measures. We give $L^2$ contraction criteria in two time-dependent metrics induced by the mirror Hessian, which reduce to the coercivity of certain conditional expectation operators. We then give an exact identity for the entropy production rate of the Sinkhorn flow, which was previously known only to be nonpositive. Examining this rate shows that the standard semigroup analysis of diffusion processes extends systematically to the Sinkhorn flow. We show that the flow induces a reversible Markov dynamics on the target marginal as an Onsager gradient flow. We define the Dirichlet form associated to its (nonlocal) infinitesimal generator, prove a PoincarÃ© inequality for it, and show that the spectral gap is strictly positive along the Sinkhorn flow whenever $\\varepsilon > 0$. Lastly, we show that the entropy decay is exponential if and only if a logarithmic Sobolev inequality (LSI) holds. We give for illustration two immediate practical use-cases for the Sinkhorn LSI: as a design principle for the latent space in which generative models are trained, and as a stopping heuristic for discrete-time algorithms.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "229",
        "title": "Dendrograms of Mixing Measures for Softmax-Gated Gaussian Mixture of Experts: Consistency without Model Sweeps",
        "author": [
            "Do Tien Hai",
            "Trung Nguyen Mai",
            "TrungTin Nguyen",
            "Nhat Ho",
            "Binh T. Nguyen",
            "Christopher Drovandi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12744",
        "abstract": "We develop a unified statistical framework for softmax-gated Gaussian mixture of experts (SGMoE) that addresses three long-standing obstacles in parameter estimation and model selection: (i) non-identifiability of gating parameters up to common translations, (ii) intrinsic gate-expert interactions that induce coupled differential relations in the likelihood, and (iii) the tight numerator-denominator coupling in the softmax-induced conditional density. Our approach introduces Voronoi-type loss functions aligned with the gate-partition geometry and establishes finite-sample convergence rates for the maximum likelihood estimator (MLE). In over-specified models, we reveal a link between the MLE's convergence rate and the solvability of an associated system of polynomial equations characterizing near-nonidentifiable directions. For model selection, we adapt dendrograms of mixing measures to SGMoE, yielding a consistent, sweep-free selector of the number of experts that attains pointwise-optimal parameter rates under overfitting while avoiding multi-size training. Simulations on synthetic data corroborate the theory, accurately recovering the expert count and achieving the predicted rates for parameter estimation while closely approximating the regression function. Under model misspecification (e.g., $\\epsilon$-contamination), the dendrogram selection criterion is robust, recovering the true number of mixture components, while the Akaike information criterion, the Bayesian information criterion, and the integrated completed likelihood tend to overselect as sample size grows. On a maize proteomics dataset of drought-responsive traits, our dendrogram-guided SGMoE selects two experts, exposes a clear mixing-measure hierarchy, stabilizes the likelihood early, and yields interpretable genotype-phenotype maps, outperforming standard criteria without multi-size training.",
        "tags": [
            "MoE"
        ]
    }
]