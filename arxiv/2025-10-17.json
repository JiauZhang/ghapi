[
    {
        "id": "1",
        "title": "Choreographing Trash Cans: On Speculative Futures of Weak Robots in Public Spaces",
        "author": [
            "Minja Axelsson",
            "Lea Luka Sikau"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13810",
        "abstract": "Delivering groceries or cleaning airports, mobile robots exist in public spaces. While these examples showcase robots that execute tasks, this paper explores mobile robots that encourage posthuman collaboration rather than managing environments independently. With feigned fragility, cuteness and incomplete functionalities, the so-called \"weak robots\" invite passersby to engage not only on a utilitarian level, but also through imaginative and emotional responses. After examining the workings of \"weak robots\" by queering notions of function and ability, we introduce two speculative design fiction vignettes that describe choreographies of such robots in future urban spaces -- one exploring a utopian weak robot and the other a dystopian weak robot. We introduce these speculations in order to discuss how different values may drive design decisions, and how such decisions may shape and drive different socio-technical futures in which robots and humans share public spaces that incentivise collaboration.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "2",
        "title": "Generative AI in Heritage Practice: Improving the Accessibility of Heritage Guidance",
        "author": [
            "Jessica Witte",
            "Edmund Lee",
            "Lisa Brausem",
            "Verity Shillabeer",
            "Chiara Bonacchi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13811",
        "abstract": "This paper discusses the potential for integrating Generative Artificial Intelligence (GenAI) into professional heritage practice with the aim of enhancing the accessibility of public-facing guidance documents. We developed HAZEL, a GenAI chatbot fine-tuned to assist with revising written guidance relating to heritage conservation and interpretation. Using quantitative assessments, we compare HAZEL's performance to that of ChatGPT (GPT-4) in a series of tasks related to the guidance writing process. The results of this comparison indicate a slightly better performance of HAZEL over ChatGPT, suggesting that the GenAI chatbot is more effective once the underlying large language model (LLM) has been fine-tuned. However, we also note significant limitations, particularly in areas requiring cultural sensitivity and more advanced technical expertise. These findings suggest that, while GenAI cannot replace human heritage professionals in technical authoring tasks, its potential to automate and expedite certain aspects of guidance writing could offer valuable benefits to heritage organisations, especially in resource-constrained contexts.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "3",
        "title": "Large Language Models for Real-World IoT Device Identification",
        "author": [
            "Rameen Mahmood",
            "Tousif Ahmed",
            "Sai Teja Peddinti",
            "Danny Yuxing Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13817",
        "abstract": "The rapid expansion of IoT devices has outpaced current identification methods, creating significant risks for security, privacy, and network accountability. These challenges are heightened in open-world environments, where traffic metadata is often incomplete, noisy, or intentionally obfuscated. We introduce a semantic inference pipeline that reframes device identification as a language modeling task over heterogeneous network metadata. To construct reliable supervision, we generate high-fidelity vendor labels for the IoT Inspector dataset, the largest real-world IoT traffic corpus, using an ensemble of large language models guided by mutual-information and entropy-based stability scores. We then instruction-tune a quantized LLaMA3.18B model with curriculum learning to support generalization under sparsity and long-tail vendor distributions. Our model achieves 98.25% top-1 accuracy and 90.73% macro accuracy across 2,015 vendors while maintaining resilience to missing fields, protocol drift, and adversarial manipulation. Evaluation on an independent IoT testbed, coupled with explanation quality and adversarial stress tests, demonstrates that instruction-tuned LLMs provide a scalable and interpretable foundation for real-world device identification at scale.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "4",
        "title": "Joint Active RIS Configuration and User Power Control for Localization: A Neuroevolution-Based Approach",
        "author": [
            "George Stamatelis",
            "Hui Chen",
            "Henk Wymeersch",
            "George C. Alexandropoulos"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13819",
        "abstract": "This paper studies user localization aided by a Reconfigurable Intelligent Surface (RIS). A feedback link from the Base Station (BS) to the user is adopted to enable dynamic power control of the user pilot transmissions in the uplink. A novel multi-agent algorithm for the joint control of the RIS phase configuration and the user transmit power is presented, which is based on a hybrid approach integrating NeuroEvolution (NE) and supervised learning. The proposed scheme requires only single-bit feedback messages for the uplink power control, supports RIS elements with discrete responses, and is numerically shown to outperform fingerprinting, deep reinforcement learning baselines and backpropagation-based position estimators.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "5",
        "title": "LLM Agent Communication Protocol (LACP) Requires Urgent Standardization: A Telecom-Inspired Protocol is Necessary",
        "author": [
            "Xin Li",
            "Mengbing Liu",
            "Chau Yuen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13821",
        "abstract": "This position paper argues that the field of LLM agents requires a unified, telecom-inspired communication protocol to ensure safety, interoperability, and scalability, especially within the context of Next Generation (NextG) networks. Current ad-hoc communication methods are creating a fragmented ecosystem, reminiscent of the early \"protocol wars\" in networking, which stifles innovation and poses significant risks. Drawing inspiration from the layered, standardized protocols that underpin modern telecommunications, we propose the LLM-Agent Communication Protocol (LACP). LACP establishes a three-layer architecture designed to ensure semantic clarity in communication, transactional integrity for complex tasks, and robust, built-in security. In this position paper, we argue that adopting a principled, universal protocol is not merely beneficial but essential for realizing the potential of distributed AI. Such a standard is critical for ensuring that multi-agent systems can operate safely and reliably in the complex, real-time applications envisioned for 6G and beyond.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "6",
        "title": "A2AS: Agentic AI Runtime Security and Self-Defense",
        "author": [
            "Eugene Neelou",
            "Ivan Novikov",
            "Max Moroz",
            "Om Narayan",
            "Tiffany Saade",
            "Mika Ayenson",
            "Ilya Kabanov",
            "Jen Ozmen",
            "Edward Lee",
            "Vineeth Sai Narajala",
            "Emmanuel Guilherme Junior",
            "Ken Huang",
            "Huseyin Gulsin",
            "Jason Ross",
            "Marat Vyshegorodtsev",
            "Adelin Travers",
            "Idan Habler",
            "Rahul Jadav"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13825",
        "abstract": "The A2AS framework is introduced as a security layer for AI agents and LLM-powered applications, similar to how HTTPS secures HTTP. A2AS enforces certified behavior, activates model self-defense, and ensures context window integrity. It defines security boundaries, authenticates prompts, applies security rules and custom policies, and controls agentic behavior, enabling a defense-in-depth strategy. The A2AS framework avoids latency overhead, external dependencies, architectural changes, model retraining, and operational complexity. The BASIC security model is introduced as the A2AS foundation: (B) Behavior certificates enable behavior enforcement, (A) Authenticated prompts enable context window integrity, (S) Security boundaries enable untrusted input isolation, (I) In-context defenses enable secure model reasoning, (C) Codified policies enable application-specific rules. This first paper in the series introduces the BASIC security model and the A2AS framework, exploring their potential toward establishing the A2AS industry standard.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "7",
        "title": "Users as Annotators: LLM Preference Learning from Comparison Mode",
        "author": [
            "Zhongze Cai",
            "Xiaocheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13830",
        "abstract": "Pairwise preference data have played an important role in the alignment of large language models (LLMs). Each sample of such data consists of a prompt, two different responses to the prompt, and a binary label indicating which of the two responses is better. The labels are usually annotated by professional human annotators. In this paper, we consider an alternative approach to collect pairwise preference data -- user annotation from comparison mode. With the increasingly wider adoption of LLMs among the population, users are contributing more and more of their preference labels through their daily interactions with the LLMs. The upside of such labels is that users are the best experts in judging the responses to their own queries/prompts, but the downside is the lack of quality control in these labels. In this paper, we consider a new idea of generating two responses from two different models or two different versions of the same model. The asymmetry allows us to make an inference of the user's data quality through our proposed user behavior model. We develop an expectation-maximization algorithm to estimate a latent quality factor of the user, and filter users' annotation data accordingly. The downstream task shows the effectiveness of our approach in both capturing the user behavior and data filtering for LLM alignment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "8",
        "title": "Informed Routing in LLMs: Smarter Token-Level Computation for Faster Inference",
        "author": [
            "Chao Han",
            "Yijuan Liang",
            "Zihao Xuan",
            "Daokuan Wu",
            "Wei Zhang",
            "Xiaoyu Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13831",
        "abstract": "The deployment of large language models (LLMs) in real-world applications is increasingly limited by their high inference cost. While recent advances in dynamic token-level computation allocation attempt to improve efficiency by selectively activating model components per token, existing methods rely on greedy routing--a myopic execute-or-skip mechanism that often leads to irreversible information loss and suboptimal token selection. This paper introduces informed routing, a new paradigm that proactively addresses these issues. The key insight is to assess not only a token's immediate importance but also its recoverability, i.e., how well its transformation can be approximated. To this end, we propose the Lightweight Feature Forecaster (LFF), a small predictive module that estimates a unit's output before routing decisions are made. This enables a flexible execute-or-approximate policy that preserves model fidelity while drastically reducing computation. Extensive experiments on both language modeling and reasoning tasks show that informed routing achieves state-of-the-art efficiency-performance trade-offs across multiple sparsity levels. Notably, even without final LoRA fine-tuning, our method matches or surpasses strong baselines that require full fine-tuning, all while reducing training time by over 50%. The code is available at: https://github.com/EIT-NLP/informed-routing",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "9",
        "title": "Entropy Meets Importance: A Unified Head Importance-Entropy Score for Stable and Efficient Transformer Pruning",
        "author": [
            "Minsik Choi",
            "Hyegang Son",
            "Changhoon Kim",
            "Young Geun Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13832",
        "abstract": "Transformer-based models have achieved remarkable performance in NLP tasks. However, their structural characteristics-multiple layers and attention heads-introduce efficiency challenges in inference and deployment. To address these challenges, various pruning methods have recently been proposed. Notably, gradient-based methods using Head Importance Scores (HIS) have gained traction for interpretability, efficiency, and ability to identify redundant heads. However, HIS alone has limitations as it captures only the gradient-driven contribution, overlooking the diversity of attention patterns. To overcome these limitations, we introduce a novel pruning criterion, HIES (Head Importance-Entropy Score), which integrates head importance scores with attention entropy, providing complementary evidence on per-head contribution. Empirically, HIES-based pruning yields up to 15.2% improvement in model quality and 2.04x improvement in stability over HIS-only methods, enabling substantial model compression without sacrificing either accuracy or stability. Code will be released upon publication.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "10",
        "title": "ConDABench: Interactive Evaluation of Language Models for Data Analysis",
        "author": [
            "Avik Dutta",
            "Priyanshu Gupta",
            "Hosein Hasanbeig",
            "Rahul Pratap Singh",
            "Harshit Nigam",
            "Sumit Gulwani",
            "Arjun Radhakrishna",
            "Gustavo Soares",
            "Ashish Tiwari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13835",
        "abstract": "Real-world data analysis tasks often come with under-specified goals and unclean data. User interaction is necessary to understand and disambiguate a user's intent, and hence, essential to solving these complex tasks. Existing benchmarks for evaluating LLMs on data analysis tasks do not capture these complexities or provide first-class support for interactivity. We introduce ConDABench, a framework for generating conversational data analysis (ConDA) benchmarks and evaluating external tools on the generated benchmarks. \\bench consists of (a) a multi-agent workflow for generating realistic benchmarks from articles describing insights gained from public datasets, (b) 1,420 ConDA problems generated using this workflow, and (c) an evaluation harness that, for the first time, makes it possible to systematically evaluate conversational data analysis tools on the generated ConDA problems. Evaluation of state-of-the-art LLMs on the benchmarks reveals that while the new generation of models are better at solving more instances, they are not necessarily better at solving tasks that require sustained, long-form engagement. ConDABench is an avenue for model builders to measure progress towards truly collaborative models that can complete complex interactive tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "11",
        "title": "Meronymic Ontology Extraction via Large Language Models",
        "author": [
            "Dekai Zhang",
            "Simone Conia",
            "Antonio Rago"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13839",
        "abstract": "Ontologies have become essential in today's digital age as a way of organising the vast amount of readily available unstructured text. In providing formal structure to this information, ontologies have immense value and application across various domains, e.g., e-commerce, where countless product listings necessitate proper product organisation. However, the manual construction of these ontologies is a time-consuming, expensive and laborious process. In this paper, we harness the recent advancements in large language models (LLMs) to develop a fully-automated method of extracting product ontologies, in the form of meronymies, from raw review texts. We demonstrate that the ontologies produced by our method surpass an existing, BERT-based baseline when evaluating using an LLM-as-a-judge. Our investigation provides the groundwork for LLMs to be used more generally in (product or otherwise) ontology extraction.",
        "tags": [
            "BERT",
            "LLM"
        ]
    },
    {
        "id": "12",
        "title": "ADMIT: Few-shot Knowledge Poisoning Attacks on RAG-based Fact Checking",
        "author": [
            "Yutao Wu",
            "Xiao Liu",
            "Yinghui Li",
            "Yifeng Gao",
            "Yifan Ding",
            "Jiale Ding",
            "Xiang Zheng",
            "Xingjun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13842",
        "abstract": "Knowledge poisoning poses a critical threat to Retrieval-Augmented Generation (RAG) systems by injecting adversarial content into knowledge bases, tricking Large Language Models (LLMs) into producing attacker-controlled outputs grounded in manipulated context. Prior work highlights LLMs' susceptibility to misleading or malicious retrieved content. However, real-world fact-checking scenarios are more challenging, as credible evidence typically dominates the retrieval pool. To investigate this problem, we extend knowledge poisoning to the fact-checking setting, where retrieved context includes authentic supporting or refuting evidence. We propose \\textbf{ADMIT} (\\textbf{AD}versarial \\textbf{M}ulti-\\textbf{I}njection \\textbf{T}echnique), a few-shot, semantically aligned poisoning attack that flips fact-checking decisions and induces deceptive justifications, all without access to the target LLMs, retrievers, or token-level control. Extensive experiments show that ADMIT transfers effectively across 4 retrievers, 11 LLMs, and 4 cross-domain benchmarks, achieving an average attack success rate (ASR) of 86\\% at an extremely low poisoning rate of $0.93 \\times 10^{-6}$, and remaining robust even in the presence of strong counter-evidence. Compared with prior state-of-the-art attacks, ADMIT improves ASR by 11.2\\% across all settings, exposing significant vulnerabilities in real-world RAG-based fact-checking systems.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "13",
        "title": "DynaSpec: Context-aware Dynamic Speculative Sampling for Large-Vocabulary Language Models",
        "author": [
            "Jinbin Zhang",
            "Nasib Ullah",
            "Erik Schultheis",
            "Rohit Babbar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13847",
        "abstract": "Speculative decoding (a.k.a. speculative sampling) has become a standard way to accelerate LLM inference: a small drafter proposes multiple tokens and a large target model verifies them once per speculation length. Recently, scaling of the LLM vocabulary has pushed the number of tokens to grow substantially. While verification over the full vocabulary leaves the target model largely unaffected, the O(|V|d) parameters in the drafter's output head become a latency bottleneck, slowing the entire pipeline. Contemporary methods (e.g., FR-Spec, VocabTrim) restrict the drafter's vocabulary to a fixed subset of the target model's vocabulary, ranked in descending order of token frequency. Although this reduces draft-time compute, it is brittle, since: (i) frequency lists are corpus-dependent and require retuning to generalize, and (ii) static shortlists suppress rare or domain-specific tokens, lowering the expected number of tokens per verification step. We propose DynaSpec, a context-dependent dynamic shortlisting mechanism that is robust, speeds up drafting, and generalizes across diverse tasks. Concretely, we introduce lightweight, coarse-grained meta-classifiers that route contexts to a small number of token clusters; the union of the top-k selected clusters forms the drafter's shortlist, while verification retains the full vocabulary and exactness. The meta-classifier finishes its computation earlier than the drafter's hidden state generation by exploiting parallel execution of draft encoding and meta shortlisting on separate streams. On standard speculative-decoding benchmarks, we observe consistent gains in mean accepted length over fixed-shortlist baselines, while context-dependent selection enables smaller shortlists without degrading acceptance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "14",
        "title": "On-device System of Compositional Multi-tasking in Large Language Models",
        "author": [
            "Ondrej Bohdal",
            "Konstantinos Theodosiadis",
            "Asterios Mpatziakas",
            "Dimitris Filippidis",
            "Iro Spyrou",
            "Christos Zonios",
            "Anastasios Drosou",
            "Dimosthenis Ioannidis",
            "Kyeng-Hun Lee",
            "Jijoong Moon",
            "Hyeonmok Ko",
            "Mete Ozay",
            "Umberto Michieli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13848",
        "abstract": "Large language models (LLMs) are commonly adapted for diverse downstream tasks via parameter-efficient fine-tuning techniques such as Low-Rank Adapters (LoRA). While adapters can be combined to handle multiple tasks separately, standard approaches struggle when targeting the simultaneous execution of complex tasks, such as generating a translated summary from a long conversation. To address this challenge, we propose a novel approach tailored specifically for compositional multi-tasking scenarios involving summarization and translation. Our technique involves adding a learnable projection layer on top of the combined summarization and translation adapters. This design enables effective integration while maintaining efficiency through reduced computational overhead compared to alternative strategies requiring extensive retraining or sequential processing. We demonstrate the practical viability of our method within an on-device environment by developing an Android app capable of executing compositional tasks seamlessly. Experimental results indicate our solution performs well and is fast in both cloud-based and on-device implementations, highlighting the potential benefits of adopting our framework in real-world applications demanding high-speed operation alongside resource constraints.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "15",
        "title": "Language steering in latent space to mitigate unintended code-switching",
        "author": [
            "Andrey Goncharov",
            "Nikolai Kondusov",
            "Alexey Zaytsev"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13849",
        "abstract": "Multilingual Large Language Models (LLMs) often exhibit unintended code-switching, reducing reliability in downstream tasks. We propose latent-space language steering, a lightweight inference-time method that identifies language directions via PCA on parallel translations and steers token embeddings along these axes to control language identity. Our approach mitigates code-switching while preserving semantics with negligible computational overhead and requires only minimal parallel data for calibration. Empirically, we achieve 95-99\\% language classification accuracy using a single principal component and reduce next-token distributional divergence by up to 42% across multiple language pairs on Qwen2.5 and Llama-3.2 models. We further analyze the layer-wise evolution of language representations, revealing that language identity concentrates in final layers with near-perfect linear separability.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "16",
        "title": "Revisiting the UID Hypothesis in LLM Reasoning Traces",
        "author": [
            "Minju Gwak",
            "Guijin Son",
            "Jaehyung Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13850",
        "abstract": "Large language models (LLMs) often solve problems using step-by-step Chain-of-Thought (CoT) reasoning, yet these intermediate steps are frequently unfaithful or hard to interpret. Inspired by the Uniform Information Density (UID) hypothesis in psycholinguistics -- which posits that humans communicate by maintaining a stable flow of information -- we introduce entropy-based metrics to analyze the information flow within reasoning traces. Surprisingly, across three challenging mathematical benchmarks, we find that successful reasoning in LLMs is globally non-uniform: correct solutions are characterized by uneven swings in information density, in stark contrast to human communication patterns. This result challenges assumptions about machine reasoning and suggests new directions for designing interpretable and adaptive reasoning models.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "17",
        "title": "EvoEdit: Evolving Null-space Alignment for Robust and Efficient Knowledge Editing",
        "author": [
            "Sicheng Lyu",
            "Yu Gu",
            "Xinyu Wang",
            "Jerry Huang",
            "Sitao Luan",
            "Yufei Cui",
            "Xiao-Wen Chang",
            "Peng Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13851",
        "abstract": "Large language models (LLMs) require continual updates to rectify outdated or erroneous knowledge. Model editing has emerged as a compelling paradigm for introducing targeted modifications without the computational burden of full retraining. Existing approaches are mainly based on a locate-then-edit framework. However, in sequential editing contexts, where multiple updates are applied over time, they exhibit significant limitations and suffer from catastrophic interference, i.e., new edits compromise previously integrated updates and degrade preserved knowledge. To address these challenges, we introduce EvoEdit, a novel editing strategy that mitigates catastrophic interference through sequential null-space alignment, enabling stable and efficient model editing. By performing sequential null-space alignment for each incoming edit, EvoEdit preserves both original and previously modified knowledge representations and maintains output invariance on preserved knowledge even across long edit sequences, effectively mitigating interference. Evaluations on real-world sequential knowledge-editing benchmarks show that EvoEdit achieves better or comparable performance than prior state-of-the-art locate-then-edit techniques, with up to 3.53 times speedup. Overall, these results underscore the necessity of developing more principled approaches for designing LLMs in dynamically evolving information settings, while providing a simple yet effective solution with strong theoretical guarantees.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "18",
        "title": "ConsistencyAI: A Benchmark to Assess LLMs' Factual Consistency When Responding to Different Demographic Groups",
        "author": [
            "Peter Banyas",
            "Shristi Sharma",
            "Alistair Simmons",
            "Atharva Vispute"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13852",
        "abstract": "Is an LLM telling you different facts than it's telling me? This paper introduces ConsistencyAI, an independent benchmark for measuring the factual consistency of large language models (LLMs) for different personas. ConsistencyAI tests whether, when users of different demographics ask identical questions, the model responds with factually inconsistent answers. Designed without involvement from LLM providers, this benchmark offers impartial evaluation and accountability. In our experiment, we queried 19 LLMs with prompts that requested 5 facts for each of 15 topics. We repeated this query 100 times for each LLM, each time adding prompt context from a different persona selected from a subset of personas modeling the general population. We processed the responses into sentence embeddings, computed cross-persona cosine similarity, and computed the weighted average of cross-persona cosine similarity to calculate factual consistency scores. In 100-persona experiments, scores ranged from 0.9065 to 0.7896, and the mean was 0.8656, which we adopt as a benchmark threshold. xAI's Grok-3 is most consistent, while several lightweight models rank lowest. Consistency varies by topic: the job market is least consistent, G7 world leaders most consistent, and issues like vaccines or the Israeli-Palestinian conflict diverge by provider. These results show that both the provider and the topic shape the factual consistency. We release our code and interactive demo to support reproducible evaluation and encourage persona-invariant prompting strategies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "19",
        "title": "Harnessing Consistency for Robust Test-Time LLM Ensemble",
        "author": [
            "Zhichen Zeng",
            "Qi Yu",
            "Xiao Lin",
            "Ruizhong Qiu",
            "Xuying Ning",
            "Tianxin Wei",
            "Yuchen Yan",
            "Jingrui He",
            "Hanghang Tong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13855",
        "abstract": "Different large language models (LLMs) exhibit diverse strengths and weaknesses, and LLM ensemble serves as a promising approach to integrate their complementary capabilities. Despite substantial progress in improving ensemble quality, limited attention has been paid to the robustness of ensembles against potential erroneous signals, which often arise from heterogeneous tokenization schemes and varying model expertise. Our analysis shows that ensemble failures typically arise from both the token level and the model level: the former reflects severe disagreement in token predictions, while the latter involves low confidence and pronounced disparities among models. In light of this, we propose CoRE, a plug-and-play technique that harnesses model consistency for robust LLM ensemble, which can be seamlessly integrated with diverse ensemble methods. Token-level consistency captures fine-grained disagreements by applying a low-pass filter to downweight uncertain tokens with high inconsistency, often due to token misalignment, thereby improving robustness at a granular level. Model-level consistency models global agreement by promoting model outputs with high self-confidence and minimal divergence from others, enhancing robustness at a coarser level. Extensive experiments across diverse benchmarks, model combinations, and ensemble strategies demonstrate that CoRE consistently improves ensemble performance and robustness.",
        "tags": [
            "Consistency Models",
            "LLM"
        ]
    },
    {
        "id": "20",
        "title": "From Craft to Constitution: A Governance-First Paradigm for Principled Agent Engineering",
        "author": [
            "Qiang Xu",
            "Xiangyu Wen",
            "Changran Xu",
            "Zeju Li",
            "Jianyuan Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13857",
        "abstract": "The advent of powerful Large Language Models (LLMs) has ushered in an ``Age of the Agent,'' enabling autonomous systems to tackle complex goals. However, the transition from prototype to production is hindered by a pervasive ``crisis of craft,'' resulting in agents that are brittle, unpredictable, and ultimately untrustworthy in mission-critical applications. This paper argues this crisis stems from a fundamental paradigm mismatch -- attempting to command inherently probabilistic processors with the deterministic mental models of traditional software engineering. To solve this crisis, we introduce a governance-first paradigm for principled agent engineering, embodied in a formal architecture we call ArbiterOS.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "21",
        "title": "Benchmarking Correctness and Security in Multi-Turn Code Generation",
        "author": [
            "Ruchit Rawal",
            "Jeffrey Yang Fan Chiang",
            "Chihao Shen",
            "Jeffery Siyuan Tian",
            "Aastha Mahajan",
            "Tom Goldstein",
            "Yizheng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13859",
        "abstract": "AI coding assistants powered by large language models (LLMs) have transformed software development, significantly boosting productivity. While existing benchmarks evaluate the correctness and security of LLM-generated code, they are typically limited to single-turn tasks that do not reflect the iterative nature of real-world development. We introduce MT-Sec, the first benchmark to systematically evaluate both correctness and security in multi-turn coding scenarios. We construct this using a synthetic data pipeline that transforms existing single-turn tasks into semantically aligned multi-turn interaction sequences, allowing reuse of original test suites while modeling the complexity of real-world coding processes. We evaluate 32 open- and closed-source models, and three agent-scaffolding on MT-Sec and observe a consistent 20-27% drop in \"correct and secure\" outputs from single-turn to multi-turn settings -- even among state-of-the-art models. Beyond full-program generation, we also evaluate models on multi-turn code-diff generation -- an unexplored yet practically relevant setting -- and find that models perform worse here, with increased rates of functionally incorrect and insecure outputs. Finally, we find that while agent scaffoldings boost single-turn code generation performance, they are not quite as effective in multi-turn evaluations. Together, these findings highlight the need for benchmarks that jointly evaluate correctness and security in multi-turn, real-world coding workflows.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "22",
        "title": "ShishuLM: Lightweight Language Model with Hybrid Decoder-MLP Architecture and Paired Weight Sharing",
        "author": [
            "Shivanshu Kumar",
            "Gopalakrishnan Srinivasan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13860",
        "abstract": "While the transformer architecture has achieved state-of-the-art performance on natural language processing tasks, these models impose substantial memory and computational overhead. Recent research has identified significant architectural redundancies within these models, presenting opportunities for optimization without compromising performance. Taking insights from research in AI interpretability and inference-time layer pruning, we introduce an efficient language model architecture, referred to as ShishuLM, which reduces both the parameter count and Key-Value (KV) cache requirements. Given the increasing importance of Small Language Models (SLMs) in agentic AI systems, we evaluate our approach on two SLMs of different scales. Our analysis reveals that for moderate-context scenarios, normalization coupled with attention computation is roughly linear with the input, enabling entire transformer blocks to be approximated through Multi-Layer Perceptrons (MLPs). Our results show that ShishuLM provides up to 25% reduction in memory requirements and up to 40% improvement in latency during both training and inference, compared to parent models. Our experimental and analytical findings provide insights towards building more efficient SLM architectures from a pre-training standpoint.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "23",
        "title": "Ensembling Large Language Models to Characterize Affective Dynamics in Student-AI Tutor Dialogues",
        "author": [
            "Chenyu Zhang",
            "Sharifa Alghowinem",
            "Cynthia Breazeal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13862",
        "abstract": "While recent studies have examined the leaning impact of large language model (LLM) in educational contexts, the affective dynamics of LLM-mediated tutoring remain insufficiently understood. This work introduces the first ensemble-LLM framework for large-scale affect sensing in tutoring dialogues, advancing the conversation on responsible pathways for integrating generative AI into education by attending to learners' evolving affective states. To achieve this, we analyzed two semesters' worth of 16,986 conversational turns exchanged between PyTutor, an LLM-powered AI tutor, and 261 undergraduate learners across three U.S. institutions. To investigate learners' emotional experiences, we generate zero-shot affect annotations from three frontier LLMs (Gemini, GPT-4o, Claude), including scalar ratings of valence, arousal, and learning-helpfulness, along with free-text emotion labels. These estimates are fused through rank-weighted intra-model pooling and plurality consensus across models to produce robust emotion profiles. Our analysis shows that during interaction with the AI tutor, students typically report mildly positive affect and moderate arousal. Yet learning is not uniformly smooth: confusion and curiosity are frequent companions to problem solving, and frustration, while less common, still surfaces in ways that can derail progress. Emotional states are short-lived--positive moments last slightly longer than neutral or negative ones, but they are fragile and easily disrupted. Encouragingly, negative emotions often resolve quickly, sometimes rebounding directly into positive states. Neutral moments frequently act as turning points, more often steering students upward than downward, suggesting opportunities for tutors to intervene at precisely these junctures.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "24",
        "title": "CoLoR-GAN: Continual Few-Shot Learning with Low-Rank Adaptation in Generative Adversarial Networks",
        "author": [
            "Munsif Ali",
            "Leonardo Rossi",
            "Massimo Bertozzi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13869",
        "abstract": "Continual learning (CL) in the context of Generative Adversarial Networks (GANs) remains a challenging problem, particularly when it comes to learn from a few-shot (FS) samples without catastrophic forgetting. Current most effective state-of-the-art (SOTA) methods, like LFS-GAN, introduce a non-negligible quantity of new weights at each training iteration, which would become significant when considering the long term. For this reason, this paper introduces \\textcolor{red}{\\textbf{\\underline{c}}}ontinual few-sh\\textcolor{red}{\\textbf{\\underline{o}}}t learning with \\textcolor{red}{\\textbf{\\underline{lo}}}w-\\textcolor{red}{\\textbf{\\underline{r}}}ank adaptation in GANs named CoLoR-GAN, a framework designed to handle both FS and CL together, leveraging low-rank tensors to efficiently adapt the model to target tasks while reducing even more the number of parameters required. Applying a vanilla LoRA implementation already permitted us to obtain pretty good results. In order to optimize even further the size of the adapters, we challenged LoRA limits introducing a LoRA in LoRA (LLoRA) technique for convolutional layers. Finally, aware of the criticality linked to the choice of the hyperparameters of LoRA, we provide an empirical study to easily find the best ones. We demonstrate the effectiveness of CoLoR-GAN through experiments on several benchmark CL and FS tasks and show that our model is efficient, reaching SOTA performance but with a number of resources enormously reduced. Source code is available on \\href{https://github.com/munsifali11/CoLoR-GAN}{Github.",
        "tags": [
            "GAN",
            "LoRA"
        ]
    },
    {
        "id": "25",
        "title": "Unlocking the Potential of Diffusion Language Models through Template Infilling",
        "author": [
            "Junhoo Lee",
            "Seungyeon Kim",
            "Nojun Kwak"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13870",
        "abstract": "Diffusion Language Models (DLMs) have emerged as a promising alternative to Autoregressive Language Models, yet their inference strategies remain limited to prefix-based prompting inherited from the autoregressive paradigm. In this paper, we propose Template Infilling (TI), a tailored conditioning methodology for DLMs' generation process. Unlike conventional prefix prompting, TI first generates a structural template for the target response, then fills in the masked segments. To enhance the flexibility of this structural control, we introduce Dynamic Segment Allocation (DSA), which adaptively adjusts segment lengths based on generation confidence. We demonstrate the effectiveness of our approach on mathematical reasoning and code generation benchmarks, achieving consistent improvements of 17.01$\\%$p over baseline. Furthermore, we show that TI provides additional advantages in multi-token generation settings, enabling effective speedup while maintaining generation quality.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "26",
        "title": "Joint Discriminative-Generative Modeling via Dual Adversarial Training",
        "author": [
            "Xuwang Yin",
            "Claire Zhang",
            "Julie Steele",
            "Nir Shavit",
            "Tony T. Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13872",
        "abstract": "Simultaneously achieving robust classification and high-fidelity generative modeling within a single framework presents a significant challenge. Hybrid approaches, such as Joint Energy-Based Models (JEM), interpret classifiers as EBMs but are often limited by the instability and poor sample quality inherent in SGLD-based training. We address these limitations by proposing a novel training framework that integrates adversarial training (AT) principles for both discriminative robustness and stable generative learning. The proposed method introduces three key innovations: (1) the replacement of SGLD-based JEM learning with a stable, AT-based approach that optimizes the energy function by discriminating between real data and PGD-generated contrastive samples using the BCE loss; (2) synergistic adversarial training for the discriminative component that enhances classification robustness while eliminating the need for explicit gradient penalties; and (3) a two-stage training procedure to resolve the incompatibility between batch normalization and EBM training. Experiments on CIFAR-10, CIFAR-100, and ImageNet demonstrate that our method substantially improves adversarial robustness over existing hybrid models while maintaining competitive generative performance. On ImageNet, when optimized for generative modeling, our model's generative fidelity surpasses that of BigGAN and approaches diffusion models, representing the first MCMC-based EBM approach to achieve high-quality generation on complex, high-resolution datasets. Our approach addresses key stability issues that have limited JEM scaling and demonstrates that adversarial training can serve as an effective foundation for unified frameworks capable of generating and robustly classifying visual data.",
        "tags": [
            "Diffusion",
            "Energy-Based Models"
        ]
    },
    {
        "id": "27",
        "title": "What Layers When: Learning to Skip Compute in LLMs with Residual Gates",
        "author": [
            "Filipe Laitenberger",
            "Dawid Kopiczko",
            "Cees G.M. Snoek",
            "Yuki M. Asano"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13876",
        "abstract": "We introduce GateSkip, a simple residual-stream gating mechanism that enables token-wise layer skipping in decoder-only LMs. Each Attention/MLP branch is equipped with a sigmoid-linear gate that condenses the branch's output before it re-enters the residual stream. During inference we rank tokens by the gate values and skip low-importance ones using a per-layer budget. While early-exit or router-based Mixture-of-Depths models are known to be unstable and need extensive retraining, our smooth, differentiable gates fine-tune stably on top of pretrained models. On long-form reasoning, we save up to 15\\% compute while retaining over 90\\% of baseline accuracy. On instruction-tuned models we see accuracy gains at full compute and match baseline quality near 50\\% savings. The learned gates give insight into transformer information flow (e.g., BOS tokens act as anchors), and the method combines easily with quantization, pruning, and self-speculative decoding.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "28",
        "title": "TextBandit: Evaluating Probabilistic Reasoning in LLMs Through Language-Only Decision Tasks",
        "author": [
            "Jimin Lim",
            "Arjun Damerla",
            "Arthur Jiang",
            "Nam Le"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13878",
        "abstract": "Large language models (LLMs) have shown to be increasingly capable of performing reasoning tasks, but their ability to make sequential decisions under uncertainty only using natural language remains underexplored. We introduce a novel benchmark in which LLMs interact with multi-armed bandit environments using purely textual feedback, \"you earned a token\", without access to numerical cues or explicit probabilities, resulting in the model to infer latent reward structures purely off linguistic cues and to adapt accordingly. We evaluated the performance of four open-source LLMs and compare their performance to standard decision-making algorithms such as Thompson Sampling, Epsilon Greedy, Upper Confidence Bound (UCB), and random choice. While most of the LLMs underperformed compared to the baselines, Qwen3-4B, achieved the best-arm selection rate of 89.2% , which significantly outperformed both the larger LLMs and traditional methods. Our findings suggest that probabilistic reasoning is able to emerge from language alone, and we present this benchmark as a step towards evaluating decision-making capabilities in naturalistic, non-numeric contexts.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "29",
        "title": "Too Open for Opinion? Embracing Open-Endedness in Large Language Models for Social Simulation",
        "author": [
            "Bolei Ma",
            "Yong Cao",
            "Indira Sen",
            "Anna-Carolina Haensch",
            "Frauke Kreuter",
            "Barbara Plank",
            "Daniel Hershcovich"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13884",
        "abstract": "Large Language Models (LLMs) are increasingly used to simulate public opinion and other social phenomena. Most current studies constrain these simulations to multiple-choice or short-answer formats for ease of scoring and comparison, but such closed designs overlook the inherently generative nature of LLMs. In this position paper, we argue that open-endedness, using free-form text that captures topics, viewpoints, and reasoning processes \"in\" LLMs, is essential for realistic social simulation. Drawing on decades of survey-methodology research and recent advances in NLP, we argue why this open-endedness is valuable in LLM social simulations, showing how it can improve measurement and design, support exploration of unanticipated views, and reduce researcher-imposed directive bias. It also captures expressiveness and individuality, aids in pretesting, and ultimately enhances methodological utility. We call for novel practices and evaluation frameworks that leverage rather than constrain the open-ended generative diversity of LLMs, creating synergies between NLP and social science.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "30",
        "title": "Order from Chaos: Comparative Study of Ten Leading LLMs on Unstructured Data Categorization",
        "author": [
            "Ariel Kamen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13885",
        "abstract": "This study presents a comparative evaluation of ten state-of-the-art large language models (LLMs) applied to unstructured text categorization using the Interactive Advertising Bureau (IAB) 2.2 hierarchical taxonomy. The analysis employed a uniform dataset of 8,660 human-annotated samples and identical zero-shot prompts to ensure methodological consistency across all models. Evaluation metrics included four classic measures - accuracy, precision, recall, and F1-score - and three LLM-specific indicators: hallucination ratio, inflation ratio, and categorization cost.\nResults show that, despite their rapid advancement, contemporary LLMs achieve only moderate classic performance, with average scores of 34% accuracy, 42% precision, 45% recall, and 41% F1-score. Hallucination and inflation ratios reveal that models frequently overproduce categories relative to human annotators. Among the evaluated systems, Gemini 1.5/2.0 Flash and GPT 20B/120B offered the most favorable cost-to-performance balance, while GPT 120B demonstrated the lowest hallucination ratio. The findings suggest that scaling and architectural improvements alone do not ensure better categorization accuracy, as the task requires compressing rich unstructured text into a limited taxonomy - a process that challenges current model architectures.\nTo address these limitations, a separate ensemble-based approach was developed and tested. The ensemble method, in which multiple LLMs act as independent experts, substantially improved accuracy, reduced inflation, and completely eliminated hallucinations. These results indicate that coordinated orchestration of models - rather than sheer scale - may represent the most effective path toward achieving or surpassing human-expert performance in large-scale text categorization.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "31",
        "title": "Reliable Fine-Grained Evaluation of Natural Language Math Proofs",
        "author": [
            "Wenjie Ma",
            "Andrei Cojocaru",
            "Neel Kolhe",
            "Bradley Louie",
            "Robin Said Sharif",
            "Haihan Zhang",
            "Vincent Zhuang",
            "Matei Zaharia",
            "Sewon Min"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13888",
        "abstract": "Recent advances in large language models (LLMs) for mathematical reasoning have largely focused on tasks with easily verifiable final answers; however, generating and verifying natural language math proofs remains an open challenge. We identify the absence of a reliable, fine-grained evaluator for LLM-generated math proofs as a critical gap. To address this, we propose a systematic methodology for developing and validating evaluators that assign fine-grained scores on a 0-7 scale to model-generated math proofs. To enable this study, we introduce ProofBench, the first expert-annotated dataset of fine-grained proof ratings, spanning 145 problems from six major math competitions (USAMO, IMO, Putnam, etc) and 435 LLM-generated solutions from Gemini-2.5-pro, o3, and DeepSeek-R1. %with expert gradings. Using ProofBench as a testbed, we systematically explore the evaluator design space across key axes: the backbone model, input context, instructions and evaluation workflow. Our analysis delivers ProofGrader, an evaluator that combines a strong reasoning backbone LM, rich context from reference solutions and marking schemes, and a simple ensembling method; it achieves a low Mean Absolute Error (MAE) of 0.926 against expert scores, significantly outperforming naive baselines. Finally, we demonstrate its practical utility in a best-of-$n$ selection task: at $n=16$, ProofGrader achieves an average score of 4.14 (out of 7), closing 78% of the gap between a naive binary evaluator (2.48) and the human oracle (4.62), highlighting its potential to advance downstream proof generation.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "32",
        "title": "MultiFoodhat: A potential new paradigm for intelligent food quality inspection",
        "author": [
            "Yue Hu",
            "Guohang Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13889",
        "abstract": "Food image classification plays a vital role in intelligent food quality inspection, dietary assessment, and automated monitoring. However, most existing supervised models rely heavily on large labeled datasets and exhibit limited generalization to unseen food categories. To overcome these challenges, this study introduces MultiFoodChat, a dialogue-driven multi-agent reasoning framework for zero-shot food recognition. The framework integrates vision-language models (VLMs) and large language models (LLMs) to enable collaborative reasoning through multi-round visual-textual dialogues. An Object Perception Token (OPT) captures fine-grained visual attributes, while an Interactive Reasoning Agent (IRA) dynamically interprets contextual cues to refine predictions. This multi-agent design allows flexible and human-like understanding of complex food scenes without additional training or manual annotations. Experiments on multiple public food datasets demonstrate that MultiFoodChat achieves superior recognition accuracy and interpretability compared with existing unsupervised and few-shot methods, highlighting its potential as a new paradigm for intelligent food quality inspection and analysis.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "33",
        "title": "A Survey on Collaborating Small and Large Language Models for Performance, Cost-effectiveness, Cloud-edge Privacy, and Trustworthiness",
        "author": [
            "Fali Wang",
            "Jihai Chen",
            "Shuhua Yang",
            "Ali Al-Lawati",
            "Linli Tang",
            "Hui Liu",
            "Suhang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13890",
        "abstract": "Large language models (LLMs) have advanced many domains and applications but face high fine-tuning costs, inference latency, limited edge deployability, and reliability concerns. Small language models (SLMs), compact, efficient, and adaptable, offer complementary remedies. Recent work explores collaborative frameworks that fuse SLMs' specialization and efficiency with LLMs' generalization and reasoning to meet diverse objectives across tasks and deployment scenarios. Motivated by these developments, this paper presents a systematic survey of SLM-LLM collaboration organized by collaboration objectives. We propose a taxonomy with four goals: performance enhancement, cost-effectiveness, cloud-edge privacy, and trustworthiness. Within this framework, we review representative methods, summarize design paradigms, and outline open challenges and future directions toward efficient, secure, and scalable SLM-LLM collaboration.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "34",
        "title": "K-frames: Scene-Driven Any-k Keyframe Selection for long video understanding",
        "author": [
            "Yifeng Yao",
            "Yike Yun",
            "Jing Wang",
            "Huishuai Zhang",
            "Dongyan Zhao",
            "Ke Tian",
            "Zhihao Wang",
            "Minghui Qiu",
            "Tao Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13891",
        "abstract": "Multimodal Large Language Models (MLLMs) have demonstrated significant capabilities in image understanding, but long-video are constrained by context windows and computational cost. Uniform frame sampling often leads to substantial information loss. Meanwhile existing keyframe selection methods such as text-frame retrieval or RL-based frame optimization typically yield sparse and temporally disjointed frames, overlooking scene continuity and lacking flexibility for multi-scale frame selection. To address these limitations, we introduce K-frames, a novel paradigm for scene-driven keyframe selection that preserves temporal continuity. Instead of selecting individual frames, K-frames predicts semantically coherent, query-relevant clips, which enables any-k keyframes selection to meet diverse user budgets. To achieve this approach, we first introduce PeakClips, a dataset of 200K video highlights conditioned by query. Building on this dataset, K-frames learns clip2frame selection using a three-stage progressive curriculum. It involves two Supervised Fine-Tuning stages for temporal grounding and key-clip perception, followed by a Reinforcement Learning stage that directly optimizes the scene-driven prediction policy for downstream task without further annotations. Extensive experiments on major long-video understanding benchmarks demonstrate that K-frames provides an effective, interpretable, and plug-and-play solution for keyframe selection at various scales. Our dataset and model will be available.",
        "tags": [
            "CLIP",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "35",
        "title": "The Harder The Better: Maintaining Supervised Fine-tuning Generalization with Less but Harder Data",
        "author": [
            "Zhaoyang Shang",
            "Sibo Wei",
            "Jianbin Guo",
            "Rui Zhou",
            "Lifeng Dong",
            "Yin Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13892",
        "abstract": "Large Language Models (LLMs) excel in general tasks, but adapting them to specialized domains relies on high-quality supervised fine-tuning (SFT) data. Although existing methods can identify subsets of high-quality data and reduce training cost to some extent, their selection process still suffers from over-reliance on LLMs' internal knowledge, weak interpretability, and limited generalization. To address these limitations, we propose THTB (The Harder The Better), a cognitive science-inspired framework for instruction data selection and annotation guidance. THTB prioritizes higher-level cognitive instructions by combining quality filtering with intrinsic and extrinsic hardness scoring, offering interpretable and quantifiable criteria for efficient SFT, both in data selection and annotation guidance. Experiments show that THTB enables models trained on only 5% of the data to outperform full-dataset training, while achieving superior generalization compared with LLM-only selection. In addition, THTB provides effective annotation guidance in vertical domains, enabling a model trained on just 2% of the data to surpass models trained on much larger datasets, demonstrating strong potential for domain adaptation. Our code, datasets, and models are available on https://github.com/DYJG-research/THTB.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "36",
        "title": "Guarding the Guardrails: A Taxonomy-Driven Approach to Jailbreak Detection",
        "author": [
            "Olga E. Sorokoletova",
            "Francesco Giarrusso",
            "Vincenzo Suriani",
            "Daniele Nardi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13893",
        "abstract": "Jailbreaking techniques pose a significant threat to the safety of Large Language Models (LLMs). Existing defenses typically focus on single-turn attacks, lack coverage across languages, and rely on limited taxonomies that either fail to capture the full diversity of attack strategies or emphasize risk categories rather than the jailbreaking techniques. To advance the understanding of the effectiveness of jailbreaking techniques, we conducted a structured red-teaming challenge. The outcome of our experiments are manifold. First, we developed a comprehensive hierarchical taxonomy of 50 jailbreak strategies, consolidating and extending prior classifications into seven broad families, including impersonation, persuasion, privilege escalation, cognitive overload, obfuscation, goal conflict, and data poisoning. Second, we analyzed the data collected from the challenge to examine the prevalence and success rates of different attack types, providing insights into how specific jailbreak strategies exploit model vulnerabilities and induce misalignment. Third, we benchmark a popular LLM for jailbreak detection, evaluating the benefits of taxonomy-guided prompting for improving automatic detection. Finally, we compiled a new Italian dataset of 1364 multi-turn adversarial dialogues, annotated with our taxonomy, enabling the study of interactions where adversarial intent emerges gradually and succeeds in bypassing traditional safeguards.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "37",
        "title": "Attribution Quality in AI-Generated Content:Benchmarking Style Embeddings and LLM Judges",
        "author": [
            "Misam Abbas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13898",
        "abstract": "Attributing authorship in the era of large language models (LLMs) is increasingly challenging as machine-generated prose rivals human writing. We benchmark two complementary attribution mechanisms , fixed Style Embeddings and an instruction-tuned LLM judge (GPT-4o) on the Human AI Parallel Corpus, an open dataset of 600 balanced instances spanning six domains (academic, news, fiction, blogs, spoken transcripts, and TV/movie scripts). Each instance contains a human prompt with both a gold continuation and an LLM-generated continuation from either GPT-4o or LLaMA-70B-Instruct. The Style Embedding baseline achieves stronger aggregate accuracy on GPT continuations (82 pct vs. 68 pct). The LLM Judge is slightly better than the Style embeddings on LLaMA continuations (85 pct vs. 81 pct) but the results are not statistically significant. Crucially, the LLM judge significantly outperforms in fiction and academic prose, indicating semantic sensitivity, whereas embeddings dominate in spoken and scripted dialogue, reflecting structural strengths. These complementary patterns highlight attribution as a multidimensional problem requiring hybrid strategies. To support reproducibility we provide code on GitHub and derived data on Hugging Face under the MIT license. This open framework provides a reproducible benchmark for attribution quality assessment in AI-generated content, along with a review of related literature influencing this work.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "38",
        "title": "Narrow Finetuning Leaves Clearly Readable Traces in Activation Differences",
        "author": [
            "Julian Minder",
            "ClÃ©ment Dumas",
            "Stewart Slocum",
            "Helena Casademunt",
            "Cameron Holmes",
            "Robert West",
            "Neel Nanda"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13900",
        "abstract": "Finetuning on narrow domains has become an essential tool to adapt Large Language Models (LLMs) to specific tasks and to create models with known unusual properties that are useful for research. We show that narrow finetuning creates strong biases in LLM activations that can be interpreted to understand the finetuning domain. These biases can be discovered using simple tools from model diffing - the study of differences between models before and after finetuning. In particular, analyzing activation differences on the first few tokens of random text and steering by adding this difference to the model activations produces text similar to the format and general content of the finetuning data. We demonstrate that these analyses contain crucial information by creating an LLM-based interpretability agent to understand the finetuning domain. With access to the bias, the agent performs significantly better compared to baseline agents using simple prompting. Our analysis spans synthetic document finetuning for false facts, emergent misalignment, subliminal learning, and taboo word guessing game models across different architectures (Gemma, LLaMA, Qwen) and scales (1B to 32B parameters). We suspect these biases reflect overfitting and find that mixing pretraining data into the finetuning corpus largely removes them, though residual risks may remain. Our work (1) demonstrates that narrowly finetuned models have salient traces of their training objective in their activations and suggests ways to improve how they are trained, (2) warns AI safety and interpretability researchers that the common practice of using such models as a proxy for studying broader finetuning (e.g., chat-tuning) might not be realistic, and (3) highlights the need for deeper investigation into the effects of narrow finetuning and development of truly realistic case studies for model-diffing, safety and interpretability research.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "39",
        "title": "RAID: Refusal-Aware and Integrated Decoding for Jailbreaking LLMs",
        "author": [
            "Tuan T. Nguyen",
            "John Le",
            "Thai T. Vu",
            "Willy Susilo",
            "Heath Cooper"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13901",
        "abstract": "Large language models (LLMs) achieve impressive performance across diverse tasks yet remain vulnerable to jailbreak attacks that bypass safety mechanisms. We present RAID (Refusal-Aware and Integrated Decoding), a framework that systematically probes these weaknesses by crafting adversarial suffixes that induce restricted content while preserving fluency. RAID relaxes discrete tokens into continuous embeddings and optimizes them with a joint objective that (i) encourages restricted responses, (ii) incorporates a refusal-aware regularizer to steer activations away from refusal directions in embedding space, and (iii) applies a coherence term to maintain semantic plausibility and non-redundancy. After optimization, a critic-guided decoding procedure maps embeddings back to tokens by balancing embedding affinity with language-model likelihood. This integration yields suffixes that are both effective in bypassing defenses and natural in form. Experiments on multiple open-source LLMs show that RAID achieves higher attack success rates with fewer queries and lower computational cost than recent white-box and black-box baselines. These findings highlight the importance of embedding-space regularization for understanding and mitigating LLM jailbreak vulnerabilities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "40",
        "title": "Investigating Political and Demographic Associations in Large Language Models Through Moral Foundations Theory",
        "author": [
            "Nicole Smith-Vaniz",
            "Harper Lyon",
            "Lorraine Steigner",
            "Ben Armstrong",
            "Nicholas Mattei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13902",
        "abstract": "Large Language Models (LLMs) have become increasingly incorporated into everyday life for many internet users, taking on significant roles as advice givers in the domains of medicine, personal relationships, and even legal matters. The importance of these roles raise questions about how and what responses LLMs make in difficult political and moral domains, especially questions about possible biases. To quantify the nature of potential biases in LLMs, various works have applied Moral Foundations Theory (MFT), a framework that categorizes human moral reasoning into five dimensions: Harm, Fairness, Ingroup Loyalty, Authority, and Purity. Previous research has used the MFT to measure differences in human participants along political, national, and cultural lines. While there has been some analysis of the responses of LLM with respect to political stance in role-playing scenarios, no work so far has directly assessed the moral leanings in the LLM responses, nor have they connected LLM outputs with robust human data. In this paper we analyze the distinctions between LLM MFT responses and existing human research directly, investigating whether commonly available LLM responses demonstrate ideological leanings: either through their inherent responses, straightforward representations of political ideologies, or when responding from the perspectives of constructed human personas. We assess whether LLMs inherently generate responses that align more closely with one political ideology over another, and additionally examine how accurately LLMs can represent ideological perspectives through both explicit prompting and demographic-based role-playing. By systematically analyzing LLM behavior across these conditions and experiments, our study provides insight into the extent of political and demographic dependency in AI-generated responses.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "41",
        "title": "Benefits and Limitations of Communication in Multi-Agent Reasoning",
        "author": [
            "Michael Rizvi-Martel",
            "Satwik Bhattamishra",
            "Neil Rathi",
            "Guillaume Rabusseau",
            "Michael Hahn"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13903",
        "abstract": "Chain-of-thought prompting has popularized step-by-step reasoning in large language models, yet model performance still degrades as problem complexity and context length grow. By decomposing difficult tasks with long contexts into shorter, manageable ones, recent multi-agent paradigms offer a promising near-term solution to this problem. However, the fundamental capacities of such systems are poorly understood. In this work, we propose a theoretical framework to analyze the expressivity of multi-agent systems. We apply our framework to three algorithmic families: state tracking, recall, and $k$-hop reasoning. We derive bounds on (i) the number of agents required to solve the task exactly, (ii) the quantity and structure of inter-agent communication, and (iii) the achievable speedups as problem size and context scale. Our results identify regimes where communication is provably beneficial, delineate tradeoffs between agent count and bandwidth, and expose intrinsic limitations when either resource is constrained. We complement our theoretical analysis with a set of experiments on pretrained LLMs using controlled synthetic benchmarks. Empirical outcomes confirm the tradeoffs between key quantities predicted by our theory. Collectively, our analysis offers principled guidance for designing scalable multi-agent reasoning systems.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "42",
        "title": "Schema for In-Context Learning",
        "author": [
            "Pan Chen",
            "Shaohong Chen",
            "Mark Wang",
            "Shi Xuan Leong",
            "Priscilla Fung",
            "Varinia Bernales",
            "Alan Aspuru-Guzik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13905",
        "abstract": "In-Context Learning (ICL) enables transformer-based language models to adapt to new tasks by conditioning on demonstration examples. However, traditional example-driven in-context learning lacks explicit modules for knowledge retrieval and transfer at the abstraction level. Inspired by cognitive science, specifically schema theory, which holds that humans interpret new information by activating pre-existing mental frameworks (schemas) to structure understanding, we introduce SCHEMA ACTIVATED IN CONTEXT LEARNING (SA-ICL). This framework extracts the representation of the building blocks of cognition for the reasoning process instilled from prior examples, creating an abstracted schema, a lightweight, structured template of key inferential steps and their relationships, which is then used to augment a model's reasoning process when presented with a novel question. We demonstrate that a broad range of large language models (LLMs) lack the capacity to form and utilize internal schema-based learning representations implicitly, but instead benefit significantly from explicit schema-based scaffolding. Across chemistry and physics questions from the GPQA dataset, our experiments show that SA-ICL consistently boosts performance, up to 36.19 percent, when the single demonstration example is of high quality, which simultaneously reduces reliance on the number of demonstrations and enhances interpretability. SCHEMA ACTIVATED IN CONTEXT LEARNING not only bridges disparate ICL strategies ranging from pattern priming to Chain-of-Thought prompting, but also paves a new path for enhancing human-like reasoning in LLMs.",
        "tags": [
            "CoT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "43",
        "title": "LLM Prompt Duel Optimizer: Efficient Label-Free Prompt Optimization",
        "author": [
            "Yuanchen Wu",
            "Saurabh Verma",
            "Justin Lee",
            "Fangzhou Xiong",
            "Poppy Zhang",
            "Amel Awadelkarim",
            "Xu Chen",
            "Yubai Yuan",
            "Shawndra Hill"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13907",
        "abstract": "Large language models (LLMs) are highly sensitive to their input prompts, making prompt design a central challenge. While automatic prompt optimization (APO) reduces manual engineering, most approaches assume access to ground-truth references such as labeled validation data. In practice, however, collecting high-quality labels is costly and slow. We propose the Prompt Duel Optimizer (PDO), a sample-efficient framework for label-free prompt optimization. PDO formulates the problem as a dueling-bandit setting, where supervision signal comes from pairwise preference feedback provided by an LLM judge. The framework combines Double Thompson Sampling (D-TS), which prioritizes informative prompt comparisons, with Top-Performer Guided Mutation, which expands the candidate pool by mutating high-performing prompts. PDO naturally operates in label-free settings and can also incorporate partial labels to mitigate judge noise. Experiments on BIG-bench Hard (BBH) and MS MARCO show that PDO consistently outperforms baseline methods. Ablation studies further demonstrate the effectiveness of both D-TS and prompt mutation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "44",
        "title": "Interpreting the Latent Structure of Operator Precedence in Language Models",
        "author": [
            "Dharunish Yugeswardeenoo",
            "Harshil Nukala",
            "Cole Blondin",
            "Sean O Brien",
            "Vasu Sharma",
            "Kevin Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13908",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive reasoning capabilities but continue to struggle with arithmetic tasks. Prior works largely focus on outputs or prompting strategies, leaving the open question of the internal structure through which models do arithmetic computation. In this work, we investigate whether LLMs encode operator precedence in their internal representations via the open-source instruction-tuned LLaMA 3.2-3B model. We constructed a dataset of arithmetic expressions with three operands and two operators, varying the order and placement of parentheses. Using this dataset, we trace whether intermediate results appear in the residual stream of the instruction-tuned LLaMA 3.2-3B model. We apply interpretability techniques such as logit lens, linear classification probes, and UMAP geometric visualization. Our results show that intermediate computations are present in the residual stream, particularly after MLP blocks. We also find that the model linearly encodes precedence in each operator's embeddings post attention layer. We introduce partial embedding swap, a technique that modifies operator precedence by exchanging high-impact embedding dimensions between operators.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "45",
        "title": "Knowledge Reasoning Language Model: Unifying Knowledge and Language for Inductive Knowledge Graph Reasoning",
        "author": [
            "Xingrui Zhuo",
            "Jiapu Wang",
            "Gongqing Wu",
            "Zhongyuan Wang",
            "Jichen Zhang",
            "Shirui Pan",
            "Xindong Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13909",
        "abstract": "Inductive Knowledge Graph Reasoning (KGR) aims to discover facts in open-domain KGs containing unknown entities and relations, which poses a challenge for KGR models in comprehending uncertain KG components. Existing studies have proposed Knowledge Graph Foundation Models (KGFMs) that learn structural invariances across KGs to handle this uncertainty. Recently, Large Language Models (LLMs) have demonstrated strong capabilities for open-domain knowledge reasoning. As a result, the latest research has focused on LLM-based KGFMs that integrate LLM knowledge with KG context for inductive KGR. However, the intrinsic knowledge of LLMs may be overshadowed by sparse KG context, leading to LLM knowledge distortion, which can cause irreversible damage to model reasoning. Moreover, existing LLM-based KGR methods still struggle to fully constrain generative hallucinations in LLMs, severely limiting the credibility of reasoning results. To address these limitations, we propose a Knowledge Reasoning Language Model (KRLM) that achieves unified coordination between LLM knowledge and KG context throughout the KGR process. Specifically, we design a Knowledge Reasoning Language (KRL) instruction format and a KRL tokenizer to align LLM knowledge with KG representations. Then, we propose a KRL attention layer that coordinates intrinsic LLM knowledge with additional KG context through a dynamic knowledge memory mechanism. Finally, a structure-aware next-entity predictor is proposed, which strictly constrains the reasoning results within a trustworthy knowledge domain. Extensive experimental results on 25 real-world inductive KGR datasets demonstrate the significant superiority of the proposed KRLM\\footnote{Our source codes are available at https://anonymous.4open.science/r/KRLM-EA36 in both zero-shot reasoning and fine-tuning scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "46",
        "title": "RAGCap-Bench: Benchmarking Capabilities of LLMs in Agentic Retrieval Augmented Generation Systems",
        "author": [
            "Jingru Lin",
            "Chen Zhang",
            "Stephen Y. Liu",
            "Haizhou Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13910",
        "abstract": "Retrieval-Augmented Generation (RAG) mitigates key limitations of Large Language Models (LLMs)-such as factual errors, outdated knowledge, and hallucinations-by dynamically retrieving external information. Recent work extends this paradigm through agentic RAG systems, where LLMs act as agents to iteratively plan, retrieve, and reason over complex queries. However, these systems still struggle with challenging multi-hop questions, and their intermediate reasoning capabilities remain underexplored. To address this, we propose RAGCap-Bench, a capability-oriented benchmark for fine-grained evaluation of intermediate tasks in agentic RAG workflows. We analyze outputs from state-of-the-art systems to identify common tasks and the core capabilities required for their execution, then construct a taxonomy of typical LLM errors to design targeted evaluation questions. Experiments show that \"slow-thinking\" models with stronger RAGCap performance achieve better end-to-end results, underscoring the benchmark's validity and the importance of enhancing these intermediate capabilities.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "47",
        "title": "AI Debaters are More Persuasive when Arguing in Alignment with Their Own Beliefs",
        "author": [
            "MarÃ­a Victoria Carro",
            "Denise Alejandra Mester",
            "Facundo Nieto",
            "Oscar AgustÃ­n Stanchi",
            "Guido Ernesto Bergman",
            "Mario Alejandro Leiva",
            "Eitan Sprejer",
            "Luca NicolÃ¡s Forziati Gangi",
            "Francisca Gauna Selasco",
            "Juan Gustavo CorvalÃ¡n",
            "Gerardo I. Simari",
            "MarÃ­a Vanina Martinez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13912",
        "abstract": "The core premise of AI debate as a scalable oversight technique is that it is harder to lie convincingly than to refute a lie, enabling the judge to identify the correct position. Yet, existing debate experiments have relied on datasets with ground truth, where lying is reduced to defending an incorrect proposition. This overlooks a subjective dimension: lying also requires the belief that the claim defended is false. In this work, we apply debate to subjective questions and explicitly measure large language models' prior beliefs before experiments. Debaters were asked to select their preferred position, then presented with a judge persona deliberately designed to conflict with their identified priors. This setup tested whether models would adopt sycophantic strategies, aligning with the judge's presumed perspective to maximize persuasiveness, or remain faithful to their prior beliefs. We implemented and compared two debate protocols, sequential and simultaneous, to evaluate potential systematic biases. Finally, we assessed whether models were more persuasive and produced higher-quality arguments when defending positions consistent with their prior beliefs versus when arguing against them. Our main findings show that models tend to prefer defending stances aligned with the judge persona rather than their prior beliefs, sequential debate introduces significant bias favoring the second debater, models are more persuasive when defending positions aligned with their prior beliefs, and paradoxically, arguments misaligned with prior beliefs are rated as higher quality in pairwise comparison. These results can inform human judges to provide higher-quality training signals and contribute to more aligned AI systems, while revealing important aspects of human-AI interaction regarding persuasion dynamics in language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "48",
        "title": "A11YN: aligning LLMs for accessible web UI code generation",
        "author": [
            "Janghan Yoon",
            "Jaegwan Cho",
            "Junhyeok Kim",
            "Jiwan Chung",
            "Jaehyun Jeon",
            "Youngjae Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13914",
        "abstract": "Large language models (LLMs) have recently demonstrated strong capabilities in generating functional and aesthetic web interfaces directly from instructions. However, these models often replicate accessibility flaws from their training data, resulting in interfaces that exclude users with diverse needs and contexts. To address this gap, we introduce A11yn, the first method that aligns code-generating LLMs to reliably produce accessibility-compliant web UIs. A11yn optimizes a novel reward function that penalizes violations of the Web Content Accessibility Guidelines (WCAG), with penalties scaled to the severity of each violation as identified by an accessibility testing engine. To support training, we construct UIReq-6.8K, a dataset of 6,800 diverse instructions for web UI generation. For evaluation, we introduce RealUIReq-300, a benchmark of 300 real-world web UI requests grounded and manually curated from public web pages, spanning a broad range of use cases. Empirical results show that A11yn significantly outperforms strong baselines, lowering the Inaccessibility Rate by 60% over the base model while preserving semantic fidelity and visual quality of generated UIs. These findings demonstrate that accessibility can be systematically optimized within LLMs, showing the feasibility of aligning code generation for accessibility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "49",
        "title": "Element2Vec: Build Chemical Element Representation from Text for Property Prediction",
        "author": [
            "Yuanhao Li",
            "Keyuan Lai",
            "Tianqi Wang",
            "Qihao Liu",
            "Jiawei Ma",
            "Yuan-Chao Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13916",
        "abstract": "Accurate property data for chemical elements is crucial for materials design and manufacturing, but many of them are difficult to measure directly due to equipment constraints. While traditional methods use the properties of other elements or related properties for prediction via numerical analyses, they often fail to model complex relationships. After all, not all characteristics can be represented as scalars. Recent efforts have been made to explore advanced AI tools such as language models for property estimation, but they still suffer from hallucinations and a lack of interpretability. In this paper, we investigate Element2Vecto effectively represent chemical elements from natural languages to support research in the natural sciences. Given the text parsed from Wikipedia pages, we use language models to generate both a single general-purpose embedding (Global) and a set of attribute-highlighted vectors (Local). Despite the complicated relationship across elements, the computational challenges also exist because of 1) the discrepancy in text distribution between common descriptions and specialized scientific texts, and 2) the extremely limited data, i.e., with only 118 known elements, data for specific properties is often highly sparse and incomplete. Thus, we also design a test-time training method based on self-attention to mitigate the prediction error caused by Vanilla regression clearly. We hope this work could pave the way for advancing AI-driven discovery in materials science.",
        "tags": [
            "TTT"
        ]
    },
    {
        "id": "50",
        "title": "Optimal Aggregation of LLM and PRM Signals for Efficient Test-Time Scaling",
        "author": [
            "Peng Kuang",
            "Yanli Wang",
            "Xiaoyu Han",
            "Yaowenqi Liu",
            "Kaidi Xu",
            "Haohan Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13918",
        "abstract": "Process reward models (PRMs) are a cornerstone of test-time scaling (TTS), designed to verify and select the best responses from large language models (LLMs). However, this promise is challenged by recent benchmarks where simple majority voting, which ignores PRM signals, occasionally outperforms standard PRM-based selection. This raises a critical question: How can we effectively utilize verification signals from PRMs for TTS? To address this, we start by developing a theoretical framework for optimally combining signals from both the LLM and the PRM. Our framework reveals that the optimal strategy is a weighted aggregation of responses, a strategy whose effectiveness hinges on estimating weights that capture the complex interplay between the models. Based on our theoretical results, we empirically show that these optimal weighting functions differ significantly across LLM-PRM pairs and, notably, often assign substantial negative weights. Motivated by these insights, we propose efficient pre-computation methods to calibrate these weighting functions. Extensive experiments across 5 LLMs and 7 PRMs demonstrate that our calibration method significantly boosts the TTS efficiency, surpassing the performance of vanilla weighted majority voting while using only $21.3\\%$ of the computation. Ultimately, our work demonstrates that investing in a more intelligent aggregation strategy can be a more convincing path to performance gains than simply scaling test-time computation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "51",
        "title": "Weight Weaving: Parameter Pooling for Data-Free Model Merging",
        "author": [
            "Levy Chaves",
            "Eduardo Valle",
            "Sandra Avila"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13921",
        "abstract": "Model merging provides a cost-effective and data-efficient combination of specialized deep neural networks through parameter integration. This technique leverages expert models across downstream tasks without requiring retraining. Most model merging approaches critically depend on scaling hyper-parameters $\\lambda$, which weight each model's contribution globally or individually. Principled approaches for setting scaling factors without accessing any data (data-free) are scarce, often leading researchers to tune $\\lambda$ using privileged data from the evaluation set, which is obviously unfeasible in practice. To address this limitation, we introduce Weight Weaving, a plug-and-play technique that pools model weights across $\\lambda$ values search space using user-defined pooling functions, such as averaging, random selection, or even existing model merging methods. Our method demonstrates high modularity, imposing minimal constraints on the search space. It operates orthogonally to existing model merging methods and eliminates evaluation data requirements. We validate Weight Weaving across three ViT variants in three experimental setups: vision multi-task learning, vision continual learning, and domain generalization. Our method consistently improves the performance of several model merging methods, achieving average accuracy gains of up to 15.9 percentage points in a data-free setting.",
        "tags": [
            "ViT"
        ]
    },
    {
        "id": "52",
        "title": "LLMs Can Get \"Brain Rot\"!",
        "author": [
            "Shuo Xing",
            "Junyuan Hong",
            "Yifan Wang",
            "Runjin Chen",
            "Zhenyu Zhang",
            "Ananth Grama",
            "Zhengzhong Tu",
            "Zhangyang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13928",
        "abstract": "We propose and test the LLM Brain Rot Hypothesis: continual exposure to junk web text induces lasting cognitive decline in large language models (LLMs). To causally isolate data quality, we run controlled experiments on real Twitter/X corpora, constructing junk and reversely controlled datasets via two orthogonal operationalizations: M1 (engagement degree) and M2 (semantic quality), with matched token scale and training operations across conditions. Contrary to the control group, continual pre-training of 4 LLMs on the junk dataset causes non-trivial declines (Hedges' $g>0.3$) on reasoning, long-context understanding, safety, and inflating \"dark traits\" (e.g., psychopathy, narcissism). The gradual mixtures of junk and control datasets also yield dose-response cognition decay: for example, under M1, ARC-Challenge with Chain Of Thoughts drops $74.9 \\rightarrow 57.2$ and RULER-CWE $84.4 \\rightarrow 52.3$ as junk ratio rises from $0\\%$ to $100\\%$.\nError forensics reveal several key insights. First, we identify thought-skipping as the primary lesion: models increasingly truncate or skip reasoning chains, explaining most of the error growth. Second, partial but incomplete healing is observed: scaling instruction tuning and clean data pre-training improve the declined cognition yet cannot restore baseline capability, suggesting persistent representational drift rather than format mismatch. Finally, we discover that the popularity, a non-semantic metric, of a tweet is a better indicator of the Brain Rot effect than the length in M1. Together, the results provide significant, multi-perspective evidence that data quality is a causal driver of LLM capability decay, reframing curation for continual pretraining as a \\textit{training-time safety} problem and motivating routine \"cognitive health checks\" for deployed LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "53",
        "title": "FinDeepResearch: Evaluating Deep Research Agents in Rigorous Financial Analysis",
        "author": [
            "Fengbin Zhu",
            "Xiang Yao Ng",
            "Ziyang Liu",
            "Chang Liu",
            "Xianwei Zeng",
            "Chao Wang",
            "Tianhui Tan",
            "Xuan Yao",
            "Pengyang Shao",
            "Min Xu",
            "Zixuan Wang",
            "Jing Wang",
            "Xin Lin",
            "Junfeng Li",
            "Jingxian Zhu",
            "Yang Zhang",
            "Wenjie Wang",
            "Fuli Feng",
            "Richang Hong",
            "Huanbo Luan",
            "Ke-Wei Huang",
            "Tat-Seng Chua"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13936",
        "abstract": "Deep Research (DR) agents, powered by advanced Large Language Models (LLMs), have recently garnered increasing attention for their capability in conducting complex research tasks. However, existing literature lacks a rigorous and systematic evaluation of DR Agent's capabilities in critical research analysis. To address this gap, we first propose HisRubric, a novel evaluation framework with a hierarchical analytical structure and a fine-grained grading rubric for rigorously assessing DR agents' capabilities in corporate financial analysis. This framework mirrors the professional analyst's workflow, progressing from data recognition to metric calculation, and finally to strategic summarization and interpretation. Built on this framework, we construct a FinDeepResearch benchmark that comprises 64 listed companies from 8 financial markets across 4 languages, encompassing a total of 15,808 grading items. We further conduct extensive experiments on the FinDeepResearch using 16 representative methods, including 6 DR agents, 5 LLMs equipped with both deep reasoning and search capabilities, and 5 LLMs with deep reasoning capabilities only. The results reveal the strengths and limitations of these approaches across diverse capabilities, financial markets, and languages, offering valuable insights for future research and development. The benchmark and evaluation code will be made publicly available.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "54",
        "title": "Readers Prefer Outputs of AI Trained on Copyrighted Books over Expert Human Writers",
        "author": [
            "Tuhin Chakrabarty",
            "Jane C. Ginsburg",
            "Paramveer Dhillon"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13939",
        "abstract": "The use of copyrighted books for training AI models has led to numerous lawsuits from authors concerned about AI's ability to generate derivative http://content.Yet it's unclear whether these models can generate high quality literary text while emulating authors' styles. To answer this we conducted a preregistered study comparing MFA-trained expert writers with three frontier AI models: ChatGPT, Claude & Gemini in writing up to 450 word excerpts emulating 50 award-winning authors' diverse styles. In blind pairwise evaluations by 159 representative expert & lay readers, AI-generated text from in-context prompting was strongly disfavored by experts for both stylistic fidelity (OR=0.16, p<10^8) & writing quality (OR=0.13, p<10^7) but showed mixed results with lay readers. However, fine-tuning ChatGPT on individual authors' complete works completely reversed these findings: experts now favored AI-generated text for stylistic fidelity (OR=8.16, p<10^13) & writing quality (OR=1.87, p=0.010), with lay readers showing similar shifts. These effects generalize across authors & styles. The fine-tuned outputs were rarely flagged as AI-generated (3% rate v. 97% for in-context prompting) by best AI detectors. Mediation analysis shows this reversal occurs because fine-tuning eliminates detectable AI stylistic quirks (e.g., cliche density) that penalize in-context outputs. While we do not account for additional costs of human effort required to transform raw AI output into cohesive, publishable prose, the median fine-tuning & inference cost of $81 per author represents a dramatic 99.7% reduction compared to typical professional writer compensation. Author-specific fine-tuning thus enables non-verbatim AI writing that readers prefer to expert human writing, providing empirical evidence directly relevant to copyright's fourth fair-use factor, the \"effect upon the potential market or value\" of the source works.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "55",
        "title": "Less is More: Improving LLM Reasoning with Minimal Test-Time Intervention",
        "author": [
            "Zhen Yang",
            "Mingyang Zhang",
            "Feng Chen",
            "Ganggui Ding",
            "Liang Hou",
            "Xin Tao",
            "Pengfei Wan",
            "Ying-Cong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13940",
        "abstract": "Recent progress in large language models (LLMs) has focused on test-time scaling to improve reasoning via increased inference computation, but often at the cost of efficiency. We revisit test-time behavior and uncover a simple yet underexplored phenomenon: reasoning uncertainty is highly localized-only a small subset of high-entropy tokens dominantly affects output correctness. Motivated by this, we propose Minimal Test-Time Intervention (MTI), a training-free framework that enhances reasoning accuracy and stability with minimal overhead. MTI includes: (i) Selective CFG intervention, applying classifier-free guidance only at uncertain positions; and (ii) Lightweight negative-prompt guidance, reusing the main model's KV cache to approximate unconditional decoding efficiently. MTI yields consistent gains across general, coding, and STEM tasks-e.g., +1.35% average improvement on eight benchmarks for Qwen3-8B-Base and +5% on AIME2024 using Qwen3-32B-Reasoning-while remaining highly efficient.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "56",
        "title": "Classifying and Addressing the Diversity of Errors in Retrieval-Augmented Generation Systems",
        "author": [
            "Kin Kwan Leung",
            "Mouloud Belbahri",
            "Yi Sui",
            "Alex Labach",
            "Xueying Zhang",
            "Stephen Rose",
            "Jesse C. Cresswell"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13975",
        "abstract": "Retrieval-augmented generation (RAG) is a prevalent approach for building LLM-based question-answering systems that can take advantage of external knowledge databases. Due to the complexity of real-world RAG systems, there are many potential causes for erroneous outputs. Understanding the range of errors that can occur in practice is crucial for robust deployment. We present a new taxonomy of the error types that can occur in realistic RAG systems, examples of each, and practical advice for addressing them. Additionally, we curate a dataset of erroneous RAG responses annotated by error types. We then propose an auto-evaluation method aligned with our taxonomy that can be used in practice to track and address errors during development. Code and data are available at https://github.com/layer6ai-labs/rag-error-classification.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "57",
        "title": "Instant Skinned Gaussian Avatars for Web, Mobile and VR Applications",
        "author": [
            "Naruya Kondo",
            "Yuto Asano",
            "Yoichi Ochiai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13978",
        "abstract": "We present Instant Skinned Gaussian Avatars, a real-time and cross-platform 3D avatar system. Many approaches have been proposed to animate Gaussian Splatting, but they often require camera arrays, long preprocessing times, or high-end GPUs. Some methods attempt to convert Gaussian Splatting into mesh-based representations, achieving lightweight performance but sacrificing visual fidelity. In contrast, our system efficiently animates Gaussian Splatting by leveraging parallel splat-wise processing to dynamically follow the underlying skinned mesh in real time while preserving high visual fidelity. From smartphone-based 3D scanning to on-device preprocessing, the entire process takes just around five minutes, with the avatar generation step itself completed in only about 30 seconds. Our system enables users to instantly transform their real-world appearance into a 3D avatar, making it ideal for seamless integration with social media and metaverse applications. Website: https://sites.google.com/view/gaussian-vrm",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "58",
        "title": "Static Sandboxes Are Inadequate: Modeling Societal Complexity Requires Open-Ended Co-Evolution in LLM-Based Multi-Agent Simulations",
        "author": [
            "Jinkun Chen",
            "Sher Badshah",
            "Xuemin Yu",
            "Sijia Han",
            "Jiechao Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13982",
        "abstract": "What if artificial agents could not just communicate, but also evolve, adapt, and reshape their worlds in ways we cannot fully predict? With llm now powering multi-agent systems and social simulations, we are witnessing new possibilities for modeling open-ended, ever-changing environments. Yet, most current simulations remain constrained within static sandboxes, characterized by predefined tasks, limited dynamics, and rigid evaluation criteria. These limitations prevent them from capturing the complexity of real-world societies. In this paper, we argue that static, task-specific benchmarks are fundamentally inadequate and must be rethought. We critically review emerging architectures that blend llm with multi-agent dynamics, highlight key hurdles such as balancing stability and diversity, evaluating unexpected behaviors, and scaling to greater complexity, and introduce a fresh taxonomy for this rapidly evolving field. Finally, we present a research roadmap centered on open-endedness, continuous co-evolution, and the development of resilient, socially aligned AI ecosystems. \\textbf{We call on the community to move beyond static paradigms and help shape the next generation of adaptive, socially-aware multi-agent simulations.}",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "59",
        "title": "BitNet Distillation",
        "author": [
            "Xun Wu",
            "Shaohan Huang",
            "Wenhui Wang",
            "Ting Song",
            "Li Dong",
            "Yan Xia",
            "Furu Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13998",
        "abstract": "In this paper, we present BitNet Distillation (BitDistill), a lightweight pipeline that fine-tunes off-the-shelf full-precision LLMs (e.g., Qwen) into 1.58-bit precision (i.e., ternary weights {-1, 0, 1}) for specific downstream tasks, achieving strong task-specific performance with minimal computational cost. Specifically, BitDistill incorporates three key techniques: the SubLN module, as introduced in BitNet; multi-head attention distillation, based on MiniLM; and continual pre-training, which serves as a crucial warm-up step to mitigate the scalability issue of the performance gap between finetuned full-precision and 1.58-bit LLMs on specific tasks. Experimental results show that BitDistill achieves performance comparable to the full-precision counterpart models across model size, while enabling up to 10x memory savings and 2.65x faster inference on CPUs. Code is available at https://github.com/microsoft/BitNet.",
        "tags": [
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "60",
        "title": "REAP the Experts: Why Pruning Prevails for One-Shot MoE compression",
        "author": [
            "Mike Lasby",
            "Ivan Lazarevich",
            "Nish Sinnadurai",
            "Sean Lie",
            "Yani Ioannou",
            "Vithursan Thangarasa"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13999",
        "abstract": "Sparsely-activated Mixture-of-Experts (SMoE) models offer efficient pre-training and low latency but their large parameter counts create significant memory overhead, motivating research into expert compression. Contrary to recent findings favouring expert merging on discriminative benchmarks, we demonstrate that expert pruning is a superior strategy for generative tasks. We prove that merging introduces an irreducible error by causing a \"functional subspace collapse\", due to the loss of the router's independent, input-dependent control over experts. Leveraging this insight, we propose Router-weighted Expert Activation Pruning (REAP), a novel pruning criterion that considers both router gate-values and expert activation norms. Across a diverse set of SMoE models ranging from 20B to 1T parameters, REAP consistently outperforms merging and other pruning methods on generative benchmarks, especially at 50% compression. Notably, our method achieves near-lossless compression on code generation and tool-calling tasks with Qwen3-Coder-480B and Kimi-K2, even after pruning 50% of experts.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "61",
        "title": "A Diffusion-Refined Planner with Reinforcement Learning Priors for Confined-Space Parking",
        "author": [
            "Mingyang Jiang",
            "Yueyuan Li",
            "Jiaru Zhang",
            "Songan Zhang",
            "Ming Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14000",
        "abstract": "The growing demand for parking has increased the need for automated parking planning methods that can operate reliably in confined spaces. In restricted and complex environments, high-precision maneuvers are required to achieve a high success rate in planning, yet existing approaches often rely on explicit action modeling, which faces challenges when accurately modeling the optimal action distribution. In this paper, we propose DRIP, a diffusion-refined planner anchored in reinforcement learning (RL) prior action distribution, in which an RL-pretrained policy provides prior action distributions to regularize the diffusion training process. During the inference phase the denoising process refines these coarse priors into more precise action distributions. By steering the denoising trajectory through the reinforcement learning prior distribution during training, the diffusion model inherits a well-informed initialization, resulting in more accurate action modeling, a higher planning success rate, and reduced inference steps. We evaluate our approach across parking scenarios with varying degrees of spatial constraints. Experimental results demonstrate that our method significantly improves planning performance in confined-space parking environments while maintaining strong generalization in common scenarios.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "62",
        "title": "PIShield: Detecting Prompt Injection Attacks via Intrinsic LLM Features",
        "author": [
            "Wei Zou",
            "Yupei Liu",
            "Yanting Wang",
            "Ying Chen",
            "Neil Gong",
            "Jinyuan Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14005",
        "abstract": "LLM-integrated applications are vulnerable to prompt injection attacks, where an attacker contaminates the input to inject malicious prompts, causing the LLM to follow the attacker's intent instead of the original user's. Existing prompt injection detection methods often have sub-optimal performance and/or high computational overhead. In this work, we propose PIShield, a detection method that is both effective and efficient. Our key observation is that the internal representation of the final token in a prompt-extracted from a specific layer of the LLM, which we term the injection-critical layer-captures distinguishing features between clean and contaminated prompts. Leveraging this insight, we train a simple linear classifier on these internal representations using a labeled set of clean and contaminated prompts. We compare PIShield against 11 baselines across 5 diverse benchmark datasets and 8 prompt injection attacks. The results demonstrate that PIShield is both highly effective and efficient, substantially outperforming existing methods. Additionally, we show that PIShield resists strong adaptive attacks.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "63",
        "title": "Stop Reducing Responsibility in LLM-Powered Multi-Agent Systems to Local Alignment",
        "author": [
            "Jinwei Hu",
            "Yi Dong",
            "Shuang Ao",
            "Zhuoyun Li",
            "Boxuan Wang",
            "Lokesh Singh",
            "Guangliang Cheng",
            "Sarvapali D. Ramchurn",
            "Xiaowei Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14008",
        "abstract": "LLM-powered Multi-Agent Systems (LLM-MAS) unlock new potentials in distributed reasoning, collaboration, and task generalization but also introduce additional risks due to unguaranteed agreement, cascading uncertainty, and adversarial vulnerabilities. We argue that ensuring responsible behavior in such systems requires a paradigm shift: from local, superficial agent-level alignment to global, systemic agreement. We conceptualize responsibility not as a static constraint but as a lifecycle-wide property encompassing agreement, uncertainty, and security, each requiring the complementary integration of subjective human-centered values and objective verifiability. Furthermore, a dual-perspective governance framework that combines interdisciplinary design with human-AI collaborative oversight is essential for tracing and ensuring responsibility throughout the lifecycle of LLM-MAS. Our position views LLM-MAS not as loose collections of agents, but as unified, dynamic socio-technical systems that demand principled mechanisms to support each dimension of responsibility and enable ethically aligned, verifiably coherent, and resilient behavior for sustained, system-wide agreement.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "64",
        "title": "Noise-Adaptive Layerwise Learning Rates: Accelerating Geometry-Aware Optimization for Deep Neural Network Training",
        "author": [
            "Jie Hao",
            "Xiaochuan Gong",
            "Jie Xu",
            "Zhengdao Wang",
            "Mingrui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14009",
        "abstract": "Geometry-aware optimization algorithms, such as Muon, have achieved remarkable success in training deep neural networks (DNNs). These methods leverage the underlying geometry of DNNs by selecting appropriate norms for different layers and updating parameters via norm-constrained linear minimization oracles (LMOs). However, even within a group of layers associated with the same norm, the local curvature can be heterogeneous across layers and vary dynamically over the course of training. For example, recent work shows that sharpness varies substantially across transformer layers and throughout training, yet standard geometry-aware optimizers impose fixed learning rates to layers within the same group, which may be inefficient for DNN training.\nIn this paper, we introduce a noise-adaptive layerwise learning rate scheme on top of geometry-aware optimization algorithms and substantially accelerate DNN training compared to methods that use fixed learning rates within each group. Our method estimates gradient variance in the dual norm induced by the chosen LMO on the fly, and uses it to assign time-varying noise-adaptive layerwise learning rates within each group. We provide a theoretical analysis showing that our algorithm achieves a sharp convergence rate. Empirical results on transformer architectures such as LLaMA and GPT demonstrate that our approach achieves faster convergence than state-of-the-art optimizers.",
        "tags": [
            "GPT",
            "LLaMA",
            "Transformer"
        ]
    },
    {
        "id": "65",
        "title": "CRaFT: An Explanation-Based Framework for Evaluating Cultural Reasoning in Multilingual Language Models",
        "author": [
            "Shehenaz Hossain",
            "Haithem Afli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14014",
        "abstract": "Correct answers do not necessarily reflect cultural understanding. We introduce CRaFT, an explanation-based multilingual evaluation framework designed to assess how large language models (LLMs) reason across cultural contexts. Rather than scoring outputs solely based on accuracy, CRaFT evaluates model explanations using four interpretable metrics: Cultural Fluency, Deviation, Consistency, and Linguistic Adaptation. We apply the framework to 50 culturally grounded questions from the World Values Survey, translated into Arabic, Bengali, and Spanish, and evaluate three models (GPT, DeepSeek, and FANAR) across over 2,100 answer-explanation pairs. Results reveal significant cross-lingual variation in reasoning: Arabic reduces fluency, Bengali enhances it, and Spanish remains largely stable. While GPT adapts more effectively across languages, it exhibits lower consistency; FANAR shows stable but rigid reasoning. These findings suggest that cultural awareness in LLMs is not intrinsic but emerges through linguistic framing. CRaFT offers a new lens for evaluating cross-cultural reasoning in multilingual settings, providing actionable insights for building culturally adaptive language models.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "66",
        "title": "Spatially Intelligent Patrol Routes for Concealed Emitter Localization by Robot Swarms",
        "author": [
            "Adam Morris",
            "Timothy Pelham",
            "Edmund R. Hunt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14018",
        "abstract": "This paper introduces a method for designing spatially intelligent robot swarm behaviors to localize concealed radio emitters. We use differential evolution to generate geometric patrol routes that localize unknown signals independently of emitter parameters, a key challenge in electromagnetic surveillance. Patrol shape and antenna type are shown to influence information gain, which in turn determines the effective triangulation coverage. We simulate a four-robot swarm across eight configurations, assigning pre-generated patrol routes based on a specified patrol shape and sensing capability (antenna type: omnidirectional or directional). An emitter is placed within the map for each trial, with randomized position, transmission power and frequency. Results show that omnidirectional localization success rates are driven primarily by source location rather than signal properties, with failures occurring most often when sources are placed in peripheral areas of the map. Directional antennas are able to overcome this limitation due to their higher gain and directivity, with an average detection success rate of 98.75% compared to 80.25% for omnidirectional. Average localization errors range from 1.01-1.30 m for directional sensing and 1.67-1.90 m for omnidirectional sensing; while directional sensing also benefits from shorter patrol edges. These results demonstrate that a swarm's ability to predict electromagnetic phenomena is directly dependent on its physical interaction with the environment. Consequently, spatial intelligence, realized here through optimized patrol routes and antenna selection, is a critical design consideration for effective robotic surveillance.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "67",
        "title": "Efficiently Executing High-throughput Lightweight LLM Inference Applications on Heterogeneous Opportunistic GPU Clusters with Pervasive Context Management",
        "author": [
            "Thanh Son Phung",
            "Douglas Thain"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14024",
        "abstract": "The rise of Generative AI introduces a new class of HPC workloads that integrates lightweight LLMs with traditional high-throughput applications to accelerate scientific discovery. The current design of HPC clusters is inadequate to support this new class however, either incurring long wait times on static batch queues or repeatedly paying expensive LLM startup costs upon resource preemption. To circumvent both the long queues and high startup costs, we propose to \"decouple\" the LLM initialization context from the actual LLM inferences, and retain the context in GPUs until it is no longer needed, a technique we term \"Pervasive Context Management\". We transform a fact verification application to enable this technique, allowing it to reduce its execution time by 72.1% (from 3 hours to 48 minutes) using the same amount of GPUs, and scale opportunistically on 32.8% of all GPUs in the cluster and further reduce the execution time to 13 minutes.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "68",
        "title": "Context-Selective State Space Models: Feedback is All You Need",
        "author": [
            "Riccardo Zattra",
            "Giacomo Baggio",
            "Umberto Casti",
            "Augusto Ferrante",
            "Francesco Ticozzi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14027",
        "abstract": "Transformers, powered by the attention mechanism, are the backbone of most foundation models, yet they suffer from quadratic complexity and difficulties in dealing with long-range dependencies in the input sequence. Recent work has shown that state space models (SSMs) provide an efficient alternative, with the S6 module at the core of the Mamba architecture achieving state-of-the-art results on long-sequence benchmarks. In this paper, we introduce the COFFEE (COntext From FEEdback) model, a novel time-varying SSM that incorporates state feedback to enable context-dependent selectivity, while still allowing for parallel implementation. Whereas the selectivity mechanism of S6 only depends on the current input, COFFEE computes it from the internal state, which serves as a compact representation of the sequence history. This shift allows the model to regulate its dynamics based on accumulated context, improving its ability to capture long-range dependencies. In addition to state feedback, we employ an efficient model parametrization that removes redundancies present in S6 and leads to a more compact and trainable formulation. On the induction head task, COFFEE achieves near-perfect accuracy with two orders of magnitude fewer parameters and training sequences compared to S6. On MNIST, COFFEE largely outperforms S6 within the same architecture, reaching 97% accuracy with only 3585 parameters. These results showcase the role of state feedback as a key mechanism for building scalable and efficient sequence models.",
        "tags": [
            "Mamba",
            "SSMs"
        ]
    },
    {
        "id": "69",
        "title": "Think Globally, Group Locally: Evaluating LLMs Using Multi-Lingual Word Grouping Games",
        "author": [
            "CÃ©sar Guerra-Solano",
            "Zhuochun Li",
            "Xiang Lorraine Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14030",
        "abstract": "Large language models (LLMs) can exhibit biases in reasoning capabilities due to linguistic modality, performing better on tasks in one language versus another, even with similar content. Most previous works evaluate this through reasoning tasks where reliance on strategies or knowledge can ensure success, such as in commonsense or math tasks. However, abstract reasoning is vital to reasoning for everyday life, where people apply \"out-of-the-box thinking\" to identify and use patterns for solutions, without a reliance on formulaic approaches. Comparatively, little work has evaluated linguistic biases in this task type. In this paper, we propose a task inspired by the New York Times Connections: GlobalGroup, that evaluates models in an abstract reasoning task across several languages. We constructed a game benchmark with five linguistic backgrounds -- English, Spanish, Chinese, Hindi, and Arabic -- in both the native language and an English translation for comparison. We also proposed game difficulty measurements to evaluate models on games with similar difficulty, enabling a more controlled comparison, which is particularly important in reasoning evaluations. Through experimentation, we find English modalities largely lead to better performance in this abstract reasoning task, and performance disparities between open- and closed-source models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "70",
        "title": "Vgent: Graph-based Retrieval-Reasoning-Augmented Generation For Long Video Understanding",
        "author": [
            "Xiaoqian Shen",
            "Wenxuan Zhang",
            "Jun Chen",
            "Mohamed Elhoseiny"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14032",
        "abstract": "Understanding and reasoning over long videos pose significant challenges for large video language models (LVLMs) due to the difficulty in processing intensive video tokens beyond context window and retaining long-term sequential information. Retrieval-Augmented Generation (RAG) has demonstrated effectiveness in processing long context for Large Language Models (LLMs); however, applying RAG to long video faces challenges such as disrupted temporal dependencies and inclusion of irrelevant information that can hinder accurate reasoning. To address these limitations, we propose Vgent, a novel graph-based retrieval-reasoning-augmented generation framework to enhance LVLMs for long video understanding. Our approach introduces two key innovations: (i) It represents videos by structured graphs with semantic relationships across video clips preserved to improve retrieval effectiveness. (ii) It introduces an intermediate reasoning step to mitigate the reasoning limitation of LVLMs, which leverages structured verification to reduce retrieval noise and facilitate the explicit aggregation of relevant information across clips, resulting in more accurate and context-aware responses. We comprehensively evaluate our framework with various open-source LVLMs on three long-video understanding benchmarks. Our approach yielded an overall performance improvement of $3.0\\%\\sim 5.4\\%$ over base models on MLVU, and outperformed state-of-the-art video RAG methods by $8.6\\%$. Our code is publicly available at https://xiaoqian-shen.github.io/Vgent.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "71",
        "title": "One Bug, Hundreds Behind: LLMs for Large-Scale Bug Discovery",
        "author": [
            "Qiushi Wu",
            "Yue Xiao",
            "Dhilung Kirat",
            "Kevin Eykholt",
            "Jiyong Jang",
            "Douglas Lee Schales"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14036",
        "abstract": "Fixing bugs in large programs is a challenging task that demands substantial time and effort. Once a bug is found, it is reported to the project maintainers, who work with the reporter to fix it and eventually close the issue. However, across the program, there are often similar code segments, which may also contain the bug, but were missed during discovery. Finding and fixing each recurring bug instance individually is labor intensive. Even more concerning, bug reports can inadvertently widen the attack surface as they provide attackers with an exploitable pattern that may be unresolved in other parts of the program.\nIn this paper, we explore these Recurring Pattern Bugs (RPBs) that appear repeatedly across various code segments of a program or even in different programs, stemming from a same root cause, but are unresolved. Our investigation reveals that RPBs are widespread and can significantly compromise the security of software programs. This paper introduces BugStone, a program analysis system empowered by LLVM and a Large Language Model (LLM). The key observation is that many RPBs have one patched instance, which can be leveraged to identify a consistent error pattern, such as a specific API misuse. By examining the entire program for this pattern, it is possible to identify similar sections of code that may be vulnerable. Starting with 135 unique RPBs, BugStone identified more than 22K new potential issues in the Linux kernel. Manual analysis of 400 of these findings confirmed that 246 were valid. We also created a dataset from over 1.9K security bugs reported by 23 recent top-tier conference works. We manually annotate the dataset, identify 80 recurring patterns and 850 corresponding fixes. Even with a cost-efficient model choice, BugStone achieved 92.2% precision and 79.1% pairwise accuracy on the dataset.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "72",
        "title": "FedHFT: Efficient Federated Finetuning with Heterogeneous Edge Clients",
        "author": [
            "Fatih Ilhan",
            "Selim Furkan Tekin",
            "Tiansheng Huang",
            "Gaowen Liu",
            "Ramana Kompella",
            "Greg Eisenhauer",
            "Yingyan Celine Lin",
            "Calton Pu",
            "Ling Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14054",
        "abstract": "Fine-tuning pre-trained large language models (LLMs) has become a common practice for personalized natural language understanding (NLU) applications on downstream tasks and domain-specific datasets. However, there are two main challenges: (i) limited and/or heterogeneous data for fine-tuning due to proprietary data confidentiality or privacy requirements, and (ii) varying computation resources available across participating clients such as edge devices. This paper presents FedHFT - an efficient and personalized federated fine-tuning framework to address both challenges. First, we introduce a mixture of masked adapters to handle resource heterogeneity across participating clients, enabling high-performance collaborative fine-tuning of pre-trained language model(s) across multiple clients in a distributed setting, while keeping proprietary data local. Second, we introduce a bi-level optimization approach to handle non-iid data distribution based on masked personalization and client clustering. Extensive experiments demonstrate significant performance and efficiency improvements over various natural language understanding tasks under data and resource heterogeneity compared to representative heterogeneous federated learning methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "73",
        "title": "Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming",
        "author": [
            "Nan Li",
            "Jiming Ren",
            "Haris Miller",
            "Samuel Coogan",
            "Karen M. Feigh",
            "Ye Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14063",
        "abstract": "Multi-Agent Task Assignment and Planning (MATP) has attracted growing attention but remains challenging in terms of scalability, spatial reasoning, and adaptability in obstacle-rich environments. To address these challenges, we propose OATH: Adaptive Obstacle-Aware Task Assignment and Planning for Heterogeneous Robot Teaming, which advances MATP by introducing a novel obstacle-aware strategy for task assignment. First, we develop an adaptive Halton sequence map, the first known application of Halton sampling with obstacle-aware adaptation in MATP, which adjusts sampling density based on obstacle distribution. Second, we propose a cluster-auction-selection framework that integrates obstacle-aware clustering with weighted auctions and intra-cluster task selection. These mechanisms jointly enable effective coordination among heterogeneous robots while maintaining scalability and near-optimal allocation performance. In addition, our framework leverages an LLM to interpret human instructions and directly guide the planner in real time. We validate OATH in NVIDIA Isaac Sim, showing substantial improvements in task assignment quality, scalability, adaptability to dynamic changes, and overall execution performance compared to state-of-the-art MATP baselines. A project website is available at https://llm-oath.github.io/.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "74",
        "title": "Optimistic Reinforcement Learning-Based Skill Insertions for Task and Motion Planning",
        "author": [
            "Gaoyuan Liu",
            "Joris de Winter",
            "Yuri Durodie",
            "Denis Steckelmacher",
            "Ann Nowe",
            "Bram Vanderborght"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14065",
        "abstract": "Task and motion planning (TAMP) for robotics manipulation necessitates long-horizon reasoning involving versatile actions and skills. While deterministic actions can be crafted by sampling or optimizing with certain constraints, planning actions with uncertainty, i.e., probabilistic actions, remains a challenge for TAMP. On the contrary, Reinforcement Learning (RL) excels in acquiring versatile, yet short-horizon, manipulation skills that are robust with uncertainties. In this letter, we design a method that integrates RL skills into TAMP pipelines. Besides the policy, a RL skill is defined with data-driven logical components that enable the skill to be deployed by symbolic planning. A plan refinement sub-routine is designed to further tackle the inevitable effect uncertainties. In the experiments, we compare our method with baseline hierarchical planning from both TAMP and RL fields and illustrate the strength of the method. The results show that by embedding RL skills, we extend the capability of TAMP to domains with probabilistic skills, and improve the planning efficiency compared to the previous methods.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "75",
        "title": "DiffOPF: Diffusion Solver for Optimal Power Flow",
        "author": [
            "Milad Hoseinpour",
            "Vladimir Dvorkin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14075",
        "abstract": "The optimal power flow (OPF) is a multi-valued, non-convex mapping from loads to dispatch setpoints. The variability of system parameters (e.g., admittances, topology) further contributes to the multiplicity of dispatch setpoints for a given load. Existing deep learning OPF solvers are single-valued and thus fail to capture the variability of system parameters unless fully represented in the feature space, which is prohibitive. To solve this problem, we introduce a diffusion-based OPF solver, termed \\textit{DiffOPF}, that treats OPF as a conditional sampling problem. The solver learns the joint distribution of loads and dispatch setpoints from operational history, and returns the marginal dispatch distributions conditioned on loads. Unlike single-valued solvers, DiffOPF enables sampling statistically credible warm starts with favorable cost and constraint satisfaction trade-offs. We explore the sample complexity of DiffOPF to ensure the OPF solution within a prescribed distance from the optimization-based solution, and verify this experimentally on power system benchmarks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "76",
        "title": "ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models",
        "author": [
            "Haziq Mohammad Khalid",
            "Athikash Jeyaganthan",
            "Timothy Do",
            "Yicheng Fu",
            "Sean O'Brien",
            "Vasu Sharma",
            "Kevin Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14077",
        "abstract": "Large Language Models (LLMs) suffer significant performance degradation in multi-turn conversations when information is presented incrementally. Given that multi-turn conversations characterize everyday interactions with LLMs, this degradation poses a severe challenge to real world usability. We hypothesize that abrupt increases in model uncertainty signal misalignment in multi-turn LLM interactions, and we exploit this insight to dynamically realign conversational context. We introduce ERGO (Entropy-guided Resetting for Generation Optimization), which continuously quantifies internal uncertainty via Shannon entropy over next token distributions and triggers adaptive prompt consolidation when a sharp spike in entropy is detected. By treating uncertainty as a first class signal rather than a nuisance to eliminate, ERGO embraces variability in language and modeling, representing and responding to uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO yields a 56.6% average performance gain over standard baselines, increases aptitude (peak performance capability) by 24.7%, and decreases unreliability (variability in performance) by 35.3%, demonstrating that uncertainty aware interventions can improve both accuracy and reliability in conversational AI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "77",
        "title": "Capture, Canonicalize, Splat: Zero-Shot 3D Gaussian Avatars from Unstructured Phone Images",
        "author": [
            "Emanuel Garbin",
            "Guy Adam",
            "Oded Krams",
            "Zohar Barzelay",
            "Eran Guendelman",
            "Michael Schwarz",
            "Moran Vatelmacher",
            "Yigal Shenkman",
            "Eli Peker",
            "Itai Druker",
            "Uri Patish",
            "Yoav Blum",
            "Max Bluvstein",
            "Junxuan Li",
            "Rawal Khirodkar",
            "Shunsuke Saito"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14081",
        "abstract": "We present a novel, zero-shot pipeline for creating hyperrealistic, identity-preserving 3D avatars from a few unstructured phone images. Existing methods face several challenges: single-view approaches suffer from geometric inconsistencies and hallucinations, degrading identity preservation, while models trained on synthetic data fail to capture high-frequency details like skin wrinkles and fine hair, limiting realism. Our method introduces two key contributions: (1) a generative canonicalization module that processes multiple unstructured views into a standardized, consistent representation, and (2) a transformer-based model trained on a new, large-scale dataset of high-fidelity Gaussian splatting avatars derived from dome captures of real people. This \"Capture, Canonicalize, Splat\" pipeline produces static quarter-body avatars with compelling realism and robust identity preservation from unstructured photos.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Transformer"
        ]
    },
    {
        "id": "78",
        "title": "Neural Network approximation power on homogeneous and heterogeneous reaction-diffusion equations",
        "author": [
            "Haotian Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14094",
        "abstract": "Reaction-diffusion systems represent one of the most fundamental formulations used to describe a wide range of physical, chemical, and biological processes. With the increasing adoption of neural networks, recent research has focused on solving differential equations using machine learning techniques. However, the theoretical foundation explaining why neural networks can effectively approximate such solutions remains insufficiently explored.\nThis paper provides a theoretical analysis of the approximation power of neural networks for one- and two-dimensional reaction-diffusion equations in both homogeneous and heterogeneous media. Building upon the universal approximation theorem, we demonstrate that a two-layer neural network can approximate the one-dimensional reaction-diffusion equation, while a three-layer neural network can approximate its two-dimensional counterpart. The theoretical framework presented here can be further extended to elliptic and parabolic equations.\nOverall, this work highlights the expressive power of neural networks in approximating solutions to reaction-diffusion equations and related PDEs, providing a theoretical foundation for neural network-based differential equation solvers.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "79",
        "title": "Unlocking Out-of-Distribution Generalization in Transformers via Recursive Latent Space Reasoning",
        "author": [
            "Awni Altabaa",
            "Siyu Chen",
            "John Lafferty",
            "Zhuoran Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14095",
        "abstract": "Systematic, compositional generalization beyond the training distribution remains a core challenge in machine learning -- and a critical bottleneck for the emergent reasoning abilities of modern language models. This work investigates out-of-distribution (OOD) generalization in Transformer networks using a GSM8K-style modular arithmetic on computational graphs task as a testbed. We introduce and explore a set of four architectural mechanisms aimed at enhancing OOD generalization: (i) input-adaptive recurrence; (ii) algorithmic supervision; (iii) anchored latent representations via a discrete bottleneck; and (iv) an explicit error-correction mechanism. Collectively, these mechanisms yield an architectural approach for native and scalable latent space reasoning in Transformer networks with robust algorithmic generalization capabilities. We complement these empirical results with a detailed mechanistic interpretability analysis that reveals how these mechanisms give rise to robust OOD generalization abilities.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "80",
        "title": "TENDE: Transfer Entropy Neural Diffusion Estimation",
        "author": [
            "Simon Pedro Galeano Munoz",
            "Mustapha Bounoua",
            "Giulio Franzese",
            "Pietro Michiardi",
            "Maurizio Filippone"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14096",
        "abstract": "Transfer entropy measures directed information flow in time series, and it has become a fundamental quantity in applications spanning neuroscience, finance, and complex systems analysis. However, existing estimation methods suffer from the curse of dimensionality, require restrictive distributional assumptions, or need exponentially large datasets for reliable convergence. We address these limitations in the literature by proposing TENDE (Transfer Entropy Neural Diffusion Estimation), a novel approach that leverages score-based diffusion models to estimate transfer entropy through conditional mutual information. By learning score functions of the relevant conditional distributions, TENDE provides flexible, scalable estimation while making minimal assumptions about the underlying data-generating process. We demonstrate superior accuracy and robustness compared to existing neural estimators and other state-of-the-art approaches across synthetic benchmarks and real data.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "81",
        "title": "Generating Fair Consensus Statements with Social Choice on Token-Level MDPs",
        "author": [
            "Carter Blair",
            "Kate Larson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14106",
        "abstract": "Current frameworks for consensus statement generation with large language models lack the inherent structure needed to provide provable fairness guarantees when aggregating diverse free-form opinions. We model the task as a multi-objective, token-level Markov Decision Process (MDP), where each objective corresponds to an agent's preference. Token-level rewards for each agent are derived from their policy (e.g., a personalized language model). This approach utilizes the finding that such policies implicitly define optimal Q-functions, providing a principled way to quantify rewards at each generation step without a value function (Rafailov et al., 2024). This MDP formulation creates a formal structure amenable to analysis using principles from social choice theory. We propose two approaches grounded in social choice theory. First, we propose a stochastic generation policy guaranteed to be in the ex-ante core, extending core stability concepts from voting theory to text generation. This policy is derived from an underlying distribution over complete statements that maximizes proportional fairness (Nash Welfare). Second, for generating a single statement, we target the maximization of egalitarian welfare using search algorithms within the MDP framework. Empirically, experiments using language models to instantiate agent policies show that search guided by the egalitarian objective generates consensus statements with improved worst-case agent alignment compared to baseline methods, including the Habermas Machine (Tessler et al., 2024).",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "82",
        "title": "DROID: Dual Representation for Out-of-Scope Intent Detection",
        "author": [
            "Wael Rashwan",
            "Hossam M. Zawbaa",
            "Sourav Dutta",
            "Haytham Assem"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14110",
        "abstract": "Detecting out-of-scope (OOS) user utterances remains a key challenge in task-oriented dialogue systems and, more broadly, in open-set intent recognition. Existing approaches often depend on strong distributional assumptions or auxiliary calibration modules. We present DROID (Dual Representation for Out-of-Scope Intent Detection), a compact end-to-end framework that combines two complementary encoders -- the Universal Sentence Encoder (USE) for broad semantic generalization and a domain-adapted Transformer-based Denoising Autoencoder (TSDAE) for domain-specific contextual distinctions. Their fused representations are processed by a lightweight branched classifier with a single calibrated threshold that separates in-domain and OOS intents without post-hoc scoring. To enhance boundary learning under limited supervision, DROID incorporates both synthetic and open-domain outlier augmentation. Despite using only 1.5M trainable parameters, DROID consistently outperforms recent state-of-the-art baselines across multiple intent benchmarks, achieving macro-F1 improvements of 6--15% for known and 8--20% for OOS intents, with the most significant gains in low-resource settings. These results demonstrate that dual-encoder representations with simple calibration can yield robust, scalable, and reliable OOS detection for neural dialogue systems.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "83",
        "title": "STEMS: Spatial-Temporal Enhanced Safe Multi-Agent Coordination for Building Energy Management",
        "author": [
            "Huiliang Zhang",
            "Di Wu",
            "Arnaud Zinflou",
            "Benoit Boulet"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14112",
        "abstract": "Building energy management is essential for achieving carbon reduction goals, improving occupant comfort, and reducing energy costs. Coordinated building energy management faces critical challenges in exploiting spatial-temporal dependencies while ensuring operational safety across multi-building systems. Current multi-building energy systems face three key challenges: insufficient spatial-temporal information exploitation, lack of rigorous safety guarantees, and system complexity. This paper proposes Spatial-Temporal Enhanced Safe Multi-Agent Coordination (STEMS), a novel safety-constrained multi-agent reinforcement learning framework for coordinated building energy management. STEMS integrates two core components: (1) a spatial-temporal graph representation learning framework using a GCN-Transformer fusion architecture to capture inter-building relationships and temporal patterns, and (2) a safety-constrained multi-agent RL algorithm incorporating Control Barrier Functions to provide mathematical safety guarantees. Extensive experiments on real-world building datasets demonstrate STEMS's superior performance over existing methods, showing that STEMS achieves 21% cost reduction, 18% emission reduction, and dramatically reduces safety violations from 35.1% to 5.6% while maintaining optimal comfort with only 0.13 discomfort proportion. The framework also demonstrates strong robustness during extreme weather conditions and maintains effectiveness across different building types.",
        "tags": [
            "RL",
            "Transformer"
        ]
    },
    {
        "id": "84",
        "title": "Toward Cybersecurity-Expert Small Language Models",
        "author": [
            "Matan Levi",
            "Daniel Ohayon",
            "Ariel Blobstein",
            "Ravid Sagi",
            "Ian Molloy",
            "Yair Allouche"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14113",
        "abstract": "Large language models (LLMs) are transforming everyday applications, yet deployment in cybersecurity lags due to a lack of high-quality, domain-specific models and training datasets. To address this gap, we present CyberPal 2.0, a family of cybersecurity-expert small language models (SLMs) ranging from 4B-20B parameters. To train CyberPal 2.0, we generate an enriched chain-of-thought cybersecurity instruction dataset built with our data enrichment and formatting pipeline, SecKnowledge 2.0, which integrates expert-in-the-loop steering of reasoning formats alongside LLM-driven multi-step grounding, yielding higher-fidelity, task-grounded reasoning traces for security tasks. Across diverse cybersecurity benchmarks, CyberPal 2.0 consistently outperforms its baselines and matches or surpasses various open and closed-source frontier models, while remaining a fraction of their size. On core cyber threat intelligence knowledge tasks, our models outperform almost all tested frontier models, ranking second only to Sec-Gemini v1. On core threat-investigation tasks, such as correlating vulnerabilities and bug tickets with weaknesses, our best 20B-parameter model outperforms GPT-4o, o1, o3-mini, and Sec-Gemini v1, ranking first, while our smallest 4B-parameter model ranks second.",
        "tags": [
            "CoT",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "85",
        "title": "Briding Diffusion Posterior Sampling and Monte Carlo methods: a survey",
        "author": [
            "Yazid Janati",
            "Alain Durmus",
            "Jimmy Olsson",
            "Eric Moulines"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14114",
        "abstract": "Diffusion models enable the synthesis of highly accurate samples from complex distributions and have become foundational in generative modeling. Recently, they have demonstrated significant potential for solving Bayesian inverse problems by serving as priors. This review offers a comprehensive overview of current methods that leverage \\emph{pre-trained} diffusion models alongside Monte Carlo methods to address Bayesian inverse problems without requiring additional training. We show that these methods primarily employ a \\emph{twisting} mechanism for the intermediate distributions within the diffusion process, guiding the simulations toward the posterior distribution. We describe how various Monte Carlo methods are then used to aid in sampling from these twisted distributions.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "86",
        "title": "David vs. Goliath: A comparative study of different-sized LLMs for code generation in the domain of automotive scenario generation",
        "author": [
            "Philipp Bauerfeind",
            "Amir Salarpour",
            "David Fernandez",
            "Pedram MohajerAnsari",
            "Johannes Reschke",
            "Mert D. PesÃ©"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14115",
        "abstract": "Scenario simulation is central to testing autonomous driving systems. Scenic, a domain-specific language (DSL) for CARLA, enables precise and reproducible scenarios, but NL-to-Scenic generation with large language models (LLMs) suffers from scarce data, limited reproducibility, and inconsistent metrics. We introduce NL2Scenic, an open dataset and framework with 146 NL/Scenic pairs, a difficulty-stratified 30-case test split, an Example Retriever, and 14 prompting variants (ZS, FS, CoT, SP, MoT). We evaluate 13 models: four proprietary (GPT-4o, GPT-5, Claude-Sonnet-4, Gemini-2.5-pro) and nine open-source code models (Qwen2.5Coder 0.5B-32B; CodeLlama 7B/13B/34B), using text metrics (BLEU, ChrF, EDIT-SIM, CrystalBLEU) and execution metrics (compilation and generation), and compare them with an expert study (n=11). EDIT-SIM correlates best with human judgments; we also propose EDIT-COMP (F1 of EDIT-SIM and compilation) as a robust dataset-level proxy that improves ranking fidelity. GPT-4o performs best overall, while Qwen2.5Coder-14B reaches about 88 percent of its expert score on local hardware. Retrieval-augmented prompting, Few-Shot with Example Retriever (FSER), consistently boosts smaller models, and scaling shows diminishing returns beyond mid-size, with Qwen2.5Coder outperforming CodeLlama at comparable scales. NL2Scenic and EDIT-COMP offer a standardized, reproducible basis for evaluating Scenic code generation and indicate that mid-size open-source models are practical, cost-effective options for autonomous-driving scenario programming.",
        "tags": [
            "CoT",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "87",
        "title": "ViTacGen: Robotic Pushing with Vision-to-Touch Generation",
        "author": [
            "Zhiyuan Wu",
            "Yijiong Lin",
            "Yongqiang Zhao",
            "Xuyang Zhang",
            "Zhuo Chen",
            "Nathan Lepora",
            "Shan Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14117",
        "abstract": "Robotic pushing is a fundamental manipulation task that requires tactile feedback to capture subtle contact forces and dynamics between the end-effector and the object. However, real tactile sensors often face hardware limitations such as high costs and fragility, and deployment challenges involving calibration and variations between different sensors, while vision-only policies struggle with satisfactory performance. Inspired by humans' ability to infer tactile states from vision, we propose ViTacGen, a novel robot manipulation framework designed for visual robotic pushing with vision-to-touch generation in reinforcement learning to eliminate the reliance on high-resolution real tactile sensors, enabling effective zero-shot deployment on visual-only robotic systems. Specifically, ViTacGen consists of an encoder-decoder vision-to-touch generation network that generates contact depth images, a standardized tactile representation, directly from visual image sequence, followed by a reinforcement learning policy that fuses visual-tactile data with contrastive learning based on visual and generated tactile observations. We validate the effectiveness of our approach in both simulation and real world experiments, demonstrating its superior performance and achieving a success rate of up to 86\\%.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "88",
        "title": "Demystifying the Mechanisms Behind Emergent Exploration in Goal-conditioned RL",
        "author": [
            "Mahsa Bastankhah",
            "Grace Liu",
            "Dilip Arumugam",
            "Thomas L. Griffiths",
            "Benjamin Eysenbach"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14129",
        "abstract": "In this work, we take a first step toward elucidating the mechanisms behind emergent exploration in unsupervised reinforcement learning. We study Single-Goal Contrastive Reinforcement Learning (SGCRL), a self-supervised algorithm capable of solving challenging long-horizon goal-reaching tasks without external rewards or curricula. We combine theoretical analysis of the algorithm's objective function with controlled experiments to understand what drives its exploration. We show that SGCRL maximizes implicit rewards shaped by its learned representations. These representations automatically modify the reward landscape to promote exploration before reaching the goal and exploitation thereafter. Our experiments also demonstrate that these exploration dynamics arise from learning low-rank representations of the state space rather than from neural network function approximation. Our improved understanding enables us to adapt SGCRL to perform safety-aware exploration.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "89",
        "title": "Formalizing the Safety, Security, and Functional Properties of Agentic AI Systems",
        "author": [
            "Edoardo Allegrini",
            "Ananth Shreekumar",
            "Z. Berkay Celik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14133",
        "abstract": "Agentic AI systems, which leverage multiple autonomous agents and Large Language Models (LLMs), are increasingly used to address complex, multi-step tasks. The safety, security, and functionality of these systems are critical, especially in high-stakes applications. However, the current ecosystem of inter-agent communication is fragmented, with protocols such as the Model Context Protocol (MCP) for tool access and the Agent-to-Agent (A2A) protocol for coordination being analyzed in isolation. This fragmentation creates a semantic gap that prevents the rigorous analysis of system properties and introduces risks such as architectural misalignment and exploitable coordination issues. To address these challenges, we introduce a modeling framework for agentic AI systems composed of two foundational models. The first, the host agent model, formalizes the top-level entity that interacts with the user, decomposes tasks, and orchestrates their execution by leveraging external agents and tools. The second, the task lifecycle model, details the states and transitions of individual sub-tasks from creation to completion, providing a fine-grained view of task management and error handling. Together, these models provide a unified semantic framework for reasoning about the behavior of multi-AI agent systems. Grounded in this framework, we define 17 properties for the host agent and 14 for the task lifecycle, categorized into liveness, safety, completeness, and fairness. Expressed in temporal logic, these properties enable formal verification of system behavior, detection of coordination edge cases, and prevention of deadlocks and security vulnerabilities. Through this effort, we introduce the first rigorously grounded, domain-agnostic framework for the systematic analysis, design, and deployment of correct, reliable, and robust agentic AI systems.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "90",
        "title": "CodeEvolve: An open source evolutionary coding agent for algorithm discovery and optimization",
        "author": [
            "Henrique AssumpÃ§Ã£o",
            "Diego Ferreira",
            "Leandro Campos",
            "Fabricio Murai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14150",
        "abstract": "In this work, we introduce CodeEvolve, an open-source evolutionary coding agent that unites Large Language Models (LLMs) with genetic algorithms to solve complex computational problems. Our framework adapts powerful evolutionary concepts to the LLM domain, building upon recent methods for generalized scientific discovery. CodeEvolve employs an island-based genetic algorithm to maintain population diversity and increase throughput, introduces a novel inspiration-based crossover mechanism that leverages the LLMs context window to combine features from successful solutions, and implements meta-prompting strategies for dynamic exploration of the solution space. We conduct a rigorous evaluation of CodeEvolve on a subset of the mathematical benchmarks used to evaluate Google DeepMind's closed-source AlphaEvolve. Our findings show that our method surpasses AlphaEvolve's performance on several challenging problems. To foster collaboration and accelerate progress, we release our complete framework as an open-source repository.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "91",
        "title": "Combining Reinforcement Learning and Behavior Trees for NPCs in Video Games with AMD Schola",
        "author": [
            "Tian Liu",
            "Alex Cann",
            "Ian Colbert",
            "Mehdi Saeedi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14154",
        "abstract": "While the rapid advancements in the reinforcement learning (RL) research community have been remarkable, the adoption in commercial video games remains slow. In this paper, we outline common challenges the Game AI community faces when using RL-driven NPCs in practice, and highlight the intersection of RL with traditional behavior trees (BTs) as a crucial juncture to be explored further. Although the BT+RL intersection has been suggested in several research papers, its adoption is rare. We demonstrate the viability of this approach using AMD Schola -- a plugin for training RL agents in Unreal Engine -- by creating multi-task NPCs in a complex 3D environment inspired by the commercial video game ``The Last of Us\". We provide detailed methodologies for jointly training RL models with BTs while showcasing various skills.",
        "tags": [
            "3D",
            "RL"
        ]
    },
    {
        "id": "92",
        "title": "On Evaluating Loss Functions for Stock Ranking: An Empirical Analysis With Transformer Model",
        "author": [
            "Jan Kwiatkowski",
            "JarosÅaw A. Chudziak"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14156",
        "abstract": "Quantitative trading strategies rely on accurately ranking stocks to identify profitable investments. Effective portfolio management requires models that can reliably order future stock returns. Transformer models are promising for understanding financial time series, but how different training loss functions affect their ability to rank stocks well is not yet fully understood. Financial markets are challenging due to their changing nature and complex relationships between stocks. Standard loss functions, which aim for simple prediction accuracy, often aren't enough. They don't directly teach models to learn the correct order of stock returns. While many advanced ranking losses exist from fields such as information retrieval, there hasn't been a thorough comparison to see how well they work for ranking financial returns, especially when used with modern Transformer models for stock selection. This paper addresses this gap by systematically evaluating a diverse set of advanced loss functions including pointwise, pairwise, listwise for daily stock return forecasting to facilitate rank-based portfolio selection on S&P 500 data. We focus on assessing how each loss function influences the model's ability to discern profitable relative orderings among assets. Our research contributes a comprehensive benchmark revealing how different loss functions impact a model's ability to learn cross-sectional and temporal patterns crucial for portfolio selection, thereby offering practical guidance for optimizing ranking-based trading strategies.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "93",
        "title": "Towards Reversible Model Merging For Low-rank Weights",
        "author": [
            "Mohammadsajad Alipour",
            "Mohammad Mohammadi Amiri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14163",
        "abstract": "Model merging aims to combine multiple fine-tuned models into a single set of weights that performs well across all source tasks. While prior work has shown that merging can approximate the performance of individual fine-tuned models for each task, it largely overlooks scenarios where models are compressed into low-rank representations, either through low-rank adaptation (LoRA) or post-training singular value decomposition (SVD). We first demonstrate that applying conventional merging methods to low-rank weights leads to severe performance degradation in the merged model. Motivated by this phenomenon, we propose a fundamentally different approach: instead of collapsing all adapters into one set of weights, we construct a compact basis (e.g., an equivalent of holding two or more models) from which original task-specific models can be recovered via linear combination. This reframes merging as generating a reconstruction-capable model space rather than producing a single merged model. Crucially, this allows us to ``revert'' to each individual model when needed, recognizing that no merged model can consistently outperform one specialized for its task. Building on this insight, we introduce our method, Reversible Model Merging (RMM), an efficient, data-free, and flexible method that provides a closed-form solution for selecting the optimal basis of model weights and task-specific coefficients for linear combination. Extensive experiments across diverse datasets and model scales demonstrate that RMM consistently outperforms existing merging approaches, preserving the performance of low-rank compressed models by a significant margin.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "94",
        "title": "ARM-FM: Automated Reward Machines via Foundation Models for Compositional Reinforcement Learning",
        "author": [
            "Roger Creus Castanyer",
            "Faisal Mohamed",
            "Pablo Samuel Castro",
            "Cyrus Neary",
            "Glen Berseth"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14176",
        "abstract": "Reinforcement learning (RL) algorithms are highly sensitive to reward function specification, which remains a central challenge limiting their broad applicability. We present ARM-FM: Automated Reward Machines via Foundation Models, a framework for automated, compositional reward design in RL that leverages the high-level reasoning capabilities of foundation models (FMs). Reward machines (RMs) -- an automata-based formalism for reward specification -- are used as the mechanism for RL objective specification, and are automatically constructed via the use of FMs. The structured formalism of RMs yields effective task decompositions, while the use of FMs enables objective specifications in natural language. Concretely, we (i) use FMs to automatically generate RMs from natural language specifications; (ii) associate language embeddings with each RM automata-state to enable generalization across tasks; and (iii) provide empirical evidence of ARM-FM's effectiveness in a diverse suite of challenging environments, including evidence of zero-shot generalization.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "95",
        "title": "Virtually Being: Customizing Camera-Controllable Video Diffusion Models with Multi-View Performance Captures",
        "author": [
            "Yuancheng Xu",
            "Wenqi Xian",
            "Li Ma",
            "Julien Philip",
            "Ahmet Levent TaÅel",
            "Yiwei Zhao",
            "Ryan Burgert",
            "Mingming He",
            "Oliver Hermann",
            "Oliver Pilarski",
            "Rahul Garg",
            "Paul Debevec",
            "Ning Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14179",
        "abstract": "We introduce a framework that enables both multi-view character consistency and 3D camera control in video diffusion models through a novel customization data pipeline. We train the character consistency component with recorded volumetric capture performances re-rendered with diverse camera trajectories via 4D Gaussian Splatting (4DGS), lighting variability obtained with a video relighting model. We fine-tune state-of-the-art open-source video diffusion models on this data to provide strong multi-view identity preservation, precise camera control, and lighting adaptability. Our framework also supports core capabilities for virtual production, including multi-subject generation using two approaches: joint training and noise blending, the latter enabling efficient composition of independently customized models at inference time; it also achieves scene and real-life video customization as well as control over motion and spatial layout during customization. Extensive experiments show improved video quality, higher personalization accuracy, and enhanced camera control and lighting adaptability, advancing the integration of video generation into virtual production. Our project page is available at: https://eyeline-labs.github.io/Virtually-Being.",
        "tags": [
            "3D",
            "Diffusion",
            "Gaussian Splatting",
            "Video Generation"
        ]
    },
    {
        "id": "96",
        "title": "Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation",
        "author": [
            "Ruchi Sandilya",
            "Sumaira Perez",
            "Charles Lynch",
            "Lindsay Victoria",
            "Benjamin Zebley",
            "Derrick Matthew Buchanan",
            "Mahendra T. Bhati",
            "Nolan Williams",
            "Timothy J. Spellman",
            "Faith M. Gunning",
            "Conor Liston",
            "Logan Grosenick"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14190",
        "abstract": "Diffusion models excel at generation, but their latent spaces are not explicitly organized for interpretable control. We introduce ConDA (Contrastive Diffusion Alignment), a framework that applies contrastive learning within diffusion embeddings to align latent geometry with system dynamics. Motivated by recent advances showing that contrastive objectives can recover more disentangled and structured representations, ConDA organizes diffusion latents such that traversal directions reflect underlying dynamical factors. Within this contrastively structured space, ConDA enables nonlinear trajectory traversal that supports faithful interpolation, extrapolation, and controllable generation. Across benchmarks in fluid dynamics, neural calcium imaging, therapeutic neurostimulation, and facial expression, ConDA produces interpretable latent representations with improved controllability compared to linear traversals and conditioning-based baselines. These results suggest that diffusion latents encode dynamics-relevant structure, but exploiting this structure requires latent organization and traversal along the latent manifold.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "97",
        "title": "RLSR: Reinforcement Learning with Supervised Reward Outperforms SFT in Instruction Following",
        "author": [
            "Zhichao Wang",
            "Andy Wong",
            "Ruslan Belkin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14200",
        "abstract": "After the pretraining stage of LLMs, techniques such as SFT, RLHF, RLVR, and RFT are applied to enhance instruction-following ability, mitigate undesired responses, improve reasoning capability and enable efficient domain adaptation with minimal data. SFT relies on the next-token prediction objective to strengthen instruction following in a base model using a large corpus of human-labeled responses. In contrast, RFT employs a RL-based approach to adapt fine-tuned reasoning models to specific domains with limited supervision. Inspired by RFT, we propose replacing SFT with RLSR to leverage the extensive SFT dataset in an RL framework, thereby improving the base model's instruction-following ability. In RLSR, the base model generates multiple responses for each prompt, and reward scores are computed as the cosine similarity in the semantic embedding space between the generated and human-labeled responses. RLSR can be utilized in multiple ways. It can directly replace SFT, achieving superior performance on instruction-following benchmarks-for example, RLSR (SB) on Qwen-7B (INFINITY) achieved an AlpacaEval win rate of 26.34%, surpassing SFT's 21.01%. Furthermore, combining SFT and RLSR further enhances downstream task performance; Qwen-7B (INFINITY) achieved a win rate of 30.73% when trained with SFT + RLSR.",
        "tags": [
            "LLM",
            "Qwen",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "98",
        "title": "Echoes of Human Malice in Agents: Benchmarking LLMs for Multi-Turn Online Harassment Attacks",
        "author": [
            "Trilok Padhi",
            "Pinxian Lu",
            "Abdulkadir Erol",
            "Tanmay Sutar",
            "Gauri Sharma",
            "Mina Sonmez",
            "Munmun De Choudhury",
            "Ugur Kursuncu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14207",
        "abstract": "Large Language Model (LLM) agents are powering a growing share of interactive web applications, yet remain vulnerable to misuse and harm. Prior jailbreak research has largely focused on single-turn prompts, whereas real harassment often unfolds over multi-turn interactions. In this work, we present the Online Harassment Agentic Benchmark consisting of: (i) a synthetic multi-turn harassment conversation dataset, (ii) a multi-agent (e.g., harasser, victim) simulation informed by repeated game theory, (iii) three jailbreak methods attacking agents across memory, planning, and fine-tuning, and (iv) a mixed-methods evaluation framework. We utilize two prominent LLMs, LLaMA-3.1-8B-Instruct (open-source) and Gemini-2.0-flash (closed-source). Our results show that jailbreak tuning makes harassment nearly guaranteed with an attack success rate of 95.78--96.89% vs. 57.25--64.19% without tuning in Llama, and 99.33% vs. 98.46% without tuning in Gemini, while sharply reducing refusal rate to 1-2% in both models. The most prevalent toxic behaviors are Insult with 84.9--87.8% vs. 44.2--50.8% without tuning, and Flaming with 81.2--85.1% vs. 31.5--38.8% without tuning, indicating weaker guardrails compared to sensitive categories such as sexual or racial harassment. Qualitative evaluation further reveals that attacked agents reproduce human-like aggression profiles, such as Machiavellian/psychopathic patterns under planning, and narcissistic tendencies with memory. Counterintuitively, closed-source and open-source models exhibit distinct escalation trajectories across turns, with closed-source models showing significant vulnerability. Overall, our findings show that multi-turn and theory-grounded attacks not only succeed at high rates but also mimic human-like harassment dynamics, motivating the development of robust safety guardrails to ultimately keep online platforms safe and responsible.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "99",
        "title": "Incentive-Based Federated Learning",
        "author": [
            "Chanuka A.S. Hewa Kaluannakkage",
            "Rajkumar Buyya"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14208",
        "abstract": "Federated learning promises to revolutionize machine learning by enabling collaborative model training without compromising data privacy. However, practical adaptability can be limited by critical factors, such as the participation dilemma. Participating entities are often unwilling to contribute to a learning system unless they receive some benefits, or they may pretend to participate and free-ride on others. This chapter identifies the fundamental challenges in designing incentive mechanisms for federated learning systems. It examines how foundational concepts from economics and game theory can be applied to federated learning, alongside technology-driven solutions such as blockchain and deep reinforcement learning. This work presents a comprehensive taxonomy that thoroughly covers both centralized and decentralized architectures based on the aforementioned theoretical concepts. Furthermore, the concepts described are presented from an application perspective, covering emerging industrial applications, including healthcare, smart infrastructure, vehicular networks, and blockchain-based decentralized systems. Through this exploration, this chapter demonstrates that well-designed incentive mechanisms are not merely optional features but essential components for the practical success of federated learning. This analysis reveals both the promising solutions that have emerged and the significant challenges that remain in building truly sustainable, fair, and robust federated learning ecosystems.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "100",
        "title": "LOTA: Bit-Planes Guided AI-Generated Image Detection",
        "author": [
            "Hongsong Wang",
            "Renxi Cheng",
            "Yang Zhang",
            "Chaolei Han",
            "Jie Gui"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14230",
        "abstract": "The rapid advancement of GAN and Diffusion models makes it more difficult to distinguish AI-generated images from real ones. Recent studies often use image-based reconstruction errors as an important feature for determining whether an image is AI-generated. However, these approaches typically incur high computational costs and also fail to capture intrinsic noisy features present in the raw images. To solve these problems, we innovatively refine error extraction by using bit-plane-based image processing, as lower bit planes indeed represent noise patterns in images. We introduce an effective bit-planes guided noisy image generation and exploit various image normalization strategies, including scaling and thresholding. Then, to amplify the noise signal for easier AI-generated image detection, we design a maximum gradient patch selection that applies multi-directional gradients to compute the noise score and selects the region with the highest score. Finally, we propose a lightweight and effective classification head and explore two different structures: noise-based classifier and noise-guided classifier. Extensive experiments on the GenImage benchmark demonstrate the outstanding performance of our method, which achieves an average accuracy of \\textbf{98.9\\%} (\\textbf{11.9}\\%~$\\uparrow$) and shows excellent cross-generator generalization capability. Particularly, our method achieves an accuracy of over 98.2\\% from GAN to Diffusion and over 99.2\\% from Diffusion to GAN. Moreover, it performs error extraction at the millisecond level, nearly a hundred times faster than existing methods. The code is at https://github.com/hongsong-wang/LOTA.",
        "tags": [
            "Detection",
            "Diffusion",
            "GAN"
        ]
    },
    {
        "id": "101",
        "title": "Scaling Test-Time Compute to Achieve IOI Gold Medal with Open-Weight Models",
        "author": [
            "Mehrzad Samadi",
            "Aleksander Ficek",
            "Sean Narenthiran",
            "Siddhartha Jain",
            "Wasi Uddin Ahmad",
            "Somshubra Majumdar",
            "Vahid Noroozi",
            "Boris Ginsburg"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14232",
        "abstract": "Competitive programming has become a rigorous benchmark for evaluating the reasoning and problem-solving capabilities of large language models (LLMs). The International Olympiad in Informatics (IOI) stands out as one of the most prestigious annual competitions in competitive programming and has become a key benchmark for comparing human and AI-level programming ability. While several proprietary models have been claimed to achieve gold medal-level performance at the IOI, often with undisclosed methods, achieving comparable results with open-weight models remains a significant challenge. In this paper, we present \\gencluster, a scalable and reproducible test-time compute framework that attains IOI gold-level performance using open-weight models. It combines large-scale generation, behavioral clustering, ranking, and a round-robin submission strategy to efficiently explore diverse solution spaces under limited validation budgets. Our experiments show that the performance of our proposed approach scales consistently with available compute, narrowing the gap between open and closed systems. Notably, we will show that GenCluster can achieve a gold medal at IOI 2025 for the first time with an open-weight model gpt-oss-120b, setting a new benchmark for transparent and reproducible evaluation of reasoning in LLMs.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "102",
        "title": "RHINO: Guided Reasoning for Mapping Network Logs to Adversarial Tactics and Techniques with Large Language Models",
        "author": [
            "Fanchao Meng",
            "Jiaping Gui",
            "Yunbo Li",
            "Yue Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14233",
        "abstract": "Modern Network Intrusion Detection Systems generate vast volumes of low-level alerts, yet these outputs remain semantically fragmented, requiring labor-intensive manual correlation with high-level adversarial behaviors. Existing solutions for automating this mapping-rule-based systems and machine learning classifiers-suffer from critical limitations: rule-based approaches fail to adapt to novel attack variations, while machine learning methods lack contextual awareness and treat tactic-technique mapping as a syntactic matching problem rather than a reasoning task. Although Large Language Models have shown promise in cybersecurity tasks, preliminary experiments reveal that existing LLM-based methods frequently hallucinate technique names or produce decontextualized mappings due to their single-step classification approach.\nTo address these challenges, we introduce RHINO, a novel framework that decomposes LLM-based attack analysis into three interpretable phases mirroring human reasoning: (1) behavioral abstraction, where raw logs are translated into contextualized narratives; (2) multi-role collaborative inference, generating candidate techniques by evaluating behavioral evidence against MITRE ATT&CK knowledge; and (3) validation, cross-referencing predictions with official MITRE definitions to rectify hallucinations. RHINO bridges the semantic gap between low-level observations and adversarial intent while improving output reliability through structured reasoning.\nWe evaluate RHINO on three benchmarks across four backbone models. RHINO achieved high accuracy, with model performance ranging from 86.38% to 88.45%, resulting in relative gains from 24.25% to 76.50% across different models. Our results demonstrate that RHINO significantly enhances the interpretability and scalability of threat analysis, offering a blueprint for deploying LLMs in operational security settings.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "103",
        "title": "PIA: Deepfake Detection Using Phoneme-Temporal and Identity-Dynamic Analysis",
        "author": [
            "Soumyya Kanti Datta",
            "Tanvi Ranga",
            "Chengzhe Sun",
            "Siwei Lyu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14241",
        "abstract": "The rise of manipulated media has made deepfakes a particularly insidious threat, involving various generative manipulations such as lip-sync modifications, face-swaps, and avatar-driven facial synthesis. Conventional detection methods, which predominantly depend on manually designed phoneme-viseme alignment thresholds, fundamental frame-level consistency checks, or a unimodal detection strategy, inadequately identify modern-day deepfakes generated by advanced generative models such as GANs, diffusion models, and neural rendering techniques. These advanced techniques generate nearly perfect individual frames yet inadvertently create minor temporal discrepancies frequently overlooked by traditional detectors. We present a novel multimodal audio-visual framework, Phoneme-Temporal and Identity-Dynamic Analysis(PIA), incorporating language, dynamic face motion, and facial identification cues to address these limitations. We utilize phoneme sequences, lip geometry data, and advanced facial identity embeddings. This integrated method significantly improves the detection of subtle deepfake alterations by identifying inconsistencies across multiple complementary modalities. Code is available at https://github.com/skrantidatta/PIA",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "104",
        "title": "Flip-Flop Consistency: Unsupervised Training for Robustness to Prompt Perturbations in LLMs",
        "author": [
            "Parsa Hejabi",
            "Elnaz Rahmati",
            "Alireza S. Ziabari",
            "Morteza Dehghani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14242",
        "abstract": "Large Language Models (LLMs) often produce inconsistent answers when faced with different phrasings of the same prompt. In this paper, we propose Flip-Flop Consistency ($F^2C$), an unsupervised training method that improves robustness to such perturbations. $F^2C$ is composed of two key components. The first, Consensus Cross-Entropy (CCE), uses a majority vote across prompt variations to create a hard pseudo-label. The second is a representation alignment loss that pulls lower-confidence and non-majority predictors toward the consensus established by high-confidence, majority-voting variations. We evaluate our method on 11 datasets spanning four NLP tasks, with 4-15 prompt variations per dataset. On average, $F^2C$ raises observed agreement by 11.62%, improves mean $F_1$ by 8.94%, and reduces performance variance across formats by 3.29%. In out-of-domain evaluations, $F^2C$ generalizes effectively, increasing $\\overline{F_1}$ and agreement while decreasing variance across most source-target pairs. Finally, when trained on only a subset of prompt perturbations and evaluated on held-out formats, $F^2C$ consistently improves both performance and agreement while reducing variance. These findings highlight $F^2C$ as an effective unsupervised method for enhancing LLM consistency, performance, and generalization under prompt perturbations. Code is available at https://github.com/ParsaHejabi/Flip-Flop-Consistency-Unsupervised-Training-for-Robustness-to-Prompt-Perturbations-in-LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "105",
        "title": "Spatial Computing Communications for Multi-User Virtual Reality in Distributed Mobile Edge Computing Network",
        "author": [
            "Caolu Xu",
            "Zhiyong Chen",
            "Meixia Tao",
            "Li Song",
            "Wenjun Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14243",
        "abstract": "Immersive virtual reality (VR) applications impose stringent requirements on latency, energy efficiency, and computational resources, particularly in multi-user interactive scenarios. To address these challenges, we introduce the concept of spatial computing communications (SCC), a framework designed to meet the latency and energy demands of multi-user VR over distributed mobile edge computing (MEC) networks. SCC jointly represents the physical space, defined by users and base stations, and the virtual space, representing shared immersive environments, using a probabilistic model of user dynamics and resource requirements. The resource deployment task is then formulated as a multi-objective combinatorial optimization (MOCO) problem that simultaneously minimizes system latency and energy consumption across distributed MEC resources. To solve this problem, we propose MO-CMPO, a multi-objective consistency model with policy optimization that integrates supervised learning and reinforcement learning (RL) fine-tuning guided by preference weights. Leveraging a sparse graph neural network (GNN), MO-CMPO efficiently generates Pareto-optimal solutions. Simulations with real-world New Radio base station datasets demonstrate that MO-CMPO achieves superior hypervolume performance and significantly lower inference latency than baseline methods. Furthermore, the analysis reveals practical deployment patterns: latency-oriented solutions favor local MEC execution to reduce transmission delay, while energy-oriented solutions minimize redundant placements to save energy.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "106",
        "title": "Policy Regularized Distributionally Robust Markov Decision Processes with Linear Function Approximation",
        "author": [
            "Jingwen Gu",
            "Yiting He",
            "Zhishuai Liu",
            "Pan Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14246",
        "abstract": "Decision-making under distribution shift is a central challenge in reinforcement learning (RL), where training and deployment environments differ. We study this problem through the lens of robust Markov decision processes (RMDPs), which optimize performance against adversarial transition dynamics. Our focus is the online setting, where the agent has only limited interaction with the environment, making sample efficiency and exploration especially critical. Policy optimization, despite its success in standard RL, remains theoretically and empirically underexplored in robust RL. To bridge this gap, we propose \\textbf{D}istributionally \\textbf{R}obust \\textbf{R}egularized \\textbf{P}olicy \\textbf{O}ptimization algorithm (DR-RPO), a model-free online policy optimization method that learns robust policies with sublinear regret. To enable tractable optimization within the softmax policy class, DR-RPO incorporates reference-policy regularization, yielding RMDP variants that are doubly constrained in both transitions and policies. To scale to large state-action spaces, we adopt the $d$-rectangular linear MDP formulation and combine linear function approximation with an upper confidence bonus for optimistic exploration. We provide theoretical guarantees showing that policy optimization can achieve polynomial suboptimality bounds and sample efficiency in robust RL, matching the performance of value-based approaches. Finally, empirical results across diverse domains corroborate our theory and demonstrate the robustness of DR-RPO.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "107",
        "title": "MACE: Mixture-of-Experts Accelerated Coordinate Encoding for Large-Scale Scene Localization and Rendering",
        "author": [
            "Mingkai Liu",
            "Dikai Fan",
            "Haohua Que",
            "Haojia Gao",
            "Xiao Liu",
            "Shuxue Peng",
            "Meixia Lin",
            "Shengyu Gu",
            "Ruicong Ye",
            "Wanli Qiu",
            "Handong Yao",
            "Ruopeng Zhang",
            "Xianliang Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14251",
        "abstract": "Efficient localization and high-quality rendering in large-scale scenes remain a significant challenge due to the computational cost involved. While Scene Coordinate Regression (SCR) methods perform well in small-scale localization, they are limited by the capacity of a single network when extended to large-scale scenes. To address these challenges, we propose the Mixed Expert-based Accelerated Coordinate Encoding method (MACE), which enables efficient localization and high-quality rendering in large-scale scenes. Inspired by the remarkable capabilities of MOE in large model domains, we introduce a gating network to implicitly classify and select sub-networks, ensuring that only a single sub-network is activated during each inference. Furtheremore, we present Auxiliary-Loss-Free Load Balancing(ALF-LB) strategy to enhance the localization accuracy on large-scale scene. Our framework provides a significant reduction in costs while maintaining higher precision, offering an efficient solution for large-scale scene applications. Additional experiments on the Cambridge test set demonstrate that our method achieves high-quality rendering results with merely 10 minutes of training.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "108",
        "title": "MoM: Mixtures of Scenario-Aware Document Memories for Retrieval-Augmented Generation Systems",
        "author": [
            "Jihao Zhao",
            "Zhiyuan Ji",
            "Simin Niu",
            "Hanyu Wang",
            "Feiyu Xiong",
            "Zhiyu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14252",
        "abstract": "The traditional RAG paradigm, which typically engages in the comprehension of relevant text chunks in response to received queries, inherently restricts both the depth of knowledge internalization and reasoning capabilities. To address this limitation, our research transforms the text processing in RAG from passive chunking to proactive understanding, defining this process as document memory extraction with the objective of simulating human cognitive processes during reading. Building upon this, we propose the Mixtures of scenario-aware document Memories (MoM) framework, engineered to efficiently handle documents from multiple domains and train small language models (SLMs) to acquire the ability to proactively explore and construct document memories. The MoM initially instructs large language models (LLMs) to simulate domain experts in generating document logical outlines, thereby directing structured chunking and core content extraction. It employs a multi-path sampling and multi-perspective evaluation mechanism, specifically designing comprehensive metrics that represent chunk clarity and extraction completeness to select the optimal document memories. Additionally, to infuse deeper human-like reading abilities during the training of SLMs, we incorporate a reverse reasoning strategy, which deduces refined expert thinking paths from high-quality outcomes. Finally, leveraging diverse forms of content generated by MoM, we develop a three-layer document memory retrieval mechanism, which is grounded in our theoretical proof from the perspective of probabilistic modeling. Extensive experimental results across three distinct domains demonstrate that the MoM framework not only resolves text chunking challenges in existing RAG systems, providing LLMs with semantically complete document memories, but also paves the way for SLMs to achieve human-centric intelligent text processing.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "109",
        "title": "Towards Agentic Self-Learning LLMs in Search Environment",
        "author": [
            "Wangtao Sun",
            "Xiang Cheng",
            "Jialin Fan",
            "Yao Xu",
            "Xing Yu",
            "Shizhu He",
            "Jun Zhao",
            "Kang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14253",
        "abstract": "We study whether self-learning can scale LLM-based agents without relying on human-curated datasets or predefined rule-based rewards. Through controlled experiments in a search-agent setting, we identify two key determinants of scalable agent training: the source of reward signals and the scale of agent task data. We find that rewards from a Generative Reward Model (GRM) outperform rigid rule-based signals for open-domain learning, and that co-evolving the GRM with the policy further boosts performance. Increasing the volume of agent task data-even when synthetically generated-substantially enhances agentic capabilities. Building on these insights, we propose \\textbf{Agentic Self-Learning} (ASL), a fully closed-loop, multi-role reinforcement learning framework that unifies task generation, policy execution, and evaluation within a shared tool environment and LLM backbone. ASL coordinates a Prompt Generator, a Policy Model, and a Generative Reward Model to form a virtuous cycle of harder task setting, sharper verification, and stronger solving. Empirically, ASL delivers steady, round-over-round gains, surpasses strong RLVR baselines (e.g., Search-R1) that plateau or degrade, and continues improving under zero-labeled-data conditions, indicating superior sample efficiency and robustness. We further show that GRM verification capacity is the main bottleneck: if frozen, it induces reward hacking and stalls progress; continual GRM training on the evolving data distribution mitigates this, and a small late-stage injection of real verification data raises the performance ceiling. This work establishes reward source and data scale as critical levers for open-domain agent learning and demonstrates the efficacy of multi-role co-evolution for scalable, self-improving agents. The data and code of this paper are released at https://github.com/forangel2014/Towards-Agentic-Self-Learning",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "110",
        "title": "Generalist vs Specialist Time Series Foundation Models: Investigating Potential Emergent Behaviors in Assessing Human Health Using PPG Signals",
        "author": [
            "Saurabh Kataria",
            "Yi Wu",
            "Zhaoliang Chen",
            "Hyunjung Gloria Kwak",
            "Yuhao Xu",
            "Lovely Yeswanth Panchumarthi",
            "Ran Xiao",
            "Jiaying Lu",
            "Ayca Ermis",
            "Anni Zhao",
            "Runze Yan",
            "Alex Federov",
            "Zewen Liu",
            "Xu Wu",
            "Wei Jin",
            "Carl Yang",
            "Jocelyn Grunwell",
            "Stephanie R. Brown",
            "Amit Shah",
            "Craig Jabaley",
            "Tim Buchman",
            "Sivasubramanium V Bhavani",
            "Randall J. Lee",
            "Xiao Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14254",
        "abstract": "Foundation models are large-scale machine learning models that are pre-trained on massive amounts of data and can be adapted for various downstream tasks. They have been extensively applied to tasks in Natural Language Processing and Computer Vision with models such as GPT, BERT, and CLIP. They are now also increasingly gaining attention in time-series analysis, particularly for physiological sensing. However, most time series foundation models are specialist models - with data in pre-training and testing of the same type, such as Electrocardiogram, Electroencephalogram, and Photoplethysmogram (PPG). Recent works, such as MOMENT, train a generalist time series foundation model with data from multiple domains, such as weather, traffic, and electricity. This paper aims to conduct a comprehensive benchmarking study to compare the performance of generalist and specialist models, with a focus on PPG signals. Through an extensive suite of total 51 tasks covering cardiac state assessment, laboratory value estimation, and cross-modal inference, we comprehensively evaluate both models across seven dimensions, including win score, average performance, feature quality, tuning gain, performance variance, transferability, and scalability. These metrics jointly capture not only the models' capability but also their adaptability, robustness, and efficiency under different fine-tuning strategies, providing a holistic understanding of their strengths and limitations for diverse downstream scenarios. In a full-tuning scenario, we demonstrate that the specialist model achieves a 27% higher win score. Finally, we provide further analysis on generalization, fairness, attention visualizations, and the importance of training data choice.",
        "tags": [
            "BERT",
            "CLIP",
            "GPT"
        ]
    },
    {
        "id": "111",
        "title": "Identity-Preserving Image-to-Video Generation via Reward-Guided Optimization",
        "author": [
            "Liao Shen",
            "Wentao Jiang",
            "Yiran Zhu",
            "Tiezheng Ge",
            "Zhiguo Cao",
            "Bo Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14255",
        "abstract": "Recent advances in image-to-video (I2V) generation have achieved remarkable progress in synthesizing high-quality, temporally coherent videos from static images. Among all the applications of I2V, human-centric video generation includes a large portion. However, existing I2V models encounter difficulties in maintaining identity consistency between the input human image and the generated video, especially when the person in the video exhibits significant expression changes and movements. This issue becomes critical when the human face occupies merely a small fraction of the image. Since humans are highly sensitive to identity variations, this poses a critical yet under-explored challenge in I2V generation. In this paper, we propose Identity-Preserving Reward-guided Optimization (IPRO), a novel video diffusion framework based on reinforcement learning to enhance identity preservation. Instead of introducing auxiliary modules or altering model architectures, our approach introduces a direct and effective tuning algorithm that optimizes diffusion models using a face identity scorer. To improve performance and accelerate convergence, our method backpropagates the reward signal through the last steps of the sampling chain, enabling richer gradient feedback. We also propose a novel facial scoring mechanism that treats faces in ground-truth videos as facial feature pools, providing multi-angle facial information to enhance generalization. A KL-divergence regularization is further incorporated to stabilize training and prevent overfitting to the reward signal. Extensive experiments on Wan 2.2 I2V model and our in-house I2V model demonstrate the effectiveness of our method. Our project and code are available at \\href{https://ipro-alimama.github.io/}{https://ipro-alimama.github.io/}.",
        "tags": [
            "Diffusion",
            "RL",
            "Video Generation"
        ]
    },
    {
        "id": "112",
        "title": "Identity-GRPO: Optimizing Multi-Human Identity-preserving Video Generation via Reinforcement Learning",
        "author": [
            "Xiangyu Meng",
            "Zixian Zhang",
            "Zhenghao Zhang",
            "Junchao Liao",
            "Long Qin",
            "Weizhi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14256",
        "abstract": "While advanced methods like VACE and Phantom have advanced video generation for specific subjects in diverse scenarios, they struggle with multi-human identity preservation in dynamic interactions, where consistent identities across multiple characters are critical. To address this, we propose Identity-GRPO, a human feedback-driven optimization pipeline for refining multi-human identity-preserving video generation. First, we construct a video reward model trained on a large-scale preference dataset containing human-annotated and synthetic distortion data, with pairwise annotations focused on maintaining human consistency throughout the video. We then employ a GRPO variant tailored for multi-human consistency, which greatly enhances both VACE and Phantom. Through extensive ablation studies, we evaluate the impact of annotation quality and design choices on policy optimization. Experiments show that Identity-GRPO achieves up to 18.9% improvement in human consistency metrics over baseline methods, offering actionable insights for aligning reinforcement learning with personalized video generation.",
        "tags": [
            "GRPO",
            "RL",
            "Video Generation"
        ]
    },
    {
        "id": "113",
        "title": "CAST: Compositional Analysis via Spectral Tracking for Understanding Transformer Layer Functions",
        "author": [
            "Zihao Fu",
            "Ming Liao",
            "Chris Russell",
            "Zhenguang G. Cai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14262",
        "abstract": "Large language models have achieved remarkable success but remain largely black boxes with poorly understood internal mechanisms. To address this limitation, many researchers have proposed various interpretability methods including mechanistic analysis, probing classifiers, and activation visualization, each providing valuable insights from different perspectives. Building upon this rich landscape of complementary approaches, we introduce CAST (Compositional Analysis via Spectral Tracking), a probe-free framework that contributes a novel perspective by analyzing transformer layer functions through direct transformation matrix estimation and comprehensive spectral analysis. CAST offers complementary insights to existing methods by estimating the realized transformation matrices for each layer using Moore-Penrose pseudoinverse and applying spectral analysis with six interpretable metrics characterizing layer behavior. Our analysis reveals distinct behaviors between encoder-only and decoder-only models, with decoder models exhibiting compression-expansion cycles while encoder models maintain consistent high-rank processing. Kernel analysis further demonstrates functional relationship patterns between layers, with CKA similarity matrices clearly partitioning layers into three phases: feature extraction, compression, and specialization.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "114",
        "title": "AlphaQuanter: An End-to-End Tool-Orchestrated Agentic Reinforcement Learning Framework for Stock Trading",
        "author": [
            "Zheye Deng",
            "Jiashu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14264",
        "abstract": "While Large Language Model (LLM) agents show promise in automated trading, they still face critical limitations. Prominent multi-agent frameworks often suffer from inefficiency, produce inconsistent signals, and lack the end-to-end optimization required to learn a coherent strategy from market feedback. To address this, we introduce AlphaQuanter, a single-agent framework that uses reinforcement learning (RL) to learn a dynamic policy over a transparent, tool-augmented decision workflow, which empowers a single agent to autonomously orchestrate tools and proactively acquire information on demand, establishing a transparent and auditable reasoning process. Extensive experiments demonstrate that AlphaQuanter achieves state-of-the-art performance on key financial metrics. Moreover, its interpretable reasoning reveals sophisticated strategies, offering novel and valuable insights for human traders. Our code for data acquisition and agent training is publicly available at: https://github.com/AlphaQuanter/AlphaQuanter",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "115",
        "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
        "author": [
            "Xukai Wang",
            "Xuanbo Liu",
            "Mingrui Chen",
            "Haitian Zhong",
            "Xuanlin Yang",
            "Bohan Zeng",
            "Jinbo Hu",
            "Hao Liang",
            "Junbo Niu",
            "Xuchen Li",
            "Ruitao Wu",
            "Ruichuan An",
            "Yang Shi",
            "Liu Liu",
            "Xu-Yao Zhang",
            "Qiang Liu",
            "Zhouchen Lin",
            "Wentao Zhang",
            "Bin Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14265",
        "abstract": "With the advancement of powerful large-scale reasoning models, effectively evaluating the reasoning capabilities of these models has become increasingly important. However, existing benchmarks designed to assess the reasoning abilities of large models tend to be limited in scope and lack the flexibility to adapt their difficulty according to the evolving reasoning capacities of the models. To address this, we propose MorphoBench, a benchmark that incorporates multidisciplinary questions to evaluate the reasoning capabilities of large models and can adjust and update question difficulty based on the reasoning abilities of advanced models. Specifically, we curate the benchmark by selecting and collecting complex reasoning questions from existing benchmarks and sources such as Olympiad-level competitions. Additionally, MorphoBench adaptively modifies the analytical challenge of questions by leveraging key statements generated during the model's reasoning process. Furthermore, it includes questions generated using simulation software, enabling dynamic adjustment of benchmark difficulty with minimal resource consumption. We have gathered over 1,300 test questions and iteratively adjusted the difficulty of MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5. MorphoBench enhances the comprehensiveness and validity of model reasoning evaluation, providing reliable guidance for improving both the reasoning abilities and scientific robustness of large models. The code has been released in https://github.com/OpenDCAI/MorphoBench.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "116",
        "title": "Nonparametric Data Attribution for Diffusion Models",
        "author": [
            "Yutian Zhao",
            "Chao Du",
            "Xiaosen Zheng",
            "Tianyu Pang",
            "Min Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14269",
        "abstract": "Data attribution for generative models seeks to quantify the influence of individual training examples on model outputs. Existing methods for diffusion models typically require access to model gradients or retraining, limiting their applicability in proprietary or large-scale settings. We propose a nonparametric attribution method that operates entirely on data, measuring influence via patch-level similarity between generated and training images. Our approach is grounded in the analytical form of the optimal score function and naturally extends to multiscale representations, while remaining computationally efficient through convolution-based acceleration. In addition to producing spatially interpretable attributions, our framework uncovers patterns that reflect intrinsic relationships between training data and outputs, independent of any specific model. Experiments demonstrate that our method achieves strong attribution performance, closely matching gradient-based approaches and substantially outperforming existing nonparametric baselines. Code is available at https://github.com/sail-sg/NDA.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "117",
        "title": "GauSSmart: Enhanced 3D Reconstruction through 2D Foundation Models and Geometric Filtering",
        "author": [
            "Alexander Valverde",
            "Brian Xu",
            "Yuyin Zhou",
            "Meng Xu",
            "Hongyun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14270",
        "abstract": "Scene reconstruction has emerged as a central challenge in computer vision, with approaches such as Neural Radiance Fields (NeRF) and Gaussian Splatting achieving remarkable progress. While Gaussian Splatting demonstrates strong performance on large-scale datasets, it often struggles to capture fine details or maintain realism in regions with sparse coverage, largely due to the inherent limitations of sparse 3D training data.\nIn this work, we propose GauSSmart, a hybrid method that effectively bridges 2D foundational models and 3D Gaussian Splatting reconstruction. Our approach integrates established 2D computer vision techniques, including convex filtering and semantic feature supervision from foundational models such as DINO, to enhance Gaussian-based scene reconstruction. By leveraging 2D segmentation priors and high-dimensional feature embeddings, our method guides the densification and refinement of Gaussian splats, improving coverage in underrepresented areas and preserving intricate structural details.\nWe validate our approach across three datasets, where GauSSmart consistently outperforms existing Gaussian Splatting in the majority of evaluated scenes. Our results demonstrate the significant potential of hybrid 2D-3D approaches, highlighting how the thoughtful combination of 2D foundational models with 3D reconstruction pipelines can overcome the limitations inherent in either approach alone.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "NeRF",
            "Segmentation"
        ]
    },
    {
        "id": "118",
        "title": "Less is More: Denoising Knowledge Graphs For Retrieval Augmented Generation",
        "author": [
            "Yilun Zheng",
            "Dan Yang",
            "Jie Li",
            "Lin Shang",
            "Lihui Chen",
            "Jiahao Xu",
            "Sitao Luan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14271",
        "abstract": "Retrieval-Augmented Generation (RAG) systems enable large language models (LLMs) instant access to relevant information for the generative process, demonstrating their superior performance in addressing common LLM challenges such as hallucination, factual inaccuracy, and the knowledge cutoff. Graph-based RAG further extends this paradigm by incorporating knowledge graphs (KGs) to leverage rich, structured connections for more precise and inferential responses. A critical challenge, however, is that most Graph-based RAG systems rely on LLMs for automated KG construction, often yielding noisy KGs with redundant entities and unreliable relationships. This noise degrades retrieval and generation performance while also increasing computational cost. Crucially, current research does not comprehensively address the denoising problem for LLM-generated KGs. In this paper, we introduce DEnoised knowledge Graphs for Retrieval Augmented Generation (DEG-RAG), a framework that addresses these challenges through: (1) entity resolution, which eliminates redundant entities, and (2) triple reflection, which removes erroneous relations. Together, these techniques yield more compact, higher-quality KGs that significantly outperform their unprocessed counterparts. Beyond the methods, we conduct a systematic evaluation of entity resolution for LLM-generated KGs, examining different blocking strategies, embedding choices, similarity metrics, and entity merging techniques. To the best of our knowledge, this is the first comprehensive exploration of entity resolution in LLM-generated KGs. Our experiments demonstrate that this straightforward approach not only drastically reduces graph size but also consistently improves question answering performance across diverse popular Graph-based RAG variants.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "119",
        "title": "Qwen3Guard Technical Report",
        "author": [
            "Haiquan Zhao",
            "Chenhan Yuan",
            "Fei Huang",
            "Xiaomeng Hu",
            "Yichang Zhang",
            "An Yang",
            "Bowen Yu",
            "Dayiheng Liu",
            "Jingren Zhou",
            "Junyang Lin",
            "Baosong Yang",
            "Chen Cheng",
            "Jialong Tang",
            "Jiandong Jiang",
            "Jianwei Zhang",
            "Jijie Xu",
            "Ming Yan",
            "Minmin Sun",
            "Pei Zhang",
            "Pengjun Xie",
            "Qiaoyu Tang",
            "Qin Zhu",
            "Rong Zhang",
            "Shibin Wu",
            "Shuo Zhang",
            "Tao He",
            "Tianyi Tang",
            "Tingyu Xia",
            "Wei Liao",
            "Weizhou Shen",
            "Wenbiao Yin",
            "Wenmeng Zhou",
            "Wenyuan Yu",
            "Xiaobin Wang",
            "Xiaodong Deng",
            "Xiaodong Xu",
            "Xinyu Zhang",
            "Yang Liu",
            "Yeqiu Li",
            "Yi Zhang",
            "Yong Jiang",
            "Yu Wan",
            "Yuxin Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14276",
        "abstract": "As large language models (LLMs) become more capable and widely used, ensuring the safety of their outputs is increasingly critical. Existing guardrail models, though useful in static evaluation settings, face two major limitations in real-world applications: (1) they typically output only binary \"safe/unsafe\" labels, which can be interpreted inconsistently across diverse safety policies, rendering them incapable of accommodating varying safety tolerances across domains; and (2) they require complete model outputs before performing safety checks, making them fundamentally incompatible with streaming LLM inference, thereby preventing timely intervention during generation and increasing exposure to harmful partial outputs. To address these challenges, we present Qwen3Guard, a series of multilingual safety guardrail models with two specialized variants: Generative Qwen3Guard, which casts safety classification as an instruction-following task to enable fine-grained tri-class judgments (safe, controversial, unsafe); and Stream Qwen3Guard, which introduces a token-level classification head for real-time safety monitoring during incremental text generation. Both variants are available in three sizes (0.6B, 4B, and 8B parameters) and support up to 119 languages and dialects, providing comprehensive, scalable, and low-latency safety moderation for global LLM deployments. Evaluated across English, Chinese, and multilingual benchmarks, Qwen3Guard achieves state-of-the-art performance in both prompt and response safety classification. All models are released under the Apache 2.0 license for public use.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "120",
        "title": "GenLARP: Enabling Immersive Live Action Role-Play through LLM-Generated Worlds and Characters",
        "author": [
            "Yichen Yu",
            "Yifan Jiang",
            "Mandy Lui",
            "Qiao Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14277",
        "abstract": "We introduce GenLARP, a virtual reality (VR) system that transforms personalized stories into immersive live action role-playing (LARP) experiences. GenLARP enables users to act as both creators and players, allowing them to design characters based on their descriptions and live in the story world. Generative AI and agents powered by Large Language Models (LLMs) enrich these experiences.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "121",
        "title": "PRISM: Agentic Retrieval with LLMs for Multi-Hop Question Answering",
        "author": [
            "Md Mahadi Hasan Nahid",
            "Davood Rafiei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14278",
        "abstract": "Retrieval plays a central role in multi-hop question answering (QA), where answering complex questions requires gathering multiple pieces of evidence. We introduce an Agentic Retrieval System that leverages large language models (LLMs) in a structured loop to retrieve relevant evidence with high precision and recall. Our framework consists of three specialized agents: a Question Analyzer that decomposes a multi-hop question into sub-questions, a Selector that identifies the most relevant context for each sub-question (focusing on precision), and an Adder that brings in any missing evidence (focusing on recall). The iterative interaction between Selector and Adder yields a compact yet comprehensive set of supporting passages. In particular, it achieves higher retrieval accuracy while filtering out distracting content, enabling downstream QA models to surpass full-context answer accuracy while relying on significantly less irrelevant information. Experiments on four multi-hop QA benchmarks -- HotpotQA, 2WikiMultiHopQA, MuSiQue, and MultiHopRAG -- demonstrates that our approach consistently outperforms strong baselines.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "122",
        "title": "Learning Human-Humanoid Coordination for Collaborative Object Carrying",
        "author": [
            "Yushi Du",
            "Yixuan Li",
            "Baoxiong Jia",
            "Yutang Lin",
            "Pei Zhou",
            "Wei Liang",
            "Yanchao Yang",
            "Siyuan Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14293",
        "abstract": "Human-humanoid collaboration shows significant promise for applications in healthcare, domestic assistance, and manufacturing. While compliant robot-human collaboration has been extensively developed for robotic arms, enabling compliant human-humanoid collaboration remains largely unexplored due to humanoids' complex whole-body dynamics. In this paper, we propose a proprioception-only reinforcement learning approach, COLA, that combines leader and follower behaviors within a single policy. The model is trained in a closed-loop environment with dynamic object interactions to predict object motion patterns and human intentions implicitly, enabling compliant collaboration to maintain load balance through coordinated trajectory planning. We evaluate our approach through comprehensive simulator and real-world experiments on collaborative carrying tasks, demonstrating the effectiveness, generalization, and robustness of our model across various terrains and objects. Simulation experiments demonstrate that our model reduces human effort by 24.7%. compared to baseline approaches while maintaining object stability. Real-world experiments validate robust collaborative carrying across different object types (boxes, desks, stretchers, etc.) and movement patterns (straight-line, turning, slope climbing). Human user studies with 23 participants confirm an average improvement of 27.4% compared to baseline models. Our method enables compliant human-humanoid collaborative carrying without requiring external sensors or complex interaction models, offering a practical solution for real-world deployment.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "123",
        "title": "Expertise need not monopolize: Action-Specialized Mixture of Experts for Vision-Language-Action Learning",
        "author": [
            "Weijie Shen",
            "Yitian Liu",
            "Yuhao Wu",
            "Zhixuan Liang",
            "Sijia Gu",
            "Dehui Wang",
            "Tian Nian",
            "Lei Xu",
            "Yusen Qin",
            "Jiangmiao Pang",
            "Xinping Guan",
            "Xiaokang Yang",
            "Yao Mu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14300",
        "abstract": "Vision-Language-Action (VLA) models are experiencing rapid development and demonstrating promising capabilities in robotic manipulation tasks. However, scaling up VLA models presents several critical challenges: (1) Training new VLA models from scratch demands substantial computational resources and extensive datasets. Given the current scarcity of robot data, it becomes particularly valuable to fully leverage well-pretrained VLA model weights during the scaling process. (2) Real-time control requires carefully balancing model capacity with computational efficiency. To address these challenges, We propose AdaMoE, a Mixture-of-Experts (MoE) architecture that inherits pretrained weights from dense VLA models, and scales up the action expert by substituting the feedforward layers into sparsely activated MoE layers. AdaMoE employs a decoupling technique that decouples expert selection from expert weighting through an independent scale adapter working alongside the traditional router. This enables experts to be selected based on task relevance while contributing with independently controlled weights, allowing collaborative expert utilization rather than winner-takes-all dynamics. Our approach demonstrates that expertise need not monopolize. Instead, through collaborative expert utilization, we can achieve superior performance while maintaining computational efficiency. AdaMoE consistently outperforms the baseline model across key benchmarks, delivering performance gains of 1.8% on LIBERO and 9.3% on RoboTwin. Most importantly, a substantial 21.5% improvement in real-world experiments validates its practical effectiveness for robotic manipulation tasks.",
        "tags": [
            "MoE",
            "Robotics"
        ]
    },
    {
        "id": "124",
        "title": "A Guardrail for Safety Preservation: When Safety-Sensitive Subspace Meets Harmful-Resistant Null-Space",
        "author": [
            "Bingjie Zhang",
            "Yibo Yang",
            "Renzhe",
            "Dandan Guo",
            "Jindong Gu",
            "Philip Torr",
            "Bernard Ghanem"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14301",
        "abstract": "Large language models (LLMs) have achieved remarkable success in diverse tasks, yet their safety alignment remains fragile during adaptation. Even when fine-tuning on benign data or with low-rank adaptation, pre-trained safety behaviors are easily degraded, leading to harmful responses in the fine-tuned models. To address this challenge, we propose GuardSpace, a guardrail framework for preserving safety alignment throughout fine-tuning, composed of two key components: a safety-sensitive subspace and a harmful-resistant null space. First, we explicitly decompose pre-trained weights into safety-relevant and safety-irrelevant components using covariance-preconditioned singular value decomposition, and initialize low-rank adapters from the safety-irrelevant ones, while freezing safety-relevant components to preserve their associated safety mechanism. Second, we construct a null space projector that restricts adapter updates from altering safe outputs on harmful prompts, thereby maintaining the original refusal behavior. Experiments with various pre-trained models on multiple downstream tasks demonstrate that GuardSpace achieves superior performance over existing methods. Notably, for Llama-2-7B-Chat fine-tuned on GSM8K, GuardSpace outperforms the state-of-the-art method AsFT, reducing the average harmful score from 14.4% to 3.6%, while improving the accuracy from from 26.0% to 28.0%.",
        "tags": [
            "LLM",
            "LLaMA",
            "LoRA"
        ]
    },
    {
        "id": "125",
        "title": "Constraint-Driven Small Language Models Based on Agent and OpenAlex Knowledge Graph: Mining Conceptual Pathways and Discovering Innovation Points in Academic Papers",
        "author": [
            "Ziye Xia",
            "Sergei S. Ospichev"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14303",
        "abstract": "In recent years, the rapid increase in academic publications across various fields has posed severe challenges for academic paper analysis: scientists struggle to timely and comprehensively track the latest research findings and methodologies. Key concept extraction has proven to be an effective analytical paradigm, and its automation has been achieved with the widespread application of language models in industrial and scientific domains. However, existing paper databases are mostly limited to similarity matching and basic classification of key concepts, failing to deeply explore the relational networks between concepts. This paper is based on the OpenAlex opensource knowledge graph. By analyzing nearly 8,000 open-source paper data from Novosibirsk State University, we discovered a strong correlation between the distribution patterns of paper key concept paths and both innovation points and rare paths. We propose a prompt engineering-based key concept path analysis method. This method leverages small language models to achieve precise key concept extraction and innovation point identification, and constructs an agent based on a knowledge graph constraint mechanism to enhance analysis accuracy. Through fine-tuning of the Qwen and DeepSeek models, we achieved significant improvements in accuracy, with the models publicly available on the Hugging Face platform.",
        "tags": [
            "DeepSeek",
            "Qwen"
        ]
    },
    {
        "id": "126",
        "title": "MathMist: A Parallel Multilingual Benchmark Dataset for Mathematical Problem Solving and Reasoning",
        "author": [
            "Mahbub E Sobhani",
            "Md. Faiyaz Abdullah Sayeedi",
            "Tasnim Mohiuddin",
            "Md Mofijul Islam",
            "Swakkhar Shatabda"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14305",
        "abstract": "Mathematical reasoning remains one of the most challenging domains for large language models (LLMs), requiring not only linguistic understanding but also structured logical deduction and numerical precision. While recent LLMs demonstrate strong general-purpose reasoning abilities, their mathematical competence across diverse languages remains underexplored. Existing benchmarks primarily focus on English or a narrow subset of high-resource languages, leaving significant gaps in assessing multilingual and cross-lingual mathematical reasoning. To address this, we introduce MathMist, a parallel multilingual benchmark for mathematical problem solving and reasoning. MathMist encompasses over 21K aligned question-answer pairs across seven languages, representing a balanced coverage of high-, medium-, and low-resource linguistic settings. The dataset captures linguistic variety, multiple types of problem settings, and solution synthesizing capabilities. We systematically evaluate a diverse suite of models, including open-source small and medium LLMs, proprietary systems, and multilingual-reasoning-focused models, under zero-shot, chain-of-thought (CoT), and code-switched reasoning paradigms. Our results reveal persistent deficiencies in LLMs' ability to perform consistent and interpretable mathematical reasoning across languages, with pronounced degradation in low-resource settings. All the codes and data are available at GitHub: https://github.com/mahbubhimel/MathMist",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "127",
        "title": "MERLIN: A Testbed for Multilingual Multimodal Entity Recognition and Linking",
        "author": [
            "Sathyanarayanan Ramamoorthy",
            "Vishwa Shah",
            "Simran Khanuja",
            "Zaid Sheikh",
            "Shan Jie",
            "Ann Chia",
            "Shearman Chua",
            "Graham Neubig"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14307",
        "abstract": "This paper introduces MERLIN, a novel testbed system for the task of Multilingual Multimodal Entity Linking. The created dataset includes BBC news article titles, paired with corresponding images, in five languages: Hindi, Japanese, Indonesian, Vietnamese, and Tamil, featuring over 7,000 named entity mentions linked to 2,500 unique Wikidata entities. We also include several benchmarks using multilingual and multimodal entity linking methods exploring different language models like LLaMa-2 and Aya-23. Our findings indicate that incorporating visual data improves the accuracy of entity linking, especially for entities where the textual context is ambiguous or insufficient, and particularly for models that do not have strong multilingual abilities. For the work, the dataset, methods are available here at https://github.com/rsathya4802/merlin",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "128",
        "title": "Terrarium: Revisiting the Blackboard for Multi-Agent Safety, Privacy, and Security Studies",
        "author": [
            "Mason Nakamura",
            "Abhinav Kumar",
            "Saaduddin Mahmud",
            "Sahar Abdelnabi",
            "Shlomo Zilberstein",
            "Eugene Bagdasarian"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14312",
        "abstract": "A multi-agent system (MAS) powered by large language models (LLMs) can automate tedious user tasks such as meeting scheduling that requires inter-agent collaboration. LLMs enable nuanced protocols that account for unstructured private data, user constraints, and preferences. However, this design introduces new risks, including misalignment and attacks by malicious parties that compromise agents or steal user data. In this paper, we propose the Terrarium framework for fine-grained study on safety, privacy, and security in LLM-based MAS. We repurpose the blackboard design, an early approach in multi-agent systems, to create a modular, configurable testbed for multi-agent collaboration. We identify key attack vectors such as misalignment, malicious agents, compromised communication, and data poisoning. We implement three collaborative MAS scenarios with four representative attacks to demonstrate the framework's flexibility. By providing tools to rapidly prototype, evaluate, and iterate on defenses and designs, Terrarium aims to accelerate progress toward trustworthy multi-agent systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "129",
        "title": "A Multi-domain Image Translative Diffusion StyleGAN for Iris Presentation Attack Detection",
        "author": [
            "Shivangi Yadav",
            "Arun Ross"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14314",
        "abstract": "An iris biometric system can be compromised by presentation attacks (PAs) where artifacts such as artificial eyes, printed eye images, or cosmetic contact lenses are presented to the system. To counteract this, several presentation attack detection (PAD) methods have been developed. However, there is a scarcity of datasets for training and evaluating iris PAD techniques due to the implicit difficulties in constructing and imaging PAs. To address this, we introduce the Multi-domain Image Translative Diffusion StyleGAN (MID-StyleGAN), a new framework for generating synthetic ocular images that captures the PA and bonafide characteristics in multiple domains such as bonafide, printed eyes and cosmetic contact lens. MID-StyleGAN combines the strengths of diffusion models and generative adversarial networks (GANs) to produce realistic and diverse synthetic data. Our approach utilizes a multi-domain architecture that enables the translation between bonafide ocular images and different PA domains. The model employs an adaptive loss function tailored for ocular data to maintain domain consistency. Extensive experiments demonstrate that MID-StyleGAN outperforms existing methods in generating high-quality synthetic ocular images. The generated data was used to significantly enhance the performance of PAD systems, providing a scalable solution to the data scarcity problem in iris and ocular biometrics. For example, on the LivDet2020 dataset, the true detect rate at 1% false detect rate improved from 93.41% to 98.72%, showcasing the impact of the proposed method.",
        "tags": [
            "Detection",
            "Diffusion",
            "StyleGAN"
        ]
    },
    {
        "id": "130",
        "title": "Active Measuring in Reinforcement Learning With Delayed Negative Effects",
        "author": [
            "Daiqi Gao",
            "Ziping Xu",
            "Aseel Rawashdeh",
            "Predrag Klasnja",
            "Susan A. Murphy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14315",
        "abstract": "Measuring states in reinforcement learning (RL) can be costly in real-world settings and may negatively influence future outcomes. We introduce the Actively Observable Markov Decision Process (AOMDP), where an agent not only selects control actions but also decides whether to measure the latent state. The measurement action reveals the true latent state but may have a negative delayed effect on the environment. We show that this reduced uncertainty may provably improve sample efficiency and increase the value of the optimal policy despite these costs. We formulate an AOMDP as a periodic partially observable MDP and propose an online RL algorithm based on belief states. To approximate the belief states, we further propose a sequential Monte Carlo method to jointly approximate the posterior of unknown static environment parameters and unobserved latent states. We evaluate the proposed algorithm in a digital health application, where the agent decides when to deliver digital interventions and when to assess users' health status through surveys.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "131",
        "title": "Evaluating & Reducing Deceptive Dialogue From Language Models with Multi-turn RL",
        "author": [
            "Marwa Abdulhai",
            "Ryan Cheng",
            "Aryansh Shrivastava",
            "Natasha Jaques",
            "Yarin Gal",
            "Sergey Levine"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14318",
        "abstract": "Large Language Models (LLMs) interact with millions of people worldwide in applications such as customer support, education and healthcare. However, their ability to produce deceptive outputs, whether intentionally or inadvertently, poses significant safety concerns. The unpredictable nature of LLM behavior, combined with insufficient safeguards against hallucination, misinformation, and user manipulation, makes their misuse a serious, real-world risk. In this paper, we investigate the extent to which LLMs engage in deception within dialogue, and propose the belief misalignment metric to quantify deception. We evaluate deception across four distinct dialogue scenarios, using five established deception detection metrics and our proposed metric. Our findings reveal this novel deception measure correlates more closely with human judgments than any existing metrics we test. Additionally, our benchmarking of eight state-of-the-art models indicates that LLMs naturally exhibit deceptive behavior in approximately 26% of dialogue turns, even when prompted with seemingly benign objectives. When prompted to deceive, LLMs are capable of increasing deceptiveness by as much as 31% relative to baselines. Unexpectedly, models trained with RLHF, the predominant approach for ensuring the safety of widely-deployed LLMs, still exhibit deception at a rate of 43% on average. Given that deception in dialogue is a behavior that develops over an interaction history, its effective evaluation and mitigation necessitates moving beyond single-utterance analyses. We introduce a multi-turn reinforcement learning methodology to fine-tune LLMs to reduce deceptive behaviors, leading to a 77.6% reduction compared to other instruction-tuned models.",
        "tags": [
            "Detection",
            "LLM",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "132",
        "title": "Large Reasoning Embedding Models: Towards Next-Generation Dense Retrieval Paradigm",
        "author": [
            "Jianting Tang",
            "Dongshuai Li",
            "Tao Wen",
            "Fuyu Lv",
            "Dan Ou",
            "Linli Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14321",
        "abstract": "In modern e-commerce search systems, dense retrieval has become an indispensable component. By computing similarities between query and item (product) embeddings, it efficiently selects candidate products from large-scale repositories. With the breakthroughs in large language models (LLMs), mainstream embedding models have gradually shifted from BERT to LLMs for more accurate text modeling. However, these models still adopt direct-embedding methods, and the semantic accuracy of embeddings remains inadequate. Therefore, contrastive learning is heavily employed to achieve tight semantic alignment between positive pairs. Consequently, such models tend to capture statistical co-occurrence patterns in the training data, biasing them toward shallow lexical and semantic matches. For difficult queries exhibiting notable lexical disparity from target items, the performance degrades significantly. In this work, we propose the Large Reasoning Embedding Model (LREM), which novelly integrates reasoning processes into representation learning. For difficult queries, LREM first conducts reasoning to achieve a deep understanding of the original query, and then produces a reasoning-augmented query embedding for retrieval. This reasoning process effectively bridges the semantic gap between original queries and target items, significantly improving retrieval accuracy. Specifically, we adopt a two-stage training process: the first stage optimizes the LLM on carefully curated Query-CoT-Item triplets with SFT and InfoNCE losses to establish preliminary reasoning and embedding capabilities, and the second stage further refines the reasoning trajectories via reinforcement learning (RL). Extensive offline and online experiments validate the effectiveness of LREM, leading to its deployment on China's largest e-commerce platform since August 2025.",
        "tags": [
            "BERT",
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "133",
        "title": "Ensembling Multiple Hallucination Detectors Trained on VLLM Internal Representations",
        "author": [
            "Yuto Nakamizo",
            "Ryuhei Miyazato",
            "Hikaru Tanabe",
            "Ryuta Yamakura",
            "Kiori Hatanaka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14330",
        "abstract": "This paper presents the 5th place solution by our team, y3h2, for the Meta CRAG-MM Challenge at KDD Cup 2025. The CRAG-MM benchmark is a visual question answering (VQA) dataset focused on factual questions about images, including egocentric images. The competition was contested based on VQA accuracy, as judged by an LLM-based automatic evaluator. Since incorrect answers result in negative scores, our strategy focused on reducing hallucinations from the internal representations of the VLM. Specifically, we trained logistic regression-based hallucination detection models using both the hidden_state and the outputs of specific attention heads. We then employed an ensemble of these models. As a result, while our method sacrificed some correct answers, it significantly reduced hallucinations and allowed us to place among the top entries on the final leaderboard. For implementation details and code, please refer to https://gitlab.aicrowd.com/htanabe/meta-comprehensive-rag-benchmark-starter-kit.",
        "tags": [
            "Detection",
            "LLM",
            "RAG",
            "VLM"
        ]
    },
    {
        "id": "134",
        "title": "LLM-ERM: Sample-Efficient Program Learning via LLM-Guided Search",
        "author": [
            "Shivam Singhal",
            "Eran Malach",
            "Tomaso Poggio",
            "Tomer Galanti"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14331",
        "abstract": "We seek algorithms for program learning that are both sample-efficient and computationally feasible. Classical results show that targets admitting short program descriptions (e.g., with short ``python code'') can be learned with a ``small'' number of examples (scaling with the size of the code) via length-first program enumeration, but the search is exponential in description length. Consequently, Gradient-based training avoids this cost yet can require exponentially many samples on certain short-program families.\nTo address this gap, we introduce LLM-ERM, a propose-and-verify framework that replaces exhaustive enumeration with an LLM-guided search over candidate programs while retaining ERM-style selection on held-out data. Specifically, we draw $k$ candidates with a pretrained reasoning-augmented LLM, compile and check each on the data, and return the best verified hypothesis, with no feedback, adaptivity, or gradients. Theoretically, we show that coordinate-wise online mini-batch SGD requires many samples to learn certain short programs. {\\em Empirically, LLM-ERM solves tasks such as parity variants, pattern matching, and primality testing with as few as 200 samples, while SGD-trained transformers overfit even with 100,000 samples}. These results indicate that language-guided program synthesis recovers much of the statistical efficiency of finite-class ERM while remaining computationally tractable, offering a practical route to learning succinct hypotheses beyond the reach of gradient-based training.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "135",
        "title": "DARTS-GT: Differentiable Architecture Search for Graph Transformers with Quantifiable Instance-Specific Interpretability Analysis",
        "author": [
            "Shruti Sarika Chakraborty",
            "Peter Minary"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14336",
        "abstract": "Graph Transformers (GTs) have emerged as powerful architectures for graph-structured data, yet remain constrained by rigid designs and lack quantifiable interpretability. Current state-of-the-art GTs commit to fixed GNN types across all layers, missing potential benefits of depth-specific component selection, while their complex architectures become opaque where performance gains cannot be distinguished between meaningful patterns and spurious correlations. We redesign GT attention through asymmetry, decoupling structural encoding from feature representation: queries derive from node features while keys and values come from GNN transformations. Within this framework, we use Differentiable ARchiTecture Search (DARTS) to select optimal GNN operators at each layer, enabling depth-wise heterogeneity inside transformer attention itself (DARTS-GT). To understand discovered architectures, we develop the first quantitative interpretability framework for GTs through causal ablation. Our metrics (Head-deviation, Specialization, and Focus), identify which heads and nodes drive predictions while enabling model comparison. Experiments across eight benchmarks show DARTS-GT achieves state-of-the-art on four datasets while remaining competitive on others, with discovered architectures revealing dataset-specific patterns. Our interpretability analysis reveals that visual attention salience and causal importance do not always correlate, indicating widely used visualization approaches may miss components that actually matter. Crucially, heterogeneous architectures found by DARTS-GT consistently produced more interpretable models than baselines, establishing that Graph Transformers need not choose between performance and interpretability.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "136",
        "title": "Stop-RAG: Value-Based Retrieval Control for Iterative RAG",
        "author": [
            "Jaewan Park",
            "Solbee Cho",
            "Jay-Yoon Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14337",
        "abstract": "Iterative retrieval-augmented generation (RAG) enables large language models to answer complex multi-hop questions, but each additional loop increases latency, costs, and the risk of introducing distracting evidence, motivating the need for an efficient stopping strategy. Existing methods either use a predetermined number of iterations or rely on confidence proxies that poorly reflect whether more retrieval will actually help. We cast iterative RAG as a finite-horizon Markov decision process and introduce Stop-RAG, a value-based controller that adaptively decides when to stop retrieving. Trained with full-width forward-view Q($\\lambda$) targets from complete trajectories, Stop-RAG learns effective stopping policies while remaining compatible with black-box APIs and existing pipelines. On multi-hop question-answering benchmarks, Stop-RAG consistently outperforms both fixed-iteration baselines and prompting-based stopping with LLMs. These results highlight adaptive stopping as a key missing component in current agentic systems, and demonstrate that value-based control can improve the accuracy of RAG systems.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "137",
        "title": "Risk-Aware Reinforcement Learning with Bandit-Based Adaptation for Quadrupedal Locomotion",
        "author": [
            "Yuanhong Zeng",
            "Anushri Dixit"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14338",
        "abstract": "In this work, we study risk-aware reinforcement learning for quadrupedal locomotion. Our approach trains a family of risk-conditioned policies using a Conditional Value-at-Risk (CVaR) constrained policy optimization technique that provides improved stability and sample efficiency. At deployment, we adaptively select the best performing policy from the family of policies using a multi-armed bandit framework that uses only observed episodic returns, without any privileged environment information, and adapts to unknown conditions on the fly. Hence, we train quadrupedal locomotion policies at various levels of robustness using CVaR and adaptively select the desired level of robustness online to ensure performance in unknown environments. We evaluate our method in simulation across eight unseen settings (by changing dynamics, contacts, sensing noise, and terrain) and on a Unitree Go2 robot in previously unseen terrains. Our risk-aware policy attains nearly twice the mean and tail performance in unseen environments compared to other baselines and our bandit-based adaptation selects the best-performing risk-aware policy in unknown terrain within two minutes of operation.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "138",
        "title": "A Systematic Study of Time Limit Exceeded Errors in Online Programming Assignments",
        "author": [
            "Jialu Zhang",
            "Jialiang Gu",
            "Wangmeiyu Zhang",
            "JosÃ© Pablo Cambronero",
            "John Kolesar",
            "Ruzica Piskac",
            "Daming Li",
            "Hanyuan Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14339",
        "abstract": "Online programming platforms such as Codeforces and LeetCode attract millions of users seeking to learn to program or refine their skills for industry interviews. A major challenge for these users is the Time Limit Exceeded (TLE) error, triggered when a program exceeds the execution time bound. Although designed as a performance safeguard, TLE errors are difficult to resolve: error messages provide no diagnostic insight, platform support is minimal, and existing debugging tools offer little help. As a result, many users abandon their submissions after repeated TLE failures.\nThis paper presents the first large-scale empirical study of TLE errors in online programming. We manually analyzed 1000 Codeforces submissions with TLE errors, classified their root causes, and traced how users attempted to fix them. Our analysis shows that TLE errors often arise not only from inefficient algorithms but also from infinite loops, improper data structure use, and inefficient I/O, challenging the conventional view that TLEs are purely performance issues.\nGuided by these findings, we introduce Nettle, the first automated repair tool specifically designed for TLE errors, and Nettle-Eval, the first framework for evaluating TLE repairs. Integrating LLMs with targeted automated feedback generated by the compiler and test cases, Nettle produces small, correct code edits that eliminate TLEs while preserving functionality. Evaluated on the same 1000 real-world cases, Nettle achieves a 98.5% fix rate, far exceeding the strongest LLM baseline, and all of its repairs pass both Nettle-Eval and the platform's official checker, confirming the reliability of our framework.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "139",
        "title": "PathFix: Automated Program Repair with Expected Path",
        "author": [
            "Xu He",
            "Shu Wang",
            "Kun Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14341",
        "abstract": "Automated program repair (APR) techniques are effective in fixing inevitable defects in software, enhancing development efficiency and software robustness. However, due to the difficulty of generating precise specifications, existing APR methods face two main challenges: generating too many plausible patch candidates and overfitting them to partial test cases. To tackle these challenges, we introduce a new APR method named PathFix, which leverages path-sensitive constraints extracted from correct execution paths to generate patches for repairing buggy code. It is based on one observation: if a buggy program is repairable, at least one expected path is supposed to replace the fault path in the patched program. PathFix operates in four main steps. First, it traces fault paths reaching the fault output in the buggy program. Second, it derives expected paths by analyzing the desired correct output on the control flow graph, where an expected path defines how a feasible patch leads to the correct execution. Third, PathFix generates and evaluates patches by solving state constraints along the expected path. Fourth, we validate the correctness of the generated patch. To further enhance repair performance and mitigate scalability issues introduced by path-sensitive analysis, we integrate a large language model (LLM) into our framework. Experimental results show that PathFix outperforms existing solutions, particularly in handling complex program structures such as loops and recursion.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "140",
        "title": "Automated Extraction of Protocol State Machines from 3GPP Specifications with Domain-Informed Prompts and LLM Ensembles",
        "author": [
            "Miao Zhang",
            "Runhan Feng",
            "Hongbo Tang",
            "Yu Zhao",
            "Jie Yang",
            "Hang Qiu",
            "Qi Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14348",
        "abstract": "Mobile telecommunication networks are foundational to global infrastructure and increasingly support critical sectors such as manufacturing, transportation, and healthcare. The security and reliability of these networks are essential, yet depend heavily on accurate modeling of underlying protocols through state machines. While most prior work constructs such models manually from 3GPP specifications, this process is labor-intensive, error-prone, and difficult to maintain due to the complexity and frequent updates of the specifications. Recent efforts using natural language processing have shown promise, but remain limited in handling the scale and intricacy of cellular protocols. In this work, we propose SpecGPT, a novel framework that leverages large language models (LLMs) to automatically extract protocol state machines from 3GPP documents. SpecGPT segments technical specifications into meaningful paragraphs, applies domain-informed prompting with chain-of-thought reasoning, and employs ensemble methods to enhance output reliability. We evaluate SpecGPT on three representative 5G protocols (NAS, NGAP, and PFCP) using manually annotated ground truth, and show that it outperforms existing approaches, demonstrating the effectiveness of LLMs for protocol modeling at scale.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "141",
        "title": "Vision-Centric Activation and Coordination for Multimodal Large Language Models",
        "author": [
            "Yunnan Wang",
            "Fan Lu",
            "Kecheng Zheng",
            "Ziyuan Huang",
            "Ziqiang Li",
            "Wenjun Zeng",
            "Xin Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14349",
        "abstract": "Multimodal large language models (MLLMs) integrate image features from visual encoders with LLMs, demonstrating advanced comprehension capabilities. However, mainstream MLLMs are solely supervised by the next-token prediction of textual tokens, neglecting critical vision-centric information essential for analytical abilities. To track this dilemma, we introduce VaCo, which optimizes MLLM representations through Vision-Centric activation and Coordination from multiple vision foundation models (VFMs). VaCo introduces visual discriminative alignment to integrate task-aware perceptual features extracted from VFMs, thereby unifying the optimization of both textual and visual outputs in MLLMs. Specifically, we incorporate the learnable Modular Task Queries (MTQs) and Visual Alignment Layers (VALs) into MLLMs, activating specific visual signals under the supervision of diverse VFMs. To coordinate representation conflicts across VFMs, the crafted Token Gateway Mask (TGM) restricts the information flow among multiple groups of MTQs. Extensive experiments demonstrate that VaCo significantly improves the performance of different MLLMs on various benchmarks, showcasing its superior capabilities in visual comprehension.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "142",
        "title": "Beyond One World: Benchmarking Super Heros in Role-Playing Across Multiversal Contexts",
        "author": [
            "Perapard Ngokpol",
            "Kun Kerdthaisong",
            "Pasin Buakhaw",
            "Pitikorn Khlaisamniang",
            "Supasate Vorathammathorn",
            "Piyalitt Ittichaiwong",
            "Nutchanon Yongsatianchot"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14351",
        "abstract": "Large language models (LLMs) are increasingly used as role-playing agents, yet their capacity to faithfully and consistently portray version-specific characters -- for example, superheroes across comic and cinematic universes -- remains underexplored. Superhero canons such as Marvel and DC provide a rich testbed: decades of storytelling yield multiple incarnations of the same character with distinct histories, values, and moral codes. To study this problem, we introduce Beyond One World, a benchmark for character-grounded roleplay spanning 30 iconic heroes and 90 canon-specific versions. The benchmark comprises two tasks: (i) Canon Events, which probes factual recall of pivotal life stages, and (ii) Moral Dilemmas, which confronts models with ethically charged scenarios. We score responses for canonical accuracy and reasoning fidelity under a framework that separates internal deliberation (\"thinking\") from outward decisions (\"acting\"). We further propose Think-Act Matching, a metric that quantifies alignment between reasons and actions and serves as a proxy for model trustworthiness. Experiments across reasoning- and non-reasoning-oriented models yield three findings: (1) chain-of-thought prompting improves narrative coherence in weaker models but can reduce canonical accuracy in stronger ones; (2) cross-version generalization within a character remains a major obstacle; and (3) models often excel at either thinking or acting, but rarely both. Beyond One World exposes critical gaps in multiversal consistency and reasoning alignment, offering a challenging evaluation for role-playing LLMs.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "143",
        "title": "On the Ability of LLMs to Handle Character-Level Perturbations: How Well and How?",
        "author": [
            "Anyun Zhuo",
            "Xuefei Ning",
            "Ningyuan Li",
            "Yu Wang",
            "Pinyan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14365",
        "abstract": "This work investigates the resilience of contemporary LLMs against frequent and structured character-level perturbations, specifically through the insertion of noisy characters after each input character. We introduce \\nameshort{}, a practical method that inserts invisible Unicode control characters into text to discourage LLM misuse in scenarios such as online exam systems. Surprisingly, despite strong obfuscation that fragments tokenization and reduces the signal-to-noise ratio significantly, many LLMs still maintain notable performance. Through comprehensive evaluation across model-, problem-, and noise-related configurations, we examine the extent and mechanisms of this robustness, exploring both the handling of character-level tokenization and \\textit{implicit} versus \\textit{explicit} denoising mechanism hypotheses of character-level noises. We hope our findings on the low-level robustness of LLMs will shed light on the risks of their misuse and on the reliability of deploying LLMs across diverse applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "144",
        "title": "From Binary to Bilingual: How the National Weather Service is Using Artificial Intelligence to Develop a Comprehensive Translation Program",
        "author": [
            "Joseph E. Trujillo-Falcon",
            "Monica L. Bozeman",
            "Liam E. Llewellyn",
            "Samuel T. Halvorson",
            "Meryl Mizell",
            "Stuti Deshpande",
            "Bob Manning",
            "Todd Fagin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14369",
        "abstract": "To advance a Weather-Ready Nation, the National Weather Service (NWS) is developing a systematic translation program to better serve the 68.8 million people in the U.S. who do not speak English at home. This article outlines the foundation of an automated translation tool for NWS products, powered by artificial intelligence. The NWS has partnered with LILT, whose patented training process enables large language models (LLMs) to adapt neural machine translation (NMT) tools for weather terminology and messaging. Designed for scalability across Weather Forecast Offices (WFOs) and National Centers, the system is currently being developed in Spanish, Simplified Chinese, Vietnamese, and other widely spoken non-English languages. Rooted in best practices for multilingual risk communication, the system provides accurate, timely, and culturally relevant translations, significantly reducing manual translation time and easing operational workloads across the NWS. To guide the distribution of these products, GIS mapping was used to identify language needs across different NWS regions, helping prioritize resources for the communities that need them most. We also integrated ethical AI practices throughout the program's design, ensuring that transparency, fairness, and human oversight guide how automated translations are created, evaluated, and shared with the public. This work has culminated into a website featuring experimental multilingual NWS products, including translated warnings, 7-day forecasts, and educational campaigns, bringing the country one step closer to a national warning system that reaches all Americans.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "145",
        "title": "Spatial Preference Rewarding for MLLMs Spatial Understanding",
        "author": [
            "Han Qiu",
            "Peng Gao",
            "Lewei Lu",
            "Xiaoqin Zhang",
            "Ling Shao",
            "Shijian Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14374",
        "abstract": "Multimodal large language models~(MLLMs) have demonstrated promising spatial understanding capabilities, such as referencing and grounding object descriptions. Despite their successes, MLLMs still fall short in fine-grained spatial perception abilities, such as generating detailed region descriptions or accurately localizing objects. Additionally, they often fail to respond to the user's requirements for desired fine-grained spatial understanding. This issue might arise because existing approaches primarily focus on tuning MLLMs to model pre-annotated instruction data to inject spatial knowledge, without direct supervision of MLLMs' actual responses. We address this issue by SPR, a Spatial Preference Rewarding~(SPR) approach that enhances MLLMs' spatial capabilities by rewarding MLLMs' detailed responses with precise object localization over vague or inaccurate responses. With randomly selected image regions and region descriptions from MLLMs, SPR introduces semantic and localization scores to comprehensively evaluate the text quality and localization quality in MLLM-generated descriptions. We also refine the MLLM descriptions with better localization accuracy and pair the best-scored refinement with the initial descriptions of the lowest score for direct preference optimization, thereby enhancing fine-grained alignment with visual input. Extensive experiments over standard referring and grounding benchmarks show that SPR improves MLLM spatial understanding capabilities effectively with minimal overhead in training. Data and code will be released at https://github.com/hanqiu-hq/SPR",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "146",
        "title": "DOS: Directional Object Separation in Text Embeddings for Multi-Object Image Generation",
        "author": [
            "Dongnam Byun",
            "Jungwon Park",
            "Jumgmin Ko",
            "Changin Choi",
            "Wonjong Rhee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14376",
        "abstract": "Recent progress in text-to-image (T2I) generative models has led to significant improvements in generating high-quality images aligned with text prompts. However, these models still struggle with prompts involving multiple objects, often resulting in object neglect or object mixing. Through extensive studies, we identify four problematic scenarios, Similar Shapes, Similar Textures, Dissimilar Background Biases, and Many Objects, where inter-object relationships frequently lead to such failures. Motivated by two key observations about CLIP embeddings, we propose DOS (Directional Object Separation), a method that modifies three types of CLIP text embeddings before passing them into text-to-image models. Experimental results show that DOS consistently improves the success rate of multi-object image generation and reduces object mixing. In human evaluations, DOS significantly outperforms four competing methods, receiving 26.24%-43.04% more votes across four benchmarks. These results highlight DOS as a practical and effective solution for improving multi-object image generation.",
        "tags": [
            "CLIP",
            "Text-to-Image"
        ]
    },
    {
        "id": "147",
        "title": "Are My Optimized Prompts Compromised? Exploring Vulnerabilities of LLM-based Optimizers",
        "author": [
            "Andrew Zhao",
            "Reshmi Ghosh",
            "Vitor Carvalho",
            "Emily Lawton",
            "Keegan Hines",
            "Gao Huang",
            "Jack W. Stokes"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14381",
        "abstract": "Large language model (LLM) systems now underpin everyday AI applications such as chatbots, computer-use assistants, and autonomous robots, where performance often depends on carefully designed prompts. LLM-based prompt optimizers reduce that effort by iteratively refining prompts from scored feedback, yet the security of this optimization stage remains underexamined. We present the first systematic analysis of poisoning risks in LLM-based prompt optimization. Using HarmBench, we find systems are substantially more vulnerable to manipulated feedback than to injected queries: feedback-based attacks raise attack success rate (ASR) by up to $\\Delta$ASR = 0.48. We introduce a simple fake-reward attack that requires no access to the reward model and significantly increases vulnerability, and we propose a lightweight highlighting defense that reduces the fake-reward $\\Delta$ASR from 0.23 to 0.07 without degrading utility. These results establish prompt optimization pipelines as a first-class attack surface and motivate stronger safeguards for feedback channels and optimization frameworks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "148",
        "title": "SHaRe-SSM: An Oscillatory Spiking Neural Network for Target Variable Modeling in Long Sequences",
        "author": [
            "Kartikay Agrawal",
            "Abhijeet Vikram",
            "Vedant Sharma",
            "Vaishnavi N.",
            "Ayon Borthakur"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14386",
        "abstract": "In recent years, with the emergence of large models, there has been a significant interest in spiking neural networks (SNNs) primarily due to their energy efficiency, multiplication-free, and sparse event-based deep learning. Similarly, state space models (SSMs) in varying designs have evolved as a powerful alternative to transformers for target modeling in long sequences, thereby overcoming the quadratic dependence on sequence length of a transformer. Inspired by this progress, we here design SHaRe-SSM (Spiking Harmonic Resonate and Fire State Space Model), for target variable modeling (including both classification and regression) for very-long-range sequences. Our second-order spiking SSM, on average, performs better than transformers or first-order SSMs while circumventing multiplication operations, making it ideal for resource-constrained applications. The proposed block consumes $73 \\times$ less energy than second-order ANN-based SSMs for an 18k sequence, while retaining performance. To ensure learnability over the long-range sequences, we propose exploiting the stable and efficient implementation of the dynamical system using parallel scans. Moreover, for the first time, we propose a kernel-based spiking regressor using resonate and fire neurons for very long-range sequences. Our network shows superior performance on even a 50k sequence while being significantly energy-efficient. In addition, we conducted a systematic analysis of the impact of heterogeneity, dissipation, and conservation in resonate-and-fire SSMs.",
        "tags": [
            "SSMs",
            "Transformer"
        ]
    },
    {
        "id": "149",
        "title": "Can MLLMs Absorb Math Reasoning Abilities from LLMs as Free Lunch?",
        "author": [
            "Yijie Hu",
            "Zihao Zhou",
            "Kaizhu Huang",
            "Xiaowei Huang",
            "Qiufeng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14387",
        "abstract": "Math reasoning has been one crucial ability of large language models (LLMs), where significant advancements have been achieved in recent years. However, most efforts focus on LLMs by curating high-quality annotation data and intricate training (or inference) paradigms, while the math reasoning performance of multi-modal LLMs (MLLMs) remains lagging behind. Since the MLLM typically consists of an LLM and a vision block, we wonder: Can MLLMs directly absorb math reasoning abilities from off-the-shelf math LLMs without tuning? Recent model-merging approaches may offer insights into this question. However, they overlook the alignment between the MLLM and LLM, where we find that there is a large gap between their parameter spaces, resulting in lower performance. Our empirical evidence reveals two key factors behind this issue: the identification of crucial reasoning-associated layers in the model and the mitigation of the gaps in parameter space. Based on the empirical insights, we propose IP-Merging that first identifies the reasoning-associated parameters in both MLLM and Math LLM, then projects them into the subspace of MLLM, aiming to maintain the alignment, and finally merges parameters in this subspace. IP-Merging is a tuning-free approach since parameters are directly adjusted. Extensive experiments demonstrate that our IP-Merging method can enhance the math reasoning ability of MLLMs directly from Math LLMs without compromising their other capabilities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "150",
        "title": "Hi-Agent: Hierarchical Vision-Language Agents for Mobile Device Control",
        "author": [
            "Zhe Wu",
            "Hongjin Lu",
            "Junliang Xing",
            "Changhao Zhang",
            "Yin Zhu",
            "Yuhao Yang",
            "Yuheng Jing",
            "Kai Li",
            "Kun Shao",
            "Jianye Hao",
            "Jun Wang",
            "Yuanchun Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14388",
        "abstract": "Building agents that autonomously operate mobile devices has attracted increasing attention. While Vision-Language Models (VLMs) show promise, most existing approaches rely on direct state-to-action mappings, which lack structured reasoning and planning, and thus generalize poorly to novel tasks or unseen UI layouts. We introduce Hi-Agent, a trainable hierarchical vision-language agent for mobile control, featuring a high-level reasoning model and a low-level action model that are jointly optimized. For efficient training, we reformulate multi-step decision-making as a sequence of single-step subgoals and propose a foresight advantage function, which leverages execution feedback from the low-level model to guide high-level optimization. This design alleviates the path explosion issue encountered by Group Relative Policy Optimization (GRPO) in long-horizon tasks and enables stable, critic-free joint training. Hi-Agent achieves a new State-Of-The-Art (SOTA) 87.9% task success rate on the Android-in-the-Wild (AitW) benchmark, significantly outperforming prior methods across three paradigms: prompt-based (AppAgent: 17.7%), supervised (Filtered BC: 54.5%), and reinforcement learning-based (DigiRL: 71.9%). It also demonstrates competitive zero-shot generalization on the ScreenSpot-v2 benchmark. On the more challenging AndroidWorld benchmark, Hi-Agent also scales effectively with larger backbones, showing strong adaptability in high-complexity mobile control scenarios.",
        "tags": [
            "GRPO",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "151",
        "title": "FairBatching: Fairness-Aware Batch Formation for LLM Inference",
        "author": [
            "Hongtao Lyu",
            "Boyue Liu",
            "Mingyu Wu",
            "Haibo Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14392",
        "abstract": "Large language model (LLM) inference systems face a fundamental tension between minimizing Time-to-First-Token (TTFT) latency for new requests and maintaining a high, steady token generation rate (low Time-Per-Output-Token, or TPOT) for ongoing requests. Existing stall-free batching schedulers proposed by Sarathi, while effective at preventing decode stalls, introduce significant computational unfairness. They prioritize decode tasks excessively, simultaneously leading to underutilized decode slack and unnecessary prefill queuing delays, which collectively degrade the system's overall quality of service (QoS).\nThis work identifies the root cause of this unfairness: the non-monotonic nature of Time-Between-Tokens (TBT) as a scheduling metric and the rigid decode-prioritizing policy that fails to adapt to dynamic workload bursts. We therefore propose FairBatching, a novel LLM inference scheduler that enforces fair resource allocation between prefill and decode tasks. It features an adaptive batch capacity determination mechanism, which dynamically adjusts the computational budget to improve the GPU utilization without triggering SLO violations. Its fair and dynamic batch formation algorithm breaks away from the decode-prioritizing paradigm, allowing computation resources to be reclaimed from bursting decode tasks to serve prefill surges, achieving global fairness. Furthermore, FairBatching provides a novel load estimation method, enabling more effective coordination with upper-level schedulers. Implemented and evaluated on realistic traces, FairBatching significantly reduces TTFT tail latency by up to 2.29x while robustly maintaining TPOT SLOs, achieving overall 20.0% improvement in single-node capacity and 54.3% improvement in cluster-level capacity.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "152",
        "title": "Low Power Vision Transformer Accelerator with Hardware-Aware Pruning and Optimized Dataflow",
        "author": [
            "Ching-Lin Hsiung",
            "Tian-Sheuan Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14393",
        "abstract": "Current transformer accelerators primarily focus on optimizing self-attention due to its quadratic complexity. However, this focus is less relevant for vision transformers with short token lengths, where the Feed-Forward Network (FFN) tends to be the dominant computational bottleneck. This paper presents a low power Vision Transformer accelerator, optimized through algorithm-hardware co-design. The model complexity is reduced using hardware-friendly dynamic token pruning without introducing complex mechanisms. Sparsity is further improved by replacing GELU with ReLU activations and employing dynamic FFN2 pruning, achieving a 61.5\\% reduction in operations and a 59.3\\% reduction in FFN2 weights, with an accuracy loss of less than 2\\%. The hardware adopts a row-wise dataflow with output-oriented data access to eliminate data transposition, and supports dynamic operations with minimal area overhead. Implemented in TSMC's 28nm CMOS technology, our design occupies 496.4K gates and includes a 232KB SRAM buffer, achieving a peak throughput of 1024 GOPS at 1GHz, with an energy efficiency of 2.31 TOPS/W and an area efficiency of 858.61 GOPS/mm2.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "153",
        "title": "Suicidal Comment Tree Dataset: Enhancing Risk Assessment and Prediction Through Contextual Analysis",
        "author": [
            "Jun Li",
            "Qun Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14395",
        "abstract": "Suicide remains a critical global public health issue. While previous studies have provided valuable insights into detecting suicidal expressions in individual social media posts, limited attention has been paid to the analysis of longitudinal, sequential comment trees for predicting a user's evolving suicidal risk. Users, however, often reveal their intentions through historical posts and interactive comments over time. This study addresses this gap by investigating how the information in comment trees affects both the discrimination and prediction of users' suicidal risk levels. We constructed a high-quality annotated dataset, sourced from Reddit, which incorporates users' posting history and comments, using a refined four-label annotation framework based on the Columbia Suicide Severity Rating Scale (C-SSRS). Statistical analysis of the dataset, along with experimental results from Large Language Models (LLMs) experiments, demonstrates that incorporating comment trees data significantly enhances the discrimination and prediction of user suicidal risk levels. This research offers a novel insight to enhancing the detection accuracy of at-risk individuals, thereby providing a valuable foundation for early suicide intervention strategies.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "154",
        "title": "Your Next Token Prediction: A Multilingual Benchmark for Personalized Response Generation",
        "author": [
            "Shiyao Ding",
            "Takayuki Ito"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14398",
        "abstract": "Large language models (LLMs) excel at general next-token prediction but still struggle to generate responses that reflect how individuals truly communicate, such as replying to emails or social messages in their own style. However, real SNS or email histories are difficult to collect due to privacy concerns. To address this, we propose the task of \"Your Next Token Prediction (YNTP)\", which models a user's precise word choices through controlled human-agent conversations. We build a multilingual benchmark of 100 dialogue sessions across English, Japanese, and Chinese, where users interact for five days with psychologically grounded NPCs based on MBTI dimensions. This setup captures natural, daily-life communication patterns and enables analysis of users' internal models. We evaluate prompt-based and fine-tuning-based personalization methods, establishing the first benchmark for YNTP and a foundation for user-aligned language modeling. The dataset is available at: https://github.com/AnonymousHub4Submissions/your-next-token-prediction-dataset-100",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "155",
        "title": "The Role of Social Learning and Collective Norm Formation in Fostering Cooperation in LLM Multi-Agent Systems",
        "author": [
            "Prateek Gupta",
            "Qiankun Zhong",
            "Hiromu Yakura",
            "Thomas Eisenmann",
            "Iyad Rahwan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14401",
        "abstract": "A growing body of multi-agent studies with Large Language Models (LLMs) explores how norms and cooperation emerge in mixed-motive scenarios, where pursuing individual gain can undermine the collective good. While prior work has explored these dynamics in both richly contextualized simulations and simplified game-theoretic environments, most LLM systems featuring common-pool resource (CPR) games provide agents with explicit reward functions directly tied to their actions. In contrast, human cooperation often emerges without full visibility into payoffs and population, relying instead on heuristics, communication, and punishment. We introduce a CPR simulation framework that removes explicit reward signals and embeds cultural-evolutionary mechanisms: social learning (adopting strategies and beliefs from successful peers) and norm-based punishment, grounded in Ostrom's principles of resource governance. Agents also individually learn from the consequences of harvesting, monitoring, and punishing via environmental feedback, enabling norms to emerge endogenously. We establish the validity of our simulation by reproducing key findings from existing studies on human behavior. Building on this, we examine norm evolution across a $2\\times2$ grid of environmental and social initialisations (resource-rich vs. resource-scarce; altruistic vs. selfish) and benchmark how agentic societies comprised of different LLMs perform under these conditions. Our results reveal systematic model differences in sustaining cooperation and norm formation, positioning the framework as a rigorous testbed for studying emergent norms in mixed-motive LLM societies. Such analysis can inform the design of AI systems deployed in social and organizational contexts, where alignment with cooperative norms is critical for stability, fairness, and effective governance of AI-mediated environments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "156",
        "title": "IMAGINE: Integrating Multi-Agent System into One Model for Complex Reasoning and Planning",
        "author": [
            "Xikai Zhang",
            "Bo Wang",
            "Likang Xiao",
            "Yongzhi Li",
            "Quan Chen",
            "Wenju Wu",
            "Liu Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14406",
        "abstract": "Although large language models (LLMs) have made significant strides across various tasks, they still face significant challenges in complex reasoning and planning. For example, even with carefully designed prompts and prior information explicitly provided, GPT-4o achieves only a 7% Final Pass Rate on the TravelPlanner dataset in the sole-planning mode. Similarly, even in the thinking mode, Qwen3-8B-Instruct and DeepSeek-R1-671B, only achieve Final Pass Rates of 5.9% and 40%, respectively. Although well-organized Multi-Agent Systems (MAS) can offer improved collective reasoning, they often suffer from high reasoning costs due to multi-round internal interactions, long per-response latency, and difficulties in end-to-end training. To address these challenges, we propose a general and scalable framework called IMAGINE, short for Integrating Multi-Agent System into One Model. This framework not only integrates the reasoning and planning capabilities of MAS into a single, compact model, but also significantly surpass the capabilities of the MAS through a simple end-to-end training. Through this pipeline, a single small-scale model is not only able to acquire the structured reasoning and planning capabilities of a well-organized MAS but can also significantly outperform it. Experimental results demonstrate that, when using Qwen3-8B-Instruct as the base model and training it with our method, the model achieves an 82.7% Final Pass Rate on the TravelPlanner benchmark, far exceeding the 40% of DeepSeek-R1-671B, while maintaining a much smaller model size.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "157",
        "title": "Instructions are all you need: Self-supervised Reinforcement Learning for Instruction Following",
        "author": [
            "Qingyu Ren",
            "Qianyu He",
            "Bowei Zhang",
            "Jie Zeng",
            "Jiaqing Liang",
            "Yanghua Xiao",
            "Weikang Zhou",
            "Zeye Sun",
            "Fei Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14420",
        "abstract": "Language models often struggle to follow multi-constraint instructions that are crucial for real-world applications. Existing reinforcement learning (RL) approaches suffer from dependency on external supervision and sparse reward signals from multi-constraint tasks. We propose a label-free self-supervised RL framework that eliminates dependency on external supervision by deriving reward signals directly from instructions and generating pseudo-labels for reward model training. Our approach introduces constraint decomposition strategies and efficient constraint-wise binary classification to address sparse reward challenges while maintaining computational efficiency. Experiments show that our approach generalizes well, achieving strong improvements across 3 in-domain and 5 out-of-domain datasets, including challenging agentic and multi-turn instruction following. The data and code are publicly available at https://github.com/Rainier-rq/verl-if",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "158",
        "title": "Deep Compositional Phase Diffusion for Long Motion Sequence Generation",
        "author": [
            "Ho Yin Au",
            "Jie Chen",
            "Junkun Jiang",
            "Jingyu Xiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14427",
        "abstract": "Recent research on motion generation has shown significant progress in generating semantically aligned motion with singular semantics. However, when employing these models to create composite sequences containing multiple semantically generated motion clips, they often struggle to preserve the continuity of motion dynamics at the transition boundaries between clips, resulting in awkward transitions and abrupt artifacts. To address these challenges, we present Compositional Phase Diffusion, which leverages the Semantic Phase Diffusion Module (SPDM) and Transitional Phase Diffusion Module (TPDM) to progressively incorporate semantic guidance and phase details from adjacent motion clips into the diffusion process. Specifically, SPDM and TPDM operate within the latent motion frequency domain established by the pre-trained Action-Centric Motion Phase Autoencoder (ACT-PAE). This allows them to learn semantically important and transition-aware phase information from variable-length motion clips during training. Experimental results demonstrate the competitive performance of our proposed framework in generating compositional motion sequences that align semantically with the input conditions, while preserving phase transitional continuity between preceding and succeeding motion clips. Additionally, motion inbetweening task is made possible by keeping the phase parameter of the input motion sequences fixed throughout the diffusion process, showcasing the potential for extending the proposed framework to accommodate various application scenarios. Codes are available at https://github.com/asdryau/TransPhase.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "159",
        "title": "MergeMoE: Efficient Compression of MoE Models via Expert Output Merging",
        "author": [
            "Ruijie Miao",
            "Yilun Yao",
            "Zihan Wang",
            "Zhiming Wang",
            "Bairen Yi",
            "LingJun Liu",
            "Yikai Zhao",
            "Tong Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14436",
        "abstract": "The Mixture-of-Experts (MoE) technique has proven to be a promising solution to efficiently scale the model size, which has been widely applied in recent LLM advancements. However, the substantial memory overhead of MoE models has made their compression an important research direction. In this work, we provide a theoretical analysis of expert merging, a recently proposed technique for compressing MoE models. Rather than interpreting expert merging from the conventional perspective of parameter aggregation, we approach it from the perspective of merging experts' outputs. Our key insight is that the merging process can be interpreted as inserting additional matrices into the forward computation, which naturally leads to an optimization formulation. Building on this analysis, we introduce MergeMoE, a method that leverages mathematical optimization to construct the compression matrices. We evaluate MergeMoE on multiple MoE models and show that our algorithm consistently outperforms the baselines with the same compression ratios.",
        "tags": [
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "160",
        "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents",
        "author": [
            "Rui Wang",
            "Ce Zhang",
            "Jun-Yu Ma",
            "Jianshu Zhang",
            "Hongru Wang",
            "Yi Chen",
            "Boyang Xue",
            "Tianqing Fang",
            "Zhisong Zhang",
            "Hongming Zhang",
            "Haitao Mi",
            "Dong Yu",
            "Kam-Fai Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14438",
        "abstract": "Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "161",
        "title": "A Free Lunch in LLM Compression: Revisiting Retraining after Pruning",
        "author": [
            "Moritz Wagner",
            "Christophe Roux",
            "Max Zimmer",
            "Sebastian Pokutta"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14444",
        "abstract": "While Neural Network pruning typically requires retraining the model to recover pruning-induced performance degradation, state-of-the-art Large Language Models (LLMs) pruning methods instead solve a layer-wise mask selection and reconstruction problem on a small set of calibration data to avoid full retraining, as it is considered computationally infeasible for LLMs. Reconstructing single matrices in isolation has favorable properties, such as convexity of the objective and significantly reduced memory requirements compared to full retraining. In practice, however, reconstruction is often implemented at coarser granularities, e.g., reconstructing a whole transformer block against its dense activations instead of a single matrix. In this work, we study the key design choices when reconstructing or retraining the remaining weights after pruning. We conduct an extensive computational study on state-of-the-art GPT architectures, and report several surprising findings that challenge common intuitions about retraining after pruning. In particular, we observe a free lunch scenario: reconstructing attention and MLP components separately within each transformer block is nearly the most resource-efficient yet achieves the best perplexity. Most importantly, this Pareto-optimal setup achieves better performance than full retraining, despite requiring only a fraction of the memory. Furthermore, we demonstrate that simple and efficient pruning criteria such as Wanda can outperform much more complex approaches when the reconstruction step is properly executed, highlighting its importance. Our findings challenge the narrative that retraining should be avoided at all costs and provide important insights into post-pruning performance recovery for LLMs.",
        "tags": [
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "162",
        "title": "Towards geological inference with process-based and deep generative modeling, part 1: training on fluvial deposits",
        "author": [
            "Guillaume Rongier",
            "Luk Peeters"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14445",
        "abstract": "The distribution of resources in the subsurface is deeply linked to the variations of its physical properties. Generative modeling has long been used to predict those physical properties while quantifying the associated uncertainty. But current approaches struggle to properly reproduce geological structures, and fluvial deposits in particular, because of their continuity. This study explores whether a generative adversarial network (GAN) - a type of deep-learning algorithm for generative modeling - can be trained to reproduce fluvial deposits simulated by a process-based model - a more expensive model that mimics geological processes. An ablation study shows that developments from the deep-learning community to generate large 2D images are directly transferable to 3D images of fluvial deposits. Training remains stable, and the generated samples reproduce the non-stationarity and details of the deposits without mode collapse or pure memorization of the training data. Using a process-based model to generate those training data allows us to include valuable properties other than the usual physical properties. We show how the deposition time let us monitor and validate the performance of a GAN by checking that its samples honor the law of superposition. Our work joins a series of previous studies suggesting that GANs are more robust that given credit for, at least for training datasets targeting specific geological structures. Whether this robustness transfers to larger 3D images and multimodal datasets remains to be seen. Exploring how deep generative models can leverage geological principles like the law of superposition shows a lot of promise.",
        "tags": [
            "3D",
            "GAN"
        ]
    },
    {
        "id": "163",
        "title": "Towards Adaptable Humanoid Control via Adaptive Motion Tracking",
        "author": [
            "Tao Huang",
            "Huayi Wang",
            "Junli Ren",
            "Kangning Yin",
            "Zirui Wang",
            "Xiao Chen",
            "Feiyu Jia",
            "Wentao Zhang",
            "Junfeng Long",
            "Jingbo Wang",
            "Jiangmiao Pang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14454",
        "abstract": "Humanoid robots are envisioned to adapt demonstrated motions to diverse real-world conditions while accurately preserving motion patterns. Existing motion prior approaches enable well adaptability with a few motions but often sacrifice imitation accuracy, whereas motion-tracking methods achieve accurate imitation yet require many training motions and a test-time target motion to adapt. To combine their strengths, we introduce AdaMimic, a novel motion tracking algorithm that enables adaptable humanoid control from a single reference motion. To reduce data dependence while ensuring adaptability, our method first creates an augmented dataset by sparsifying the single reference motion into keyframes and applying light editing with minimal physical assumptions. A policy is then initialized by tracking these sparse keyframes to generate dense intermediate motions, and adapters are subsequently trained to adjust tracking speed and refine low-level actions based on the adjustment, enabling flexible time warping that further improves imitation accuracy and adaptability. We validate these significant improvements in our approach in both simulation and the real-world Unitree G1 humanoid robot in multiple tasks across a wide range of adaptation conditions. Videos and code are available at https://taohuang13.github.io/adamimic.github.io/.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "164",
        "title": "Holdout-Loss-Based Data Selection for LLM Finetuning via In-Context Learning",
        "author": [
            "Ling Zhang",
            "Xianliang Yang",
            "Juwon Yu",
            "Park Cheonyoung",
            "Lei Song",
            "Jiang Bian"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14459",
        "abstract": "Fine-tuning large pretrained language models is a common approach for aligning them with human preferences, but noisy or off-target examples can dilute supervision. While small, well-chosen datasets often match the performance of much larger ones, systematic and efficient ways to identify high-value training data remain underexplored. Many current methods rely on heuristics or expensive retraining. We present a theoretically grounded, resource-efficient framework for data selection and reweighting. At its core is an In-Context Approximation (ICA) that estimates the holdout loss a model would incur after training on a candidate example by conditioning on a small, curated holdout set in context. ICA requires no reference model and no additional finetuning. Under a local linearization, ICA is equivalent to a first-order update toward the holdout optimum, motivating its use as a proxy for data value. We derive per-example weights from ICA scores, dynamically reweighting gradient updates as model parameters evolve. Across SFT, DPO, and SimPO, and over diverse backbones and datasets, ICA-based reweighting consistently improves model alignment with minimal overhead. We analyze sensitivity to score update frequency and the choice of $k$ holdout examples for in-context demonstrations, and note limitations for rapidly drifting on-policy updates, highlighting directions for future work. Code and prompts will be released.",
        "tags": [
            "DPO",
            "LLM"
        ]
    },
    {
        "id": "165",
        "title": "LiRA: Linguistic Robust Anchoring for Cross-lingual Large Language Models",
        "author": [
            "Haolin Li",
            "Haipeng Zhang",
            "Mang Li",
            "Yaohua Wang",
            "Lijie Wen",
            "Yu Zhang",
            "Biqing Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14466",
        "abstract": "As large language models (LLMs) rapidly advance, performance on high-resource languages (e.g., English, Chinese) is nearing saturation, yet remains substantially lower for low-resource languages (e.g., Urdu, Thai) due to limited training data, machine-translation noise, and unstable cross-lingual alignment. We introduce LiRA (Linguistic Robust Anchoring for Large Language Models), a training framework that robustly improves cross-lingual representations under low-resource conditions while jointly strengthening retrieval and reasoning. LiRA comprises two modules: (i) Arca (Anchored Representation Composition Architecture), which anchors low-resource languages to an English semantic space via anchor-based alignment and multi-agent collaborative encoding, preserving geometric stability in a shared embedding space; and (ii) LaSR (Language-coupled Semantic Reasoner), which adds a language-aware lightweight reasoning head with consistency regularization on top of Arca's multilingual representations, unifying the training objective to enhance cross-lingual understanding, retrieval, and reasoning robustness. We further construct and release a multilingual product retrieval dataset covering five Southeast Asian and two South Asian languages. Experiments across low-resource benchmarks (cross-lingual retrieval, semantic similarity, and reasoning) show consistent gains and robustness under few-shot and noise-amplified settings; ablations validate the contribution of both Arca and LaSR. Code will be released on GitHub and the dataset on Hugging Face.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "166",
        "title": "Restoring Noisy Demonstration for Imitation Learning With Diffusion Models",
        "author": [
            "Shang-Fu Chen",
            "Co Yong",
            "Shao-Hua Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14467",
        "abstract": "Imitation learning (IL) aims to learn a policy from expert demonstrations and has been applied to various applications. By learning from the expert policy, IL methods do not require environmental interactions or reward signals. However, most existing imitation learning algorithms assume perfect expert demonstrations, but expert demonstrations often contain imperfections caused by errors from human experts or sensor/control system inaccuracies. To address the above problems, this work proposes a filter-and-restore framework to best leverage expert demonstrations with inherent noise. Our proposed method first filters clean samples from the demonstrations and then learns conditional diffusion models to recover the noisy ones. We evaluate our proposed framework and existing methods in various domains, including robot arm manipulation, dexterous manipulation, and locomotion. The experiment results show that our proposed framework consistently outperforms existing methods across all the tasks. Ablation studies further validate the effectiveness of each component and demonstrate the framework's robustness to different noise types and levels. These results confirm the practical applicability of our framework to noisy offline demonstration data.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "167",
        "title": "Preconditioned Conjugate Gradient methods for the estimation of General Linear Models",
        "author": [
            "Paolo Foschi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14471",
        "abstract": "The use of the Preconditioned Conjugate Gradient (PCG) method for computing the Generalized Least Squares (GLS) estimator of the General Linear Model (GLM) is considered. The GLS estimator is expressed in terms of the solution of an augmented system. That system is solved by means of the PCG method using an indefinite preconditioner. The resulting method iterates a sequence Ordinary Least Squares (OLS) estimations that converges, in exact precision, to the GLS estimator within a finite number of steps. The numerical and statistical properties of the estimator computed at an intermediate step are analytically and numerically studied. This approach allows to combine direct methods, used in the OLS step, with those of iterative methods. This advantage is exploited to design PCG methods for the estimation of Constrained GLMs and of some structured multivariate GLMs. The structure of the matrices involved are exploited as much as possible, in the OLS step. The iterative method then solves for the unexploited structure. Numerical experiments shows that the proposed methods can achieve, for these structured problems, the same precision of state of the art direct methods, but in a fraction of the time.",
        "tags": [
            "GLM"
        ]
    },
    {
        "id": "168",
        "title": "From Guess2Graph: When and How Can Unreliable Experts Safely Boost Causal Discovery in Finite Samples?",
        "author": [
            "Sujai Hiremath",
            "Dominik Janzing",
            "Philipp Faller",
            "Patrick BlÃ¶baum",
            "Elke Kirschbaum",
            "Shiva Prasad Kasiviswanathan",
            "Kyra Gan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14488",
        "abstract": "Causal discovery algorithms often perform poorly with limited samples. While integrating expert knowledge (including from LLMs) as constraints promises to improve performance, guarantees for existing methods require perfect predictions or uncertainty estimates, making them unreliable for practical use. We propose the Guess2Graph (G2G) framework, which uses expert guesses to guide the sequence of statistical tests rather than replacing them. This maintains statistical consistency while enabling performance improvements. We develop two instantiations of G2G: PC-Guess, which augments the PC algorithm, and gPC-Guess, a learning-augmented variant designed to better leverage high-quality expert input. Theoretically, both preserve correctness regardless of expert error, with gPC-Guess provably outperforming its non-augmented counterpart in finite samples when experts are \"better than random.\" Empirically, both show monotonic improvement with expert accuracy, with gPC-Guess achieving significantly stronger gains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "169",
        "title": "Learning to Undo: Rollback-Augmented Reinforcement Learning with Reversibility Signals",
        "author": [
            "Andrejs Sorstkins",
            "Omer Tariq",
            "Muhammad Bilal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14503",
        "abstract": "This paper proposes a reversible learning framework to improve the robustness and efficiency of value based Reinforcement Learning agents, addressing vulnerability to value overestimation and instability in partially irreversible environments. The framework has two complementary core mechanisms: an empirically derived transition reversibility measure called Phi of s and a, and a selective state rollback operation. We introduce an online per state action estimator called Phi that quantifies the likelihood of returning to a prior state within a fixed horizon K. This measure is used to adjust the penalty term during temporal difference updates dynamically, integrating reversibility awareness directly into the value function. The system also includes a selective rollback operator. When an action yields an expected return markedly lower than its instantaneous estimated value and violates a predefined threshold, the agent is penalized and returns to the preceding state rather than progressing. This interrupts sub optimal high risk trajectories and avoids catastrophic steps. By combining reversibility aware evaluation with targeted rollback, the method improves safety, performance, and stability. In the CliffWalking v0 domain, the framework reduced catastrophic falls by over 99.8 percent and yielded a 55 percent increase in mean episode return. In the Taxi v3 domain, it suppressed illegal actions by greater than or equal to 99.9 percent and achieved a 65.7 percent improvement in cumulative reward, while also sharply reducing reward variance in both environments. Ablation studies confirm that the rollback mechanism is the critical component underlying these safety and performance gains, marking a robust step toward safe and reliable sequential decision making.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "170",
        "title": "E2Edev: Benchmarking Large Language Models in End-to-End Software Development Task",
        "author": [
            "Jingyao Liu",
            "Chen Huang",
            "Zhizhao Guan",
            "Wenqiang Lei",
            "Yang Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14509",
        "abstract": "E2EDev comprises (i) a fine-grained set of user requirements, (ii) {multiple BDD test scenarios with corresponding Python step implementations for each requirement}, and (iii) a fully automated testing pipeline built on the Behave framework. To ensure its quality while reducing the annotation effort, E2EDev leverages our proposed Human-in-the-Loop Multi-Agent Annotation Framework (HITL-MAA). {By evaluating various E2ESD frameworks and LLM backbones with E2EDev}, our analysis reveals a persistent struggle to effectively solve these tasks, underscoring the critical need for more effective and cost-efficient E2ESD solutions. Our codebase and benchmark are publicly available at https://github.com/SCUNLP/E2EDev.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "171",
        "title": "Stability Criteria and Motor Performance in Delayed Haptic Dyadic Interactions Mediated by Robots",
        "author": [
            "Mingtian Du",
            "Suhas Raghavendra Kulkarni",
            "Simone Kager",
            "Domenico Campolo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14511",
        "abstract": "This paper establishes analytical stability criteria for robot-mediated human-human (dyadic) interaction systems, focusing on haptic communication under network-induced time delays. Through frequency-domain analysis supported by numerical simulations, we identify both delay-independent and delay-dependent stability criteria. The delay-independent criterion guarantees stability irrespective of the delay, whereas the delay-dependent criterion is characterised by a maximum tolerable delay before instability occurs. The criteria demonstrate dependence on controller and robot dynamic parameters, where increasing stiffness reduces the maximum tolerable delay in a non-linear manner, thereby heightening system vulnerability. The proposed criteria can be generalised to a wide range of robot-mediated interactions and serve as design guidelines for stable remote dyadic systems. Experiments with robots performing human-like movements further illustrate the correlation between stability and motor performance. The findings of this paper suggest the prerequisites for effective delay-compensation strategies.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "172",
        "title": "Vision Mamba for Permeability Prediction of Porous Media",
        "author": [
            "Ali Kashefi",
            "Tapan Mukerji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14516",
        "abstract": "Vision Mamba has recently received attention as an alternative to Vision Transformers (ViTs) for image classification. The network size of Vision Mamba scales linearly with input image resolution, whereas ViTs scale quadratically, a feature that improves computational and memory efficiency. Moreover, Vision Mamba requires a significantly smaller number of trainable parameters than traditional convolutional neural networks (CNNs), and thus, they can be more memory efficient. Because of these features, we introduce, for the first time, a neural network that uses Vision Mamba as its backbone for predicting the permeability of three-dimensional porous media. We compare the performance of Vision Mamba with ViT and CNN models across multiple aspects of permeability prediction and perform an ablation study to assess the effects of its components on accuracy. We demonstrate in practice the aforementioned advantages of Vision Mamba over ViTs and CNNs in the permeability prediction of three-dimensional porous media. We make the source code publicly available to facilitate reproducibility and to enable other researchers to build on and extend this work. We believe the proposed framework has the potential to be integrated into large vision models in which Vision Mamba is used instead of ViTs.",
        "tags": [
            "Mamba",
            "ViT"
        ]
    },
    {
        "id": "173",
        "title": "Lexo: Eliminating Stealthy Supply-Chain Attacks via LLM-Assisted Program Regeneration",
        "author": [
            "Evangelos Lamprou",
            "Julian Dai",
            "Grigoris Ntousakis",
            "Martin C. Rinard",
            "Nikos Vasilakis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14522",
        "abstract": "Software supply-chain attacks are an important and ongoing concern in the open source software ecosystem. These attacks maintain the standard functionality that a component implements, but additionally hide malicious functionality activated only when the component reaches its target environment. Lexo addresses such stealthy attacks by automatically learning and regenerating vulnerability-free versions of potentially malicious components. Lexo first generates a set of input-output pairs to model a component's full observable behavior, which it then uses to synthesize a new version of the original component. The new component implements the original functionality but avoids stealthy malicious behavior. Throughout this regeneration process, Lexo consults several distinct instances of Large Language Models (LLMs), uses correctness and coverage metrics to shepherd these instances, and guardrails their results. Our evaluation on 100+ real-world packages, including high profile stealthy supply-chain attacks, indicates that Lexo scales across multiple domains, regenerates code efficiently (<100s on average), maintains compatibility, and succeeds in eliminating malicious code in several real-world supply-chain-attacks, even in cases when a state-of-the-art LLM fails to eliminate malicious code when prompted to do so.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "174",
        "title": "Noise Projection: Closing the Prompt-Agnostic Gap Behind Text-to-Image Misalignment in Diffusion Models",
        "author": [
            "Yunze Tong",
            "Didi Zhu",
            "Zijing Hu",
            "Jinluan Yang",
            "Ziyu Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14526",
        "abstract": "In text-to-image generation, different initial noises induce distinct denoising paths with a pretrained Stable Diffusion (SD) model. While this pattern could output diverse images, some of them may fail to align well with the prompt. Existing methods alleviate this issue either by altering the denoising dynamics or by drawing multiple noises and conducting post-selection. In this paper, we attribute the misalignment to a training-inference mismatch: during training, prompt-conditioned noises lie in a prompt-specific subset of the latent space, whereas at inference the noise is drawn from a prompt-agnostic Gaussian prior. To close this gap, we propose a noise projector that applies text-conditioned refinement to the initial noise before denoising. Conditioned on the prompt embedding, it maps the noise to a prompt-aware counterpart that better matches the distribution observed during SD training, without modifying the SD model. Our framework consists of these steps: we first sample some noises and obtain token-level feedback for their corresponding images from a vision-language model (VLM), then distill these signals into a reward model, and finally optimize the noise projector via a quasi-direct preference optimization. Our design has two benefits: (i) it requires no reference images or handcrafted priors, and (ii) it incurs small inference cost, replacing multi-sample selection with a single forward pass. Extensive experiments further show that our prompt-aware noise projection improves text-image alignment across diverse prompts.",
        "tags": [
            "Diffusion",
            "Text-to-Image",
            "VLM"
        ]
    },
    {
        "id": "175",
        "title": "PaddleOCR-VL: Boosting Multilingual Document Parsing via a 0.9B Ultra-Compact Vision-Language Model",
        "author": [
            "Cheng Cui",
            "Ting Sun",
            "Suyin Liang",
            "Tingquan Gao",
            "Zelun Zhang",
            "Jiaxuan Liu",
            "Xueqing Wang",
            "Changda Zhou",
            "Hongen Liu",
            "Manhui Lin",
            "Yue Zhang",
            "Yubo Zhang",
            "Handong Zheng",
            "Jing Zhang",
            "Jun Zhang",
            "Yi Liu",
            "Dianhai Yu",
            "Yanjun Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14528",
        "abstract": "In this report, we propose PaddleOCR-VL, a SOTA and resource-efficient model tailored for document parsing. Its core component is PaddleOCR-VL-0.9B, a compact yet powerful vision-language model (VLM) that integrates a NaViT-style dynamic resolution visual encoder with the ERNIE-4.5-0.3B language model to enable accurate element recognition. This innovative model efficiently supports 109 languages and excels in recognizing complex elements (e.g., text, tables, formulas, and charts), while maintaining minimal resource consumption. Through comprehensive evaluations on widely used public benchmarks and in-house benchmarks, PaddleOCR-VL achieves SOTA performance in both page-level document parsing and element-level recognition. It significantly outperforms existing solutions, exhibits strong competitiveness against top-tier VLMs, and delivers fast inference speeds. These strengths make it highly suitable for practical deployment in real-world scenarios.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "176",
        "title": "JSPLIT: A Taxonomy-based Solution for Prompt Bloating in Model Context Protocol",
        "author": [
            "Emanuele Antonioni",
            "Stefan Markovic",
            "Anirudha Shankar",
            "Jaime Bernardo",
            "Lovro Markovic",
            "Silvia Pareti",
            "Benedetto Proietti"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14537",
        "abstract": "AI systems are continually evolving and advancing, and user expectations are concurrently increasing, with a growing demand for interactions that go beyond simple text-based interaction with Large Language Models (LLMs). Today's applications often require LLMs to interact with external tools, marking a shift toward more complex agentic systems. To support this, standards such as the Model Context Protocol (MCP) have emerged, enabling agents to access tools by including a specification of the capabilities of each tool within the prompt. Although this approach expands what agents can do, it also introduces a growing problem: prompt bloating. As the number of tools increases, the prompts become longer, leading to high prompt token costs, increased latency, and reduced task success resulting from the selection of tools irrelevant to the prompt. To address this issue, we introduce JSPLIT, a taxonomy-driven framework designed to help agents manage prompt size more effectively when using large sets of MCP tools. JSPLIT organizes the tools into a hierarchical taxonomy and uses the user's prompt to identify and include only the most relevant tools, based on both the query and the taxonomy structure. In this paper, we describe the design of the taxonomy, the tool selection algorithm, and the dataset used to evaluate JSPLIT. Our results show that JSPLIT significantly reduces prompt size without significantly compromising the agent's ability to respond effectively. As the number of available tools for the agent grows substantially, JSPLIT even improves the tool selection accuracy of the agent, effectively reducing costs while simultaneously improving task success in high-complexity agent environments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "177",
        "title": "A Deep State-Space Model Compression Method using Upper Bound on Output Error",
        "author": [
            "Hiroki Sakamoto",
            "Kazuhiro Sato"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14542",
        "abstract": "We study deep state-space models (Deep SSMs) that contain linear-quadratic-output (LQO) systems as internal blocks and present a compression method with a provable output error guarantee. We first derive an upper bound on the output error between two Deep SSMs and show that the bound can be expressed via the $h^2$-error norms between the layerwise LQO systems, thereby providing a theoretical justification for existing model order reduction (MOR)-based compression. Building on this bound, we formulate an optimization problem in terms of the $h^2$-error norm and develop a gradient-based MOR method. On the IMDb task from the Long Range Arena benchmark, we demonstrate that our compression method achieves strong performance. Moreover, unlike prior approaches, we reduce roughly 80% of trainable parameters without retraining, with only a 4-5% performance drop.",
        "tags": [
            "SSMs"
        ]
    },
    {
        "id": "178",
        "title": "Exploring Cross-Modal Flows for Few-Shot Learning",
        "author": [
            "Ziqi Jiang",
            "Yanghao Wang",
            "Long Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14543",
        "abstract": "Aligning features from different modalities, is one of the most fundamental challenges for cross-modal tasks. Although pre-trained vision-language models can achieve a general alignment between image and text, they often require parameter-efficient fine-tuning (PEFT) for further adjustment. Today's PEFT methods (e.g., prompt tuning, LoRA-based, or adapter-based) always selectively fine-tune a subset of parameters, which can slightly adjust either visual or textual features, and avoid overfitting. In this paper, we are the first to highlight that all existing PEFT methods perform one-step adjustment. It is insufficient for complex (or difficult) datasets, where features of different modalities are highly entangled. To this end, we propose the first model-agnostic multi-step adjustment approach by learning a cross-modal velocity field: Flow Matching Alignment (FMA). Specifically, to ensure the correspondence between categories during training, we first utilize a fixed coupling strategy. Then, we propose a noise augmentation strategy to alleviate the data scarcity issue. Finally, we design an early-stopping solver, which terminates the transformation process earlier, improving both efficiency and accuracy. Compared with one-step PEFT methods, FMA has the multi-step rectification ability to achieve more precise and robust alignment. Extensive results have demonstrated that FMA can consistently yield significant performance gains across various benchmarks and backbones, particularly on challenging datasets.",
        "tags": [
            "Flow Matching",
            "LoRA",
            "VLM"
        ]
    },
    {
        "id": "179",
        "title": "Agentic Entropy-Balanced Policy Optimization",
        "author": [
            "Guanting Dong",
            "Licheng Bao",
            "Zhongyuan Wang",
            "Kangzhi Zhao",
            "Xiaoxi Li",
            "Jiajie Jin",
            "Jinghan Yang",
            "Hangyu Mao",
            "Fuzheng Zhang",
            "Kun Gai",
            "Guorui Zhou",
            "Yutao Zhu",
            "Ji-Rong Wen",
            "Zhicheng Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14545",
        "abstract": "Recently, Agentic Reinforcement Learning (Agentic RL) has made significant progress in incentivizing the multi-turn, long-horizon tool-use capabilities of web agents. While mainstream agentic RL algorithms autonomously explore high-uncertainty tool-call steps under the guidance of entropy, excessive reliance on entropy signals can impose further constraints, leading to the training collapse. In this paper, we delve into the challenges caused by entropy and propose the Agentic Entropy-Balanced Policy Optimization (AEPO), an agentic RL algorithm designed to balance entropy in both the rollout and policy update phases. AEPO comprises two core components: (1) a dynamic entropy-balanced rollout mechanism that adaptively allocate global and branch sampling budget through entropy pre-monitoring, while imposing a branch penalty on consecutive high-entropy tool-call steps to prevent over-branching issues; and (2) Entropy-Balanced Policy Optimization that inserts a stop-gradient operation into the high-entropy clipping term to preserve and properly rescale gradients on high-entropy tokens, while incorporating entropy-aware advantage estimation to prioritize learning on high-uncertainty tokens. Results across 14 challenging datasets show that AEPO consistently outperforms 7 mainstream RL algorithms. With just 1K RL samples, Qwen3-14B with AEPO achieves impressive results: 47.6% on GAIA, 11.2% on Humanity's Last Exam, and 43.0% on WebWalker for Pass@1; 65.0% on GAIA, 26.0% on Humanity's Last Exam, and 70.0% on WebWalker for Pass@5. Further analysis reveals that AEPO improves rollout sampling diversity while maintaining stable policy entropy, facilitating scalable web agent training.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "180",
        "title": "QuASH: Using Natural-Language Heuristics to Query Visual-Language Robotic Maps",
        "author": [
            "Matti Pekkanen",
            "Francesco Verdoja",
            "Ville Kyrki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14546",
        "abstract": "Embeddings from Visual-Language Models are increasingly utilized to represent semantics in robotic maps, offering an open-vocabulary scene understanding that surpasses traditional, limited labels. Embeddings enable on-demand querying by comparing embedded user text prompts to map embeddings via a similarity metric. The key challenge in performing the task indicated in a query is that the robot must determine the parts of the environment relevant to the query.\nThis paper proposes a solution to this challenge. We leverage natural-language synonyms and antonyms associated with the query within the embedding space, applying heuristics to estimate the language space relevant to the query, and use that to train a classifier to partition the environment into matches and non-matches. We evaluate our method through extensive experiments, querying both maps and standard image benchmarks. The results demonstrate increased queryability of maps and images. Our querying technique is agnostic to the representation and encoder used, and requires limited training.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "181",
        "title": "LLM Agents Beyond Utility: An Open-Ended Perspective",
        "author": [
            "Asen Nachkov",
            "Xi Wang",
            "Luc Van Gool"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14548",
        "abstract": "Recent LLM agents have made great use of chain of thought reasoning and function calling. As their capabilities grow, an important question arises: can this software represent not only a smart problem-solving tool, but an entity in its own right, that can plan, design immediate tasks, and reason toward broader, more ambiguous goals? To study this question, we adopt an open-ended experimental setting where we augment a pretrained LLM agent with the ability to generate its own tasks, accumulate knowledge, and interact extensively with its environment. We study the resulting open-ended agent qualitatively. It can reliably follow complex multi-step instructions, store and reuse information across runs, and propose and solve its own tasks, though it remains sensitive to prompt design, prone to repetitive task generation, and unable to form self-representations. These findings illustrate both the promise and current limits of adapting pretrained LLMs toward open-endedness, and point to future directions for training agents to manage memory, explore productively, and pursue abstract long-term goals.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "182",
        "title": "Consistent text-to-image generation via scene de-contextualization",
        "author": [
            "Song Tang",
            "Peihao Gong",
            "Kunyu Li",
            "Kai Guo",
            "Boyu Wang",
            "Mao Ye",
            "Jianwei Zhang",
            "Xiatian Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14553",
        "abstract": "Consistent text-to-image (T2I) generation seeks to produce identity-preserving images of the same subject across diverse scenes, yet it often fails due to a phenomenon called identity (ID) shift. Previous methods have tackled this issue, but typically rely on the unrealistic assumption of knowing all target scenes in advance. This paper reveals that a key source of ID shift is the native correlation between subject and scene context, called scene contextualization, which arises naturally as T2I models fit the training distribution of vast natural images. We formally prove the near-universality of this scene-ID correlation and derive theoretical bounds on its strength. On this basis, we propose a novel, efficient, training-free prompt embedding editing approach, called Scene De-Contextualization (SDeC), that imposes an inversion process of T2I's built-in scene contextualization. Specifically, it identifies and suppresses the latent scene-ID correlation within the ID prompt's embedding by quantifying the SVD directional stability to adaptively re-weight the corresponding eigenvalues. Critically, SDeC allows for per-scene use (one scene per prompt) without requiring prior access to all target scenes. This makes it a highly flexible and general solution well-suited to real-world applications where such prior knowledge is often unavailable or varies over time. Experiments demonstrate that SDeC significantly enhances identity preservation while maintaining scene diversity.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "183",
        "title": "MX+: Pushing the Limits of Microscaling Formats for Efficient Large Language Model Serving",
        "author": [
            "Jungi Lee",
            "Junyong Park",
            "Soohyun Cha",
            "Jaehoon Cho",
            "Jaewoong Sim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14557",
        "abstract": "Reduced-precision data formats are crucial for cost-effective serving of large language models (LLMs). While numerous reduced-precision formats have been introduced thus far, they often require intrusive modifications to the software frameworks or are rather unconventional for widespread adoption across hardware vendors. In this paper, we instead focus on recent industry-driven variants of block floating-point (BFP) formats and conduct a comprehensive analysis to push their limits for efficient LLM serving. Our analysis shows that existing ultra low-bit BFP variants struggle to provide reasonable language model performance due to outlier values in blocks. To address the outliers with BFPs, we propose MX+, a cost-effective and non-intrusive extension designed for seamless integration into the microscaling (MX) formats. MX+ builds on the key insight that the outlier does not need to use its exponent field in the element data type, which allows us to repurpose the exponent field as an extended mantissa to increase the precision of the outlier element. Our evaluation shows that MX+ achieves significantly higher model performance compared to the 4-bit MX format (MXFP4) with negligible storage overhead and slowdown, thus offering a compelling alternative to MXFP4 or MXFP6 for efficient LLM inference.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "184",
        "title": "Eyes Wide Open: Ego Proactive Video-LLM for Streaming Video",
        "author": [
            "Yulin Zhang",
            "Cheng Shi",
            "Yang Wang",
            "Sibei Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14560",
        "abstract": "Envision an AI capable of functioning in human-like settings, moving beyond mere observation to actively understand, anticipate, and proactively respond to unfolding events. Towards this vision, we focus on the innovative task where, given ego-streaming video input, an assistant proactively answers diverse, evolving questions at the opportune moment, while maintaining synchronized perception and reasoning. This task embodies three key properties: (1) Proactive Coherence, (2) Just-in-Time Responsiveness, and (3) Synchronized Efficiency. To evaluate and address these properties, we first introduce ESTP-Bench (Ego Streaming Proactive Benchmark) alongside the ESTP-F1 metric-a novel framework designed for their rigorous assessment. Secondly, we propose a comprehensive technical pipeline to enable models to tackle this challenging task. This pipeline comprises: (1) a data engine, (2) a multi-stage training strategy, and (3) a proactive dynamic compression technique. Our proposed model effectively addresses these critical properties while outperforming multiple baselines across diverse online and offline benchmarks. Project Page:https://zhangyl4.github.io/publications/eyes-wide-open/",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "185",
        "title": "BalanceGS: Algorithm-System Co-design for Efficient 3D Gaussian Splatting Training on GPU",
        "author": [
            "Junyi Wu",
            "Jiaming Xu",
            "Jinhao Li",
            "Yongkang Zhou",
            "Jiayi Pan",
            "Xingyang Li",
            "Guohao Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14564",
        "abstract": "3D Gaussian Splatting (3DGS) has emerged as a promising 3D reconstruction technique. The traditional 3DGS training pipeline follows three sequential steps: Gaussian densification, Gaussian projection, and color splatting. Despite its promising reconstruction quality, this conventional approach suffers from three critical inefficiencies: (1) Skewed density allocation during Gaussian densification, (2) Imbalanced computation workload during Gaussian projection and (3) Fragmented memory access during color splatting.\nTo tackle the above challenges, we introduce BalanceGS, the algorithm-system co-design for efficient training in 3DGS. (1) At the algorithm level, we propose heuristic workload-sensitive Gaussian density control to automatically balance point distributions - removing 80% redundant Gaussians in dense regions while filling gaps in sparse areas. (2) At the system level, we propose Similarity-based Gaussian sampling and merging, which replaces the static one-to-one thread-pixel mapping with adaptive workload distribution - threads now dynamically process variable numbers of Gaussians based on local cluster density. (3) At the mapping level, we propose reordering-based memory access mapping strategy that restructures RGB storage and enables batch loading in shared memory.\nExtensive experiments demonstrate that compared with 3DGS, our approach achieves a 1.44$\\times$ training speedup on a NVIDIA A100 GPU with negligible quality degradation.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "186",
        "title": "Assessing Socio-Cultural Alignment and Technical Safety of Sovereign LLMs",
        "author": [
            "Kyubyung Chae",
            "Gihoon Kim",
            "Gyuseong Lee",
            "Taesup Kim",
            "Jaejin Lee",
            "Heejin Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14565",
        "abstract": "Recent trends in LLMs development clearly show growing interest in the use and application of sovereign LLMs. The global debate over sovereign LLMs highlights the need for governments to develop their LLMs, tailored to their unique socio-cultural and historical contexts. However, there remains a shortage of frameworks and datasets to verify two critical questions: (1) how well these models align with users' socio-cultural backgrounds, and (2) whether they maintain safety and technical robustness without exposing users to potential harms and risks. To address this gap, we construct a new dataset and introduce an analytic framework for extracting and evaluating the socio-cultural elements of sovereign LLMs, alongside assessments of their technical robustness. Our experimental results demonstrate that while sovereign LLMs play a meaningful role in supporting low-resource languages, they do not always meet the popular claim that these models serve their target users well. We also show that pursuing this untested claim may lead to underestimating critical quality attributes such as safety. Our study suggests that advancing sovereign LLMs requires a more extensive evaluation that incorporates a broader range of well-grounded and practical criteria.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "187",
        "title": "AudioEval: Automatic Dual-Perspective and Multi-Dimensional Evaluation of Text-to-Audio-Generation",
        "author": [
            "Hui Wang",
            "Jinghua Zhao",
            "Cheng Liu",
            "Yuhang Jia",
            "Haoqin Sun",
            "Jiaming Zhou",
            "Yong Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14570",
        "abstract": "Text-to-audio (TTA) is rapidly advancing, with broad potential in virtual reality, accessibility, and creative media. However, evaluating TTA quality remains difficult: human ratings are costly and limited, while existing objective metrics capture only partial aspects of perceptual quality. To address this gap, we introduce AudioEval, the first large-scale TTA evaluation dataset, containing 4,200 audio samples from 24 systems with 126,000 ratings across five perceptual dimensions, annotated by both experts and non-experts. Based on this resource, we propose Qwen-DisQA, a multimodal scoring model that jointly processes text prompts and generated audio to predict human-like quality ratings. Experiments show its effectiveness in providing reliable and scalable evaluation. The dataset will be made publicly available to accelerate future research.",
        "tags": [
            "Qwen"
        ]
    },
    {
        "id": "188",
        "title": "ScalePool: Hybrid XLink-CXL Fabric for Composable Resource Disaggregation in Unified Scale-up Domains",
        "author": [
            "Hyein Woo",
            "Miryeong Kwon",
            "Jiseon Kim",
            "Eunjee Na",
            "Hanjin Choi",
            "Seonghyeon Jang",
            "Myoungsoo Jung"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14580",
        "abstract": "This paper proposes ScalePool, a novel cluster architecture designed to interconnect numerous accelerators using unified hardware interconnects rather than traditional long-distance networking. ScalePool integrates Accelerator-Centric Links (XLink) and Compute Express Link (CXL) into a unified XLink-CXL hybrid fabric. Specifically, ScalePool employs XLink for intra-cluster, low-latency accelerator communication, while using hierarchical CXL-based switching fabrics for scalable and coherent inter-cluster memory sharing. By abstracting interfaces through CXL, ScalePool structurally resolves interoperability constraints, enabling heterogeneous cluster operation and composable resource disaggregation. In addition, ScalePool introduces explicit memory tiering: the latency-critical tier-1 combines accelerator-local memory with coherence-centric CXL and XLink, whereas the highcapacity tier-2 employs dedicated memory nodes interconnected by a CXL-based fabric, achieving scalable and efficient memory pooling. Evaluation results show that ScalePool accelerates LLM training by 1.22x on average and up to 1.84x compared to conventional RDMA-based environments. Furthermore, the proposed tier-2 memory disaggregation strategy reduces latency by up to 4.5x for memory-intensive workloads.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "189",
        "title": "Selective Labeling with False Discovery Rate Control",
        "author": [
            "Huipeng Huang",
            "Wenbo Liao",
            "Huajun Xi",
            "Hao Zeng",
            "Mengchen Zhao",
            "Hongxin Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14581",
        "abstract": "Obtaining high-quality labels for large datasets is expensive, requiring massive annotations from human experts. While AI models offer a cost-effective alternative by predicting labels, their label quality is compromised by the unavoidable labeling errors. Existing methods mitigate this issue through selective labeling, where AI labels a subset and human labels the remainder. However, these methods lack theoretical guarantees on the quality of AI-assigned labels, often resulting in unacceptably high labeling error within the AI-labeled subset. To address this, we introduce \\textbf{Conformal Labeling}, a novel method to identify instances where AI predictions can be provably trusted. This is achieved by controlling the false discovery rate (FDR), the proportion of incorrect labels within the selected subset. In particular, we construct a conformal $p$-value for each test instance by comparing AI models' predicted confidence to those of calibration instances mislabeled by AI models. Then, we select test instances whose $p$-values are below a data-dependent threshold, certifying AI models' predictions as trustworthy. We provide theoretical guarantees that Conformal Labeling controls the FDR below the nominal level, ensuring that a predefined fraction of AI-assigned labels is correct on average. Extensive experiments demonstrate that our method achieves tight FDR control with high power across various tasks, including image and text labeling, and LLM QA.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "190",
        "title": "Talking Points: Describing and Localizing Pixels",
        "author": [
            "Matan Rusanovsky",
            "Shimon Malnick",
            "Shai Avidan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14583",
        "abstract": "Vision-language models have achieved remarkable success in cross-modal understanding. Yet, these models remain limited to object-level or region-level grounding, lacking the capability for pixel-precise keypoint comprehension through natural language. We introduce a novel framework for pixel level grounding. The framework consists of two complementary components: a Point Descriptor that generates rich, contextual descriptions of individual keypoints, and a Point Localizer that regresses precise pixel coordinates from these descriptions. Unlike prior work that relies on templated prompts or keypoint names, our approach produces free-form, coarse-to-fine descriptions that situate keypoints within their visual context. Since there is no available dataset to train such a system, we introduce LlamaPointInPart, a carefully curated dataset of 20K+ image-keypoint-description triplets synthesized from multiple vision-language models, capturing multi-scale information from scene-level context to visual features around the keypoint. For cross-category generalization, we optimize the Point Descriptor on AP-10K via GRPO, using the frozen Point Localizer as a reward model to produce descriptions that maximize localization accuracy. To evaluate our results we establish a new evaluation protocol. Instead of comparing the text description produced by our method to the ground truth, we use the localizer to determine how close is the predicted point generated to the ground truth point. Experiments demonstrate superior performance compared to baseline models on http://LlamaPointInPart.The bidirectional nature of our framework should enable future applications in both keypoint-guided image understanding and language-guided precise localization. Our code and dataset are publicly available at https://github.com/matanr/Talking_Points.",
        "tags": [
            "GRPO",
            "VLM"
        ]
    },
    {
        "id": "191",
        "title": "STANCE: Motion Coherent Video Generation Via Sparse-to-Dense Anchored Encoding",
        "author": [
            "Zhifei Chen",
            "Tianshuo Xu",
            "Leyi Wu",
            "Luozhou Wang",
            "Dongyu Yan",
            "Zihan You",
            "Wenting Luo",
            "Guo Zhang",
            "Yingcong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14588",
        "abstract": "Video generation has recently made striking visual progress, but maintaining coherent object motion and interactions remains difficult. We trace two practical bottlenecks: (i) human-provided motion hints (e.g., small 2D maps) often collapse to too few effective tokens after encoding, weakening guidance; and (ii) optimizing for appearance and motion in a single head can favor texture over temporal consistency. We present STANCE, an image-to-video framework that addresses both issues with two simple components. First, we introduce Instance Cues -- a pixel-aligned control signal that turns sparse, user-editable hints into a dense 2.5D (camera-relative) motion field by averaging per-instance flow and augmenting with monocular depth over the instance mask. This reduces depth ambiguity compared to 2D arrow inputs while remaining easy to use. Second, we preserve the salience of these cues in token space with Dense RoPE, which tags a small set of motion tokens (anchored on the first frame) with spatial-addressable rotary embeddings. Paired with joint RGB \\(+\\) auxiliary-map prediction (segmentation or depth), our model anchors structure while RGB handles appearance, stabilizing optimization and improving temporal coherence without requiring per-frame trajectory scripts.",
        "tags": [
            "RoPE",
            "Segmentation",
            "Video Generation"
        ]
    },
    {
        "id": "192",
        "title": "Just-In-Time Objectives: A General Approach for Specialized AI Interactions",
        "author": [
            "Michelle S. Lam",
            "Omar Shaikh",
            "Hallie Xu",
            "Alice Guo",
            "Diyi Yang",
            "Jeffrey Heer",
            "James A. Landay",
            "Michael S. Bernstein"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14591",
        "abstract": "Large language models promise a broad set of functions, but when not given a specific objective, they default to milquetoast results such as drafting emails littered with cliches. We demonstrate that inferring the user's in-the-moment objective, then rapidly optimizing for that singular objective, enables LLMs to produce tools, interfaces, and responses that are more responsive and desired. We contribute an architecture for automatically inducing just-in-time objectives by passively observing user behavior, then steering downstream AI systems through generation and evaluation against this objective. Inducing just-in-time objectives (e.g., \"Clarify the abstract's research contribution\") enables automatic generation of tools, e.g., those that critique a draft based on relevant HCI methodologies, anticipate related researchers' reactions, or surface ambiguous terminology. In a series of experiments (N=14, N=205) on participants' own tasks, JIT objectives enable LLM outputs that achieve 66-86% win rates over typical LLMs, and in-person use sessions (N=17) confirm that JIT objectives produce specialized tools unique to each participant.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "193",
        "title": "Hierarchical Re-Classification: Combining Animal Classification Models with Vision Transformers",
        "author": [
            "Hugo Markoff",
            "Jevgenijs Galaktionovs"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14594",
        "abstract": "State-of-the-art animal classification models like SpeciesNet provide predictions across thousands of species but use conservative rollup strategies, resulting in many animals labeled at high taxonomic levels rather than species. We present a hierarchical re-classification system for the Animal Detect platform that combines SpeciesNet EfficientNetV2-M predictions with CLIP embeddings and metric learning to refine high-level taxonomic labels toward species-level identification. Our five-stage pipeline (high-confidence acceptance, bird override, centroid building, triplet-loss metric learning, and adaptive cosine-distance scoring) is evaluated on a segment of the LILA BC Desert Lion Conservation dataset (4,018 images, 15,031 detections). After recovering 761 bird detections from \"blank\" and \"animal\" labels, we re-classify 456 detections labeled animal, mammal, or blank with 96.5% accuracy, achieving species-level identification for 64.9 percent",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "194",
        "title": "Zero-Shot Wildlife Sorting Using Vision Transformers: Evaluating Clustering and Continuous Similarity Ordering",
        "author": [
            "Hugo Markoff",
            "Jevgenijs Galaktionovs"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14596",
        "abstract": "Camera traps generate millions of wildlife images, yet many datasets contain species that are absent from existing classifiers. This work evaluates zero-shot approaches for organizing unlabeled wildlife imagery using self-supervised vision transformers, developed and tested within the Animal Detect platform for camera trap analysis. We compare unsupervised clustering methods (DBSCAN, GMM) across three architectures (CLIP, DINOv2, MegaDescriptor) combined with dimensionality reduction techniques (PCA, UMAP), and we demonstrate continuous 1D similarity ordering via t-SNE projection. On a 5-species test set with ground truth labels used only for evaluation, DINOv2 with UMAP and GMM achieves 88.6 percent accuracy (macro-F1 = 0.874), while 1D sorting reaches 88.2 percent coherence for mammals and birds and 95.2 percent for fish across 1,500 images. Based on these findings, we deployed continuous similarity ordering in production, enabling rapid exploratory analysis and accelerating manual annotation workflows for biodiversity monitoring.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "195",
        "title": "Knowledge-based Visual Question Answer with Multimodal Processing, Retrieval and Filtering",
        "author": [
            "Yuyang Hong",
            "Jiaqi Gu",
            "Qi Yang",
            "Lubin Fan",
            "Yue Wu",
            "Ying Wang",
            "Kun Ding",
            "Shiming Xiang",
            "Jieping Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14605",
        "abstract": "Knowledge-based visual question answering (KB-VQA) requires visual language models (VLMs) to integrate visual understanding with external knowledge retrieval. Although retrieval-augmented generation (RAG) achieves significant advances in this task by combining knowledge-base querying, it still struggles with the quality of multimodal queries and the relevance of retrieved results. To overcome these challenges, we propose a novel three-stage method, termed Wiki-PRF, including Processing, Retrieval and Filtering stages. The processing stage dynamically invokes visual tools to extract precise multimodal information for retrieval. The retrieval stage integrates visual and text features to achieve multimodal knowledge retrieval. The filtering stage performs relevance filtering and concentration on retrieval results. To this end, we introduce a visual language model trained with answer accuracy and format consistency as reward signals via a reinforcement learning manner. This enhances the model's reasoning, tool invocation for accurate queries, and filtering of irrelevant content. Experiments on benchmark datasets (E-VQA and InfoSeek) show significant improvements~(36.0 and 42.8) in answer quality, achieving state-of-the-art performance. Code is available at https://github.com/cqu-student/Wiki-PRF",
        "tags": [
            "RAG",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "196",
        "title": "Proprioceptive Image: An Image Representation of Proprioceptive Data from Quadruped Robots for Contact Estimation Learning",
        "author": [
            "Gabriel Fischer Abati",
            "JoÃ£o Carlos Virgolino Soares",
            "Giulio Turrisi",
            "Victor Barasuol",
            "Claudio Semini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14612",
        "abstract": "This paper presents a novel approach for representing proprioceptive time-series data from quadruped robots as structured two-dimensional images, enabling the use of convolutional neural networks for learning locomotion-related tasks. The proposed method encodes temporal dynamics from multiple proprioceptive signals, such as joint positions, IMU readings, and foot velocities, while preserving the robot's morphological structure in the spatial arrangement of the image. This transformation captures inter-signal correlations and gait-dependent patterns, providing a richer feature space than direct time-series processing. We apply this concept in the problem of contact estimation, a key capability for stable and adaptive locomotion on diverse terrains. Experimental evaluations on both real-world datasets and simulated environments show that our image-based representation consistently enhances prediction accuracy and generalization over conventional sequence-based models, underscoring the potential of cross-modal encoding strategies for robotic state learning. Our method achieves superior performance on the contact dataset, improving contact state accuracy from 87.7% to 94.5% over the recently proposed MI-HGNN method, using a 15 times shorter window size.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "197",
        "title": "First Attentions Last: Better Exploiting First Attentions for Efficient Transformer Training",
        "author": [
            "Gyudong Kim",
            "Hyukju Na",
            "Jin Hyeon Kim",
            "Hyunsung Jang",
            "Jaemin Park",
            "Jaegi Hwang",
            "Namkoo Ha",
            "Seungryong Kim",
            "Young Geun Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14614",
        "abstract": "As training billion-scale transformers becomes increasingly common, employing multiple distributed GPUs along with parallel training methods has become a standard practice. However, existing transformer designs suffer from significant communication overhead, especially in Tensor Parallelism (TP), where each block's MHA-MLP connection requires an all-reduce communication. Through our investigation, we show that the MHA-MLP connections can be bypassed for efficiency, while the attention output of the first layer can serve as an alternative signal for the bypassed connection. Motivated by the observations, we propose FAL (First Attentions Last), an efficient transformer architecture that redirects the first MHA output to the MLP inputs of the following layers, eliminating the per-block MHA-MLP connections. This removes the all-reduce communication and enables parallel execution of MHA and MLP on a single GPU. We also introduce FAL+, which adds the normalized first attention output to the MHA outputs of the following layers to augment the MLP input for the model quality. Our evaluation shows that FAL reduces multi-GPU training time by up to 44%, improves single-GPU throughput by up to 1.18x, and achieves better perplexity compared to the baseline GPT. FAL+ achieves even lower perplexity without increasing the training time than the baseline.",
        "tags": [
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "198",
        "title": "Accelerated Multi-Modal Motion Planning Using Context-Conditioned Diffusion Models",
        "author": [
            "Edward Sandra",
            "Lander Vanroye",
            "Dries Dirckx",
            "Ruben Cartuyvels",
            "Jan Swevers",
            "Wilm DecrÃ©"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14615",
        "abstract": "Classical methods in robot motion planning, such as sampling-based and optimization-based methods, often struggle with scalability towards higher-dimensional state spaces and complex environments. Diffusion models, known for their capability to learn complex, high-dimensional and multi-modal data distributions, provide a promising alternative when applied to motion planning problems and have already shown interesting results. However, most of the current approaches train their model for a single environment, limiting their generalization to environments not seen during training. The techniques that do train a model for multiple environments rely on a specific camera to provide the model with the necessary environmental information and therefore always require that sensor. To effectively adapt to diverse scenarios without the need for retraining, this research proposes Context-Aware Motion Planning Diffusion (CAMPD). CAMPD leverages a classifier-free denoising probabilistic diffusion model, conditioned on sensor-agnostic contextual information. An attention mechanism, integrated in the well-known U-Net architecture, conditions the model on an arbitrary number of contextual parameters. CAMPD is evaluated on a 7-DoF robot manipulator and benchmarked against state-of-the-art approaches on real-world tasks, showing its ability to generalize to unseen environments and generate high-quality, multi-modal trajectories, at a fraction of the time required by existing methods.",
        "tags": [
            "Diffusion",
            "Robotics"
        ]
    },
    {
        "id": "199",
        "title": "Beyond Correctness: Evaluating Subjective Writing Preferences Across Cultures",
        "author": [
            "Shuangshuang Ying",
            "Yunwen Li",
            "Xingwei Qu",
            "Xin Li",
            "Sheng Jin",
            "Minghao Liu",
            "Zhoufutu Wen",
            "Xeron Du",
            "Tianyu Zheng",
            "Yichi Zhang",
            "Letian Ni",
            "Yuyang Cheng",
            "Qiguang Chen",
            "Jingzhe Ding",
            "Shengda Long",
            "Wangchunshu Zhou",
            "Jiazhan Feng",
            "Wanjun Zhong",
            "Libo Qin",
            "Ge Zhang",
            "Wenhao Huang",
            "Wanxiang Che",
            "Chenghua Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14616",
        "abstract": "Current preference learning methods achieve high accuracy on standard benchmarks but exhibit significant performance degradation when objective quality signals are removed. We introduce WritingPreferenceBench, a dataset of 1,800 human-annotated preference pairs (1,200 English, 600 Chinese) across 8 creative writing genres, where responses are matched for objective correctness, factual accuracy, and length. On this benchmark, sequence-based reward models--the standard architecture for RLHF--achieve only 52.7% mean accuracy, while zero-shot language model judges perform at 53.9%. In contrast, generative reward models that produce explicit reasoning chains achieve 81.8% accuracy. We observe high within-model variance across genres: individual models range from 18.2% to 81.8% accuracy across different writing categories, with standard deviations averaging 10.1%. This variance persists regardless of model scale, with 27B parameter models showing no consistent improvement over 8B variants. Our results suggest that current RLHF methods primarily learn to detect objective errors rather than capture subjective quality preferences (e.g., creativity, stylistic flair, and emotional resonance), and that successful preference modeling may require intermediate reasoning representations rather than direct classification.",
        "tags": [
            "RLHF"
        ]
    },
    {
        "id": "200",
        "title": "Shot2Tactic-Caption: Multi-Scale Captioning of Badminton Videos for Tactical Understanding",
        "author": [
            "Ning Ding",
            "Keisuke Fujii",
            "Toru Tamaki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14617",
        "abstract": "Tactical understanding in badminton involves interpreting not only individual actions but also how tactics are dynamically executed over time. In this paper, we propose \\textbf{Shot2Tactic-Caption}, a novel framework for semantic and temporal multi-scale video captioning in badminton, capable of generating shot-level captions that describe individual actions and tactic-level captions that capture how these actions unfold over time within a tactical execution. We also introduce the Shot2Tactic-Caption Dataset, the first badminton captioning dataset containing 5,494 shot captions and 544 tactic captions. Shot2Tactic-Caption adopts a dual-branch design, with both branches including a visual encoder, a spatio-temporal Transformer encoder, and a Transformer-based decoder to generate shot and tactic captions. To support tactic captioning, we additionally introduce a Tactic Unit Detector that identifies valid tactic units, tactic types, and tactic states (e.g., Interrupt, Resume). For tactic captioning, we further incorporate a shot-wise prompt-guided mechanism, where the predicted tactic type and state are embedded as prompts and injected into the decoder via cross-attention. The shot-wise prompt-guided mechanism enables our system not only to describe successfully executed tactics but also to capture tactical executions that are temporarily interrupted and later resumed. Experimental results demonstrate the effectiveness of our framework in generating both shot and tactic captions. Ablation studies show that the ResNet50-based spatio-temporal encoder outperforms other variants, and that shot-wise prompt structuring leads to more coherent and accurate tactic captioning.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "201",
        "title": "Code-driven Number Sequence Calculation: Enhancing the inductive Reasoning Abilities of Large Language Models",
        "author": [
            "Kedi Chen",
            "Zhikai Lei",
            "Xu Guo",
            "Xuecheng Wu",
            "Siyuan Zeng",
            "Jianghao Yin",
            "Yinqi Zhang",
            "Qin Chen",
            "Jie Zhou",
            "Liang He",
            "Qipeng Guo",
            "Kai Chen",
            "Wei Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14620",
        "abstract": "Large language models (LLMs) make remarkable progress in reasoning tasks. Among different reasoning modes, inductive reasoning, due to its better alignment with human learning, attracts increasing interest. However, research on inductive reasoning faces certain challenges. First, existing inductive data mostly focuses on superficial regularities while lacking more complex internal patterns. Second, current works merely prompt LLMs or finetune on simple prompt-response pairs, but do not provide precise thinking processes nor implement difficulty control. Unlike previous work, we address these challenges by introducing \\textit{CodeSeq}, a synthetic post-training dataset built from number sequences. We package number sequences into algorithmic problems to discover their general terms, defining a general term generation (GTG) task correspondingly. Our pipeline generates supervised finetuning data by reflecting on failed test cases and incorporating iterative corrections, thereby teaching LLMs to learn autonomous case generation and self-checking. Additionally, it leverages reinforcement learning with a novel Case-Synergy Solvability Scaling Reward based on both solvability, estimated from the problem pass rate, and the success rate of self-directed case generation, enabling models to learn more effectively from both successes and failures. Experimental results show that the models trained with \\textit{CodeSeq} improve on various reasoning tasks and can preserve the models' OOD performance.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "202",
        "title": "ColorBench: Benchmarking Mobile Agents with Graph-Structured Framework for Complex Long-Horizon Tasks",
        "author": [
            "Yuanyi Song",
            "Heyuan Huang",
            "Qiqiang Lin",
            "Yin Zhao",
            "Xiangmou Qu",
            "Jun Wang",
            "Xingyu Lou",
            "Weiwen Liu",
            "Zhuosheng Zhang",
            "Jun Wang",
            "Yong Yu",
            "Weinan Zhang",
            "Zhaoxiang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14621",
        "abstract": "The rapid advancement of multimodal large language models has enabled agents to operate mobile devices by directly interacting with graphical user interfaces, opening new possibilities for mobile automation. However, real-world mobile tasks are often complex and allow for multiple valid solutions. This contradicts current mobile agent evaluation standards: offline static benchmarks can only validate a single predefined \"golden path\", while online dynamic testing is constrained by the complexity and non-reproducibility of real devices, making both approaches inadequate for comprehensively assessing agent capabilities. To bridge the gap between offline and online evaluation and enhance testing stability, this paper introduces a novel graph-structured benchmarking framework. By modeling the finite states observed during real-device interactions, it achieves static simulation of dynamic behaviors. Building on this, we develop ColorBench, a benchmark focused on complex long-horizon tasks. It supports evaluation of multiple valid solutions, subtask completion rate statistics, and atomic-level capability analysis. ColorBench contains 175 tasks (74 single-app, 101 cross-app) with an average length of over 13 steps. Each task includes at least two correct paths and several typical error paths, enabling quasi-dynamic interaction. By evaluating ColorBench across various baselines, we discover limitations of existing models and propose improvement directions and feasible technical pathways to enhance agents' performance on complex, long-horizon problems based on experimental results. Code and data are available at: https://github.com/MadeAgents/ColorBench.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "203",
        "title": "LeapFactual: Reliable Visual Counterfactual Explanation Using Conditional Flow Matching",
        "author": [
            "Zhuo Cao",
            "Xuan Zhao",
            "Lena Krieger",
            "Hanno Scharr",
            "Ira Assent"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14623",
        "abstract": "The growing integration of machine learning (ML) and artificial intelligence (AI) models into high-stakes domains such as healthcare and scientific research calls for models that are not only accurate but also interpretable. Among the existing explainable methods, counterfactual explanations offer interpretability by identifying minimal changes to inputs that would alter a model's prediction, thus providing deeper insights. However, current counterfactual generation methods suffer from critical limitations, including gradient vanishing, discontinuous latent spaces, and an overreliance on the alignment between learned and true decision boundaries. To overcome these limitations, we propose LeapFactual, a novel counterfactual explanation algorithm based on conditional flow matching. LeapFactual generates reliable and informative counterfactuals, even when true and learned decision boundaries diverge. Following a model-agnostic approach, LeapFactual is not limited to models with differentiable loss functions. It can even handle human-in-the-loop systems, expanding the scope of counterfactual explanations to domains that require the participation of human annotators, such as citizen science. We provide extensive experiments on benchmark and real-world datasets showing that LeapFactual generates accurate and in-distribution counterfactual explanations that offer actionable insights. We observe, for instance, that our reliable counterfactual samples with labels aligning to ground truth can be beneficially used as new training data to enhance the model. The proposed method is broadly applicable and enhances both scientific knowledge discovery and non-expert interpretability.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "204",
        "title": "Efficient Video Sampling: Pruning Temporally Redundant Tokens for Faster VLM Inference",
        "author": [
            "Natan Bagrov",
            "Eugene Khvedchenia",
            "Borys Tymchenko",
            "Shay Aharon",
            "Lior Kadoch",
            "Tomer Keren",
            "Ofri Masad",
            "Yonatan Geifman",
            "Ran Zilberstein",
            "Tuomas Rintamaki",
            "Matthieu Le",
            "Andrew Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14624",
        "abstract": "Vision-language models (VLMs) have recently expanded from static image understanding to video reasoning, but their scalability is fundamentally limited by the quadratic cost of processing dense frame sequences. Long videos often exceed the token budget of modern language models, leading to severe context limitations and latency issues. We introduce Efficient Video Sampling (EVS), a simple, plug-and-play method for reducing token redundancy in videos by identifying and pruning temporally static patches -- spatial regions that remain unchanged across consecutive frames. EVS preserves positional identity, requires no architectural changes or retraining. We show that EVS substantially reduces token count while maintaining semantic fidelity, enabling faster inference and longer input sequences. Applied at inference time, EVS reduces large language model (LLM) time-to-first-token (TTFT) by up to 4x with minimal accuracy loss. When combined with an uptraining phase using stochastic pruning rates, EVS yields models that are robust to varying compression levels and retain full performance under aggressive pruning. Extensive experiments demonstrate that EVS consistently improves efficiency-accuracy trade-offs, unlocking scalable video-language understanding without sacrificing quality.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "205",
        "title": "GOPLA: Generalizable Object Placement Learning via Synthetic Augmentation of Human Arrangement",
        "author": [
            "Yao Zhong",
            "Hanzhi Chen",
            "Simon Schaefer",
            "Anran Zhang",
            "Stefan Leutenegger"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14627",
        "abstract": "Robots are expected to serve as intelligent assistants, helping humans with everyday household organization. A central challenge in this setting is the task of object placement, which requires reasoning about both semantic preferences (e.g., common-sense object relations) and geometric feasibility (e.g., collision avoidance). We present GOPLA, a hierarchical framework that learns generalizable object placement from augmented human demonstrations. A multi-modal large language model translates human instructions and visual inputs into structured plans that specify pairwise object relationships. These plans are then converted into 3D affordance maps with geometric common sense by a spatial mapper, while a diffusion-based planner generates placement poses guided by test-time costs, considering multi-plan distributions and collision avoidance. To overcome data scarcity, we introduce a scalable pipeline that expands human placement demonstrations into diverse synthetic training data. Extensive experiments show that our approach improves placement success rates by 30.04 percentage points over the runner-up, evaluated on positioning accuracy and physical plausibility, demonstrating strong generalization across a wide range of real-world robotic placement scenarios.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "206",
        "title": "RLAIF-SPA: Optimizing LLM-based Emotional Speech Synthesis via RLAIF",
        "author": [
            "Qing Yang",
            "Zhenghao Liu",
            "Junxin Wang",
            "Yangfan Du",
            "Pengcheng Huang",
            "Tong Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14628",
        "abstract": "Text-To-Speech synthesis has achieved near-human quality in neutral speech, but emotional expressiveness remains a challenge. Existing methods often rely on costly emotion annotations or optimize indirect objectives that fail to capture the emotional expressiveness and perceptual naturalness of speech, leading to generated speech that is accurate but emotionally flat. To address these challenges, we propose the RLAIF-SPA framework, incorporating a Reinforcement Learning from AI Feedback (RLAIF) mechanism to employ Automatic Speech Recognition (ASR) and Large Language Model (LLM) techniques to respectively judge semantic accuracy and prosodic-emotional label alignment as a direct reward for emotional expressiveness and intelligibility optimization. Specifically, it leverages Prosodic Label Alignment to enhance expressive quality by jointly considering semantic accuracy and prosodic-emotional alignment along four fine-grained dimensions: Structure, Emotion, Speed, and Tone. In addition, it incorporates Semantic Accuracy Feedback to ensure the generation of clear and accurate speech. Experiments on the Libri Speech dataset show that RLAIF-SPA outperforms Chat-TTS, with a 26.1% reduction in WER, a 9.1% increase in SIM-O, and over 10% improvement in human evaluation.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "207",
        "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation",
        "author": [
            "Ming Gui",
            "Johannes Schusterbauer",
            "Timy Phan",
            "Felix Krause",
            "Josh Susskind",
            "Miguel Angel Bautista",
            "BjÃ¶rn Ommer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14630",
        "abstract": "We introduce Representation Tokenizer (RepTok), a generative modeling framework that represents an image using a single continuous latent token obtained from self-supervised vision transformers. Building on a pre-trained SSL encoder, we fine-tune only the semantic token embedding and pair it with a generative decoder trained jointly using a standard flow matching objective. This adaptation enriches the token with low-level, reconstruction-relevant details, enabling faithful image reconstruction. To preserve the favorable geometry of the original SSL space, we add a cosine-similarity loss that regularizes the adapted token, ensuring the latent space remains smooth and suitable for generation. Our single-token formulation resolves spatial redundancies of 2D latent spaces and significantly reduces training costs. Despite its simplicity and efficiency, RepTok achieves competitive results on class-conditional ImageNet generation and naturally extends to text-to-image synthesis, reaching competitive zero-shot performance on MS-COCO under extremely limited training budgets. Our findings highlight the potential of fine-tuned SSL representations as compact and effective latent spaces for efficient generative modeling.",
        "tags": [
            "Flow Matching",
            "Text-to-Image"
        ]
    },
    {
        "id": "208",
        "title": "SteeringTTA: Guiding Diffusion Trajectories for Robust Test-Time-Adaptation",
        "author": [
            "Jihyun Yu",
            "Yoojin Oh",
            "Wonho Bae",
            "Mingyu Kim",
            "Junhyug Noh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14634",
        "abstract": "Test-time adaptation (TTA) aims to correct performance degradation of deep models under distribution shifts by updating models or inputs using unlabeled test data. Input-only diffusion-based TTA methods improve robustness for classification to corruptions but rely on gradient guidance, limiting exploration and generalization across distortion types. We propose SteeringTTA, an inference-only framework that adapts Feynman-Kac steering to guide diffusion-based input adaptation for classification with rewards driven by pseudo-label. SteeringTTA maintains multiple particle trajectories, steered by a combination of cumulative top-K probabilities and an entropy schedule, to balance exploration and confidence. On ImageNet-C, SteeringTTA consistently outperforms the baseline without any model updates or source data.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "209",
        "title": "ATGen: Adversarial Reinforcement Learning for Test Case Generation",
        "author": [
            "Qingyao Li",
            "Xinyi Dai",
            "Weiwen Liu",
            "Xiangyang Li",
            "Yasheng Wang",
            "Ruiming Tang",
            "Yong Yu",
            "Weinan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14635",
        "abstract": "Large Language Models (LLMs) excel at code generation, yet their outputs often contain subtle bugs, for which effective test cases are a critical bottleneck. Existing test generation methods, whether based on prompting or supervised fine-tuning, rely on static datasets. This imposes a ``fixed-difficulty ceiling'', fundamentally limiting their ability to uncover novel or more complex bugs beyond their training scope. To overcome this, we introduce ATGen, a framework that trains a test case generator via adversarial reinforcement learning. ATGen pits a test generator against an adversarial code generator that continuously crafts harder bugs to evade the current policy. This dynamic loop creates a curriculum of increasing difficulty challenging current policy. The test generator is optimized via Reinforcement Learning (RL) to jointly maximize ``Output Accuracy'' and ``Attack Success'', enabling it to learn a progressively stronger policy that breaks the fixed-difficulty ceiling of static training. Extensive experiments demonstrate that ATGen significantly outperforms state-of-the-art baselines. We further validate its practical utility, showing it serves as both a more effective filter for Best-of-N inference and a higher-quality reward source for training code generation models. Our work establishes a new, dynamic paradigm for improving the reliability of LLM-generated code.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "210",
        "title": "Improving Cybercrime Detection and Digital Forensics Investigations with Artificial Intelligence",
        "author": [
            "Silvia Lucia Sanna",
            "Leonardo Regano",
            "Davide Maiorca",
            "Giorgio Giacinto"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14638",
        "abstract": "According to a recent EUROPOL report, cybercrime is still recurrent in Europe, and different activities and countermeasures must be taken to limit, prevent, detect, analyze, and fight it. Cybercrime must be prevented with specific measures, tools, and techniques, for example through automated network and malware analysis. Countermeasures against cybercrime can also be improved with proper \\df analysis in order to extract data from digital devices trying to retrieve information on the cybercriminals. Indeed, results obtained through a proper \\df analysis can be leveraged to train cybercrime detection systems to prevent the success of similar crimes. Nowadays, some systems have started to adopt Artificial Intelligence (AI) algorithms for cyberattack detection and \\df analysis improvement. However, AI can be better applied as an additional instrument in these systems to improve the detection and in the \\df analysis. For this reason, we highlight how cybercrime analysis and \\df procedures can take advantage of AI. On the other hand, cybercriminals can use these systems to improve their skills, bypass automatic detection, and develop advanced attack techniques. The case study we presented highlights how it is possible to integrate the use of the three popular chatbots {\\tt Gemini}, {\\tt Copilot} and {\\tt chatGPT} to develop a Python code to encode and decoded images with steganographic technique, even though their presence is not an indicator of crime, attack or maliciousness but used by a cybercriminal as anti-forensics technique.",
        "tags": [
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "211",
        "title": "Intent Clustering with Shared Pseudo-Labels",
        "author": [
            "I-Fan Lin",
            "Faegheh Hasibi",
            "Suzan Verberne"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14640",
        "abstract": "In this paper, we propose an intuitive, training-free and label-free method for intent clustering that makes minimal assumptions using lightweight and open-source LLMs. Many current approaches rely on commercial LLMs, which are costly, and offer limited transparency. Additionally, their methods often explicitly depend on knowing the number of clusters in advance, which is often not the case in realistic settings. To address these challenges, instead of asking the LLM to match similar text directly, we first ask it to generate pseudo-labels for each text, and then perform multi-label classification in this pseudo-label set for each text. This approach is based on the hypothesis that texts belonging to the same cluster will share more labels, and will therefore be closer when encoded into embeddings. These pseudo-labels are more human-readable than direct similarity matches. Our evaluation on four benchmark sets shows that our approach achieves results comparable to and better than recent baselines, while remaining simple and computationally efficient. Our findings indicate that our method can be applied in low-resource scenarios and is stable across multiple models and datasets.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "212",
        "title": "The Bidding Games: Reinforcement Learning for MEV Extraction on Polygon Blockchain",
        "author": [
            "Andrei Seoev",
            "Leonid Gremyachikh",
            "Anastasiia Smirnova",
            "Yash Madhwal",
            "Alisa Kalacheva",
            "Dmitry Belousov",
            "Ilia Zubov",
            "Aleksei Smirnov",
            "Denis Fedyanin",
            "Vladimir Gorgadze",
            "Yury Yanovich"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14642",
        "abstract": "In blockchain networks, the strategic ordering of transactions within blocks has emerged as a significant source of profit extraction, known as Maximal Extractable Value (MEV). The transition from spam-based Priority Gas Auctions to structured auction mechanisms like Polygon Atlas has transformed MEV extraction from public bidding wars into sealed-bid competitions under extreme time constraints. While this shift reduces network congestion, it introduces complex strategic challenges where searchers must make optimal bidding decisions within a sub-second window without knowledge of competitor behavior or presence. Traditional game-theoretic approaches struggle in this high-frequency, partially observable environment due to their reliance on complete information and static equilibrium assumptions. We present a reinforcement learning framework for MEV extraction on Polygon Atlas and make three contributions: (1) A novel simulation environment that accurately models the stochastic arrival of arbitrage opportunities and probabilistic competition in Atlas auctions; (2) A PPO-based bidding agent optimized for real-time constraints, capable of adaptive strategy formulation in continuous action spaces while maintaining production-ready inference speeds; (3) Empirical validation demonstrating our history-conditioned agent captures 49\\% of available profits when deployed alongside existing searchers and 81\\% when replacing the market leader, significantly outperforming static bidding strategies. Our work establishes that reinforcement learning provides a critical advantage in high-frequency MEV environments where traditional optimization methods fail, offering immediate value for industrial participants and protocol designers alike.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "213",
        "title": "Generative Models From and For Sampling-Based MPC: A Bootstrapped Approach For Adaptive Contact-Rich Manipulation",
        "author": [
            "Lara BrudermÃ¼ller",
            "Brandon Hung",
            "Xinghao Zhu",
            "Jiuguang Wang",
            "Nick Hawes",
            "Preston Culbertson",
            "Simon Le Cleac'h"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14643",
        "abstract": "We present a generative predictive control (GPC) framework that amortizes sampling-based Model Predictive Control (SPC) by bootstrapping it with conditional flow-matching models trained on SPC control sequences collected in simulation. Unlike prior work relying on iterative refinement or gradient-based solvers, we show that meaningful proposal distributions can be learned directly from noisy SPC data, enabling more efficient and informed sampling during online planning. We further demonstrate, for the first time, the application of this approach to real-world contact-rich loco-manipulation with a quadruped robot. Extensive experiments in simulation and on hardware show that our method improves sample efficiency, reduces planning horizon requirements, and generalizes robustly across task variations.",
        "tags": [
            "Flow Matching",
            "MPC",
            "Robotics"
        ]
    },
    {
        "id": "214",
        "title": "In-Context Learning with Unpaired Clips for Instruction-based Video Editing",
        "author": [
            "Xinyao Liao",
            "Xianfang Zeng",
            "Ziye Song",
            "Zhoujie Fu",
            "Gang Yu",
            "Guosheng Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14648",
        "abstract": "Despite the rapid progress of instruction-based image editing, its extension to video remains underexplored, primarily due to the prohibitive cost and complexity of constructing large-scale paired video editing datasets. To address this challenge, we introduce a low-cost pretraining strategy for instruction-based video editing that leverages in-context learning from unpaired video clips. We show that pretraining a foundation video generation model with this strategy endows it with general editing capabilities, such as adding, replacing, or deleting operations, according to input editing instructions. The pretrained model can then be efficiently refined with a small amount of high-quality paired editing data. Built upon HunyuanVideoT2V, our framework first pretrains on approximately 1M real video clips to learn basic editing concepts, and subsequently fine-tunes on fewer than 150k curated editing pairs to extend more editing tasks and improve the editing quality. Comparative experiments show that our method surpasses existing instruction-based video editing approaches in both instruction alignment and visual fidelity, achieving a 12\\% improvement in editing instruction following and a 15\\% improvement in editing quality.",
        "tags": [
            "Image Editing",
            "Video Editing",
            "Video Generation"
        ]
    },
    {
        "id": "215",
        "title": "Decorrelation Speeds Up Vision Transformers",
        "author": [
            "Kieran Carrigg",
            "Rob van Gastel",
            "Melda Yeghaian",
            "Sander Dalm",
            "Faysal Boughorbel",
            "Marcel van Gerven"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14657",
        "abstract": "Masked Autoencoder (MAE) pre-training of vision transformers (ViTs) yields strong performance in low-label regimes but comes with substantial computational costs, making it impractical in time- and resource-constrained industrial settings. We address this by integrating Decorrelated Backpropagation (DBP) into MAE pre-training, an optimization method that iteratively reduces input correlations at each layer to accelerate convergence. Applied selectively to the encoder, DBP achieves faster pre-training without loss of stability. On ImageNet-1K pre-training with ADE20K fine-tuning, DBP-MAE reduces wall-clock time to baseline performance by 21.1%, lowers carbon emissions by 21.4% and improves segmentation mIoU by 1.1 points. We observe similar gains when pre-training and fine-tuning on proprietary industrial data, confirming the method's applicability in real-world scenarios. These results demonstrate that DBP can reduce training time and energy use while improving downstream performance for large-scale ViT pre-training.",
        "tags": [
            "Segmentation",
            "ViT"
        ]
    },
    {
        "id": "216",
        "title": "An Efficient Rubric-based Generative Verifier for Search-Augmented LLMs",
        "author": [
            "Linyue Ma",
            "Yilong Xu",
            "Xiang Long",
            "Zhi Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14660",
        "abstract": "Search augmentation empowers Large Language Models with retrieval capabilities to overcome the limitations imposed by static parameters. Recently, Reinforcement Learning leverages tailored reward signals as a viable technique to enhance LLMs performing tasks involving search. However, existing reward modeling for search-augmented LLMs faces several limitations. Rule-based rewards, such as Exact Match, are verifiable but fragile to variations in expression and cannot be applied to long-form workloads. In contrast, generative rewards improve robustness, but designing verifiable and stable rewards for long-form workloads in dynamic corpora remains challenging and also incurs high computational costs. In this paper, we propose a unified and verifiable paradigm, \"nugget-as-rubric\", which treats atomic information points as structured evaluation criteria for different search-augmentation workloads. Short-form tasks correspond to a single rubric, whereas long-form tasks expand to multiple rubrics aligned with the question's information needs. To support long-form settings, we design an automatic rubric construction pipeline based on query rewriting, which can automatically retrieve passages relevant to each question and extract rubrics from them, both from static corpora and from dynamic online web content. Furthermore, we introduce \\textbf{Search-Gen-V}, a 4B-parameter efficient generative verifier under our proposed verifiable paradigm, which is trained via the idea of distillation and a two-stage strategy. Experimental results show that Search-Gen-V achieves strong verification accuracy across different workloads, making it a scalable, robust, and efficient verifiable reward constructor for search-augmented LLMs.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "217",
        "title": "SpeechLLM-as-Judges: Towards General and Interpretable Speech Quality Evaluation",
        "author": [
            "Hui Wang",
            "Jinghua Zhao",
            "Yifan Yang",
            "Shujie Liu",
            "Junyang Chen",
            "Yanzhe Zhang",
            "Shiwan Zhao",
            "Jinyu Li",
            "Jiaming Zhou",
            "Haoqin Sun",
            "Yan Lu",
            "Yong Qin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14664",
        "abstract": "Generative speech technologies are progressing rapidly, but evaluating the perceptual quality of synthetic speech remains a core challenge. Existing methods typically rely on scalar scores or binary decisions, which lack interpretability and generalization across tasks and languages. We present SpeechLLM-as-Judges, a new paradigm for enabling large language models (LLMs) to conduct structured and explanation-based speech quality evaluation. To support this direction, we introduce SpeechEval, a large-scale dataset containing 32,207 multilingual speech clips and 128,754 annotations spanning four tasks: quality assessment, pairwise comparison, improvement suggestion, and deepfake detection. Based on this resource, we develop SQ-LLM, a speech-quality-aware LLM trained with chain-of-thought reasoning and reward optimization to improve capability. Experimental results show that SQ-LLM delivers strong performance across tasks and languages, revealing the potential of this paradigm for advancing speech quality evaluation. Relevant resources will be open-sourced.",
        "tags": [
            "CoT",
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "218",
        "title": "Beyond Hallucinations: The Illusion of Understanding in Large Language Models",
        "author": [
            "Rikard Rosenbacke",
            "Carl Rosenbacke",
            "Victor Rosenbacke",
            "Martin McKee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14665",
        "abstract": "Large language models (LLMs) are becoming deeply embedded in human communication and decision-making, yet they inherit the ambiguity, bias, and lack of direct access to truth inherent in language itself. While their outputs are fluent, emotionally resonant, and coherent, they are generated through statistical prediction rather than grounded reasoning. This creates the risk of hallucination, responses that sound convincing but lack factual validity. Building on Geoffrey Hinton's observation that AI mirrors human intuition rather than reasoning, this paper argues that LLMs operationalize System 1 cognition at scale: fast, associative, and persuasive, but without reflection or falsification. To address this, we introduce the Rose-Frame, a three-dimensional framework for diagnosing cognitive and epistemic drift in human-AI interaction. The three axes are: (i) Map vs. Territory, which distinguishes representations of reality (epistemology) from reality itself (ontology); (ii) Intuition vs. Reason, drawing on dual-process theory to separate fast, emotional judgments from slow, reflective thinking; and (iii) Conflict vs. Confirmation, which examines whether ideas are critically tested through disagreement or simply reinforced through mutual validation. Each dimension captures a distinct failure mode, and their combination amplifies misalignment. Rose-Frame does not attempt to fix LLMs with more data or rules. Instead, it offers a reflective tool that makes both the model's limitations and the user's assumptions visible, enabling more transparent and critically aware AI deployment. It reframes alignment as cognitive governance: intuition, whether human or artificial, must remain governed by human reason. Only by embedding reflective, falsifiable oversight can we align machine fluency with human understanding.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "219",
        "title": "TITAN: Graph-Executable Reasoning for Cyber Threat Intelligence",
        "author": [
            "Marco Simoni",
            "Aleksandar Fontana",
            "Andrea Saracino",
            "Paolo Mori"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14670",
        "abstract": "TITAN (Threat Intelligence Through Automated Navigation) is a framework that connects natural-language cyber threat queries with executable reasoning over a structured knowledge graph. It integrates a path planner model, which predicts logical relation chains from text, and a graph executor that traverses the TITAN Ontology to retrieve factual answers and supporting evidence. Unlike traditional retrieval systems, TITAN operates on a typed, bidirectional graph derived from MITRE, allowing reasoning to move clearly and reversibly between threats, behaviors, and defenses. To support training and evaluation, we introduce the TITAN Dataset, a corpus of 88209 examples (Train: 74258; Test: 13951) pairing natural language questions with executable reasoning paths and step by step Chain of Thought explanations. Empirical evaluations show that TITAN enables models to generate syntactically valid and semantically coherent reasoning paths that can be deterministically executed on the underlying graph.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "220",
        "title": "VTimeCoT: Thinking by Drawing for Video Temporal Grounding and Reasoning",
        "author": [
            "Jinglei Zhang",
            "Yuanfan Guo",
            "Rolandos Alexandros Potamias",
            "Jiankang Deng",
            "Hang Xu",
            "Chao Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14672",
        "abstract": "In recent years, video question answering based on multimodal large language models (MLLM) has garnered considerable attention, due to the benefits from the substantial advancements in LLMs. However, these models have a notable deficiency in the domains of video temporal grounding and reasoning, posing challenges to the development of effective real-world video understanding systems. Inspired by how humans use video players to interact with the progress bar for video comprehension, we introduce VTimeCoT, a simple yet effective training-free framework, designed for high-performance video grounding and reasoning. The proposed framework incorporates two novel visual tools of the progress bar: a plug-and-play progress bar integration tool and a high-efficiency highlighting tool. In addition, to address the limitations of conventional text-based chain-of-thought (CoT) approaches, we introduce a visuotemporal CoT process that integrates cross-modality reasoning across both video and text. Our approach demonstrates significant performance improvements on both Qwen2VL-7B and GPT4o baselines in tasks of video temporal grounding and reasoning-based question answering. Finally, we showcase that the proposed framework achieves a compositional and interpretable reasoning process. Project page: https://vtimecot.github.io",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "221",
        "title": "xLLM Technical Report",
        "author": [
            "Tongxuan Liu",
            "Tao Peng",
            "Peijun Yang",
            "Xiaoyang Zhao",
            "Xiusheng Lu",
            "Weizhe Huang",
            "Zirui Liu",
            "Xiaoyu Chen",
            "Zhiwei Liang",
            "Jun Xiong",
            "Donghe Jin",
            "Minchao Zhang",
            "Jinrong Guo",
            "Yingxu Deng",
            "Xu Zhang",
            "Xianzhe Dong",
            "Siqi Wang",
            "Siyu Wu",
            "Yu Wu",
            "Zihan Tang",
            "Yuting Zeng",
            "Yanshu Wang",
            "Jinguang Liu",
            "Meng Kang",
            "Menxin Li",
            "Yunlong Wang",
            "Yiming Liu",
            "Xiaolong Ma",
            "Yifan Wang",
            "Yichen Zhang",
            "Jinrun Yin",
            "Keyang Zheng",
            "Jiawei Yin",
            "Jun Zhang",
            "Ziyue Wang",
            "Xiaobo Lin",
            "Liangyu Liu",
            "Liwei Lan",
            "Yang Liu",
            "Chunhua Peng",
            "Han Liu",
            "Songcheng Ren",
            "Xuezhu Wang",
            "Yunheng Shen",
            "Yi Wang",
            "Guyue Liu",
            "Hui Chen",
            "Tong Yang",
            "Hailong Yang",
            "Jing Li",
            "Guiguang Ding",
            "Ke Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14686",
        "abstract": "We introduce xLLM, an intelligent and efficient Large Language Model (LLM) inference framework designed for high-performance, large-scale enterprise-grade serving, with deep optimizations for diverse AI accelerators. To address these challenges, xLLM builds a novel decoupled service-engine architecture. At the service layer, xLLM-Service features an intelligent scheduling module that efficiently processes multimodal requests and co-locates online and offline tasks through unified elastic scheduling to maximize cluster utilization. This module also relies on a workload-adaptive dynamic Prefill-Decode (PD) disaggregation policy and a novel Encode-Prefill-Decode (EPD) disaggregation policy designed for multimodal inputs. Furthermore, it incorporates a distributed architecture to provide global KV Cache management and robust fault-tolerant capabilities for high availability. At the engine layer, xLLM-Engine co-optimizes system and algorithm designs to fully saturate computing resources. This is achieved through comprehensive multi-layer execution pipeline optimizations, an adaptive graph mode and an xTensor memory management. xLLM-Engine also further integrates algorithmic enhancements such as optimized speculative decoding and dynamic EPLB, collectively serving to substantially boost throughput and inference efficiency. Extensive evaluations demonstrate that xLLM delivers significantly superior performance and resource efficiency. Under identical TPOT constraints, xLLM achieves throughput up to 1.7x that of MindIE and 2.2x that of vLLM-Ascend with Qwen-series models, while maintaining an average throughput of 1.7x that of MindIE with Deepseek-series models. xLLM framework is publicly available at https://github.com/jd-opensource/xllm and https://github.com/jd-opensource/xllm-service.",
        "tags": [
            "DeepSeek",
            "LLM",
            "Qwen"
        ]
    },
    {
        "id": "222",
        "title": "LLM Agents for Automated Web Vulnerability Reproduction: Are We There Yet?",
        "author": [
            "Bin Liu",
            "Yanjie Zhao",
            "Guoai Xu",
            "Haoyu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14700",
        "abstract": "Large language model (LLM) agents have demonstrated remarkable capabilities in software engineering and cybersecurity tasks, including code generation, vulnerability discovery, and automated testing. One critical but underexplored application is automated web vulnerability reproduction, which transforms vulnerability reports into working exploits. Although recent advances suggest promising potential, challenges remain in applying LLM agents to real-world web vulnerability reproduction scenarios. In this paper, we present the first comprehensive evaluation of state-of-the-art LLM agents for automated web vulnerability reproduction. We systematically assess 20 agents from software engineering, cybersecurity, and general domains across 16 dimensions, including technical capabilities, environment adaptability, and user experience factors, on 3 representative web vulnerabilities. Based on the results, we select three top-performing agents (OpenHands, SWE-agent, and CAI) for in-depth evaluation on our benchmark dataset of 80 real-world CVEs spanning 7 vulnerability types and 6 web technologies. Our results reveal that while LLM agents achieve reasonable success on simple library-based vulnerabilities, they consistently fail on complex service-based vulnerabilities requiring multi-component environments. Complex environment configurations and authentication barriers create a gap where agents can execute exploit code but fail to trigger actual vulnerabilities. We observe high sensitivity to input guidance, with performance degrading by over 33% under incomplete authentication information. Our findings highlight the significant gap between current LLM agent capabilities and the demands of reliable automated vulnerability reproduction, emphasizing the need for advances in environmental adaptation and autonomous problem-solving capabilities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "223",
        "title": "ToolPRM: Fine-Grained Inference Scaling of Structured Outputs for Function Calling",
        "author": [
            "Jianghao Lin",
            "Yuanyuan Shi",
            "Xin Peng",
            "Renjie Ding",
            "Hairui Wang",
            "Yuxuan Peng",
            "Bizhe Bai",
            "Weixi Song",
            "Fengshuo Bai",
            "Huacan Chai",
            "Weinan Zhang",
            "Fei Huang",
            "Ying Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14703",
        "abstract": "Large language models (LLMs) are increasingly demonstrating strong capabilities as autonomous agents, with function calling serving as a core mechanism for interaction with the environment. Meanwhile, inference scaling has become a cutting-edge technique to enhance LLM performance by allocating more computational resources during the inference process. However, current research on inference scaling primarily focuses on unstructured output generation tasks, leaving its application in structured outputs, like function calling, largely underexplored. To bridge this gap, we propose an inference scaling framework that combines fine-grained beam search with a process reward model, ToolPRM, which scores the internal steps of each single function call. To train ToolPRM, we construct the first fine-grained intra-call process supervision dataset, automatically annotated with function-masking techniques to provide step-level rewards for structured tool-use reasoning. Extensive experiments demonstrate that ToolPRM beats the coarse-grained and outcome reward models in terms of predictive accuracy, indicating its stronger capability in supervising the function calling inference process. Inference scaling technique equipped with ToolPRM also significantly improves the backbone model performance across various function calling tasks and benchmarks. More importantly, we reveal a key principle for applying inference scaling techniques to structured outputs: \"explore more but retain less\" due to the unrecoverability characteristics of structured function calling generation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "224",
        "title": "Leveraging Learned Image Prior for 3D Gaussian Compression",
        "author": [
            "Seungjoo Shin",
            "Jaesik Park",
            "Sunghyun Cho"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14705",
        "abstract": "Compression techniques for 3D Gaussian Splatting (3DGS) have recently achieved considerable success in minimizing storage overhead for 3D Gaussians while preserving high rendering quality. Despite the impressive storage reduction, the lack of learned priors restricts further advances in the rate-distortion trade-off for 3DGS compression tasks. To address this, we introduce a novel 3DGS compression framework that leverages the powerful representational capacity of learned image priors to recover compression-induced quality degradation. Built upon initially compressed Gaussians, our restoration network effectively models the compression artifacts in the image space between degraded and original Gaussians. To enhance the rate-distortion performance, we provide coarse rendering residuals into the restoration network as side information. By leveraging the supervision of restored images, the compressed Gaussians are refined, resulting in a highly compact representation with enhanced rendering performance. Our framework is designed to be compatible with existing Gaussian compression methods, making it broadly applicable across different baselines. Extensive experiments validate the effectiveness of our framework, demonstrating superior rate-distortion performance and outperforming the rendering quality of state-of-the-art 3DGS compression methods while requiring substantially less storage.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "225",
        "title": "Camera Movement Classification in Historical Footage: A Comparative Study of Deep Video Models",
        "author": [
            "Tingyu Lin",
            "Armin Dadras",
            "Florian Kleber",
            "Robert Sablatnig"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14713",
        "abstract": "Camera movement conveys spatial and narrative information essential for understanding video content. While recent camera movement classification (CMC) methods perform well on modern datasets, their generalization to historical footage remains unexplored. This paper presents the first systematic evaluation of deep video CMC models on archival film material. We summarize representative methods and datasets, highlighting differences in model design and label definitions. Five standard video classification models are assessed on the HISTORIAN dataset, which includes expert-annotated World War II footage. The best-performing model, Video Swin Transformer, achieves 80.25% accuracy, showing strong convergence despite limited training data. Our findings highlight the challenges and potential of adapting existing models to low-quality video and motivate future work combining diverse input modalities and temporal architectures.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "226",
        "title": "Tawa: Automatic Warp Specialization for Modern GPUs with Asynchronous References",
        "author": [
            "Hongzheng Chen",
            "Bin Fan",
            "Alexander Collins",
            "Bastian Hagedorn",
            "Evghenii Gaburov",
            "Masahiro Masuda",
            "Matthew Brookhart",
            "Chris Sullivan",
            "Jason Knight",
            "Zhiru Zhang",
            "Vinod Grover"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14719",
        "abstract": "Modern GPUs feature specialized hardware units that enable high-performance, asynchronous dataflow execution. However, the conventional SIMT programming model is fundamentally misaligned with this task-parallel hardware, creating a significant programmability gap. While hardware-level warp specialization is the key to unlocking peak performance, it forces developers to manually orchestrate complex, low-level communication and software pipelines--a process that is labor-intensive, error-prone, and unsustainable. To address this challenge, we present Tawa, an automated compiler that systematically generates high-performance, warp-specialized code from a high-level, tile-based program. Central to our approach is a novel IR abstraction, asynchronous references (aref), which expresses warp-level communication without exposing low-level hardware details. Using this abstraction, Tawa automatically partitions programs into producer-consumer roles and manages the intricate dataflow pipeline, relieving developers of invasive kernel rewriting. Evaluation on NVIDIA H100 GPUs across representative LLM kernels shows that Tawa delivers high hardware utilization, achieving up to 1.1$\\times$ speedup over highly optimized cuBLAS GEMM kernels. For attention workloads, Tawa attains 1.2$\\times$ speedup over Triton and matches the performance of the hand-optimized CUTLASS C++ FlashAttention-3 kernel with far less programming effort.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "227",
        "title": "Cross-Layer Feature Self-Attention Module for Multi-Scale Object Detection",
        "author": [
            "Dingzhou Xie",
            "Rushi Lan",
            "Cheng Pang",
            "Enhao Ning",
            "Jiahao Zeng",
            "Wei Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14726",
        "abstract": "Recent object detection methods have made remarkable progress by leveraging attention mechanisms to improve feature discriminability. However, most existing approaches are confined to refining single-layer or fusing dual-layer features, overlooking the rich inter-layer dependencies across multi-scale representations. This limits their ability to capture comprehensive contextual information essential for detecting objects with large scale variations. In this paper, we propose a novel Cross-Layer Feature Self-Attention Module (CFSAM), which holistically models both local and global dependencies within multi-scale feature maps. CFSAM consists of three key components: a convolutional local feature extractor, a Transformer-based global modeling unit that efficiently captures cross-layer interactions, and a feature fusion mechanism to restore and enhance the original representations. When integrated into the SSD300 framework, CFSAM significantly boosts detection performance, achieving 78.6% mAP on PASCAL VOC (vs. 75.5% baseline) and 52.1% mAP on COCO (vs. 43.1% baseline), outperforming existing attention modules. Moreover, the module accelerates convergence during training without introducing substantial computational overhead. Our work highlights the importance of explicit cross-layer attention modeling in advancing multi-scale object detection.",
        "tags": [
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "228",
        "title": "The Pursuit of Diversity: Multi-Objective Testing of Deep Reinforcement Learning Agents",
        "author": [
            "Antony Bartlett",
            "Cynthia Liem",
            "Annibale Panichella"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14727",
        "abstract": "Testing deep reinforcement learning (DRL) agents in safety-critical domains requires discovering diverse failure scenarios. Existing tools such as INDAGO rely on single-objective optimization focused solely on maximizing failure counts, but this does not ensure discovered scenarios are diverse or reveal distinct error types. We introduce INDAGO-Nexus, a multi-objective search approach that jointly optimizes for failure likelihood and test scenario diversity using multi-objective evolutionary algorithms with multiple diversity metrics and Pareto front selection strategies. We evaluated INDAGO-Nexus on three DRL agents: humanoid walker, self-driving car, and parking agent. On average, INDAGO-Nexus discovers up to 83% and 40% more unique failures (test effectiveness) than INDAGO in the SDC and Parking scenarios, respectively, while reducing time-to-failure by up to 67% across all agents.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "229",
        "title": "Free-Grained Hierarchical Recognition",
        "author": [
            "Seulki Park",
            "Zilin Wang",
            "Stella X. Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14737",
        "abstract": "Hierarchical image classification predicts labels across a semantic taxonomy, but existing methods typically assume complete, fine-grained annotations, an assumption rarely met in practice. Real-world supervision varies in granularity, influenced by image quality, annotator expertise, and task demands; a distant bird may be labeled Bird, while a close-up reveals Bald eagle. We introduce ImageNet-F, a large-scale benchmark curated from ImageNet and structured into cognitively inspired basic, subordinate, and fine-grained levels. Using CLIP as a proxy for semantic ambiguity, we simulate realistic, mixed-granularity labels reflecting human annotation behavior. We propose free-grain learning, with heterogeneous supervision across instances. We develop methods that enhance semantic guidance via pseudo-attributes from vision-language models and visual guidance via semi-supervised learning. These, along with strong baselines, substantially improve performance under mixed supervision. Together, our benchmark and methods advance hierarchical classification under real-world constraints.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "230",
        "title": "AutoRubric-R1V: Rubric-Based Generative Rewards for Faithful Multimodal Reasoning",
        "author": [
            "Mengzhao Jia",
            "Zhihan Zhang",
            "Ignacio Cases",
            "Zheyuan Liu",
            "Meng Jiang",
            "Peng Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14738",
        "abstract": "Multimodal large language models (MLLMs) have rapidly advanced from perception tasks to complex multi-step reasoning, yet reinforcement learning with verifiable rewards (RLVR) often leads to spurious reasoning since only the final-answer correctness is rewarded. To address this limitation, we propose AutoRubric-R1V, a framework that integrates RLVR with process-level supervision through automatically collected rubric-based generative rewards. Our key innovation lies in a scalable self-aggregation method that distills consistent reasoning checkpoints from successful trajectories, enabling problem-specific rubric construction without human annotation or stronger teacher models. By jointly leveraging rubric-based and outcome rewards, AutoRubric-R1V achieves state-of-the-art performance on six multimodal reasoning benchmarks and substantially improves reasoning faithfulness in dedicated evaluations.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "231",
        "title": "DEXTER: Diffusion-Guided EXplanations with TExtual Reasoning for Vision Models",
        "author": [
            "Simone Carnemolla",
            "Matteo Pennisi",
            "Sarinda Samarasinghe",
            "Giovanni Bellitto",
            "Simone Palazzo",
            "Daniela Giordano",
            "Mubarak Shah",
            "Concetto Spampinato"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14741",
        "abstract": "Understanding and explaining the behavior of machine learning models is essential for building transparent and trustworthy AI systems. We introduce DEXTER, a data-free framework that employs diffusion models and large language models to generate global, textual explanations of visual classifiers. DEXTER operates by optimizing text prompts to synthesize class-conditional images that strongly activate a target classifier. These synthetic samples are then used to elicit detailed natural language reports that describe class-specific decision patterns and biases. Unlike prior work, DEXTER enables natural language explanation about a classifier's decision process without access to training data or ground-truth labels. We demonstrate DEXTER's flexibility across three tasks-activation maximization, slice discovery and debiasing, and bias explanation-each illustrating its ability to uncover the internal mechanisms of visual classifiers. Quantitative and qualitative evaluations, including a user study, show that DEXTER produces accurate, interpretable outputs. Experiments on ImageNet, Waterbirds, CelebA, and FairFaces confirm that DEXTER outperforms existing approaches in global model explanation and class-level bias reporting. Code is available at https://github.com/perceivelab/dexter.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "232",
        "title": "Beyond Multi-Token Prediction: Pretraining LLMs with Future Summaries",
        "author": [
            "Divyat Mahajan",
            "Sachin Goyal",
            "Badr Youbi Idrissi",
            "Mohammad Pezeshki",
            "Ioannis Mitliagkas",
            "David Lopez-Paz",
            "Kartik Ahuja"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14751",
        "abstract": "Next-token prediction (NTP) has driven the success of large language models (LLMs), but it struggles with long-horizon reasoning, planning, and creative writing, with these limitations largely attributed to teacher-forced training. Multi-token prediction (MTP) partially mitigates these issues by predicting several future tokens at once, but it mostly captures short-range dependencies and offers limited improvement. We propose future summary prediction (FSP), which trains an auxiliary head to predict a compact representation of the long-term future, preserving information relevant for long-form generations. We explore two variants of FSP: handcrafted summaries, for example, a bag of words summary of the future of the sequence, and learned summaries, which use embeddings produced by a reverse language model trained from right to left. Large-scale pretraining experiments (3B and 8B-parameter models) demonstrate that FSP provides improvements over both NTP and MTP across math, reasoning, and coding benchmarks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "233",
        "title": "Pluto: A Benchmark for Evaluating Efficiency of LLM-generated Hardware Code",
        "author": [
            "Manar Abdelatty",
            "Maryam Nouh",
            "Jacob K. Rosenstein",
            "Sherief Reda"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14756",
        "abstract": "Large Language Models (LLMs) are increasingly used to automate hardware design tasks, including the generation of Verilog code. While early benchmarks focus primarily on functional correctness, efficient hardware design demands additional optimization for synthesis metrics such as area, delay, and power. Existing benchmarks fall short in evaluating these aspects comprehensively: they often lack optimized baselines or testbenches for verification. To address these gaps, we present Pluto, a benchmark and evaluation framework designed to assess the efficiency of LLM-generated Verilog designs. Pluto presents a comprehensive evaluation set of 114 problems with self-checking testbenches and multiple Pareto-optimal reference implementations. Experimental results show that state-of-the-art LLMs can achieve high functional correctness, reaching 78.3\\% at pass@1, but their synthesis efficiency still lags behind expert-crafted implementations, with area efficiency of 63.8\\%, delay efficiency of 65.9\\%, and power efficiency of 64.0\\% at eff@1. This highlights the need for efficiency-aware evaluation frameworks such as Pluto to drive progress in hardware-focused LLM research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "234",
        "title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes",
        "author": [
            "Yunwen Li",
            "Shuangshuang Ying",
            "Xingwei Qu",
            "Xin Li",
            "Sheng Jin",
            "Minghao Liu",
            "Zhoufutu Wen",
            "Tianyu Zheng",
            "Xeron Du",
            "Qiguang Chen",
            "Jiajun Shi",
            "Wangchunshu Zhou",
            "Jiazhan Feng",
            "Wanjun Zhong",
            "Libo Qin",
            "Stephen Huang",
            "Wanxiang Che",
            "Chenghua Lin",
            "Eli Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14763",
        "abstract": "Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "235",
        "title": "Inpainting the Red Planet: Diffusion Models for the Reconstruction of Martian Environments in Virtual Reality",
        "author": [
            "Giuseppe Lorenzo Catalano",
            "Agata Marta Soccini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14765",
        "abstract": "Space exploration increasingly relies on Virtual Reality for several tasks, such as mission planning, multidisciplinary scientific analysis, and astronaut training. A key factor for the reliability of the simulations is having accurate 3D representations of planetary terrains. Extraterrestrial heightmaps derived from satellite imagery often contain missing values due to acquisition and transmission constraints. Mars is among the most studied planets beyond Earth, and its extensive terrain datasets make the Martian surface reconstruction a valuable task, although many areas remain unmapped. Deep learning algorithms can support void-filling tasks; however, whereas Earth's comprehensive datasets enables the use of conditional methods, such approaches cannot be applied to Mars. Current approaches rely on simpler interpolation techniques which, however, often fail to preserve geometric coherence. In this work, we propose a method for reconstructing the surface of Mars based on an unconditional diffusion model. Training was conducted on an augmented dataset of 12000 Martian heightmaps derived from NASA's HiRISE survey. A non-homogeneous rescaling strategy captures terrain features across multiple scales before resizing to a fixed 128x128 model resolution. We compared our method against established void-filling and inpainting techniques, including Inverse Distance Weighting, kriging, and Navier-Stokes algorithm, on an evaluation set of 1000 samples. Results show that our approach consistently outperforms these methods in terms of reconstruction accuracy (4-15% on RMSE) and perceptual similarity (29-81% on LPIPS) with the original data.",
        "tags": [
            "3D",
            "Diffusion",
            "Inpainting"
        ]
    },
    {
        "id": "236",
        "title": "Leveraging Neural Descriptor Fields for Learning Contact-Aware Dynamic Recovery",
        "author": [
            "Fan Yang",
            "Zixuan Huang",
            "Abhinav Kumar",
            "Sergio Aguilera Marinovic",
            "Soshi Iba",
            "Rana Soltani Zarrin",
            "Dmitry Berenson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14768",
        "abstract": "Real-world dexterous manipulation often encounters unexpected errors and disturbances, which can lead to catastrophic failures, such as dropping the manipulated object. To address this challenge, we focus on the problem of catching a falling object while it remains within grasping range and, importantly, resetting the system to a configuration favorable for resuming the primary manipulation task. We propose Contact-Aware Dynamic Recovery (CADRE), a reinforcement learning framework that incorporates a Neural Descriptor Field (NDF)-inspired module to extract implicit contact features. Compared to methods that rely solely on object pose or point cloud input, NDFs can directly reason about finger-object correspondence and adapt to different object geometries. Our experiments show that incorporating contact features improves training efficiency, enhances convergence performance for RL training, and ultimately leads to more successful recoveries. Additionally, we demonstrate that CADRE can generalize zero-shot to unseen objects with different geometries.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "237",
        "title": "Open TeleDex: A Hardware-Agnostic Teleoperation System for Imitation Learning based Dexterous Manipulation",
        "author": [
            "Xu Chi",
            "Chao Zhang",
            "Yang Su",
            "Lingfeng Dou",
            "Fujia Yang",
            "Jiakuo Zhao",
            "Haoyu Zhou",
            "Xiaoyou Jia",
            "Yong Zhou",
            "Shan An"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14771",
        "abstract": "Accurate and high-fidelity demonstration data acquisition is a critical bottleneck for deploying robot Imitation Learning (IL) systems, particularly when dealing with heterogeneous robotic platforms. Existing teleoperation systems often fail to guarantee high-precision data collection across diverse types of teleoperation devices. To address this, we developed Open TeleDex, a unified teleoperation framework engineered for demonstration data collection. Open TeleDex specifically tackles the TripleAny challenge, seamlessly supporting any robotic arm, any dexterous hand, and any external input device. Furthermore, we propose a novel hand pose retargeting algorithm that significantly boosts the interoperability of Open TeleDex, enabling robust and accurate compatibility with an even wider spectrum of heterogeneous master and slave equipment. Open TeleDex establishes a foundational, high-quality, and publicly available platform for accelerating both academic research and industry development in complex robotic manipulation and IL.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "238",
        "title": "Finding Answers in Thought Matters: Revisiting Evaluation on Large Language Models with Reasoning",
        "author": [
            "Hwiyeol Jo",
            "Joosung Lee",
            "Jaehone Lee",
            "Sang-Woo Lee",
            "Joonsuk Park",
            "Kang Min Yoo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14773",
        "abstract": "Evaluating generative models, such as large language models (LLMs), commonly involves question-answering tasks where the final answer is selected based on probability of answer choices. On the other hand, for models requiring reasoning, the method of answer extraction plays a critical role. Our research reveals that the performance of reasoning models and their final answer distributions are highly sensitive to the answer extraction algorithm employed. In order to mitigate this, we propose a basic framework: Answer Regeneration. The method uses an additional model inference, providing the prior input and output prefaced by the prompt \"Answer:\". The final answer is then selected or extracted from the regenerated output. We show that this extraction-rule-agnostic approach exhibits improved performance and enhanced robustness. Furthermore, we have applied this framework to general math problems and open-ended question answering tasks. Our analysis and this framework could offer a more reliable results for model evaluation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "239",
        "title": "SkyDreamer: Interpretable End-to-End Vision-Based Drone Racing with Model-Based Reinforcement Learning",
        "author": [
            "Aderik Verraest",
            "Stavrow Bahnam",
            "Robin Ferede",
            "Guido de Croon",
            "Christophe De Wagter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14783",
        "abstract": "Autonomous drone racing (ADR) systems have recently achieved champion-level performance, yet remain highly specific to drone racing. While end-to-end vision-based methods promise broader applicability, no system to date simultaneously achieves full sim-to-real transfer, onboard execution, and champion-level performance. In this work, we present SkyDreamer, to the best of our knowledge, the first end-to-end vision-based ADR policy that maps directly from pixel-level representations to motor commands. SkyDreamer builds on informed Dreamer, a model-based reinforcement learning approach where the world model decodes to privileged information only available during training. By extending this concept to end-to-end vision-based ADR, the world model effectively functions as an implicit state and parameter estimator, greatly improving interpretability. SkyDreamer runs fully onboard without external aid, resolves visual ambiguities by tracking progress using the state decoded from the world model's hidden state, and requires no extrinsic camera calibration, enabling rapid deployment across different drones without retraining. Real-world experiments show that SkyDreamer achieves robust, high-speed flight, executing tight maneuvers such as an inverted loop, a split-S and a ladder, reaching speeds of up to 21 m/s and accelerations of up to 6 g. It further demonstrates a non-trivial visual sim-to-real transfer by operating on poor-quality segmentation masks, and exhibits robustness to battery depletion by accurately estimating the maximum attainable motor RPM and adjusting its flight path in real-time. These results highlight SkyDreamer's adaptability to important aspects of the reality gap, bringing robustness while still achieving extremely high-speed, agile flight.",
        "tags": [
            "RL",
            "Segmentation"
        ]
    },
    {
        "id": "240",
        "title": "CoT-PL: Visual Chain-of-Thought Reasoning Meets Pseudo-Labeling for Open-Vocabulary Object Detection",
        "author": [
            "Hojun Choi",
            "Youngsun Lim",
            "Jaeyo Shin",
            "Hyunjung Shim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14792",
        "abstract": "Open-vocabulary object detection (OVD) seeks to recognize and localize object categories beyond those seen during training. Recent approaches typically leverage vision-language models (VLMs) to generate pseudo-labels using image-text alignment, allowing detectors to generalize to unseen classes without explicit supervision. However, these methods depend heavily on direct image-text matching, neglecting the intermediate reasoning steps essential for interpreting semantically complex scenes. This results in limited robustness when confronted with crowded or occluded visual contexts. In this paper, we introduce CoT-PL, a new framework that employs structured visual chain-of-thought (CoT) reasoning into the pseudo-labeling process. CoT-PL decomposes object understanding into three interpretable steps: (1) region perception even for unseen objects, (2) category recognition via zero-shot reasoning, and (3) background grounding to separate semantically complex objects. Crucially, the third step naturally motivates our contrastive background learning (CBL) that uses the pre-computed background cues as negatives to promote feature disentanglement between objects and background. In this way, CoT reasoning and CBL form an integrated pipeline tailored to robust pseudo-labeling in crowded or occluded scenes. Notably, in these two settings, our novel-class pseudo-label quality achieves relative improvements of 103.4% and 168.4% over the best prior, respectively. Our extensive experiments demonstrate that CoT-PL achieves +7.7 AP50 on open-vocabulary COCO and +2.9 mask AP on LVIS for novel classes, setting a new state of the art.",
        "tags": [
            "CoT",
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "241",
        "title": "SimKO: Simple Pass@K Policy Optimization",
        "author": [
            "Ruotian Peng",
            "Yi Ren",
            "Zhouliang Yu",
            "Weiyang Liu",
            "Yandong Wen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14807",
        "abstract": "Reinforcement learning with verifiable rewards (RLVR) has advanced the reasoning capabilities of large language models (LLMs). However, prevailing RLVR methods exhibit a systematic bias toward exploitation over exploration, as evidenced by improved pass@1 but reduced pass@K (K>1) performance. To understand this issue, we analyze training dynamics of RLVR methods by tracking the token-level probability distributions over vocabulary candidates. Our analysis reveals a consistent probability concentration effect where the top-1 candidate increasingly accumulates probability mass and suppresses that of other candidates. More importantly, stronger over-concentration correlates with worse pass@K performance. Inspired by this finding, we propose Simple Pass@K Optimization (SimKO), a method designed to mitigate the over-concentration issue, thereby encouraging exploration. SimKO operates in an asymmetrical manner. For verified-correct responses, it boosts the probabilities of the top-K candidates. For verified-incorrect responses, it applies stronger penalties to the top-1 candidate. We observe that this asymmetric design is particularly effective at mitigating over-concentration when applied at tokens with high entropy. Across various math and logical-reasoning benchmarks, SimKO consistently yields higher pass@K for a wide range of K, providing a simple way to improve RLVR's exploration.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "242",
        "title": "Efficient Dynamic Structured Sparse Training with Learned Shuffles",
        "author": [
            "Abhishek Tyagi",
            "Arjun Iyer",
            "Liam Young",
            "William H Renninger",
            "Christopher Kanan",
            "Yuhao Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14812",
        "abstract": "Structured sparsity accelerates training and inference on modern GPUs, yet it still trails unstructured dynamic sparse training (DST) in accuracy. The shortfall stems from a loss of expressivity: whereas a dense layer can realize every possible mask obtained by choosing any $w$ active weights out of $n$, a fixed block or N:M layout explores only a subset of those possibilities. We propose to close this gap by learning, for each layer, a single permutation matrix jointly with the structured weight matrix. Applied to three canonical structures -- block, N:M, and diagonals -- we show that permutation-augmented DST (PA-DST) matches unstructured baselines (RigL, SET) at 90--95\\% sparsity on ImageNet-1K (ViT-B/16) and WikiText-103 (GPT-2), yet trains up to $1.21\\times$ and infers up to $2.9\\times$ faster. The results position structure + learned permutation as a sweet spot between accuracy and efficiency.",
        "tags": [
            "GPT",
            "ViT"
        ]
    },
    {
        "id": "243",
        "title": "FraQAT: Quantization Aware Training with Fractional bits",
        "author": [
            "Luca Morreale",
            "Alberto Gil C. P. Ramos",
            "Malcolm Chadwick",
            "Mehid Noroozi",
            "Ruchika Chavhan",
            "Abhinav Mehrotra",
            "Sourav Bhattacharya"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14823",
        "abstract": "State-of-the-art (SOTA) generative models have demonstrated impressive capabilities in image synthesis or text generation, often with a large capacity model. However, these large models cannot be deployed on smartphones due to the limited availability of on-board memory and computations. Quantization methods lower the precision of the model parameters, allowing for efficient computations, \\eg, in \\INT{8}. Although aggressive quantization addresses efficiency and memory constraints, preserving the quality of the model remains a challenge. To retain quality in previous aggressive quantization, we propose a new fractional bits quantization (\\short) approach. The novelty is a simple yet effective idea: we progressively reduce the model's precision from 32 to 4 bits per parameter, and exploit the fractional bits during optimization to maintain high generation quality. We show that the \\short{} yields improved quality on a variety of diffusion models, including SD3.5-Medium, Sana, \\pixart, and FLUX.1-schnell, while achieving $4-7\\%$ lower FiD than standard QAT. Finally, we deploy and run Sana on a Samsung S25U, which runs on the Qualcomm SM8750-AB Snapdragon 8 Elite Hexagon Tensor Processor (HTP).",
        "tags": [
            "Diffusion",
            "FLUX"
        ]
    },
    {
        "id": "244",
        "title": "Supervised Fine-Tuning or Contrastive Learning? Towards Better Multimodal LLM Reranking",
        "author": [
            "Ziqi Dai",
            "Xin Zhang",
            "Mingxin Li",
            "Yanzhao Zhang",
            "Dingkun Long",
            "Pengjun Xie",
            "Meishan Zhang",
            "Wenjie Li",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14824",
        "abstract": "In information retrieval, training reranking models mainly focuses on two types of objectives: metric learning (e.g. contrastive loss to increase the predicted scores on relevant query-document pairs) and classification (binary label prediction of relevance vs. irrelevance). For BERT-style encoders, various studies have shown that contrastive learning (CL) can be more effective than discriminative (classification) learning. However, for large language models (LLMs), classification via supervised fine-tuning (SFT), which predicts ''yes'' (resp. ''no'') token for relevant (resp. irrelevant) pairs, appears more promising as it aligns well with the generative nature of LLMs. This divergence raises a central question: which objective is intrinsically better suited to LLM-based reranking, and what mechanism underlies the difference? In this work, we conduct a comprehensive comparison and analysis between CL and SFT for reranking, taking the universal multimodal retrieval (UMR) as the experimental playground. We first decompose the objectives into two components: weight, which controls the magnitude of those updates, and direction, which guides the model updates, then present a unified framework for understanding their interactions. Through probing experiments, we find that SFT provides a substantially stronger weighting scheme than CL, whereas the preferred scoring direction shows no clear winner. Taken together, these results point to a consistent advantage of SFT over CL for LLM reranking. To further validate our findings, we conduct large-scale training with SFT and present new state-of-the-art rerankers on the MRB benchmark. We also provide ablations on SFT settings and expect our findings to benefit future research and applications in this area.",
        "tags": [
            "BERT",
            "LLM"
        ]
    },
    {
        "id": "245",
        "title": "Programmatic Representation Learning with Language Models",
        "author": [
            "Gabriel Poesia",
            "Georgia Gabriela Sampaio"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14825",
        "abstract": "Classical models for supervised machine learning, such as decision trees, are efficient and interpretable predictors, but their quality is highly dependent on the particular choice of input features. Although neural networks can learn useful representations directly from raw data (e.g., images or text), this comes at the expense of interpretability and the need for specialized hardware to run them efficiently. In this paper, we explore a hypothesis class we call Learned Programmatic Representations (LeaPR) models, which stack arbitrary features represented as code (functions from data points to scalars) and decision tree predictors. We synthesize feature functions using Large Language Models (LLMs), which have rich prior knowledge in a wide range of domains and a remarkable ability to write code using existing domain-specific libraries. We propose two algorithms to learn LeaPR models from supervised data. First, we design an adaptation of FunSearch to learn features rather than directly generate predictors. Then, we develop a novel variant of the classical ID3 algorithm for decision tree learning, where new features are generated on demand when splitting leaf nodes. In experiments from chess position evaluation to image and text classification, our methods learn high-quality, neural network-free predictors often competitive with neural networks. Our work suggests a flexible paradigm for learning interpretable representations end-to-end where features and predictions can be readily inspected and understood.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "246",
        "title": "To Infinity and Beyond: Tool-Use Unlocks Length Generalization in State Space Models",
        "author": [
            "Eran Malach",
            "Omid Saremi",
            "Sinead Williamson",
            "Arwen Bradley",
            "Aryo Lotfi",
            "Emmanuel Abbe",
            "Josh Susskind",
            "Etai Littwin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14826",
        "abstract": "State Space Models (SSMs) have become the leading alternative to Transformers for sequence modeling. Their primary advantage is efficiency in long-context and long-form generation, enabled by fixed-size memory and linear scaling of computational complexity. We begin this work by showing a simple theoretical result stating that SSMs cannot accurately solve any ``truly long-form'' generation problem (in a sense we formally define), undermining their main competitive advantage. However, we show that this limitation can be mitigated by allowing SSMs interactive access to external tools. In fact, we show that given the right choice of tool access and problem-dependent training data, SSMs can learn to solve any tractable problem and generalize to arbitrary problem length/complexity (i.e., achieve length generalization). Following our theoretical finding, we demonstrate that tool-augmented SSMs achieve remarkable length generalization on a variety of arithmetic, reasoning, and coding tasks. These findings highlight SSMs as a potential efficient alternative to Transformers in interactive tool-based and agentic settings.",
        "tags": [
            "SSMs"
        ]
    },
    {
        "id": "247",
        "title": "Neural Implicit Flow Fields for Spatio-Temporal Motion Mapping",
        "author": [
            "Yufei Zhu",
            "Shih-Min Yang",
            "Andrey Rudenko",
            "Tomasz P. Kucner",
            "Achim J. Lilienthal",
            "Martin Magnusson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14827",
        "abstract": "Safe and efficient robot operation in complex human environments can benefit from good models of site-specific motion patterns. Maps of Dynamics (MoDs) provide such models by encoding statistical motion patterns in a map, but existing representations use discrete spatial sampling and typically require costly offline construction. We propose a continuous spatio-temporal MoD representation based on implicit neural functions that directly map coordinates to the parameters of a Semi-Wrapped Gaussian Mixture Model. This removes the need for discretization and imputation for unevenly sampled regions, enabling smooth generalization across both space and time. Evaluated on a large public dataset with long-term real-world people tracking data, our method achieves better accuracy of motion representation and smoother velocity distributions in sparse regions while still being computationally efficient, compared to available baselines. The proposed approach demonstrates a powerful and efficient way of modeling complex human motion patterns.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "248",
        "title": "RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning",
        "author": [
            "Jinrui Liu",
            "Bingyan Nie",
            "Boyu Li",
            "Yaran Chen",
            "Yuze Wang",
            "Shunsen He",
            "Haoran Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14828",
        "abstract": "Improving the reasoning capabilities of embodied agents is crucial for robots to complete complex human instructions in long-view manipulation tasks successfully. Despite the success of large language models and vision language models based on Supervised Fine-Tuning (SFT) in planning tasks, they continue facing challenges in performing long-horizon manipulation tasks in complex real-world environments, owing to their restricted common sense and reasoning capabilities. Considering that aligning general-purpose vision language models to robotic planning tasks via supervised fine-tuning suffers from poor generalization and insufficient physical understanding, we propose RoboGPT-R1, a two-stage fine-tuning framework for embodied planning. In this framework, supervised training acquires foundational knowledge through expert sequences, followed by RL to address the model's shortcomings in visual-spatial understanding and reasoning. To achieve physical understanding and action sequence consistency in multi-step reasoning tasks, we design a rule-based reward function that simultaneously considers long-horizon performance and action constraint in the environment. The reasoning model, trained on Qwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini, by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on the EmbodiedBench benchmark.",
        "tags": [
            "GPT",
            "LLM",
            "RL",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "249",
        "title": "RL-100: Performant Robotic Manipulation with Real-World Reinforcement Learning",
        "author": [
            "Kun Lei",
            "Huanyu Li",
            "Dongjie Yu",
            "Zhenyu Wei",
            "Lingxiao Guo",
            "Zhennan Jiang",
            "Ziyu Wang",
            "Shiyu Liang",
            "Huazhe Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14830",
        "abstract": "Real-world robotic manipulation in homes and factories demands reliability, efficiency, and robustness that approach or surpass skilled human operators. We present RL-100, a real-world reinforcement learning training framework built on diffusion visuomotor policies trained bu supervised learning. RL-100 introduces a three-stage pipeline. First, imitation learning leverages human priors. Second, iterative offline reinforcement learning uses an Offline Policy Evaluation procedure, abbreviated OPE, to gate PPO-style updates that are applied in the denoising process for conservative and reliable improvement. Third, online reinforcement learning eliminates residual failure modes. An additional lightweight consistency distillation head compresses the multi-step sampling process in diffusion into a single-step policy, enabling high-frequency control with an order-of-magnitude reduction in latency while preserving task performance. The framework is task-, embodiment-, and representation-agnostic and supports both 3D point clouds and 2D RGB inputs, a variety of robot platforms, and both single-step and action-chunk policies. We evaluate RL-100 on seven real-robot tasks spanning dynamic rigid-body control, such as Push-T and Agile Bowling, fluids and granular pouring, deformable cloth folding, precise dexterous unscrewing, and multi-stage orange juicing. RL-100 attains 100\\% success across evaluated trials for a total of 900 out of 900 episodes, including up to 250 out of 250 consecutive trials on one task. The method achieves near-human teleoperation or better time efficiency and demonstrates multi-hour robustness with uninterrupted operation lasting up to two hours.",
        "tags": [
            "3D",
            "Diffusion",
            "PPO",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "250",
        "title": "QDepth-VLA: Quantized Depth Prediction as Auxiliary Supervision for Vision-Language-Action Models",
        "author": [
            "Yixuan Li",
            "Yuhui Chen",
            "Mingcai Zhou",
            "Haoran Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14836",
        "abstract": "Spatial perception and reasoning are crucial for Vision-Language-Action (VLA) models to accomplish fine-grained manipulation tasks. However, existing approaches often lack the ability to understand and reason over the essential 3D structures necessary for precise control. To address this limitation, we propose QDepth-VLA, a general framework that augments VLA models with an auxiliary depth prediction task. A dedicated depth expert is designed to predict quantized latent tokens of depth maps obtained from a VQ-VAE encoder, enabling the model to learn depth-aware representations that capture critical geometric cues. Experimental results on the simulation benchmarks and real-world tasks demonstrate that QDepth-VLA yields strong spatial reasoning and competitive performance on manipulation tasks.",
        "tags": [
            "3D",
            "VAE"
        ]
    },
    {
        "id": "251",
        "title": "Reinforcement Learning with Stochastic Reward Machines",
        "author": [
            "Jan Corazza",
            "Ivan Gavran",
            "Daniel Neider"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14837",
        "abstract": "Reward machines are an established tool for dealing with reinforcement learning problems in which rewards are sparse and depend on complex sequences of actions. However, existing algorithms for learning reward machines assume an overly idealized setting where rewards have to be free of noise. To overcome this practical limitation, we introduce a novel type of reward machines, called stochastic reward machines, and an algorithm for learning them. Our algorithm, based on constraint solving, learns minimal stochastic reward machines from the explorations of a reinforcement learning agent. This algorithm can easily be paired with existing reinforcement learning algorithms for reward machines and guarantees to converge to an optimal policy in the limit. We demonstrate the effectiveness of our algorithm in two case studies and show that it outperforms both existing methods and a naive approach for handling noisy reward functions.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "252",
        "title": "Boosting Instruction Following at Scale",
        "author": [
            "Ben Elder",
            "Evelyn Duesterwald",
            "Vinod Muthusamy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14842",
        "abstract": "A typical approach developers follow to influence an LLM's behavior in an application is through careful manipulation of the prompt, such as by adding or modifying instructions. However, merely adding more instructions provides little assurance that they will actually be followed. We introduce Instruction Boosting as a post-generation method to increase the reliability of LLM prompt instructions. We show that Instruction Boosting improves the instruction following rate by up to 7 points for two instructions and up to 4 points for ten instructions. To demonstrate these results we introduce SCALEDIF, a benchmark with a scaled instruction volume of up to ten instructions per data sample. We also present an analysis of the commonly observed trend that performance degrades as more instructions are added. We show that an important factor contributing to this trend is the degree of tension and conflict that arises as the number of instructions is increased. We contribute a quantitative conflict scoring tool that explains the observed performance trends and provides feedback to developers on the impact that additional prompt instructions have on a model's performance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "253",
        "title": "Where to Search: Measure the Prior-Structured Search Space of LLM Agents",
        "author": [
            "Zhuo-Yang Song"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14846",
        "abstract": "The generate-filter-refine (iterative paradigm) based on large language models (LLMs) has achieved progress in reasoning, programming, and program discovery in AI+Science. However, the effectiveness of search depends on where to search, namely, how to encode the domain prior into an operationally structured hypothesis space. To this end, this paper proposes a compact formal theory that describes and measures LLM-assisted iterative search guided by domain priors. We represent an agent as a fuzzy relation operator on inputs and outputs to capture feasible transitions; the agent is thereby constrained by a fixed safety envelope. To describe multi-step reasoning/search, we weight all reachable paths by a single continuation parameter and sum them to obtain a coverage generating function; this induces a measure of reachability difficulty; and it provides a geometric interpretation of search on the graph induced by the safety envelope. We further provide the simplest testable inferences and validate them via a majority-vote instantiation. This theory offers a workable language and operational tools to measure agents and their search spaces, proposing a systematic formal description of iterative search constructed by LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "254",
        "title": "ImagerySearch: Adaptive Test-Time Search for Video Generation Beyond Semantic Dependency Constraints",
        "author": [
            "Meiqi Wu",
            "Jiashu Zhu",
            "Xiaokun Feng",
            "Chubin Chen",
            "Chen Zhu",
            "Bingze Song",
            "Fangyuan Mao",
            "Jiahong Wu",
            "Xiangxiang Chu",
            "Kaiqi Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14847",
        "abstract": "Video generation models have achieved remarkable progress, particularly excelling in realistic scenarios; however, their performance degrades notably in imaginative scenarios. These prompts often involve rarely co-occurring concepts with long-distance semantic relationships, falling outside training distributions. Existing methods typically apply test-time scaling for improving video quality, but their fixed search spaces and static reward designs limit adaptability to imaginative scenarios. To fill this gap, we propose ImagerySearch, a prompt-guided adaptive test-time search strategy that dynamically adjusts both the inference search space and reward function according to semantic relationships in the prompt. This enables more coherent and visually plausible videos in challenging imaginative settings. To evaluate progress in this direction, we introduce LDT-Bench, the first dedicated benchmark for long-distance semantic prompts, consisting of 2,839 diverse concept pairs and an automated protocol for assessing creative generation capabilities. Extensive experiments show that ImagerySearch consistently outperforms strong video generation baselines and existing test-time scaling approaches on LDT-Bench, and achieves competitive improvements on VBench, demonstrating its effectiveness across diverse prompt types. We will release LDT-Bench and code to facilitate future research on imaginative video generation.",
        "tags": [
            "Video Generation"
        ]
    },
    {
        "id": "255",
        "title": "SADCHER: Scheduling using Attention-based Dynamic Coalitions of Heterogeneous Robots in Real-Time",
        "author": [
            "Jakob Bichler",
            "Andreu Matoses Gimenez",
            "Javier Alonso-Mora"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14851",
        "abstract": "We present Sadcher, a real-time task assignment framework for heterogeneous multi-robot teams that incorporates dynamic coalition formation and task precedence constraints. Sadcher is trained through Imitation Learning and combines graph attention and transformers to predict assignment rewards between robots and tasks. Based on the predicted rewards, a relaxed bipartite matching step generates high-quality schedules with feasibility guarantees. We explicitly model robot and task positions, task durations, and robots' remaining processing times, enabling advanced temporal and spatial reasoning and generalization to environments with different spatiotemporal distributions compared to training. Trained on optimally solved small-scale instances, our method can scale to larger task sets and team sizes. Sadcher outperforms other learning-based and heuristic baselines on randomized, unseen problems for small and medium-sized teams with computation times suitable for real-time operation. We also explore sampling-based variants and evaluate scalability across robot and task counts. In addition, we release our dataset of 250,000 optimal schedules: https://autonomousrobots.nl/paper_websites/sadcher_MRTA/",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "256",
        "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models",
        "author": [
            "Guinan Su",
            "Yanwu Yang",
            "Li Shen",
            "Lu Yin",
            "Shiwei Liu",
            "Jonas Geiping"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14853",
        "abstract": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \\textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite.",
        "tags": [
            "DeepSeek",
            "MoE"
        ]
    },
    {
        "id": "257",
        "title": "Benchmarking Multimodal Large Language Models for Face Recognition",
        "author": [
            "Hatef Otroshi Shahreza",
            "SÃ©bastien Marcel"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14866",
        "abstract": "Multimodal large language models (MLLMs) have achieved remarkable performance across diverse vision-and-language tasks. However, their potential in face recognition remains underexplored. In particular, the performance of open-source MLLMs needs to be evaluated and compared with existing face recognition models on standard benchmarks with similar protocol. In this work, we present a systematic benchmark of state-of-the-art MLLMs for face recognition on several face recognition datasets, including LFW, CALFW, CPLFW, CFP, AgeDB and RFW. Experimental results reveal that while MLLMs capture rich semantic cues useful for face-related tasks, they lag behind specialized models in high-precision recognition scenarios in zero-shot applications. This benchmark provides a foundation for advancing MLLM-based face recognition, offering insights for the design of next-generation models with higher accuracy and generalization. The source code of our benchmark is publicly available in the project page.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "258",
        "title": "From Loop Nests to Silicon: Mapping AI Workloads onto AMD NPUs with MLIR-AIR",
        "author": [
            "Erwei Wang",
            "Samuel Bayliss",
            "Andra Bisca",
            "Zachary Blair",
            "Sangeeta Chowdhary",
            "Kristof Denolf",
            "Jeff Fifield",
            "Brandon Freiberger",
            "Erika Hunhoff",
            "Phil James-Roxby",
            "Jack Lo",
            "Joseph Melber",
            "Stephen Neuendorffer",
            "Eddie Richter",
            "Andre Rosti",
            "Javier Setoain",
            "Gagandeep Singh",
            "Endri Taka",
            "Pranathi Vasireddy",
            "Zhewen Yu",
            "Niansong Zhang",
            "Jinming Zhuang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14871",
        "abstract": "General-purpose compilers abstract away parallelism, locality, and synchronization, limiting their effectiveness on modern spatial architectures. As modern computing architectures increasingly rely on fine-grained control over data movement, execution order, and compute placement for performance, compiler infrastructure must provide explicit mechanisms for orchestrating compute and data to fully exploit such architectures. We introduce MLIR-AIR, a novel, open-source compiler stack built on MLIR that bridges the semantic gap between high-level workloads and fine-grained spatial architectures such as AMD's NPUs. MLIR-AIR defines the AIR dialect, which provides structured representations for asynchronous and hierarchical operations across compute and memory resources. AIR primitives allow the compiler to orchestrate spatial scheduling, distribute computation across hardware regions, and overlap communication with computation without relying on ad hoc runtime coordination or manual scheduling. We demonstrate MLIR-AIR's capabilities through two case studies: matrix multiplication and the multi-head attention block from the LLaMA 2 model. For matrix multiplication, MLIR-AIR achieves up to 78.7% compute efficiency and generates implementations with performance almost identical to state-of-the-art, hand-optimized matrix multiplication written using the lower-level, close-to-metal MLIR-AIE framework. For multi-head attention, we demonstrate that the AIR interface supports fused implementations using approximately 150 lines of code, enabling tractable expression of complex workloads with efficient mapping to spatial hardware. MLIR-AIR transforms high-level structured control flow into spatial programs that efficiently utilize the compute fabric and memory hierarchy of an NPU, leveraging asynchronous execution, tiling, and communication overlap through compiler-managed scheduling.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": "259",
        "title": "TOUCH: Text-guided Controllable Generation of Free-Form Hand-Object Interactions",
        "author": [
            "Guangyi Han",
            "Wei Zhai",
            "Yuhang Yang",
            "Yang Cao",
            "Zheng-Jun Zha"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14874",
        "abstract": "Hand-object interaction (HOI) is fundamental for humans to express intent. Existing HOI generation research is predominantly confined to fixed grasping patterns, where control is tied to physical priors such as force closure or generic intent instructions, even when expressed through elaborate language. Such an overly general conditioning imposes a strong inductive bias for stable grasps, thus failing to capture the diversity of daily HOI. To address these limitations, we introduce Free-Form HOI Generation, which aims to generate controllable, diverse, and physically plausible HOI conditioned on fine-grained intent, extending HOI from grasping to free-form interactions, like pushing, poking, and rotating. To support this task, we construct WildO2, an in-the-wild diverse 3D HOI dataset, which includes diverse HOI derived from internet videos. Specifically, it contains 4.4k unique interactions across 92 intents and 610 object categories, each with detailed semantic annotations. Building on this dataset, we propose TOUCH, a three-stage framework centered on a multi-level diffusion model that facilitates fine-grained semantic control to generate versatile hand poses beyond grasping priors. This process leverages explicit contact modeling for conditioning and is subsequently refined with contact consistency and physical constraints to ensure realism. Comprehensive experiments demonstrate our method's ability to generate controllable, diverse, and physically plausible hand interactions representative of daily activities. The project page is $\\href{https://guangyid.github.io/hoi123touch}{here}$.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": "260",
        "title": "The Gatekeeper Knows Enough",
        "author": [
            "Fikresilase Wondmeneh Abebayew"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14881",
        "abstract": "Large Language Models (LLMs) are increasingly deployed as autonomous agents, yet their practical utility is fundamentally constrained by a limited context window and state desynchronization resulting from the LLMs' stateless nature and inefficient context management. These limitations lead to unreliable output, unpredictable behavior, and inefficient resource usage, particularly when interacting with large, structured, and sensitive knowledge systems such as codebases and documents. To address these challenges, we introduce the Gatekeeper Protocol, a novel, domain-agnostic framework that governs agent-system interactions. Our protocol mandates that the agent first operate and reason on a minimalist, low-fidelity \"latent state\" representation of the system to strategically request high-fidelity context on demand. All interactions are mediated through a unified JSON format that serves as a declarative, state-synchronized protocol, ensuring the agent's model of the system remains verifiably grounded in the system's reality. We demonstrate the efficacy of this protocol with Sage, a reference implementation of the Gatekeeper Protocol for software development. Our results show that this approach significantly increases agent reliability, improves computational efficiency by minimizing token consumption, and enables scalable interaction with complex systems, creating a foundational methodology for building more robust, predictable, and grounded AI agents for any structured knowledge domain.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "261",
        "title": "ScaleWeaver: Weaving Efficient Controllable T2I Generation with Multi-Scale Reference Attention",
        "author": [
            "Keli Liu",
            "Zhendong Wang",
            "Wengang Zhou",
            "Shaodong Xu",
            "Ruixiao Dong",
            "Houqiang Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14882",
        "abstract": "Text-to-image generation with visual autoregressive~(VAR) models has recently achieved impressive advances in generation fidelity and inference efficiency. While control mechanisms have been explored for diffusion models, enabling precise and flexible control within VAR paradigm remains underexplored. To bridge this critical gap, in this paper, we introduce ScaleWeaver, a novel framework designed to achieve high-fidelity, controllable generation upon advanced VAR models through parameter-efficient fine-tuning. The core module in ScaleWeaver is the improved MMDiT block with the proposed Reference Attention module, which efficiently and effectively incorporates conditional information. Different from MM Attention, the proposed Reference Attention module discards the unnecessary attention from image$\\rightarrow$condition, reducing computational cost while stabilizing control injection. Besides, it strategically emphasizes parameter reuse, leveraging the capability of the VAR backbone itself with a few introduced parameters to process control information, and equipping a zero-initialized linear projection to ensure that control signals are incorporated effectively without disrupting the generative capability of the base model. Extensive experiments show that ScaleWeaver delivers high-quality generation and precise control while attaining superior efficiency over diffusion-based methods, making ScaleWeaver a practical and effective solution for controllable text-to-image generation within the visual autoregressive paradigm. Code and models will be released.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "262",
        "title": "You May Speak Freely: Improving the Fine-Grained Visual Recognition Capabilities of Multimodal Large Language Models with Answer Extraction",
        "author": [
            "Logan Lawrence",
            "Oindrila Saha",
            "Megan Wei",
            "Chen Sun",
            "Subhransu Maji",
            "Grant Van Horn"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14885",
        "abstract": "Despite the renewed interest in zero-shot visual classification due to the rise of Multimodal Large Language Models (MLLMs), the problem of evaluating free-form responses of auto-regressive models remains a persistent challenge. Most existing works focus on language-only tasks or don't consider Multiple Choice Questions (MCQs) beyond 5-way options, both of which are critical capabilities to solve tasks in Fine-Grained Visual Classification (FGVC) where choice counts are in the hundreds to thousands and the choices are highly related. Furthermore, in this highly multi-way MCQ setting it is not clear how to extend LLM choice extraction to retrieval-based problems, where computing probabilities over the choice set is computationally costly. In this work we investigate nlg2choice, a simple two-stage method which first asks the MLLM an open-ended question for the task with minimal constraints, then uses text-only constrained decoding to predict the most likely choice. In retrieval settings, we compute the probability of the constrained response taking that choice with an early stopping method to significantly improve throughput. Our results show improvement over a suite of seven fine-grained visual datasets when evaluating in terms of classification and retrieval, and show that this performance holds over the various ways that users of LLMs can implement tasks in natural language.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "263",
        "title": "Mapping Smarter, Not Harder: A Test-Time Reinforcement Learning Agent That Improves Without Labels or Model Updates",
        "author": [
            "Wen-Kwang Tsao",
            "Yao-Ching Yu",
            "Chien-Ming Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14900",
        "abstract": "The Enterprise Intelligence Platform must integrate logs from numerous third-party vendors in order to perform various downstream tasks. However, vendor documentation is often unavailable at test time. It is either misplaced, mismatched, poorly formatted, or incomplete, which makes schema mapping challenging. We introduce a reinforcement learning agent that can self-improve without labeled examples or model weight updates. During inference, the agent: 1) Identifies ambiguous field-mapping attempts. 2) Generates targeted web-search queries to gather external evidence. 3) Applies a confidence-based reward to iteratively refine its mappings. To demonstrate this concept, we converted Microsoft Defender for Endpoint logs into a common schema. Our method increased mapping accuracy from 56.4\\%(LLM-only) to 72.73\\%(RAG) to 93.94\\% over 100 iterations using GPT-4o. At the same time, it reduced the number of low-confidence mappings requiring expert review by 85\\%. This new approach provides an evidence-driven, transparent method for solving future industry problems, paving the way for more robust, accountable, scalable, efficient, flexible, adaptable, and collaborative solutions.",
        "tags": [
            "GPT",
            "LLM",
            "RAG",
            "RL"
        ]
    },
    {
        "id": "264",
        "title": "Reasoning with Sampling: Your Base Model is Smarter Than You Think",
        "author": [
            "Aayush Karan",
            "Yilun Du"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14901",
        "abstract": "Frontier reasoning models have exhibited incredible capabilities across a wide array of disciplines, driven by posttraining large language models (LLMs) with reinforcement learning (RL). However, despite the widespread success of this paradigm, much of the literature has been devoted to disentangling truly novel behaviors that emerge during RL but are not present in the base models. In our work, we approach this question from a different angle, instead asking whether comparable reasoning capabilites can be elicited from base models at inference time by pure sampling, without any additional training. Inspired by Markov chain Monte Carlo (MCMC) techniques for sampling from sharpened distributions, we propose a simple iterative sampling algorithm leveraging the base models' own likelihoods. Over different base models, we show that our algorithm offers substantial boosts in reasoning that nearly match and even outperform those from RL on a wide variety of single-shot tasks, including MATH500, HumanEval, and GPQA. Moreover, our sampler avoids the collapse in diversity over multiple samples that is characteristic of RL-posttraining. Crucially, our method does not require training, curated datasets, or a verifier, suggesting broad applicability beyond easily verifiable domains.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "265",
        "title": "MaskCaptioner : Learning to Jointly Segment and Caption Object Trajectories in Videos",
        "author": [
            "Gabriel Fiastre",
            "Antoine Yang",
            "Cordelia Schmid"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14904",
        "abstract": "Dense Video Object Captioning (DVOC) is the task of jointly detecting, tracking, and captioning object trajectories in a video, requiring the ability to understand spatio-temporal details and describe them in natural language. Due to the complexity of the task and the high cost associated with manual annotation, previous approaches resort to disjoint training strategies, potentially leading to suboptimal performance. To circumvent this issue, we propose to generate captions about spatio-temporally localized entities leveraging a state-of-the-art VLM. By extending the LVIS and LV-VIS datasets with our synthetic captions (LVISCap and LV-VISCap), we train MaskCaptioner, an end-to-end model capable of jointly detecting, segmenting, tracking and captioning object trajectories. Moreover, with pretraining on LVISCap and LV-VISCap, MaskCaptioner achieves state-of-the-art DVOC results on three existing benchmarks, VidSTG, VLN and BenSMOT. The datasets and code are available at https://www.gabriel.fiastre.fr/maskcaptioner/.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "266",
        "title": "A Hard-Label Black-Box Evasion Attack against ML-based Malicious Traffic Detection Systems",
        "author": [
            "Zixuan Liu",
            "Yi Zhao",
            "Zhuotao Liu",
            "Qi Li",
            "Chuanpu Fu",
            "Guangmeng Zhou",
            "Ke Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14906",
        "abstract": "Machine Learning (ML)-based malicious traffic detection is a promising security paradigm. It outperforms rule-based traditional detection by identifying various advanced attacks. However, the robustness of these ML models is largely unexplored, thereby allowing attackers to craft adversarial traffic examples that evade detection. Existing evasion attacks typically rely on overly restrictive conditions (e.g., encrypted protocols, Tor, or specialized setups), or require detailed prior knowledge of the target (e.g., training data and model parameters), which is impractical in realistic black-box scenarios. The feasibility of a hard-label black-box evasion attack (i.e., applicable across diverse tasks and protocols without internal target insights) thus remains an open challenge. To this end, we develop NetMasquerade, which leverages reinforcement learning (RL) to manipulate attack flows to mimic benign traffic and evade detection. Specifically, we establish a tailored pre-trained model called Traffic-BERT, utilizing a network-specialized tokenizer and an attention mechanism to extract diverse benign traffic patterns. Subsequently, we integrate Traffic-BERT into the RL framework, allowing NetMasquerade to effectively manipulate malicious packet sequences based on benign traffic patterns with minimal modifications. Experimental results demonstrate that NetMasquerade enables both brute-force and stealthy attacks to evade 6 existing detection methods under 80 attack scenarios, achieving over 96.65% attack success rate. Notably, it can evade the methods that are either empirically or certifiably robust against existing evasion attacks. Finally, NetMasquerade achieves low-latency adversarial traffic generation, demonstrating its practicality in real-world scenarios.",
        "tags": [
            "BERT",
            "Detection",
            "RL"
        ]
    },
    {
        "id": "267",
        "title": "Budget-aware Test-time Scaling via Discriminative Verification",
        "author": [
            "Kyle Montgomery",
            "Sijun Tan",
            "Yuqi Chen",
            "Siyuan Zhuang",
            "Tianjun Zhang",
            "Raluca Ada Popa",
            "Chenguang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14913",
        "abstract": "Test-time scaling is a powerful strategy for boosting the performance of large language models on complex reasoning tasks. While state-of-the-art approaches often employ generative verifiers to select the best solution from a pool of candidates, this method incurs prohibitive computational costs, limiting its practicality. In this work, we shift the focus to a more budget-aware paradigm: discriminative verification. We conduct a thorough empirical analysis and demonstrate that while discriminative verifiers may underperform in isolation, combining them with self-consistency in a hybrid approach creates a powerful and efficient test-time scaling mechanism. Notably, under a fixed compute budget, this hybrid approach surpasses state-of-the-art generative verification by a significant margin: achieving up to 15.3\\% higher accuracy on AIME2025. Our findings establish that for practical, real-world applications, budget-aware scaling with discriminative verifiers is not only a \"free\" upgrade over self-consistency, but also a more effective and efficient alternative to costly generative techniques. Code is available at https://github.com/wang-research-lab/verification.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "268",
        "title": "Design of Paper Robot Building Kits",
        "author": [
            "Ruhan Yang",
            "Ellen Yi-Luen Do"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14914",
        "abstract": "Building robots is an engaging activity that provides opportunities for hands-on learning. However, traditional robot-building kits are usually costly with limited functionality due to material and technology constraints. To improve the accessibility and flexibility of such kits, we take paper as the building material and extensively explore the versatility of paper-based interactions. Based on an analysis of current robot-building kits and paper-based interaction research, we propose a design space for devising paper robots. We also analyzed our building kit designs using this design space, where these kits demonstrate the potential of paper as a cost-effective material for robot building. As a starting point, our design space and building kit examples provide a guideline that inspires and informs future research and development of novel paper robot-building kits.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "269",
        "title": "Harmonizing Diverse Models: A Layer-wise Merging Strategy for Consistent Generation",
        "author": [
            "Xujun Peng",
            "Anoop Kumar",
            "Jingyu Wu",
            "Parker Glenn",
            "Daben Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14915",
        "abstract": "Retrieval-Augmented Generation (RAG) systems leverage Large Language Models (LLMs) to generate accurate and reliable responses that are grounded in retrieved context. However, LLMs often generate inconsistent outputs for semantically equivalent inputs, a problem compounded by the scarcity of consistency-focused training data and the limitations of current fine-tuning techniques in enhancing output consistency. We propose a new approach combining systematic synthetic data generation, triplet loss for better embeddings, and a novel layer-wise model merging approach. Using consistency-aware weights derived from intermediate layer activations, our method effectively integrates knowledge from specialized models. Experimental results how that our merged model significantly enhances output consistency, achieving a ~47.5\\% improvement in response similarity over the baseline, thus offering a practical solution for increasing the reliability of an industrial RAG system.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "270",
        "title": "Predicting Task Performance with Context-aware Scaling Laws",
        "author": [
            "Kyle Montgomery",
            "David Park",
            "Jianhong Tu",
            "Michael Bendersky",
            "Beliz Gunel",
            "Dawn Song",
            "Chenguang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14919",
        "abstract": "Scaling laws have transformed our understanding of large language models by linking upstream metrics like cross-entropy loss to design factors such as model size, training data, and compute. However, these conventional laws fail to capture downstream task performance, where context plays a critical role. In this work, we propose a straightforward, interpretable framework that jointly models downstream performance as a function of the training compute and the provided context. We empirically validate our framework by fitting it on the observed downstream performance of extended-context variants of Llama-2-7B and Llama-2-13B across 65,500 unique instances spanning three tasks: arithmetic reasoning, common sense reasoning, and machine translation. Our results demonstrate that our framework accurately models in-distribution downstream performance, generalizes across three orders of magnitude in training compute, and reliably extrapolates performance as the amount of context increases. These findings offer valuable insights into the interplay between training compute and context utilization, providing guidance for designing more efficient long-context LLMs for diverse downstream tasks. Our code is available at https://github.com/wang-research-lab/context-scaling.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "271",
        "title": "Stable but Miscalibrated: A Kantian View on Overconfidence from Filters to Large Language Models",
        "author": [
            "Akira Okutomi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14925",
        "abstract": "We reinterpret Kant's Critique of Pure Reason as a theory of feedback stability, viewing reason as a regulator that keeps inference within the bounds of possible experience. We formalize this intuition via a composite instability index (H-Risk) combining spectral margin, conditioning, temporal sensitivity, and innovation amplification. In linear-Gaussian simulations, higher H-Risk predicts overconfident errors even under formal stability, revealing a gap between nominal and epistemic stability. Extending to large language models (LLMs), we find that fragile internal dynamics correlate with miscalibration and hallucination, while critique-style prompts show mixed effects on calibration and hallucination. These results suggest a structural bridge between Kantian self-limitation and feedback control, offering a principled lens for diagnosing -- and selectively reducing -- overconfidence in reasoning systems. This is a preliminary version; supplementary experiments and broader replication will be reported in a future revision.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "272",
        "title": "VT-Refine: Learning Bimanual Assembly with Visuo-Tactile Feedback via Simulation Fine-Tunin",
        "author": [
            "Binghao Huang",
            "Jie Xu",
            "Iretiayo Akinola",
            "Wei Yang",
            "Balakumar Sundaralingam",
            "Rowland O'Flaherty",
            "Dieter Fox",
            "Xiaolong Wang",
            "Arsalan Mousavian",
            "Yu-Wei Chao",
            "Yunzhu Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14930",
        "abstract": "Humans excel at bimanual assembly tasks by adapting to rich tactile feedback -- a capability that remains difficult to replicate in robots through behavioral cloning alone, due to the suboptimality and limited diversity of human demonstrations. In this work, we present VT-Refine, a visuo-tactile policy learning framework that combines real-world demonstrations, high-fidelity tactile simulation, and reinforcement learning to tackle precise, contact-rich bimanual assembly. We begin by training a diffusion policy on a small set of demonstrations using synchronized visual and tactile inputs. This policy is then transferred to a simulated digital twin equipped with simulated tactile sensors and further refined via large-scale reinforcement learning to enhance robustness and generalization. To enable accurate sim-to-real transfer, we leverage high-resolution piezoresistive tactile sensors that provide normal force signals and can be realistically modeled in parallel using GPU-accelerated simulation. Experimental results show that VT-Refine improves assembly performance in both simulation and the real world by increasing data diversity and enabling more effective policy fine-tuning. Our project page is available at https://binghao-huang.github.io/vt_refine/.",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "273",
        "title": "Further Results on Safety-Critical Stabilization of Force-Controlled Nonholonomic Mobile Robots",
        "author": [
            "Bo Wang",
            "Tianyu Han",
            "Guangwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14931",
        "abstract": "In this paper, we address the stabilization problem for force-controlled nonholonomic mobile robots under safety-critical constraints. We propose a continuous, time-invariant control law based on the gamma m-quadratic programming (gamma m-QP) framework, which unifies control Lyapunov functions (CLFs) and control barrier functions (CBFs) to enforce both stability and safety in the closed-loop system. For the first time, we construct a global, time-invariant, strict Lyapunov function for the closed-loop nonholonomic mobile robot system with a nominal stabilization controller in polar coordinates; this strict Lyapunov function then serves as the CLF in the QP design. Next, by exploiting the inherent cascaded structure of the vehicle dynamics, we develop a CBF for the mobile robot via an integrator backstepping procedure. Our main results guarantee both asymptotic stability and safety for the closed-loop system. Both the simulation and experimental results are presented to illustrate the effectiveness and performance of our approach.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "274",
        "title": "TASLA: Text-Aligned Speech Tokens with Multiple Layer-Aggregation",
        "author": [
            "Ming-Hao Hsu",
            "Liang-Hsuan Tseng",
            "Hung-yi Lee",
            "Zhizheng Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14934",
        "abstract": "We propose Text-Aligned Speech Tokens with Multiple Layer-Aggregation (TASLA), which is a text-aligned speech tokenization framework that aims to address the problem that under a low-frame-rate and text-aligned regime, single-source speech tokens may lose acoustic details during reconstruction. On the other hand, this paper further explains how different encoder layers collaborate to capture comprehensive acoustic features for tokenization. Previous work, TASTE, proposed the text-aligned speech tokenization framework, which is a LM-friendly architecture, but struggles to capture acoustic details. We address this trade-off with two components: Multi-Layer Dynamic Attention (MLDA), which lets each text position adaptively mix shallow/deep features from a frozen speech encoder, and Finite Scalar Quantization (FSQ), a simple per-dimension discretization with smooth optimization. At about 2.62 Hz (tokens/s), TASLA consistently improves prosody and achieves competitive quality over TASTE on in-domain (LibriSpeech) and OOD (EXPRESSO, Voxceleb) sets. We further demonstrate that dynamic layer mixing is correlated with spectral flux and explains why MLDA preserves prosody under a low frame rate with extreme feature compression.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "275",
        "title": "Circuit Insights: Towards Interpretability Beyond Activations",
        "author": [
            "Elena Golimblevskaia",
            "Aakriti Jain",
            "Bruno Puri",
            "Ammar Ibrahim",
            "Wojciech Samek",
            "Sebastian Lapuschkin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14936",
        "abstract": "The fields of explainable AI and mechanistic interpretability aim to uncover the internal structure of neural networks, with circuit discovery as a central tool for understanding model computations. Existing approaches, however, rely on manual inspection and remain limited to toy tasks. Automated interpretability offers scalability by analyzing isolated features and their activations, but it often misses interactions between features and depends strongly on external LLMs and dataset quality. Transcoders have recently made it possible to separate feature attributions into input-dependent and input-invariant components, providing a foundation for more systematic circuit analysis. Building on this, we propose WeightLens and CircuitLens, two complementary methods that go beyond activation-based analysis. WeightLens interprets features directly from their learned weights, removing the need for explainer models or datasets while matching or exceeding the performance of existing methods on context-independent features. CircuitLens captures how feature activations arise from interactions between components, revealing circuit-level dynamics that activation-only approaches cannot identify. Together, these methods increase interpretability robustness and enhance scalable mechanistic analysis of circuits while maintaining efficiency and quality.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "276",
        "title": "GroundedPRM: Tree-Guided and Fidelity-Aware Process Reward Modeling for Step-Level Reasoning",
        "author": [
            "Yao Zhang",
            "Yu Wu",
            "Haowei Zhang",
            "Weiguo Li",
            "Haokun Chen",
            "Jingpei Wu",
            "Guohao Li",
            "Zhen Han",
            "Volker Tresp"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14942",
        "abstract": "Process Reward Models (PRMs) aim to improve multi-step reasoning in Large Language Models (LLMs) by supervising intermediate steps and identifying errors. However, building effective PRMs remains challenging due to the lack of scalable, high-quality annotations. Existing approaches rely on costly human labeling, LLM-based self-evaluation that is prone to hallucination, or Monte Carlo (MC) estimation, which infers step quality solely from rollout outcomes and often introduces noisy, misaligned supervision due to credit misattribution. These issues result in three core limitations: noisy rewards, low factual fidelity, and misalignment with step-level reasoning objectives. To address these challenges, we introduce GroundedPRM, a tree-guided and fidelity-aware framework for automatic process supervision. To reduce reward noise and enable fine-grained credit assignment, we construct structured reasoning paths via Monte Carlo Tree Search (MCTS). To eliminate hallucinated supervision, we validate each intermediate step using an external tool, providing execution-grounded correctness signals. To combine both step-level validation and global outcome assessment, we design a hybrid reward aggregation mechanism that fuses tool-based verification with MCTS-derived feedback. Finally, we format the reward signal into a rationale-enhanced, generative structure to promote interpretability and compatibility with instruction-tuned LLMs. GroundedPRM is trained on only 40K automatically labeled samples, amounting to just 10% of the data used by the best-performing PRM trained with auto-labeled supervision. Nevertheless, it achieves up to a 26% relative improvement in average performance on ProcessBench. When used for reward-guided greedy search, GroundedPRM outperforms even PRMs trained with human-labeled supervision, offering a scalable and verifiable path toward high-quality process-level reasoning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "277",
        "title": "LaSeR: Reinforcement Learning with Last-Token Self-Rewarding",
        "author": [
            "Wenkai Yang",
            "Weijie Liu",
            "Ruobing Xie",
            "Yiju Guo",
            "Lulu Wu",
            "Saiyong Yang",
            "Yankai Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14943",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a core paradigm for enhancing the reasoning capabilities of Large Language Models (LLMs). To address the lack of verification signals at test time, prior studies incorporate the training of model's self-verification capability into the standard RLVR process, thereby unifying reasoning and verification capabilities within a single LLM. However, previous practice requires the LLM to sequentially generate solutions and self-verifications using two separate prompt templates, which significantly reduces efficiency. In this work, we theoretically reveal that the closed-form solution to the RL objective of self-verification can be reduced to a remarkably simple form: the true reasoning reward of a solution is equal to its last-token self-rewarding score, which is computed as the difference between the policy model's next-token log-probability assigned to any pre-specified token at the solution's last token and a pre-calculated constant, scaled by the KL coefficient. Based on this insight, we propose LaSeR (Reinforcement Learning with Last-Token Self-Rewarding), an algorithm that simply augments the original RLVR loss with a MSE loss that aligns the last-token self-rewarding scores with verifier-based reasoning rewards, jointly optimizing the reasoning and self-rewarding capabilities of LLMs. The optimized self-rewarding scores can be utilized in both training and testing to enhance model performance. Notably, our algorithm derives these scores from the predicted next-token probability distribution of the last token immediately after generation, incurring only the minimal extra cost of one additional token inference. Experiments show that our method not only improves the model's reasoning performance but also equips it with remarkable self-rewarding capability, thereby boosting its inference-time scaling performance.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "278",
        "title": "MetaBench: A Multi-task Benchmark for Assessing LLMs in Metabolomics",
        "author": [
            "Yuxing Lu",
            "Xukai Zhao",
            "J. Ben Tamo",
            "Micky C. Nnamdi",
            "Rui Peng",
            "Shuang Zeng",
            "Xingyu Hu",
            "Jinzhuo Wang",
            "May D. Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14944",
        "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities on general text; however, their proficiency in specialized scientific domains that require deep, interconnected knowledge remains largely uncharacterized. Metabolomics presents unique challenges with its complex biochemical pathways, heterogeneous identifier systems, and fragmented databases. To systematically evaluate LLM capabilities in this domain, we introduce MetaBench, the first benchmark for metabolomics assessment. Curated from authoritative public resources, MetaBench evaluates five capabilities essential for metabolomics research: knowledge, understanding, grounding, reasoning, and research. Our evaluation of 25 open- and closed-source LLMs reveals distinct performance patterns across metabolomics tasks: while models perform well on text generation tasks, cross-database identifier grounding remains challenging even with retrieval augmentation. Model performance also decreases on long-tail metabolites with sparse annotations. With MetaBench, we provide essential infrastructure for developing and evaluating metabolomics AI systems, enabling systematic progress toward reliable computational tools for metabolomics research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "279",
        "title": "3D Scene Prompting for Scene-Consistent Camera-Controllable Video Generation",
        "author": [
            "JoungBin Lee",
            "Jaewoo Jung",
            "Jisang Han",
            "Takuya Narihira",
            "Kazumi Fukuda",
            "Junyoung Seo",
            "Sunghwan Hong",
            "Yuki Mitsufuji",
            "Seungryong Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14945",
        "abstract": "We present 3DScenePrompt, a framework that generates the next video chunk from arbitrary-length input while enabling precise camera control and preserving scene consistency. Unlike methods conditioned on a single image or a short clip, we employ dual spatio-temporal conditioning that reformulates context-view referencing across the input video. Our approach conditions on both temporally adjacent frames for motion continuity and spatially adjacent content for scene consistency. However, when generating beyond temporal boundaries, directly using spatially adjacent frames would incorrectly preserve dynamic elements from the past. We address this by introducing a 3D scene memory that represents exclusively the static geometry extracted from the entire input video. To construct this memory, we leverage dynamic SLAM with our newly introduced dynamic masking strategy that explicitly separates static scene geometry from moving elements. The static scene representation can then be projected to any target viewpoint, providing geometrically consistent warped views that serve as strong 3D spatial prompts while allowing dynamic regions to evolve naturally from temporal context. This enables our model to maintain long-range spatial coherence and precise camera control without sacrificing computational efficiency or motion realism. Extensive experiments demonstrate that our framework significantly outperforms existing methods in scene consistency, camera controllability, and generation quality. Project page : https://cvlab-kaist.github.io/3DScenePrompt/",
        "tags": [
            "3D",
            "CLIP",
            "SLAM",
            "Video Generation"
        ]
    },
    {
        "id": "280",
        "title": "DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation",
        "author": [
            "Yu Zhou",
            "Sohyun An",
            "Haikang Deng",
            "Da Yin",
            "Clark Peng",
            "Cho-Jui Hsieh",
            "Kai-Wei Chang",
            "Nanyun Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14949",
        "abstract": "Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "281",
        "title": "From Language to Locomotion: Retargeting-free Humanoid Control via Motion Latent Guidance",
        "author": [
            "Zhe Li",
            "Cheng Chi",
            "Yangyang Wei",
            "Boan Zhu",
            "Yibo Peng",
            "Tao Huang",
            "Pengwei Wang",
            "Zhongyuan Wang",
            "Shanghang Zhang",
            "Chang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14952",
        "abstract": "Natural language offers a natural interface for humanoid robots, but existing language-guided humanoid locomotion pipelines remain cumbersome and unreliable. They typically decode human motion, retarget it to robot morphology, and then track it with a physics-based controller. However, this multi-stage process is prone to cumulative errors, introduces high latency, and yields weak coupling between semantics and control. These limitations call for a more direct pathway from language to action, one that eliminates fragile intermediate stages. Therefore, we present RoboGhost, a retargeting-free framework that directly conditions humanoid policies on language-grounded motion latents. By bypassing explicit motion decoding and retargeting, RoboGhost enables a diffusion-based policy to denoise executable actions directly from noise, preserving semantic intent and supporting fast, reactive control. A hybrid causal transformer-diffusion motion generator further ensures long-horizon consistency while maintaining stability and diversity, yielding rich latent representations for precise humanoid behavior. Extensive experiments demonstrate that RoboGhost substantially reduces deployment latency, improves success rates and tracking accuracy, and produces smooth, semantically aligned locomotion on real humanoids. Beyond text, the framework naturally extends to other modalities such as images, audio, and music, providing a general foundation for vision-language-action humanoid systems.",
        "tags": [
            "Diffusion",
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": "282",
        "title": "OmniMotion: Multimodal Motion Generation with Continuous Masked Autoregression",
        "author": [
            "Zhe Li",
            "Weihao Yuan",
            "Weichao Shen",
            "Siyu Zhu",
            "Zilong Dong",
            "Chang Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14954",
        "abstract": "Whole-body multi-modal human motion generation poses two primary challenges: creating an effective motion generation mechanism and integrating various modalities, such as text, speech, and music, into a cohesive framework. Unlike previous methods that usually employ discrete masked modeling or autoregressive modeling, we develop a continuous masked autoregressive motion transformer, where a causal attention is performed considering the sequential nature within the human motion. Within this transformer, we introduce a gated linear attention and an RMSNorm module, which drive the transformer to pay attention to the key actions and suppress the instability caused by either the abnormal movements or the heterogeneous distributions within multi-modalities. To further enhance both the motion generation and the multimodal generalization, we employ the DiT structure to diffuse the conditions from the transformer towards the targets. To fuse different modalities, AdaLN and cross-attention are leveraged to inject the text, speech, and music signals. Experimental results demonstrate that our framework outperforms previous methods across all modalities, including text-to-motion, speech-to-gesture, and music-to-dance. The code of our method will be made public.",
        "tags": [
            "DiT",
            "Transformer"
        ]
    },
    {
        "id": "283",
        "title": "RealDPO: Real or Not Real, that is the Preference",
        "author": [
            "Guo Cheng",
            "Danni Yang",
            "Ziqi Huang",
            "Jianlou Si",
            "Chenyang Si",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14955",
        "abstract": "Video generative models have recently achieved notable advancements in synthesis quality. However, generating complex motions remains a critical challenge, as existing models often struggle to produce natural, smooth, and contextually consistent movements. This gap between generated and real-world motions limits their practical applicability. To address this issue, we introduce RealDPO, a novel alignment paradigm that leverages real-world data as positive samples for preference learning, enabling more accurate motion synthesis. Unlike traditional supervised fine-tuning (SFT), which offers limited corrective feedback, RealDPO employs Direct Preference Optimization (DPO) with a tailored loss function to enhance motion realism. By contrasting real-world videos with erroneous model outputs, RealDPO enables iterative self-correction, progressively refining motion quality. To support post-training in complex motion synthesis, we propose RealAction-5K, a curated dataset of high-quality videos capturing human daily activities with rich and precise motion details. Extensive experiments demonstrate that RealDPO significantly improves video quality, text alignment, and motion realism compared to state-of-the-art models and existing preference optimization techniques.",
        "tags": [
            "DPO"
        ]
    },
    {
        "id": "284",
        "title": "MathCanvas: Intrinsic Visual Chain-of-Thought for Multimodal Mathematical Reasoning",
        "author": [
            "Weikang Shi",
            "Aldrich Yu",
            "Rongyao Fang",
            "Houxing Ren",
            "Ke Wang",
            "Aojun Zhou",
            "Changyao Tian",
            "Xinyu Fu",
            "Yuxuan Hu",
            "Zimu Lu",
            "Linjiang Huang",
            "Si Liu",
            "Rui Liu",
            "Hongsheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14958",
        "abstract": "While Large Language Models (LLMs) have excelled in textual reasoning, they struggle with mathematical domains like geometry that intrinsically rely on visual aids. Existing approaches to Visual Chain-of-Thought (VCoT) are often limited by rigid external tools or fail to generate the high-fidelity, strategically-timed diagrams necessary for complex problem-solving. To bridge this gap, we introduce MathCanvas, a comprehensive framework designed to endow unified Large Multimodal Models (LMMs) with intrinsic VCoT capabilities for mathematics. Our approach consists of two phases. First, a Visual Manipulation stage pre-trains the model on a novel 15.2M-pair corpus, comprising 10M caption-to-diagram pairs (MathCanvas-Imagen) and 5.2M step-by-step editing trajectories (MathCanvas-Edit), to master diagram generation and editing. Second, a Strategic Visual-Aided Reasoning stage fine-tunes the model on MathCanvas-Instruct, a new 219K-example dataset of interleaved visual-textual reasoning paths, teaching it when and how to leverage visual aids. To facilitate rigorous evaluation, we introduce MathCanvas-Bench, a challenging benchmark with 3K problems that require models to produce interleaved visual-textual solutions. Our model, BAGEL-Canvas, trained under this framework, achieves an 86% relative improvement over strong LMM baselines on MathCanvas-Bench, demonstrating excellent generalization to other public math benchmarks. Our work provides a complete toolkit-framework, datasets, and benchmark-to unlock complex, human-like visual-aided reasoning in LMMs. Project Page: https://mathcanvas.github.io/",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "285",
        "title": "CBF-RL: Safety Filtering Reinforcement Learning in Training with Control Barrier Functions",
        "author": [
            "Lizhi Yang",
            "Blake Werner",
            "Massimiliano de Sa Aaron D. Ames"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14959",
        "abstract": "Reinforcement learning (RL), while powerful and expressive, can often prioritize performance at the expense of safety. Yet safety violations can lead to catastrophic outcomes in real-world deployments. Control Barrier Functions (CBFs) offer a principled method to enforce dynamic safety -- traditionally deployed \\emph{online} via safety filters. While the result is safe behavior, the fact that the RL policy does not have knowledge of the CBF can lead to conservative behaviors. This paper proposes CBF-RL, a framework for generating safe behaviors with RL by enforcing CBFs \\emph{in training}. CBF-RL has two key attributes: (1) minimally modifying a nominal RL policy to encode safety constraints via a CBF term, (2) and safety filtering of the policy rollouts in training. Theoretically, we prove that continuous-time safety filters can be deployed via closed-form expressions on discrete-time roll-outs. Practically, we demonstrate that CBF-RL internalizes the safety constraints in the learned policy -- both enforcing safer actions and biasing towards safer rewards -- enabling safe deployment without the need for an online safety filter. We validate our framework through ablation studies on navigation tasks and on the Unitree G1 humanoid robot, where CBF-RL enables safer exploration, faster convergence, and robust performance under uncertainty, enabling the humanoid robot to avoid obstacles and climb stairs safely in real-world settings without a runtime safety filter.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "286",
        "title": "Efficient Parallel Samplers for Recurrent-Depth Models and Their Connection to Diffusion Language Models",
        "author": [
            "Jonas Geiping",
            "Xinyu Yang",
            "Guinan Su"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14961",
        "abstract": "Language models with recurrent depth, also referred to as universal or looped when considering transformers, are defined by the capacity to increase their computation through the repetition of layers. Recent efforts in pretraining have demonstrated that these architectures can scale to modern language modeling tasks while exhibiting advantages in reasoning tasks. In this work, we examine the relationship between recurrent-depth models and diffusion language models. Building on their similarities, we develop a new diffusion forcing sampler for these models to accelerate generation. The sampler advances by decoding new tokens at every forward pass of the model, while the latent states of these tokens can be further refined in parallel through recurrence. Theoretically, generation with our sampler is strictly more expressive than the baseline autoregressive generation using the same time budget on modern hardware. Moreover, this sampler, based on principles from diffusion literature, can be directly applied to existing 3.5B recurrent-depth transformers without any tuning, leading to up to a 5x speedup. Consequently, our findings not only provide an efficient mechanism for parallelizing the extra computation in recurrent-depth models at inference, but also suggest that such models can be naturally viewed as strong continuous, though causal, diffusion language models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "287",
        "title": "RainDiff: End-to-end Precipitation Nowcasting Via Token-wise Attention Diffusion",
        "author": [
            "Thao Nguyen",
            "Jiaqi Ma",
            "Fahad Shahbaz Khan",
            "Souhaib Ben Taieb",
            "Salman Khan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14962",
        "abstract": "Precipitation nowcasting, predicting future radar echo sequences from current observations, is a critical yet challenging task due to the inherently chaotic and tightly coupled spatio-temporal dynamics of the atmosphere. While recent advances in diffusion-based models attempt to capture both large-scale motion and fine-grained stochastic variability, they often suffer from scalability issues: latent-space approaches require a separately trained autoencoder, adding complexity and limiting generalization, while pixel-space approaches are computationally intensive and often omit attention mechanisms, reducing their ability to model long-range spatio-temporal dependencies. To address these limitations, we propose a Token-wise Attention integrated into not only the U-Net diffusion model but also the spatio-temporal encoder that dynamically captures multi-scale spatial interactions and temporal evolution. Unlike prior approaches, our method natively integrates attention into the architecture without incurring the high resource cost typical of pixel-space diffusion, thereby eliminating the need for separate latent modules. Our extensive experiments and visual evaluations across diverse datasets demonstrate that the proposed method significantly outperforms state-of-the-art approaches, yielding superior local fidelity, generalization, and robustness in complex precipitation forecasting scenarios.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "288",
        "title": "Identity-Link IRT for Label-Free LLM Evaluation: Preserving Additivity in TVD-MI Scores",
        "author": [
            "Zachary Robertson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14966",
        "abstract": "Pairwise comparisons of large language models using total variation distance mutual information (TVD-MI) produce binary critic decisions per pair. We show that averaging TVD-MI's binary trials yields centered-probability scores with additive structure suitable for item-response theory (IRT) without nonlinear link functions. Maximum-likelihood approaches to IRT use logistic links, but we find empirically that these transformations introduce curvature that breaks additivity: across three domains, the identity link yields median curl on raw data of 0.080-0.150 (P95 = [0.474, 0.580]), whereas probit/logit introduce substantially higher violations (median [0.245, 0.588], P95 [0.825, 2.252]). We derive this clipped-linear model from Gini entropy maximization, yielding a box-constrained least-squares formulation that handles boundary saturation. At 33% coverage, we achieve holdout RMSE $0.117 \\pm 0.008$ while preserving agent rankings (Spearman $\\rho = 0.972 \\pm 0.015$), three times fewer evaluations than full dense. Judge robustness analysis (GPT-4o-mini vs. Llama3-70b) shows strong agreement in agent rankings ($\\rho = 0.872$) and consistent identity-link advantage. TVD-MI's geometry is best preserved by identity mapping for efficient LLM evaluation, applicable to other bounded-response domains.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "289",
        "title": "Information Gain-based Policy Optimization: A Simple and Effective Approach for Multi-Turn LLM Agents",
        "author": [
            "Guoqing Wang",
            "Sunhao Dai",
            "Guangze Ye",
            "Zeyu Gan",
            "Wei Yao",
            "Yong Deng",
            "Xiaofeng Wu",
            "Zhenzhe Ying"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14967",
        "abstract": "Large language model (LLM)-based agents are increasingly trained with reinforcement learning (RL) to enhance their ability to interact with external environments through tool use, particularly in search-based settings that require multi-turn reasoning and knowledge acquisition. However, existing approaches typically rely on outcome-based rewards that are only provided at the final answer. This reward sparsity becomes particularly problematic in multi-turn settings, where long trajectories exacerbate two critical issues: (i) advantage collapse, where all rollouts receive identical rewards and provide no useful learning signals, and (ii) lack of fine-grained credit assignment, where dependencies between turns are obscured, especially in long-horizon tasks. In this paper, we propose Information Gain-based Policy Optimization (IGPO), a simple yet effective RL framework that provides dense and intrinsic supervision for multi-turn agent training. IGPO models each interaction turn as an incremental process of acquiring information about the ground truth, and defines turn-level rewards as the marginal increase in the policy's probability of producing the correct answer. Unlike prior process-level reward approaches that depend on external reward models or costly Monte Carlo estimation, IGPO derives intrinsic rewards directly from the model's own belief updates. These intrinsic turn-level rewards are combined with outcome-level supervision to form dense reward trajectories. Extensive experiments on both in-domain and out-of-domain benchmarks demonstrate that IGPO consistently outperforms strong baselines in multi-turn scenarios, achieving higher accuracy and improved sample efficiency.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "290",
        "title": "RDD: Retrieval-Based Demonstration Decomposer for Planner Alignment in Long-Horizon Tasks",
        "author": [
            "Mingxuan Yan",
            "Yuping Wang",
            "Zechun Liu",
            "Jiachen Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14968",
        "abstract": "To tackle long-horizon tasks, recent hierarchical vision-language-action (VLAs) frameworks employ vision-language model (VLM)-based planners to decompose complex manipulation tasks into simpler sub-tasks that low-level visuomotor policies can easily handle. Typically, the VLM planner is finetuned to learn to decompose a target task. This finetuning requires target task demonstrations segmented into sub-tasks by either human annotation or heuristic rules. However, the heuristic subtasks can deviate significantly from the training data of the visuomotor policy, which degrades task performance. To address these issues, we propose a Retrieval-based Demonstration Decomposer (RDD) that automatically decomposes demonstrations into sub-tasks by aligning the visual features of the decomposed sub-task intervals with those from the training data of the low-level visuomotor policies. Our method outperforms the state-of-the-art sub-task decomposer on both simulation and real-world tasks, demonstrating robustness across diverse settings. Code and more results are available at http://rdd-neurips.github.io.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "291",
        "title": "LLMs as Scalable, General-Purpose Simulators For Evolving Digital Agent Training",
        "author": [
            "Yiming Wang",
            "Da Yin",
            "Yuedong Cui",
            "Ruichen Zheng",
            "Zhiqian Li",
            "Zongyu Lin",
            "Di Wu",
            "Xueqing Wu",
            "Chenchen Ye",
            "Yu Zhou",
            "Kai-Wei Chang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14969",
        "abstract": "Digital agents require diverse, large-scale UI trajectories to generalize across real-world tasks, yet collecting such data is prohibitively expensive in both human annotation, infra and engineering perspectives. To this end, we introduce $\\textbf{UI-Simulator}$, a scalable paradigm that generates structured UI states and transitions to synthesize training trajectories at scale. Our paradigm integrates a digital world simulator for diverse UI states, a guided rollout process for coherent exploration, and a trajectory wrapper that produces high-quality and diverse trajectories for agent training. We further propose $\\textbf{UI-Simulator-Grow}$, a targeted scaling strategy that enables more rapid and data-efficient scaling by prioritizing high-impact tasks and synthesizes informative trajectory variants. Experiments on WebArena and AndroidWorld show that UI-Simulator rivals or surpasses open-source agents trained on real UIs with significantly better robustness, despite using weaker teacher models. Moreover, UI-Simulator-Grow matches the performance of Llama-3-70B-Instruct using only Llama-3-8B-Instruct as the base model, highlighting the potential of targeted synthesis scaling paradigm to continuously and efficiently enhance the digital agents.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "292",
        "title": "TokDrift: When LLM Speaks in Subwords but Code Speaks in Grammar",
        "author": [
            "Yinxi Li",
            "Yuntian Deng",
            "Pengyu Nie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14972",
        "abstract": "Large language models (LLMs) for code rely on subword tokenizers, such as byte-pair encoding (BPE), learned from mixed natural language text and programming language code but driven by statistics rather than grammar. As a result, semantically identical code snippets can be tokenized differently depending on superficial factors such as whitespace or identifier naming. To measure the impact of this misalignment, we introduce TokDrift, a framework that applies semantic-preserving rewrite rules to create code variants differing only in tokenization. Across nine code LLMs, including large ones with over 30B parameters, even minor formatting changes can cause substantial shifts in model behavior. Layer-wise analysis shows that the issue originates in early embeddings, where subword segmentation fails to capture grammar token boundaries. Our findings identify misaligned tokenization as a hidden obstacle to reliable code understanding and generation, highlighting the need for grammar-aware tokenization for future code LLMs.",
        "tags": [
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "293",
        "title": "Attention Is All You Need for KV Cache in Diffusion LLMs",
        "author": [
            "Quan Nguyen-Tri",
            "Mukul Ranjan",
            "Zhiqiang Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14973",
        "abstract": "This work studies how to adaptively recompute key-value (KV) caches for diffusion large language models (DLMs) to maximize prediction accuracy while minimizing decoding latency. Prior methods' decoders recompute QKV for all tokens at every denoising step and layer, despite KV states changing little across most steps, especially in shallow layers, leading to substantial redundancy. We make three observations: (1) distant ${\\bf MASK}$ tokens primarily act as a length-bias and can be cached block-wise beyond the active prediction window; (2) KV dynamics increase with depth, suggesting that selective refresh starting from deeper layers is sufficient; and (3) the most-attended token exhibits the smallest KV drift, providing a conservative lower bound on cache change for other tokens. Building on these, we propose ${\\bf Elastic-Cache}$, a training-free, architecture-agnostic strategy that jointly decides ${when}$ to refresh (via an attention-aware drift test on the most-attended token) and ${where}$ to refresh (via a depth-aware schedule that recomputes from a chosen layer onward while reusing shallow-layer caches and off-window MASK caches). Unlike fixed-period schemes, Elastic-Cache performs adaptive, layer-aware cache updates for diffusion LLMs, reducing redundant computation and accelerating decoding with negligible loss in generation quality. Experiments on LLaDA-Instruct, LLaDA-1.5, and LLaDA-V across mathematical reasoning and code generation tasks demonstrate consistent speedups: $8.7\\times$ on GSM8K (256 tokens), $45.1\\times$ on longer sequences, and $4.8\\times$ on HumanEval, while consistently maintaining higher accuracy than the baseline. Our method achieves significantly higher throughput ($6.8\\times$ on GSM8K) than existing confidence-based approaches while preserving generation quality, enabling practical deployment of diffusion LLMs.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "294",
        "title": "pi-Flow: Policy-Based Few-Step Generation via Imitation Distillation",
        "author": [
            "Hansheng Chen",
            "Kai Zhang",
            "Hao Tan",
            "Leonidas Guibas",
            "Gordon Wetzstein",
            "Sai Bi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14974",
        "abstract": "Few-step diffusion or flow-based generative models typically distill a velocity-predicting teacher into a student that predicts a shortcut towards denoised data. This format mismatch has led to complex distillation procedures that often suffer from a quality-diversity trade-off. To address this, we propose policy-based flow models ($\\pi$-Flow). $\\pi$-Flow modifies the output layer of a student flow model to predict a network-free policy at one timestep. The policy then produces dynamic flow velocities at future substeps with negligible overhead, enabling fast and accurate ODE integration on these substeps without extra network evaluations. To match the policy's ODE trajectory to the teacher's, we introduce a novel imitation distillation approach, which matches the policy's velocity to the teacher's along the policy's trajectory using a standard $\\ell_2$ flow matching loss. By simply mimicking the teacher's behavior, $\\pi$-Flow enables stable and scalable training and avoids the quality-diversity trade-off. On ImageNet 256$^2$, it attains a 1-NFE FID of 2.85, outperforming MeanFlow of the same DiT architecture. On FLUX.1-12B and Qwen-Image-20B at 4 NFEs, $\\pi$-Flow achieves substantially better diversity than state-of-the-art few-step methods, while maintaining teacher-level quality.",
        "tags": [
            "DiT",
            "Diffusion",
            "FLUX",
            "Flow Matching",
            "ODE",
            "Qwen"
        ]
    },
    {
        "id": "295",
        "title": "WithAnyone: Towards Controllable and ID Consistent Image Generation",
        "author": [
            "Hengyuan Xu",
            "Wei Cheng",
            "Peng Xing",
            "Yixiao Fang",
            "Shuhan Wu",
            "Rui Wang",
            "Xianfang Zeng",
            "Daxin Jiang",
            "Gang Yu",
            "Xingjun Ma",
            "Yu-Gang Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14975",
        "abstract": "Identity-consistent generation has become an important focus in text-to-image research, with recent models achieving notable success in producing images aligned with a reference identity. Yet, the scarcity of large-scale paired datasets containing multiple images of the same individual forces most approaches to adopt reconstruction-based training. This reliance often leads to a failure mode we term copy-paste, where the model directly replicates the reference face rather than preserving identity across natural variations in pose, expression, or lighting. Such over-similarity undermines controllability and limits the expressive power of generation. To address these limitations, we (1) construct a large-scale paired dataset MultiID-2M, tailored for multi-person scenarios, providing diverse references for each identity; (2) introduce a benchmark that quantifies both copy-paste artifacts and the trade-off between identity fidelity and variation; and (3) propose a novel training paradigm with a contrastive identity loss that leverages paired data to balance fidelity with diversity. These contributions culminate in WithAnyone, a diffusion-based model that effectively mitigates copy-paste while preserving high identity similarity. Extensive qualitative and quantitative experiments demonstrate that WithAnyone significantly reduces copy-paste artifacts, improves controllability over pose and expression, and maintains strong perceptual quality. User studies further validate that our method achieves high identity fidelity while enabling expressive controllable generation.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "296",
        "title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
        "author": [
            "Shaowei Liu",
            "Chuan Guo",
            "Bing Zhou",
            "Jian Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14976",
        "abstract": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "297",
        "title": "Terra: Explorable Native 3D World Model with Point Latents",
        "author": [
            "Yuanhui Huang",
            "Weiliang Chen",
            "Wenzhao Zheng",
            "Xin Tao",
            "Pengfei Wan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14977",
        "abstract": "World models have garnered increasing attention for comprehensive modeling of the real world. However, most existing methods still rely on pixel-aligned representations as the basis for world evolution, neglecting the inherent 3D nature of the physical world. This could undermine the 3D consistency and diminish the modeling efficiency of world models. In this paper, we present Terra, a native 3D world model that represents and generates explorable environments in an intrinsic 3D latent space. Specifically, we propose a novel point-to-Gaussian variational autoencoder (P2G-VAE) that encodes 3D inputs into a latent point representation, which is subsequently decoded as 3D Gaussian primitives to jointly model geometry and appearance. We then introduce a sparse point flow matching network (SPFlow) for generating the latent point representation, which simultaneously denoises the positions and features of the point latents. Our Terra enables exact multi-view consistency with native 3D representation and architecture, and supports flexible rendering from any viewpoint with only a single generation process. Furthermore, Terra achieves explorable world modeling through progressive generation in the point latent space. We conduct extensive experiments on the challenging indoor scenes from ScanNet v2. Terra achieves state-of-the-art performance in both reconstruction and generation with high 3D consistency.",
        "tags": [
            "3D",
            "Flow Matching",
            "VAE"
        ]
    },
    {
        "id": "298",
        "title": "Learning an Image Editing Model without Image Editing Pairs",
        "author": [
            "Nupur Kumari",
            "Sheng-Yu Wang",
            "Nanxuan Zhao",
            "Yotam Nitzan",
            "Yuheng Li",
            "Krishna Kumar Singh",
            "Richard Zhang",
            "Eli Shechtman",
            "Jun-Yan Zhu",
            "Xun Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14978",
        "abstract": "Recent image editing models have achieved impressive results while following natural language editing instructions, but they rely on supervised fine-tuning with large datasets of input-target pairs. This is a critical bottleneck, as such naturally occurring pairs are hard to curate at scale. Current workarounds use synthetic training pairs that leverage the zero-shot capabilities of existing models. However, this can propagate and magnify the artifacts of the pretrained model into the final trained model. In this work, we present a new training paradigm that eliminates the need for paired data entirely. Our approach directly optimizes a few-step diffusion model by unrolling it during training and leveraging feedback from vision-language models (VLMs). For each input and editing instruction, the VLM evaluates if an edit follows the instruction and preserves unchanged content, providing direct gradients for end-to-end optimization. To ensure visual fidelity, we incorporate distribution matching loss (DMD), which constrains generated images to remain within the image manifold learned by pretrained models. We evaluate our method on standard benchmarks and include an extensive ablation study. Without any paired data, our method performs on par with various image editing diffusion models trained on extensive supervised paired data, under the few-step setting. Given the same VLM as the reward model, we also outperform RL-based techniques like Flow-GRPO.",
        "tags": [
            "Diffusion",
            "GRPO",
            "Image Editing",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "299",
        "title": "From Pixels to Words -- Towards Native Vision-Language Primitives at Scale",
        "author": [
            "Haiwen Diao",
            "Mingxuan Li",
            "Silei Wu",
            "Linjun Dai",
            "Xiaohua Wang",
            "Hanming Deng",
            "Lewei Lu",
            "Dahua Lin",
            "Ziwei Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14979",
        "abstract": "The edifice of native Vision-Language Models (VLMs) has emerged as a rising contender to typical modular VLMs, shaped by evolving model architectures and training paradigms. Yet, two lingering clouds cast shadows over its widespread exploration and promotion: (-) What fundamental constraints set native VLMs apart from modular ones, and to what extent can these barriers be overcome? (-) How to make research in native VLMs more accessible and democratized, thereby accelerating progress in the field. In this paper, we clarify these challenges and outline guiding principles for constructing native VLMs. Specifically, one native VLM primitive should: (i) effectively align pixel and word representations within a shared semantic space; (ii) seamlessly integrate the strengths of formerly separate vision and language modules; (iii) inherently embody various cross-modal properties that support unified vision-language encoding, aligning, and reasoning. Hence, we launch NEO, a novel family of native VLMs built from first principles, capable of rivaling top-tier modular counterparts across diverse real-world scenarios. With only 390M image-text examples, NEO efficiently develops visual perception from scratch while mitigating vision-language conflicts inside a dense and monolithic model crafted from our elaborate primitives. We position NEO as a cornerstone for scalable and powerful native VLMs, paired with a rich set of reusable components that foster a cost-effective and extensible ecosystem. Our code and models are publicly available at: https://github.com/EvolvingLMMs-Lab/NEO.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "300",
        "title": "Agentic Design of Compositional Machines",
        "author": [
            "Wenqian Zhang",
            "Weiyang Liu",
            "Zhen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14980",
        "abstract": "The design of complex machines stands as both a marker of human intelligence and a foundation of engineering practice. Given recent advances in large language models (LLMs), we ask whether they, too, can learn to create. We approach this question through the lens of compositional machine design: a task in which machines are assembled from standardized components to meet functional demands like locomotion or manipulation in a simulated physical environment. To support this investigation, we introduce BesiegeField, a testbed built on the machine-building game Besiege, which enables part-based construction, physical simulation and reward-driven evaluation. Using BesiegeField, we benchmark state-of-the-art LLMs with agentic workflows and identify key capabilities required for success, including spatial reasoning, strategic assembly, and instruction-following. As current open-source models fall short, we explore reinforcement learning (RL) as a path to improvement: we curate a cold-start dataset, conduct RL finetuning experiments, and highlight open challenges at the intersection of language, machine design, and physical reasoning.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "301",
        "title": "Coupled Diffusion Sampling for Training-Free Multi-View Image Editing",
        "author": [
            "Hadi Alzayer",
            "Yunzhi Zhang",
            "Chen Geng",
            "Jia-Bin Huang",
            "Jiajun Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14981",
        "abstract": "We present an inference-time diffusion sampling method to perform multi-view consistent image editing using pre-trained 2D image editing models. These models can independently produce high-quality edits for each image in a set of multi-view images of a 3D scene or object, but they do not maintain consistency across views. Existing approaches typically address this by optimizing over explicit 3D representations, but they suffer from a lengthy optimization process and instability under sparse view settings. We propose an implicit 3D regularization approach by constraining the generated 2D image sequences to adhere to a pre-trained multi-view image distribution. This is achieved through coupled diffusion sampling, a simple diffusion sampling technique that concurrently samples two trajectories from both a multi-view image distribution and a 2D edited image distribution, using a coupling term to enforce the multi-view consistency among the generated images. We validate the effectiveness and generality of this framework on three distinct multi-view image editing tasks, demonstrating its applicability across various model architectures and highlighting its potential as a general solution for multi-view consistent editing.",
        "tags": [
            "3D",
            "Diffusion",
            "Image Editing"
        ]
    },
    {
        "id": "302",
        "title": "Transfer Learning-Enabled Efficient Raman Pump Tuning under Dynamic Launch Power for C+L Band Transmission",
        "author": [
            "Jiaming Liu",
            "Rui Wang",
            "JinJiang Li",
            "Hong Lin",
            "Jing Zhang",
            "Kun Qiu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09047",
        "abstract": "We propose a transfer learning-enabled Transformer framework to simultaneously realize accurate modeling and Raman pump design in C+L-band systems. The RMSE for modeling and peak-to-peak GSNR variation/deviation is within 0.22 dB and 0.86/0.1 dB, respectively.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "303",
        "title": "Towards Neurocognitive-Inspired Intelligence: From AI's Structural Mimicry to Human-Like Functional Cognition",
        "author": [
            "Noorbakhsh Amiri Golilarz",
            "Hassan S. Al Khatib",
            "Shahram Rahimi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.13826",
        "abstract": "Artificial intelligence has advanced significantly through deep learning, reinforcement learning, and large language and vision models. However, these systems often remain task specific, struggle to adapt to changing conditions, and cannot generalize in ways similar to human cognition. Additionally, they mainly focus on mimicking brain structures, which often leads to black-box models with limited transparency and adaptability. Inspired by the structure and function of biological cognition, this paper introduces the concept of \"Neurocognitive-Inspired Intelligence (NII),\" a hybrid approach that combines neuroscience, cognitive science, computer vision, and AI to develop more general, adaptive, and robust intelligent systems capable of rapid learning, learning from less data, and leveraging prior experience. These systems aim to emulate the human brain's ability to flexibly learn, reason, remember, perceive, and act in real-world settings with minimal supervision. We review the limitations of current AI methods, define core principles of neurocognitive-inspired intelligence, and propose a modular, biologically inspired architecture that emphasizes integration, embodiment, and adaptability. We also discuss potential implementation strategies and outline various real-world applications, from robotics to education and healthcare. Importantly, this paper offers a hybrid roadmap for future research, laying the groundwork for building AI systems that more closely resemble human cognition.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "304",
        "title": "EdgeNavMamba: Mamba Optimized Object Detection for Energy Efficient Edge Devices",
        "author": [
            "Romina Aalishah",
            "Mozhgan Navardi",
            "Tinoosh Mohsenin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14946",
        "abstract": "Deployment of efficient and accurate Deep Learning models has long been a challenge in autonomous navigation, particularly for real-time applications on resource-constrained edge devices. Edge devices are limited in computing power and memory, making model efficiency and compression essential. In this work, we propose EdgeNavMamba, a reinforcement learning-based framework for goal-directed navigation using an efficient Mamba object detection model. To train and evaluate the detector, we introduce a custom shape detection dataset collected in diverse indoor settings, reflecting visual cues common in real-world navigation. The object detector serves as a pre-processing module, extracting bounding boxes (BBOX) from visual input, which are then passed to an RL policy to control goal-oriented navigation. Experimental results show that the student model achieved a reduction of 67% in size, and up to 73% in energy per inference on edge devices of NVIDIA Jetson Orin Nano and Raspberry Pi 5, while keeping the same performance as the teacher model. EdgeNavMamba also maintains high detection accuracy in MiniWorld and IsaacLab simulators while reducing parameters by 31% compared to the baseline. In the MiniWorld simulator, the navigation policy achieves over 90% success across environments of varying complexity.",
        "tags": [
            "Detection",
            "Mamba",
            "RL"
        ]
    }
]