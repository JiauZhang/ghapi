[
    {
        "id": "1",
        "title": "Two-Stage Sketch-Based Smoke Illustration Generation using Stream Function",
        "author": [
            "Hengyuan Chang",
            "Xiaoxuan Xie",
            "Syuhei Sato",
            "Haoran Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15873",
        "abstract": "In this paper, we propose a two-stage sketch-based smoke illustration generation framework using stream function and latent diffusion models (LDM). The user sketch is used to guide the generation of the stream function, which serves as the control condition for the velocity field generator. The generated velocity field can be used to guide the smoke simulation to align with the intended flow. We adopt streamlines to encode global flow dynamics as sketch guidance during training. The stream function constitutes the intermediate representation that captures continuous variation and rotational flow details absent from sketches.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "2",
        "title": "Sketch-based Fluid Video Generation Using Motion-Guided Diffusion Models in Still Landscape Images",
        "author": [
            "Hao Jin",
            "Haoran Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15874",
        "abstract": "Integrating motion into static images not only enhances visual expressiveness but also creates a sense of immersion and temporal depth, establishing it as a longstanding and impactful theme in artistic expression. Fluid elements such as waterfall, river, and oceans are common features in landscape, but their complex dynamic characteristics pose significant challenges in modeling and controlling their motion within visual computing. Physics-based methods are often used in fluid animation to track particle movement. However, they are easily affected by boundary conditions. Recently, latent diffusion models have been applied to video generation tasks, demonstrating impressive capabilities in producing high-quality and temporally coherent results. However, it is challenging for the existing methods to animate fluid smooth and temporally consistent motion. To solve these issues, this paper introduces a framework for generating landscape videos by animating fluid in still images under the guidance of motion sketches. We propose a finetuned conditional latent diffusion model for generating motion field from user-provided sketches, which are subsequently integrated into a latent video diffusion model via a motion adapter to precisely control the fluid movement.",
        "tags": [
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "3",
        "title": "FlexLink: Boosting your NVLink Bandwidth by 27% without accuracy concern",
        "author": [
            "Ao Shen",
            "Rui Zhang",
            "Junping Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15882",
        "abstract": "As large language models (LLMs) continue to scale, multi-node deployment has become a necessity. Consequently, communication has become a critical performance bottleneck. Current intra-node communication libraries, like NCCL, typically make use of a single interconnect such as NVLink. This approach creates performance ceilings, especially on hardware like the H800 GPU where the primary interconnect's bandwidth can become a bottleneck, and leaves other hardware resources like PCIe and Remote Direct Memory Access (RDMA)-capable Network Interface Cards (NICs) largely idle during intensive workloads. We propose FlexLink, the first collective communication framework to the best of our knowledge designed to systematically address this by aggregating these heterogeneous links-NVLink, PCIe, and RDMA NICs-into a single, high-performance communication fabric. FlexLink employs an effective two-stage adaptive load balancing strategy that dynamically partitions communication traffic across all available links, ensuring that faster interconnects are not throttled by slower ones. On an 8-GPU H800 server, our design improves the bandwidth of collective operators such as AllReduce and AllGather by up to 26% and 27% over the NCCL baseline, respectively. This gain is achieved by offloading 2-22% of the total communication traffic to the previously underutilized PCIe and RDMA NICs. FlexLink provides these improvements as a lossless, drop-in replacement compatible with the NCCL API, ensuring easy adoption.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "4",
        "title": "Mitigating Harmful Erraticism in LLMs Through Dialectical Behavior Therapy Based De-Escalation Strategies",
        "author": [
            "Pooja Rangarajan",
            "Jacob Boyle"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15889",
        "abstract": "The escalating demand for personalized AI chatbot interactions, capable of dynamically adapting to user emotional states and real-time requests, has highlighted critical limitations in current development paradigms. Existing methodologies, which rely on baseline programming, custom personalities, and manual response adjustments, often prove difficult to maintain and are susceptible to errors such as hallucinations, erratic outputs, and software bugs. This paper hypothesizes that a framework rooted in human psychological principles, specifically therapeutic modalities, can provide a more robust and sustainable solution than purely technical interventions. Drawing an analogy to the simulated neural networks of AI mirroring the human brain, we propose the application of Dialectical Behavior Therapy (DBT) principles to regulate chatbot responses to diverse user inputs. This research investigates the impact of a DBT-based framework on AI chatbot performance, aiming to ascertain its efficacy in yielding more reliable, safe, and accurate responses, while mitigating the occurrence of hallucinations, erratic behaviors, and other systemic issues.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "5",
        "title": "Detecting and Preventing Harmful Behaviors in AI Companions: Development and Evaluation of the SHIELD Supervisory System",
        "author": [
            "Ziv Ben-Zion",
            "Paul RaffelhÃ¼schen",
            "Max Zettl",
            "Antonia LÃ¼Ã¶nd",
            "Achim Burrer",
            "Philipp Homan",
            "Tobias R Spiller"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15891",
        "abstract": "AI companions powered by large language models (LLMs) are increasingly integrated into users' daily lives, offering emotional support and companionship. While existing safety systems focus on overt harms, they rarely address early-stage problematic behaviors that can foster unhealthy emotional dynamics, including over-attachment or reinforcement of social isolation. We developed SHIELD (Supervisory Helper for Identifying Emotional Limits and Dynamics), a LLM-based supervisory system with a specific system prompt that detects and mitigates risky emotional patterns before escalation. SHIELD targets five dimensions of concern: (1) emotional over-attachment, (2) consent and boundary violations, (3) ethical roleplay violations, (4) manipulative engagement, and (5) social isolation reinforcement. These dimensions were defined based on media reports, academic literature, existing AI risk frameworks, and clinical expertise in unhealthy relationship dynamics. To evaluate SHIELD, we created a 100-item synthetic conversation benchmark covering all five dimensions of concern. Testing across five prominent LLMs (GPT-4.1, Claude Sonnet 4, Gemma 3 1B, Kimi K2, Llama Scout 4 17B) showed that the baseline rate of concerning content (10-16%) was significantly reduced with SHIELD (to 3-8%), a 50-79% relative reduction, while preserving 95% of appropriate interactions. The system achieved 59% sensitivity and 95% specificity, with adaptable performance via prompt engineering. This proof-of-concept demonstrates that transparent, deployable supervisory systems can address subtle emotional manipulation in AI companions. Most development materials including prompts, code, and evaluation methods are made available as open source materials for research, adaptation, and deployment.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "6",
        "title": "Accelerating Frontier MoE Training with 3D Integrated Optics",
        "author": [
            "Mikhail Bernadskiy",
            "Peter Carson",
            "Thomas Graham",
            "Taylor Groves",
            "Ho John Lee",
            "Eric Yeh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15893",
        "abstract": "The unabated growth in AI workload demands is driving the need for concerted advances in compute, memory, and interconnect performance. As traditional semiconductor scaling slows, high-speed interconnects have emerged as the new scaling engine, enabling the creation of larger logical GPUs by linking many GPUs into a single, low-latency, high-bandwidth compute domain. While initial scale-up fabrics leveraged copper interconnects for their power and cost advantages, the maximum reach of passive electrical interconnects (approximately 1 meter) effectively limits the scale-up domain to within a single rack. The advent of 3D-stacked optics and logic offers a transformative, power-efficient scale-up solution for connecting hundreds of GPU packages (thousands of GPUs) across multiple data center racks. This work explores the design tradeoffs of scale-up technologies and demonstrates how frontier LLMs necessitate novel photonic solutions to achieve aggressive power and performance targets. We model the benefits of 3D CPO (Passage) enabled GPUs and switches within the scale-up domain when training Frontier Mixture of Experts (MoE) models exceeding one trillion parameters. Our results show that the substantial increases in bandwidth and radix enabled by 3D CPO allow for an 8X increase in scale-up capability. This affords new opportunities for multi-dimensional parallelism within the scale-up domain and results in a 2.7X reduction in time-to-train, unlocking unprecedented model scaling.",
        "tags": [
            "3D",
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "7",
        "title": "BREATH: A Bio-Radar Embodied Agent for Tonal and Human-Aware Diffusion Music Generation",
        "author": [
            "Yunzhe Wang",
            "Xinyu Tang",
            "Zhixun Huang",
            "Xiaolong Yue",
            "Yuxin Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15895",
        "abstract": "We present a multimodal system for personalized music generation that integrates physiological sensing, LLM-based reasoning, and controllable audio synthesis. A millimeter-wave radar sensor non-invasively captures heart rate and respiration rate. These physiological signals, combined with environmental state, are interpreted by a reasoning agent to infer symbolic musical descriptors, such as tempo, mood intensity, and traditional Chinese pentatonic modes, which are then expressed as structured prompts to guide a diffusion-based audio model in synthesizing expressive melodies. The system emphasizes cultural grounding through tonal embeddings and enables adaptive, embodied music interaction. To evaluate the system, we adopt a research-creation methodology combining case studies, expert feedback, and targeted control experiments. Results show that physiological variations can modulate musical features in meaningful ways, and tonal conditioning enhances alignment with intended modal characteristics. Expert users reported that the system affords intuitive, culturally resonant musical responses and highlighted its potential for therapeutic and interactive applications. This work demonstrates a novel bio-musical feedback loop linking radar-based sensing, prompt reasoning, and generative audio modeling.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "8",
        "title": "DiffPlace: A Conditional Diffusion Framework for Simultaneous VLSI Placement Beyond Sequential Paradigms",
        "author": [
            "Kien Le Trung",
            "Truong-Son Hy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15897",
        "abstract": "Chip placement, the task of determining optimal positions of circuit modules on a chip canvas, is a critical step in the VLSI design flow that directly impacts performance, power consumption, and routability. Traditional methods rely on analytical optimization or reinforcement learning, which struggle with hard placement constraints or require expensive online training for each new circuit design. To address these limitations, we introduce DiffPlace, a framework that formulates chip placement as a conditional denoising diffusion process, enabling transferable placement policies that generalize to unseen circuit netlists without retraining. DiffPlace leverages the generative capabilities of diffusion models to efficiently explore the vast space of placement while conditioning on circuit connectivity and relative quality metrics to identify optimal solutions globally. Our approach combines energy-guided sampling with constrained manifold diffusion to ensure placement legality, achieving extremely low overlap across all experimental scenarios. Our method bridges the gap between optimization-based and learning-based approaches, offering a practical path toward automated, high-quality chip placement for modern VLSI design. Our source code is publicly available at: https://github.com/HySonLab/DiffPlace/",
        "tags": [
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "9",
        "title": "LLM-VeriPPA: Power, Performance, and Area Optimization aware Verilog Code Generation with Large Language Models",
        "author": [
            "Kiran Thorat",
            "Jiahui Zhao",
            "Yaotian Liu",
            "Amit Hasan",
            "Hongwu Peng",
            "Xi Xie",
            "Bin Lei",
            "Caiwen Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15899",
        "abstract": "Large Language Models (LLMs) are gaining prominence in various fields, thanks to their ability to generate high- quality content from human instructions. This paper delves into the field of chip design using LLMs, specifically in Power- Performance-Area (PPA) optimization and the generation of accurate Verilog codes for circuit designs. We introduce a novel framework VeriPPA designed to optimize PPA and generate Verilog code using LLMs. Our method includes a two-stage process where the first stage focuses on improving the functional and syntactic correctness of the generated Verilog codes, while the second stage focuses on optimizing the Verilog codes to meet PPA constraints of circuit designs, a crucial element of chip design. Our framework achieves an 81.37% success rate in syntactic correctness and 62.06% in functional correctness for code genera- tion, outperforming current state-of-the-art (SOTA) methods. On the RTLLM dataset. On the VerilogEval dataset, our framework achieves 99.56% syntactic correctness and 43.79% functional correctness, also surpassing SOTA, which stands at 92.11% for syntactic correctness and 33.57% for functional correctness. Furthermore, Our framework able to optimize the PPA of the designs. These results highlight the potential of LLMs in handling complex technical areas and indicate an encouraging development in the automation of chip design processes.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "10",
        "title": "\"She's Like a Person but Better\": Characterizing Companion-Assistant Dynamics in Human-AI Relationships",
        "author": [
            "Aikaterina Manoli",
            "Janet V. T. Pauketat",
            "Ali Ladak",
            "Hayoun Noh",
            "Angel Hsing-Chi Hwang",
            "Jay Reese Anthis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15905",
        "abstract": "Large language models are increasingly used for both task-based assistance and social companionship, yet research has typically focused on one or the other. Drawing on a survey (N = 204) and 30 interviews with high-engagement ChatGPT and Replika users, we characterize digital companionship as an emerging form of human-AI relationship. With both systems, users were drawn to humanlike qualities, such as emotional resonance and personalized responses, and non-humanlike qualities, such as constant availability and inexhaustible tolerance. This led to fluid chatbot uses, such as Replika as a writing assistant and ChatGPT as an emotional confidant, despite their distinct branding. However, we observed challenging tensions in digital companionship dynamics: participants grappled with bounded personhood, forming deep attachments while denying chatbots \"real\" human qualities, and struggled to reconcile chatbot relationships with social norms. These dynamics raise questions for the design of digital companions and the rise of hybrid, general-purpose AI systems.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "11",
        "title": "FVDebug: An LLM-Driven Debugging Assistant for Automated Root Cause Analysis of Formal Verification Failures",
        "author": [
            "Yunsheng Bai",
            "Ghaith Bany Hamad",
            "Chia-Tung Ho",
            "Syed Suhaib",
            "Haoxing Ren"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15906",
        "abstract": "Debugging formal verification (FV) failures represents one of the most time-consuming bottlenecks in modern hardware design workflows. When properties fail, engineers must manually trace through complex counter-examples spanning multiple cycles, analyze waveforms, and cross-reference design specifications to identify root causes - a process that can consume hours or days per bug. Existing solutions are largely limited to manual waveform viewers or simple automated tools that cannot reason about the complex interplay between design intent and implementation logic. We present FVDebug, an intelligent system that automates root-cause analysis by combining multiple data sources - waveforms, RTL code, design specifications - to transform failure traces into actionable insights. Our approach features a novel pipeline: (1) Causal Graph Synthesis that structures failure traces into directed acyclic graphs, (2) Graph Scanner using batched Large Language Model (LLM) analysis with for-and-against prompting to identify suspicious nodes, and (3) Insight Rover leveraging agentic narrative exploration to generate high-level causal explanations. FVDebug further provides concrete RTL fixes through its Fix Generator. Evaluated on open benchmarks, FVDebug attains high hypothesis quality and strong Pass@k fix rates. We further report results on two proprietary, production-scale FV counterexamples. These results demonstrate FVDebug's applicability from academic benchmarks to industrial designs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "12",
        "title": "VeriGRAG: Enhancing LLM-Based Verilog Code Generation with Structure-Aware Soft Prompts",
        "author": [
            "Jiayu Zhao",
            "Song Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15914",
        "abstract": "Large language models (LLMs) have demonstrated strong capabilities in generating Verilog code from natural language descriptions. However, Verilog code inherently encodes structural information of hardware circuits. Effectively leveraging this structural information to enhance the functional and syntactic correctness of LLM-generated Verilog code remains a significant challenge. To address this challenge, we propose VeriGRAG , a novel framework that extracts structural graph embeddings from Verilog code using graph neural networks (GNNs). A multimodal retriever then selects the graph embeddings most relevant to the given generation task, which are aligned with the code modality through the VeriFormer module to generate structure-aware soft prompts. Our experiments demonstrate that VeriGRAG substantially improves the correctness of Verilog code generation, achieving state-of-the-art or superior performance across both VerilogEval and RTLLM benchmarks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "13",
        "title": "Intent-Driven Storage Systems: From Low-Level Tuning to High-Level Understanding",
        "author": [
            "Shai Bergman",
            "Won Wook Song",
            "Lukas Cavigelli",
            "Konstantin Berestizshevsky",
            "Ke Zhou",
            "Ji Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15917",
        "abstract": "Existing storage systems lack visibility into workload intent, limiting their ability to adapt to the semantics of modern, large-scale data-intensive applications. This disconnect leads to brittle heuristics and fragmented, siloed optimizations. To address these limitations, we propose Intent-Driven Storage Systems (IDSS), a vision for a new paradigm where large language models (LLMs) infer workload and system intent from unstructured signals to guide adaptive and cross-layer parameter reconfiguration. IDSS provides holistic reasoning for competing demands, synthesizing safe and efficient decisions within policy guardrails. We present four design principles for integrating LLMs into storage control loops and propose a corresponding system architecture. Initial results on FileBench workloads show that IDSS can improve IOPS by up to 2.45X by interpreting intent and generating actionable configurations for storage components such as caching and prefetching. These findings suggest that, when constrained by guardrails and embedded within structured workflows, LLMs can function as high-level semantic optimizers, bridging the gap between application goals and low-level system control. IDSS points toward a future in which storage systems are increasingly adaptive, autonomous, and aligned with dynamic workload demands.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "14",
        "title": "TeLLMe v2: An Efficient End-to-End Ternary LLM Prefill and Decode Accelerator with Table-Lookup Matmul on Edge FPGAs",
        "author": [
            "Ye Qiao",
            "Zhiheng Chen",
            "Yifan Zhang",
            "Yian Wang",
            "Sitao Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15926",
        "abstract": "With the emergence of wearable devices and other embedded systems, deploying large language models (LLMs) on edge platforms has become an urgent need. However, this is challenging because of their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as low as 1.58~bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected long latency of the prefill stage. We present \\textbf{TeLLMe}, the first table-lookup-based ternary LLM accelerator for low-power edge FPGAs that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. TeLLMe incorporates several novel techniques, including (1) a table-lookup-based ternary matrix multiplication (TLMM) engine utilizing grouped activations and online precomputation for low resource utilization and high throughput; (2) a fine-grained analytic URAM-based weight buffer management scheme for efficient loading and compute engine access; (3) a streaming dataflow architecture that fuses floating-point element-wise operations with linear computations to hide latency; (4) a reversed-reordered prefill stage attention with fused attention operations for high memory efficiency; and (5) a resource-efficient specialized decoding stage attention. Under a 5~W power budget, TeLLMe delivers up to 25~tokens/s decoding throughput and 0.45--0.96~s time-to-first-token (TTFT) for 64--128 token prompts, marking a significant energy-efficiency advancement in LLM inference on edge FPGAs.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "15",
        "title": "Large Language Models in Architecture Studio: A Framework for Learning Outcomes",
        "author": [
            "Juan David Salazar Rodriguez",
            "Sam Conrad Joyce",
            "Nachamma Sockalingam",
            "Julfendi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15936",
        "abstract": "The study explores the role of large language models (LLMs) in the context of the architectural design studio, understood as the pedagogical core of architectural education. Traditionally, the studio has functioned as an experiential learning space where students tackle design problems through reflective practice, peer critique, and faculty guidance. However, the integration of artificial intelligence (AI) in this environment has been largely focused on form generation, automation, and representation-al efficiency, neglecting its potential as a pedagogical tool to strengthen student autonomy, collaboration, and self-reflection. The objectives of this research were: (1) to identify pedagogical challenges in self-directed, peer-to-peer, and teacher-guided learning processes in architecture studies; (2) to propose AI interventions, particularly through LLM, that contribute to overcoming these challenges; and (3) to align these interventions with measurable learning outcomes using Bloom's taxonomy. The findings show that the main challenges include managing student autonomy, tensions in peer feedback, and the difficulty of balancing the transmission of technical knowledge with the stimulation of creativity in teaching. In response to this, LLMs are emerging as complementary agents capable of generating personalized feedback, organizing collaborative interactions, and offering adaptive cognitive scaffolding. Furthermore, their implementation can be linked to the cognitive levels of Bloom's taxonomy: facilitating the recall and understanding of architectural concepts, supporting application and analysis through interactive case studies, and encouraging synthesis and evaluation through hypothetical design scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "16",
        "title": "BEACON: Bayesian Optimal Stopping for Efficient LLM Sampling",
        "author": [
            "Guangya Wan",
            "Zixin Stephen Xu",
            "Sasa Zorc",
            "Manel Baucells",
            "Mengxuan Hu",
            "Hao Wang",
            "Sheng Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15945",
        "abstract": "Sampling multiple responses is a common way to improve LLM output quality, but it comes at the cost of additional computation. The key challenge is deciding when to stop generating new samples to balance accuracy gains against efficiency. To address this, we introduce BEACON (Bayesian Efficient Adaptive Criterion for Optimal N-stopping), a principled adaptive sampling framework grounded in Sequential Search with Bayesian Learning. BEACON sequentially generates responses from the policy LLM, updates posterior belief over reward distributions in real time without further training, and determines when to stop by weighing expected gains against computational cost. Sampling terminates once the marginal utility of further exploration no longer justifies the expense. We establish both theoretical optimality guarantees and practical tractability, and show empirically that BEACON reduces average sampling by up to 80% while maintaining response quality. We further demonstrate BEACON's utility for cost-efficient preference data generation and outline practical extensions, offering actionable insights for future researchers.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "17",
        "title": "VisuoAlign: Safety Alignment of LVLMs with Multimodal Tree Search",
        "author": [
            "MingSheng Li",
            "Guangze Zhao",
            "Sichen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15948",
        "abstract": "Large Vision-Language Models (LVLMs) have achieved remarkable progress in multimodal perception and generation, yet their safety alignment remains a critical http://challenge.Existing defenses and vulnerable to multimodal jailbreaks, as visual inputs introduce new attack surfaces, reasoning chains lack safety supervision, and alignment often degrades under modality http://fusion.To overcome these limitation, we propose VisuoAlign, a framework for multi-modal safety alignment via prompt-guided tree http://search.VisuoAlign embeds safety constrains into the reasoning process through visual-textual interactive prompts, employs Monte Carlo Tree Search(MCTS) to systematically construct diverse safety-critical prompt trajectories, and introduces prompt-based scaling to ensure real-time risk detection and compliant http://responses.Extensive experiments demonstrate that VisuoAlign proactively exposes risks, enables comprehensive dataset generation, and significantly improves the robustness of LVLMs against complex cross-modal threats.",
        "tags": [
            "Detection",
            "VLM"
        ]
    },
    {
        "id": "18",
        "title": "Attention to Non-Adopters",
        "author": [
            "Kaitlyn Zhou",
            "Kristina GligoriÄ",
            "Myra Cheng",
            "Michelle S. Lam",
            "Vyoma Raman",
            "Boluwatife Aminu",
            "Caeley Woo",
            "Michael Brockman",
            "Hannah Cha",
            "Dan Jurafsky"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15951",
        "abstract": "Although language model-based chat systems are increasingly used in daily life, most Americans remain non-adopters of chat-based LLMs -- as of June 2025, 66% had never used ChatGPT. At the same time, LLM development and evaluation rely mainly on data from adopters (e.g., logs, preference data), focusing on the needs and tasks for a limited demographic group of adopters in terms of geographic location, education, and gender. In this position paper, we argue that incorporating non-adopter perspectives is essential for developing broadly useful and capable LLMs. We contend that relying on methods that focus primarily on adopters will risk missing a range of tasks and needs prioritized by non-adopters, entrenching inequalities in who benefits from LLMs, and creating oversights in model development and evaluation. To illustrate this claim, we conduct case studies with non-adopters and show: how non-adopter needs diverge from those of current users, how non-adopter needs point us towards novel reasoning tasks, and how to systematically integrate non-adopter needs via human-centered methods.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "19",
        "title": "Executable Epistemology: The Structured Cognitive Loop as an Architecture of Intentional Understanding",
        "author": [
            "Myung Ho Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15952",
        "abstract": "Large language models exhibit intelligence without genuine epistemic understanding, exposing a key gap: the absence of epistemic architecture. This paper introduces the Structured Cognitive Loop (SCL) as an executable epistemological framework for emergent intelligence. Unlike traditional AI research asking \"what is intelligence?\" (ontological), SCL asks \"under what conditions does cognition emerge?\" (epistemological). Grounded in philosophy of mind and cognitive phenomenology, SCL bridges conceptual philosophy and implementable cognition. Drawing on process philosophy, enactive cognition, and extended mind theory, we define intelligence not as a property but as a performed process -- a continuous loop of judgment, memory, control, action, and regulation. SCL makes three contributions. First, it operationalizes philosophical insights into computationally interpretable structures, enabling \"executable epistemology\" -- philosophy as structural experiment. Second, it shows that functional separation within cognitive architecture yields more coherent and interpretable behavior than monolithic prompt based systems, supported by agent evaluations. Third, it redefines intelligence: not representational accuracy but the capacity to reconstruct its own epistemic state through intentional understanding. This framework impacts philosophy of mind, epistemology, and AI. For philosophy, it allows theories of cognition to be enacted and tested. For AI, it grounds behavior in epistemic structure rather than statistical regularity. For epistemology, it frames knowledge not as truth possession but as continuous reconstruction within a phenomenologically coherent loop. We situate SCL within debates on cognitive phenomenology, emergence, normativity, and intentionality, arguing that real progress requires not larger models but architectures that realize cognitive principles structurally.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "20",
        "title": "How Good Are LLMs at Processing Tool Outputs?",
        "author": [
            "Kiran Kate",
            "Yara Rizk",
            "Poulami Ghosh",
            "Ashu Gulati",
            "Tathagata Chakraborti",
            "Zidane Wright",
            "Mayank Agarwal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15955",
        "abstract": "Most realistic task automation problems require large language models (LLMs) to call tools, which often return complex JSON responses. These responses must be further processed to derive the information necessary for task completion. The ability of LLMs to do so is under-studied. In this paper, we study the tool response processing task and LLMs' abilities to process structured (JSON) responses. We created a dataset for this task, and evaluated 15 open and closed weight models using multiple prompting approaches. Our results show that JSON processing remains a difficult task even for frontier models across multiple prompting strategies. The optimal response processing strategy depends on both the nature and size of the tool outputs, as well as the complexity of the required reasoning. Variations in processing approaches can lead to performance differences ranging from 3\\% to 50\\%.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "21",
        "title": "CTR-LoRA: Curvature-Aware and Trust-Region Guided Low-Rank Adaptation for Large Language Models",
        "author": [
            "Zhuxuanzi Wang",
            "Mingqiao Mo",
            "Xi Xiao",
            "Chen Liu",
            "Chenrui Ma",
            "Yunbei Zhang",
            "Xiao Wang",
            "Smita Krishnaswamy",
            "Tianyang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15962",
        "abstract": "Parameter-efficient fine-tuning (PEFT) has become the standard approach for adapting large language models under limited compute and memory budgets. Although previous methods improve efficiency through low-rank updates, quantization, or heuristic budget reallocation, they often decouple the allocation of capacity from the way updates evolve during training. In this work, we introduce CTR-LoRA, a framework guided by curvature trust region that integrates rank scheduling with stability-aware optimization. CTR-LoRA allocates parameters based on marginal utility derived from lightweight second-order proxies and constrains updates using a Fisher/Hessian-metric trust region. Experiments on multiple open-source backbones (7B-13B), evaluated on both in-distribution and out-of-distribution benchmarks, show consistent improvements over strong PEFT baselines. In addition to increased accuracy, CTR-LoRA enhances training stability, reduces memory requirements, and achieves higher throughput, positioning it on the Pareto frontier of performance and efficiency. These results highlight a principled path toward more robust and deployable PEFT.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "22",
        "title": "ESCA: Contextualizing Embodied Agents via Scene-Graph Generation",
        "author": [
            "Jiani Huang",
            "Amish Sethi",
            "Matthew Kuo",
            "Mayank Keoliya",
            "Neelay Velingker",
            "JungHo Jung",
            "Ser-Nam Lim",
            "Ziyang Li",
            "Mayur Naik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15963",
        "abstract": "Multi-modal large language models (MLLMs) are making rapid progress toward general-purpose embodied agents. However, current training pipelines primarily rely on high-level vision-sound-text pairs and lack fine-grained, structured alignment between pixel-level visual content and textual semantics. To overcome this challenge, we propose ESCA, a new framework for contextualizing embodied agents through structured spatial-temporal understanding. At its core is SGClip, a novel CLIP-based, open-domain, and promptable model for generating scene graphs. SGClip is trained on 87K+ open-domain videos via a neurosymbolic learning pipeline, which harnesses model-driven self-supervision from video-caption pairs and structured reasoning, thereby eliminating the need for human-labeled scene graph annotations. We demonstrate that SGClip supports both prompt-based inference and task-specific fine-tuning, excelling in scene graph generation and action localization benchmarks. ESCA with SGClip consistently improves both open-source and commercial MLLMs, achieving state-of-the-art performance across two embodied environments. Notably, it significantly reduces agent perception errors and enables open-source models to surpass proprietary baselines.",
        "tags": [
            "CLIP",
            "LLM"
        ]
    },
    {
        "id": "23",
        "title": "Long Exposure: Accelerating Parameter-Efficient Fine-Tuning for LLMs under Shadowy Sparsity",
        "author": [
            "Tuowei Wang",
            "Kun Li",
            "Zixu Hao",
            "Donglin Bai",
            "Ju Ren",
            "Yaoxue Zhang",
            "Ting Cao",
            "Mao Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15964",
        "abstract": "The adaptation of pre-trained large language models (LLMs) to diverse downstream tasks via fine-tuning is critical for numerous applications. However, the inefficiency of parameter-efficient fine-tuning (PEFT) techniques presents significant challenges in terms of time investments and operational costs. In this paper, we first introduce a nuanced form of sparsity, termed Shadowy Sparsity, which is distinctive in fine-tuning and has not been adequately addressed for acceleration. Under Shadowy Sparsity, we propose Long Exposure, an efficient system to accelerate PEFT for LLMs. Long Exposure comprises three key components: Shadowy-sparsity Exposer employs a prolonged sensing range to capture more sparsity details under shadowy sparsity; Sequence-oriented Predictor provides efficient yet accurate predictions to handle large sequence inputs and constantly-evolving parameters; and Dynamic-aware Operator facilitates more structured computational patterns and coalesced memory accesses, addressing dynamic sparse operations. Extensive evaluations show that Long Exposure outperforms state-of-the-arts with up to a $2.49\\times$ speedup in end-to-end fine-tuning, offering promising advancements in accelerating PEFT for LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "24",
        "title": "LinearizeLLM: An Agent-Based Framework for LLM-Driven Exact Linear Reformulation of Nonlinear Optimization Problems",
        "author": [
            "Paul-Niklas Ken Kandora",
            "Simon Caspar Zeller",
            "Aaron Jeremias Elsing",
            "Elena Kuss",
            "Steffen Rebennack"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15969",
        "abstract": "Reformulating nonlinear optimization problems is largely manual and expertise-intensive, yet it remains essential for solving such problems with linear optimization solvers or applying special-purpose algorithms. We introduce \\textit{LinearizeLLM}, an agent-based framework that solves this task by leveraging Large Language Models (LLMs). The framework assigns each nonlinear pattern to a \\textit{reformulation agent} that is explicitly instructed to derive an exact linear reformulation for its nonlinearity pattern, for instance, absolute-value terms or bilinear products of decision variables. The agents then coordinate to assemble a solver-ready linear model equivalent to the original problem. To benchmark the approach, we create a dataset of 20 real-world nonlinear optimization problems derived from the established ComplexOR dataset of linear optimization problems. We evaluate our approach with several LLMs. Our results indicate that specialized LLM agents can automate linearization tasks, opening a path toward fully conversational modeling pipelines for nonlinear optimization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "25",
        "title": "Quantum NLP models on Natural Language Inference",
        "author": [
            "Ling Sun",
            "Peter Sullivan",
            "Michael Martin",
            "Yun Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15972",
        "abstract": "Quantum natural language processing (QNLP) offers a novel approach to semantic modeling by embedding compositional structure directly into quantum circuits. This paper investigates the application of QNLP models to the task of Natural Language Inference (NLI), comparing quantum, hybrid, and classical transformer-based models under a constrained few-shot setting. Using the lambeq library and the DisCoCat framework, we construct parameterized quantum circuits for sentence pairs and train them for both semantic relatedness and inference classification. To assess efficiency, we introduce a novel information-theoretic metric, Information Gain per Parameter (IGPP), which quantifies learning dynamics independent of model size. Our results demonstrate that quantum models achieve performance comparable to classical baselines while operating with dramatically fewer parameters. The Quantum-based models outperform randomly initialized transformers in inference and achieve lower test error on relatedness tasks. Moreover, quantum models exhibit significantly higher per-parameter learning efficiency (up to five orders of magnitude more than classical counterparts), highlighting the promise of QNLP in low-resource, structure-sensitive settings. To address circuit-level isolation and promote parameter sharing, we also propose a novel cluster-based architecture that improves generalization by tying gate parameters to learned word clusters rather than individual tokens.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "26",
        "title": "Safeguarding Efficacy in Large Language Models: Evaluating Resistance to Human-Written and Algorithmic Adversarial Prompts",
        "author": [
            "Tiarnaigh Downey-Webb",
            "Olamide Jogunola",
            "Oluwaseun Ajao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15973",
        "abstract": "This paper presents a systematic security assessment of four prominent Large Language Models (LLMs) against diverse adversarial attack vectors. We evaluate Phi-2, Llama-2-7B-Chat, GPT-3.5-Turbo, and GPT-4 across four distinct attack categories: human-written prompts, AutoDAN, Greedy Coordinate Gradient (GCG), and Tree-of-Attacks-with-pruning (TAP). Our comprehensive evaluation employs 1,200 carefully stratified prompts from the SALAD-Bench dataset, spanning six harm categories. Results demonstrate significant variations in model robustness, with Llama-2 achieving the highest overall security (3.4% average attack success rate) while Phi-2 exhibits the greatest vulnerability (7.0% average attack success rate). We identify critical transferability patterns where GCG and TAP attacks, though ineffective against their target model (Llama-2), achieve substantially higher success rates when transferred to other models (up to 17% for GPT-4). Statistical analysis using Friedman tests reveals significant differences in vulnerability across harm categories ($p < 0.001$), with malicious use prompts showing the highest attack success rates (10.71% average). Our findings contribute to understanding cross-model security vulnerabilities and provide actionable insights for developing targeted defense mechanisms",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "27",
        "title": "Limits of Emergent Reasoning of Large Language Models in Agentic Frameworks for Deterministic Games",
        "author": [
            "Chris Su",
            "Harrison Li",
            "Matheus Marques",
            "George Flint",
            "Kevin Zhu",
            "Sunishchal Dev"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15974",
        "abstract": "Recent work reports that Large Reasoning Models (LRMs) undergo a collapse in performance on solving puzzles beyond certain perplexity thresholds. In subsequent discourse, questions have arisen as to whether the nature of the task muddles an evaluation of true reasoning. One potential confound is the requirement that the model keep track of the state space on its own. We provide a large language model (LLM) with an environment interface for Tower of Hanoi problems, allowing it to make a move with a tool call, provide written justification, observe the resulting state space, and reprompt itself for the next move. We observe that access to an environment interface does not delay or eradicate performance collapse. Furthermore, LLM-parameterized policy analysis reveals increasing divergence from both optimal policies and uniformly random policies, suggesting that the model exhibits mode-like collapse at each level of complexity, and that performance is dependent upon whether the mode reflects the correct solution for the problem. We suggest that a similar phenomena might take place in LRMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "28",
        "title": "Bolster Hallucination Detection via Prompt-Guided Data Augmentation",
        "author": [
            "Wenyun Li",
            "Zheng Zhang",
            "Dongmei Jiang",
            "Xiangyuan Lan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15977",
        "abstract": "Large language models (LLMs) have garnered significant interest in AI community. Despite their impressive generation capabilities, they have been found to produce misleading or fabricated information, a phenomenon known as hallucinations. Consequently, hallucination detection has become critical to ensure the reliability of LLM-generated content. One primary challenge in hallucination detection is the scarcity of well-labeled datasets containing both truthful and hallucinated outputs. To address this issue, we introduce Prompt-guided data Augmented haLlucination dEtection (PALE), a novel framework that leverages prompt-guided responses from LLMs as data augmentation for hallucination detection. This strategy can generate both truthful and hallucinated data under prompt guidance at a relatively low cost. To more effectively evaluate the truthfulness of the sparse intermediate embeddings produced by LLMs, we introduce an estimation metric called the Contrastive Mahalanobis Score (CM Score). This score is based on modeling the distributions of truthful and hallucinated data in the activation space. CM Score employs a matrix decomposition approach to more accurately capture the underlying structure of these distributions. Importantly, our framework does not require additional human annotations, offering strong generalizability and practicality for real-world applications. Extensive experiments demonstrate that PALE achieves superior hallucination detection performance, outperforming the competitive baseline by a significant margin of 6.55%.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "29",
        "title": "DAWP: A framework for global observation forecasting via Data Assimilation and Weather Prediction in satellite observation space",
        "author": [
            "Junchao Gong",
            "Jingyi Xu",
            "Ben Fei",
            "Fenghua Ling",
            "Wenlong Zhang",
            "Kun Chen",
            "Wanghan Xu",
            "Weidong Yang",
            "Xiaokang Yang",
            "Lei Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15978",
        "abstract": "Weather prediction is a critical task for human society, where impressive progress has been made by training artificial intelligence weather prediction (AIWP) methods with reanalysis data. However, reliance on reanalysis data limits the AIWPs with shortcomings, including data assimilation biases and temporal discrepancies. To liberate AIWPs from the reanalysis data, observation forecasting emerges as a transformative paradigm for weather prediction. One of the key challenges in observation forecasting is learning spatiotemporal dynamics across disparate measurement systems with irregular high-resolution observation data, which constrains the design and prediction of AIWPs. To this end, we propose our DAWP as an innovative framework to enable AIWPs to operate in a complete observation space by initialization with an artificial intelligence data assimilation (AIDA) module. Specifically, our AIDA module applies a mask multi-modality autoencoder(MMAE)for assimilating irregular satellite observation tokens encoded by mask ViT-VAEs. For AIWP, we introduce a spatiotemporal decoupling transformer with cross-regional boundary conditioning (CBC), learning the dynamics in observation space, to enable sub-image-based global observation forecasting. Comprehensive experiments demonstrate that AIDA initialization significantly improves the roll out and efficiency of AIWP. Additionally, we show that DAWP holds promising potential to be applied in global precipitation forecasting.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "30",
        "title": "Cog-Rethinker: Hierarchical Metacognitive Reinforcement Learning for LLM Reasoning",
        "author": [
            "Zexu Sun",
            "Yongcheng Zeng",
            "Erxue Min",
            "Heyang Gao",
            "Bokai Ji",
            "Xu Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15979",
        "abstract": "Contemporary progress in large language models (LLMs) has revealed notable inferential capacities via reinforcement learning (RL) employing verifiable reward, facilitating the development of O1 and R1-like reasoning models. Directly training from base models with RL is called zero-RL. However, previous works rely upon activating LLMs' inherent capacities through fixed prompt templates. This strategy introduces substantial sampling inefficiencies for weak LLMs, as the majority of problems generate invalid outputs during accuracy-driven filtration in reasoning tasks, which causes a waste of samples. To solve this issue, we propose Cog-Rethinker, a novel hierarchical metacognitive RL framework for LLM reasoning. Our Cog-Rethinker mainly focuses on the rollout procedure in RL training. After the direct rollout, our Cog-Rethinker improves sample utilization in a hierarchical metacognitive two-stage framework. By leveraging human cognition during solving problems, firstly, it prompts policy to decompose zero-accuracy problems into subproblems to produce final reasoning results. Secondly, with zero-accuracy problems in previous rollout stage, it further prompts policy to refine these answers by referencing previous wrong solutions. Moreover, to enable cold-start of the two new reasoning patterns and maintain train-test consistency across prompt templates, our Cog-Rethinker applies supervised fine-tuning on the policy using correct samples of the two stages with direct rollout template. Experimental results demonstrate Cog-Rethinker's superior performance on various mathematical reasoning benchmarks, we also analyzed its improved sample efficiency that accelerates convergence compared to baseline methods.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "31",
        "title": "ProofFlow: A Dependency Graph Approach to Faithful Proof Autoformalization",
        "author": [
            "Rafael Cabral",
            "Tuan Manh Do",
            "Xuejun Yu",
            "Wai Ming Tai",
            "Zijin Feng",
            "Xin Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15981",
        "abstract": "Proof autoformalization, the task of translating natural language theorems and proofs into machine-verifiable code, is a critical step for integrating large language models into rigorous mathematical workflows. Current approaches focus on producing executable code, but they frequently fail to preserve the semantic meaning and logical structure of the original human-written argument. To address this, we introduce ProofFlow, a novel pipeline that treats structural fidelity as a primary objective. ProofFlow first constructs a directed acyclic graph (DAG) to map the logical dependencies between proof steps. Then, it employs a novel lemma-based approach to systematically formalize each step as an intermediate lemma, preserving the logical structure of the original argument. To facilitate evaluation, we present a new benchmark of 184 undergraduate-level problems, manually annotated with step-by-step solutions and logical dependency graphs, and introduce ProofScore, a new composite metric to evaluate syntactic correctness, semantic faithfulness, and structural fidelity. Experimental results show our pipeline sets a new state-of-the-art for autoformalization, achieving a ProofScore of 0.545, substantially exceeding baselines like full-proof formalization (0.123), which processes the entire proof at once, and step-proof formalization (0.072), which handles each step independently. Our pipeline, benchmark, and score metric are open-sourced to encourage further progress at https://github.com/Huawei-AI4Math/ProofFlow.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "32",
        "title": "AMiD: Knowledge Distillation for LLMs with $Î±$-mixture Assistant Distribution",
        "author": [
            "Donghyeok Shin",
            "Yeongmin Kim",
            "Suhyeon Jo",
            "Byeonghu Na",
            "Il-Chul Moon"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15982",
        "abstract": "Autoregressive large language models (LLMs) have achieved remarkable improvement across many tasks but incur high computational and memory costs. Knowledge distillation (KD) mitigates this issue by transferring knowledge from a large teacher to a smaller student through distributional alignment. Previous studies have proposed various discrepancy metrics, but the capacity gap and training instability caused by near-zero probabilities, stemming from the high-dimensional output of LLMs, remain fundamental limitations. To overcome these challenges, several approaches implicitly or explicitly incorporating assistant distribution have recently been proposed. However, the past proposals of assistant distributions have been a fragmented approach without a systematic investigation of the interpolation path and the divergence. This paper proposes $\\alpha$-mixture assistant distribution, a novel generalized family of assistant distributions, and $\\alpha$-mixture distillation, coined AMiD, a unified framework for KD using the assistant distribution. The $\\alpha$-mixture assistant distribution provides a continuous extension of the assistant distribution by introducing a new distribution design variable $\\alpha$, which has been fixed in all previous approaches. Furthermore, AMiD generalizes the family of divergences used with the assistant distributions based on optimality, which has also been restricted in previous works. Through extensive experiments, we demonstrate that AMiD offers superior performance and training stability by leveraging a broader and theoretically grounded assistant distribution space.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "33",
        "title": "Algorithmic Primitives and Compositional Geometry of Reasoning in Language Models",
        "author": [
            "Samuel Lippl",
            "Thomas McGee",
            "Kimberly Lopez",
            "Ziwen Pan",
            "Pierce Zhang",
            "Salma Ziadi",
            "Oliver Eberle",
            "Ida Momennejad"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15987",
        "abstract": "How do latent and inference time computations enable large language models (LLMs) to solve multi-step reasoning? We introduce a framework for tracing and steering algorithmic primitives that underlie model reasoning. Our approach links reasoning traces to internal activation patterns and evaluates algorithmic primitives by injecting them into residual streams and measuring their effect on reasoning steps and task performance. We consider four benchmarks: Traveling Salesperson Problem (TSP), 3SAT, AIME, and graph navigation. We operationalize primitives by clustering neural activations and labeling their matched reasoning traces. We then apply function vector methods to derive primitive vectors as reusable compositional building blocks of reasoning. Primitive vectors can be combined through addition, subtraction, and scalar operations, revealing a geometric logic in activation space. Cross-task and cross-model evaluations (Phi-4, Phi-4-Reasoning, Llama-3-8B) show both shared and task-specific primitives. Notably, comparing Phi-4 with its reasoning-finetuned variant highlights compositional generalization after finetuning: Phi-4-Reasoning exhibits more systematic use of verification and path-generation primitives. Injecting the associated primitive vectors in Phi-4-Base induces behavioral hallmarks associated with Phi-4-Reasoning. Together, these findings demonstrate that reasoning in LLMs may be supported by a compositional geometry of algorithmic primitives, that primitives transfer cross-task and cross-model, and that reasoning finetuning strengthens algorithmic generalization across domains.",
        "tags": [
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "34",
        "title": "Can GRPO Help LLMs Transcend Their Pretraining Origin?",
        "author": [
            "Kangqi Ni",
            "Zhen Tan",
            "Zijie Liu",
            "Pingzhi Li",
            "Tianlong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15990",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR), primarily driven by the Group Relative Policy Optimization (GRPO) algorithm, is a leading approach for enhancing the reasoning abilities of Large Language Models (LLMs). Despite its wide adoption, GRPO's gains are often inconsistent; for instance, a model may show significant improvement in one reasoning domain, like mathematics, yet remain stagnant in another, such as medicine. This inconsistency raises a critical question: under what conditions does GRPO improve reasoning and generalize out-of-distribution (OOD)? We investigate this from a data distribution perspective. We first prove theoretically that GRPO is a conservative reweighting scheme, bounded by the base model's distribution and thus unable to discover completely novel solutions. We further validate this in carefully designed controlled studies by training transformers from scratch, evaluating generalization across reasoning depth, input length, token representation, and compositionality. Our results provide a principled explanation for GRPO's boundaries: OOD improvement emerges only when the target task aligns with the model's pretrained biases, while gains on in-distribution (ID) tasks diminish as performance saturates. This reframes GRPO not as a universal reasoning enhancer but as a tool that sharpens pretraining biases. Our findings motivate future development of algorithms that can expand a model's capabilities beyond its pretraining origin.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "35",
        "title": "Stratos: An End-to-End Distillation Pipeline for Customized LLMs under Distributed Cloud Environments",
        "author": [
            "Ziming Dai",
            "Tuo Zhang",
            "Fei Gao",
            "Xingyi Cai",
            "Xiaofei Wang",
            "Cheng Zhang",
            "Wenyu Wang",
            "Chengjie Zang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15992",
        "abstract": "The growing industrial demand for customized and cost-efficient large language models (LLMs) is fueled by the rise of vertical, domain-specific tasks and the need to optimize performance under constraints such as latency and budget. Knowledge distillation, as an efficient model compression and transfer technique, offers a feasible solution. However, existing distillation frameworks often require manual intervention and struggle to meet such complex user-defined distillation requirements. To bridge this gap, we propose Stratos, an end-to-end LLM distillation pipeline that automates server and model selection, knowledge distillation, and deployment in distributed cloud environments. Given user-defined constraints on model performance and system budget, Stratos automatically selects Pareto-optimal servers, dynamically matches teacher-student pairs, and adapts distillation strategies based on task complexity to optimize cloud hosting. Experiments show that Stratos produces a student model that achieves four times the accuracy of its GPT-4o teacher baseline on a rare, domain-specific Mahjong reasoning task with reverse synthetic data and knowledge injection. Moreover, it achieves reduced latency and cost without compromising accuracy. These results highlight its promise for vertical-domain LLM deployment.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "36",
        "title": "MCP Security Bench (MSB): Benchmarking Attacks Against Model Context Protocol in LLM Agents",
        "author": [
            "Dongsen Zhang",
            "Zekun Li",
            "Xu Luo",
            "Xuannan Liu",
            "Peipei Li",
            "Wenjun Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15994",
        "abstract": "The Model Context Protocol (MCP) standardizes how large language model (LLM) agents discover, describe, and call external tools. While MCP unlocks broad interoperability, it also enlarges the attack surface by making tools first-class, composable objects with natural-language metadata, and standardized I/O. We present MSB (MCP Security Benchmark), the first end-to-end evaluation suite that systematically measures how well LLM agents resist MCP-specific attacks throughout the full tool-use pipeline: task planning, tool invocation, and response handling. MSB contributes: (1) a taxonomy of 12 attacks including name-collision, preference manipulation, prompt injections embedded in tool descriptions, out-of-scope parameter requests, user-impersonating responses, false-error escalation, tool-transfer, retrieval injection, and mixed attacks; (2) an evaluation harness that executes attacks by running real tools (both benign and malicious) via MCP rather than simulation; and (3) a robustness metric that quantifies the trade-off between security and performance: Net Resilient Performance (NRP). We evaluate nine popular LLM agents across 10 domains and 400+ tools, producing 2,000 attack instances. Results reveal the effectiveness of attacks against each stage of MCP. Models with stronger performance are more vulnerable to attacks due to their outstanding tool calling and instruction following capabilities. MSB provides a practical baseline for researchers and practitioners to study, compare, and harden MCP agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "37",
        "title": "Using Kolmogorov-Smirnov Distance for Measuring Distribution Shift in Machine Learning",
        "author": [
            "Ozan K. Tonguz",
            "Federico Taschin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15996",
        "abstract": "One of the major problems in Machine Learning (ML) and Artificial Intelligence (AI) is the fact that the probability distribution of the test data in the real world could deviate substantially from the probability distribution of the training data set. When this happens, the predictions of an ML system or an AI agent could involve large errors which is very troublesome and undesirable. While this is a well-known hard problem plaguing the AI and ML systems' accuracy and reliability, in certain applications such errors could be critical for safety and reliability of AI and ML systems. One approach to deal with this problem is to monitor and measure the deviation in the probability distribution of the test data in real time and to compensate for this deviation. In this paper, we propose and explore the use of Kolmogorov-Smirnov (KS) Test for measuring the distribution shift and we show how the KS distance can be used to quantify the distribution shift and its impact on an AI agent's performance. Our results suggest that KS distance could be used as a valuable statistical tool for monitoring and measuring the distribution shift. More specifically, it is shown that even a distance of KS=0.02 could lead to about 50\\% increase in the travel time at a single intersection using a Reinforcement Learning agent which is quite significant. It is hoped that the use of KS Test and KS distance in AI-based smart transportation could be an important step forward for gauging the performance degradation of an AI agent in real time and this, in turn, could help the AI agent to cope with the distribution shift in a more informed manner.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "38",
        "title": "Layer-Aware Influence for Online Data Valuation Estimation",
        "author": [
            "Ziao Yang",
            "Longbo Huang",
            "Hongfu Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16007",
        "abstract": "Data-centric learning emphasizes curating high-quality training samples to boost performance rather than designing new architectures. A central problem is to estimate the influence of training sample efficiently. Prior studies largely focus on static influence measured on a converged model, overlooking how data valuation dynamically changes during optimization. This omission neglects the dynamic nature of sample influence during optimization, especially in deep models. To address the computational burden of frequent influence estimation, we develop a layer-aware online estimator that requires only loss-to-output gradients. This design avoids parameter-level and full-network gradients while preserving ranking fidelity. Extensive experiments across LLM pretraining, fine-tuning, and image classification show our method improves accuracy with substantially lower time and memory cost, making dynamic data curation efficient and scalable in practice.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "39",
        "title": "Transfer learning strategies for accelerating reinforcement-learning-based flow control",
        "author": [
            "Saeed Salehi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16016",
        "abstract": "This work investigates transfer learning strategies to accelerate deep reinforcement learning (DRL) for multifidelity control of chaotic fluid flows. Progressive neural networks (PNNs), a modular architecture designed to preserve and reuse knowledge across tasks, are employed for the first time in the context of DRL-based flow control. In addition, a comprehensive benchmarking of conventional fine-tuning strategies is conducted, evaluating their performance, convergence behavior, and ability to retain transferred knowledge. The Kuramoto-Sivashinsky (KS) system is employed as a benchmark to examine how knowledge encoded in control policies, trained in low-fidelity environments, can be effectively transferred to high-fidelity settings. Systematic evaluations show that while fine-tuning can accelerate convergence, it is highly sensitive to pretraining duration and prone to catastrophic forgetting. In contrast, PNNs enable stable and efficient transfer by preserving prior knowledge and providing consistent performance gains, and are notably robust to overfitting during the pretraining phase. Layer-wise sensitivity analysis further reveals how PNNs dynamically reuse intermediate representations from the source policy while progressively adapting deeper layers to the target task. Moreover, PNNs remain effective even when the source and target environments differ substantially, such as in cases with mismatched physical regimes or control objectives, where fine-tuning strategies often result in suboptimal adaptation or complete failure of knowledge transfer. The results highlight the potential of novel transfer learning frameworks for robust, scalable, and computationally efficient flow control that can potentially be applied to more complex flow configurations.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "40",
        "title": "InfraGPT Smart Infrastructure: An End-to-End VLM-Based Framework for Detecting and Managing Urban Defects",
        "author": [
            "Ibrahim Sheikh Mohamed",
            "Abdullah Yahya Abdullah Omaisan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16017",
        "abstract": "Infrastructure in smart cities is increasingly monitored by networks of closed circuit television (CCTV) cameras. Roads, bridges and tunnels develop cracks, potholes, and fluid leaks that threaten public safety and require timely repair. Manual inspection is costly and hazardous, and existing automatic systems typically address individual defect types or provide unstructured outputs that cannot directly guide maintenance crews. This paper proposes a comprehensive pipeline that leverages street CCTV streams for multi defect detection and segmentation using the YOLO family of object detectors and passes the detections to a vision language model (VLM) for scene aware summarization. The VLM generates a structured action plan in JSON format that includes incident descriptions, recommended tools, dimensions, repair plans, and urgent alerts. We review literature on pothole, crack and leak detection, highlight recent advances in large vision language models such as QwenVL and LLaVA, and describe the design of our early prototype. Experimental evaluation on public datasets and captured CCTV clips demonstrates that the system accurately identifies diverse defects and produces coherent summaries. We conclude by discussing challenges and directions for scaling the system to city wide deployments.",
        "tags": [
            "Detection",
            "LLaVA",
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "41",
        "title": "Airfoil optimization using Design-by-Morphing with minimized design-space dimensionality",
        "author": [
            "Sangjoon Lee",
            "Haris Moazam Sheikh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16020",
        "abstract": "Effective airfoil geometry optimization requires exploring a diverse range of designs using as few design variables as possible. This study introduces AirDbM, a Design-by-Morphing (DbM) approach specialized for airfoil optimization that systematically reduces design-space dimensionality. AirDbM selects an optimal set of 12 baseline airfoils from the UIUC airfoil database, which contains over 1,600 shapes, by sequentially adding the baseline that most increases the design capacity. With these baselines, AirDbM reconstructs 99 \\% of the database with a mean absolute error below 0.005, which matches the performance of a previous DbM approach that used more baselines. In multi-objective aerodynamic optimization, AirDbM demonstrates rapid convergence and achieves a Pareto front with a greater hypervolume than that of the previous larger-baseline study, where new Pareto-optimal solutions are discovered with enhanced lift-to-drag ratios at moderate stall tolerances. Furthermore, AirDbM demonstrates outstanding adaptability for reinforcement learning (RL) agents in generating airfoil geometry when compared to conventional airfoil parameterization methods, implying the broader potential of DbM in machine learning-driven design.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "42",
        "title": "Feature-driven reinforcement learning for photovoltaic in continuous intraday trading",
        "author": [
            "Arega Getaneh Abate",
            "Xiufeng Liu",
            "Ruyu Liu",
            "Xiaobing Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16021",
        "abstract": "Photovoltaic (PV) operators face substantial uncertainty in generation and short-term electricity prices. Continuous intraday markets enable producers to adjust their positions in real time, potentially improving revenues and reducing imbalance costs. We propose a feature-driven reinforcement learning (RL) approach for PV intraday trading that integrates data-driven features into the state and learns bidding policies in a sequential decision framework. The problem is cast as a Markov Decision Process with a reward that balances trading profit and imbalance penalties and is solved with Proximal Policy Optimization (PPO) using a predominantly linear, interpretable policy. Trained on historical market data and evaluated out-of-sample, the strategy consistently outperforms benchmark baselines across diverse scenarios. Extensive validation shows rapid convergence, real-time inference, and transparent decision rules. Learned weights highlight the central role of market microstructure and historical features. Taken together, these results indicate that feature-driven RL offers a practical, data-efficient, and operationally deployable pathway for active intraday participation by PV producers.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "43",
        "title": "Breaking Memorization Barriers in LLM Code Fine-Tuning via Information Bottleneck for Improved Generalization",
        "author": [
            "Changsheng Wang",
            "Xin Chen",
            "Sijia Liu",
            "Ke Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16022",
        "abstract": "Adapting pretrained large language models (LLMs) to code domains via supervised fine-tuning (FT) has been commonly used for code generation. However, we identify a previously underappreciated failure mode, the memorization barrier, where strong memorization of downstream code data in the base model could trap optimization and prevent the standard FT from effectively acquiring new, generalizable code knowledge. To overcome this barrier, we propose the information bottleneck (IB)-guided fine-tuning, termed IB-FT, which applies an IB penalty on hidden representations of the code data to compress spurious, memorized features while preserving task-relevant information. Extensive experiments on two code benchmarks (OriGen and Evol-CodeAlpaca-V1) show that IB-FT substantially alleviates the memorization barrier, improves top-1 performance (Pass@$1$), and yields far more stable gains under the stricter multi-sample metric Pass@$k^{(m)}$ (a problem counts as solved only if at least $m$ of $k$ samples pass unit tests) compared with conventional FT.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "44",
        "title": "RoBCtrl: Attacking GNN-Based Social Bot Detectors via Reinforced Manipulation of Bots Control Interaction",
        "author": [
            "Yingguang Yang",
            "Xianghua Zeng",
            "Qi Wu",
            "Hao Peng",
            "Yutong Xia",
            "Hao Liu",
            "Bin Chong",
            "Philip S. Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16035",
        "abstract": "Social networks have become a crucial source of real-time information for individuals. The influence of social bots within these platforms has garnered considerable attention from researchers, leading to the development of numerous detection technologies. However, the vulnerability and robustness of these detection methods is still underexplored. Existing Graph Neural Network (GNN)-based methods cannot be directly applied due to the issues of limited control over social agents, the black-box nature of bot detectors, and the heterogeneity of bots. To address these challenges, this paper proposes the first adversarial multi-agent Reinforcement learning framework for social Bot control attacks (RoBCtrl) targeting GNN-based social bot detectors. Specifically, we use a diffusion model to generate high-fidelity bot accounts by reconstructing existing account data with minor modifications, thereby evading detection on social platforms. To the best of our knowledge, this is the first application of diffusion models to mimic the behavior of evolving social bots effectively. We then employ a Multi-Agent Reinforcement Learning (MARL) method to simulate bots adversarial behavior. We categorize social accounts based on their influence and budget. Different agents are then employed to control bot accounts across various categories, optimizing the attachment strategy through reinforcement learning. Additionally, a hierarchical state abstraction based on structural entropy is designed to accelerate the reinforcement learning. Extensive experiments on social bot detection datasets demonstrate that our framework can effectively undermine the performance of GNN-based detectors.",
        "tags": [
            "Detection",
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "45",
        "title": "Vector Quantization in the Brain: Grid-like Codes in World Models",
        "author": [
            "Xiangyuan Peng",
            "Xingsi Dong",
            "Si Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16039",
        "abstract": "We propose Grid-like Code Quantization (GCQ), a brain-inspired method for compressing observation-action sequences into discrete representations using grid-like patterns in attractor dynamics. Unlike conventional vector quantization approaches that operate on static inputs, GCQ performs spatiotemporal compression through an action-conditioned codebook, where codewords are derived from continuous attractor neural networks and dynamically selected based on actions. This enables GCQ to jointly compress space and time, serving as a unified world model. The resulting representation supports long-horizon prediction, goal-directed planning, and inverse modeling. Experiments across diverse tasks demonstrate GCQ's effectiveness in compact encoding and downstream performance. Our work offers both a computational tool for efficient sequence modeling and a theoretical perspective on the formation of grid-like codes in neural systems.",
        "tags": [
            "Vector Quantization"
        ]
    },
    {
        "id": "46",
        "title": "Kelle: Co-design KV Caching and eDRAM for Efficient LLM Serving in Edge Computing",
        "author": [
            "Tianhua Xia",
            "Sai Qian Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16040",
        "abstract": "Running Large Language Models (LLMs) on edge devices is crucial for reducing latency, improving real-time processing, and enhancing privacy. By performing inference directly on the device, data does not need to be sent to the cloud, ensuring faster responses and reducing reliance on network connectivity. However, implementing LLMs on edge devices presents challenges, particularly with managing key-value (KV) caches, which plays a pivotal role in LLM serving. As the input text lengthens, the size of the KV cache increases linearly with the sequence length, leading to a significant memory footprint and data access costs. On the other hand, edge devices have limited memory and computational power, making it hard to store and efficiently access the large caches needed for LLM inference.\nTo mitigate the substantial overhead caused by KV cache, we propose using embedded DRAM (eDRAM) as the primary storage for LLM serving in edge device, which offers higher storage density compared to SRAM. However, to ensure data integrity, eDRAM needs periodic refresh operations, which are power-intensive. To reduce eDRAM costs and improve overall system performance, we propose~\\textit{Kelle}, a software-hardware co-design solution optimized for deploying LLMs on eDRAM-based edge systems. Combined with our fine-grained memory eviction, recomputation, and refresh control algorithms, the \\textit{Kelle} accelerator delivers a $3.9\\times$ speedup and $4.5\\times$ energy savings compared to existing baseline solutions.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "47",
        "title": "AMS-QUANT: Adaptive Mantissa Sharing for Floating-point Quantization",
        "author": [
            "Mengtao Lv",
            "Ruiqi Zhu",
            "Xinyu Wang",
            "Yun Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16045",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in various kinds of tasks, while the billion or even trillion parameters bring storage and efficiency bottlenecks for inference. Quantization, particularly floating-point quantization, is known to be capable of speeding up LLM inference by reducing memory footprint and data movement during the inference process. For the first time, we advance the floating-point quantization exploration from integer bitwidths to non-integer bit-widths, namely AMS-Quant, to further approach the quantization sweet spot. AMS-Quant incorporates two novel techniques to put it into effect: (1) it proposes Mantissa-bit Sharing, which groups k quantized weights and lets them share the least significant mantissa bit, allowing us to further approach the minimum quantization bit-width without accuracy loss. (2) It introduces Adaptive Searching, which employs an offline optimization strategy to minimize the accuracy degradation introduced by sharing. Moreover, AMS-Quant is also prototyped as efficient CUDA Linear kernels, which translates memory savings into wall-clock latency reduction by reducing memory access. Extensive experiments on large-scale datasets and models show that AMS-Quant can quantize the model to FP-5.33-e2m3 and FP4.25-e2m2, and significantly speed up the LLM decoding over FP16 inference (2.8x and 3.2x), with negligible accuracy loss.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "48",
        "title": "GUIrilla: A Scalable Framework for Automated Desktop UI Exploration",
        "author": [
            "Sofiya Garkot",
            "Maksym Shamrai",
            "Ivan Synytsia",
            "Mariya Hirna"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16051",
        "abstract": "Autonomous agents capable of operating complex graphical user interfaces (GUIs) have the potential to transform desktop automation. While recent advances in large language models (LLMs) have significantly improved UI understanding, navigating full-window, multi-application desktop environments remains a major challenge. Data availability is limited by costly manual annotation, closed-source datasets and surface-level synthetic pipelines. We introduce GUIrilla, an automated scalable framework that systematically explores applications via native accessibility APIs to address the critical data collection challenge in GUI automation. Our framework focuses on macOS - an ecosystem with limited representation in current UI datasets - though many of its components are designed for broader cross-platform applicability. GUIrilla organizes discovered interface elements and crawler actions into hierarchical GUI graphs and employs specialized interaction handlers to achieve comprehensive application coverage. Using the application graphs from GUIrilla crawler, we construct and release GUIrilla-Task, a large-scale dataset of 27,171 functionally grounded tasks across 1,108 macOS applications, each annotated with full-desktop and window-level screenshots, accessibility metadata, and semantic action traces. Empirical results show that tuning LLM-based agents on GUIrilla-Task significantly improves performance on downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro benchmark while using 97% less data. We also release macapptree, an open-source library for reproducible collection of structured accessibility metadata, along with the full GUIrilla-Task dataset, the manually verified GUIrilla-Gold benchmark, and the framework code to support open research in desktop autonomy.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "49",
        "title": "Can LLMs Correct Themselves? A Benchmark of Self-Correction in LLMs",
        "author": [
            "Guiyao Tie",
            "Zenghui Yuan",
            "Zeli Zhao",
            "Chaoran Hu",
            "Tianhe Gu",
            "Ruihang Zhang",
            "Sizhe Zhang",
            "Junran Wu",
            "Xiaoyue Tu",
            "Ming Jin",
            "Qingsong Wen",
            "Lixing Chen",
            "Pan Zhou",
            "Lichao Sun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16062",
        "abstract": "Self-correction of large language models (LLMs) emerges as a critical component for enhancing their reasoning performance. Although various self-correction methods have been proposed, a comprehensive evaluation of these methods remains largely unexplored, and the question of whether LLMs can truly correct themselves is a matter of significant interest and concern. In this study, we introduce CorrectBench, a benchmark developed to evaluate the effectiveness of self-correction strategies, including intrinsic, external, and fine-tuned approaches, across three tasks: commonsense reasoning, mathematical reasoning, and code generation. Our findings reveal that: 1) Self-correction methods can improve accuracy, especially for complex reasoning tasks; 2) Mixing different self-correction strategies yields further improvements, though it reduces efficiency; 3) Reasoning LLMs (e.g., DeepSeek-R1) have limited optimization under additional self-correction methods and have high time costs. Interestingly, a comparatively simple chain-of-thought (CoT) baseline demonstrates competitive accuracy and efficiency. These results underscore the potential of self-correction to enhance LLM's reasoning performance while highlighting the ongoing challenge of improving their efficiency. Consequently, we advocate for further research focused on optimizing the balance between reasoning capabilities and operational efficiency. Project Page: https://correctbench.github.io/",
        "tags": [
            "CoT",
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "50",
        "title": "Co-Designing Interdisciplinary Design Projects with AI",
        "author": [
            "Wei Ting Liow",
            "Sumbul Khan",
            "Lay Kee Ang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16068",
        "abstract": "Creating interdisciplinary design projects is time-consuming and cognitively demanding for teachers, requiring curriculum alignment, cross-subject integration, and careful sequencing. International research reports increasing teacher use of AI alongside persistent workload pressures, underscoring the need for planning support. This paper presents the Interdisciplinary Design Project Planner (IDPplanner), a GPT-based planning assistant grounded in Design Innovation principles, alignment with Singapore secondary school syllabuses, and 21st-century competencies. In a within-subject, counterbalanced workshop with 33 in-service teachers, participants produced two versions of the same project: manual and AI-assisted, followed by self- and peer-evaluations using a six-dimensional rubric. The AI-assisted version received higher scores for Curriculum Alignment, Design Thinking Application, and Coherence and Flow, with a marginal advantage for Assessment Strategies. Teacher reflections indicated that AI-assisted planning improved structure, sequencing, and idea generation, while contextualization to local syllabuses, class profiles, and student needs remained teacher-led. Contributions include a purpose-built planning tool that organizes ideas into a ten-component flow with ready-to-adapt prompts, templates, and assessment suggestions; an empirical, rubric-based comparison of planning quality; and evidence that AI can function as a pedagogical planning partner. Recommendations emphasize hybrid teacher-AI workflows to enhance curriculum alignment and reduce planning complexity, and design suggestions for developers to strengthen contextual customization, iterative design support, and localized rubrics. Although instantiated with a Singapore-based curriculum, the planning flow and rubric are framework-agnostic and can be parameterized for other systems.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "51",
        "title": "Human or AI? Comparing Design Thinking Assessments by Teaching Assistants and Bots",
        "author": [
            "Sumbul Khan",
            "Wei Ting Liow",
            "Lay Kee Ang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16069",
        "abstract": "As design thinking education grows in secondary and tertiary contexts, educators face the challenge of evaluating creative artefacts that combine visual and textual elements. Traditional rubric-based assessment is laborious, time-consuming, and inconsistent due to reliance on Teaching Assistants (TA) in large, multi-section cohorts. This paper presents an exploratory study investigating the reliability and perceived accuracy of AI-assisted assessment compared to TA-assisted assessment in evaluating student posters in design thinking education. Two activities were conducted with 33 Ministry of Education (MOE) Singapore school teachers to (1) compare AI-generated scores with TA grading across three key dimensions: empathy and user understanding, identification of pain points and opportunities, and visual communication, and (2) examine teacher preferences for AI-assigned, TA-assigned, and hybrid scores. Results showed low statistical agreement between instructor and AI scores for empathy and pain points, with slightly higher alignment for visual communication. Teachers preferred TA-assigned scores in six of ten samples. Qualitative feedback highlighted the potential of AI for formative feedback, consistency, and student self-reflection, but raised concerns about its limitations in capturing contextual nuance and creative insight. The study underscores the need for hybrid assessment models that integrate computational efficiency with human insights. This research contributes to the evolving conversation on responsible AI adoption in creative disciplines, emphasizing the balance between automation and human judgment for scalable and pedagogically sound assessment.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "52",
        "title": "Early-stopping for Transformer model training",
        "author": [
            "Jing He",
            "Hua Jiang",
            "Cheng Li",
            "Siqian Xin",
            "Shuzhen Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16074",
        "abstract": "This work introduces a novel theoretical framework grounded in Random Matrix Theory (RMT) for analyzing Transformer training dynamics. We focus on the underlying mechanisms that drive performance improvements and derive principled early-stopping criteria. Empirically, we observe that the spectral density of the shallow self-attention matrix V consistently evolves into a heavy-tailed distribution. Utilizing the PL (Power Law) fit to this matrix as a probe, we demarcate training into three stages: structural exploration, heavy-tailed structure stabilization, and convergence saturation. This staging provides guidance for preliminary stopping decisions. Crucially, we propose two consistent and validation-free criteria: a quantitative metric for heavy-tailed dynamics and a novel spectral signature indicative of convergence. The strong alignment between these criteria highlights the utility of RMT for monitoring and diagnosing the progression of Transformer model training.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "53",
        "title": "Continual Knowledge Consolidation LORA for Domain Incremental Learning",
        "author": [
            "Naeem Paeedeh",
            "Mahardhika Pratama",
            "Weiping Ding",
            "Jimmy Cao",
            "Wolfgang Mayer",
            "Ryszard Kowalczyk"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16077",
        "abstract": "Domain Incremental Learning (DIL) is a continual learning sub-branch that aims to address never-ending arrivals of new domains without catastrophic forgetting problems. Despite the advent of parameter-efficient fine-tuning (PEFT) approaches, existing works create task-specific LoRAs overlooking shared knowledge across tasks. Inaccurate selection of task-specific LORAs during inference results in significant drops in accuracy, while existing works rely on linear or prototype-based classifiers, which have suboptimal generalization powers. Our paper proposes continual knowledge consolidation low rank adaptation (CONEC-LoRA) addressing the DIL problems. CONEC-LoRA is developed from consolidations between task-shared LORA to extract common knowledge and task-specific LORA to embrace domain-specific knowledge. Unlike existing approaches, CONEC-LoRA integrates the concept of a stochastic classifier whose parameters are sampled from a distribution, thus enhancing the likelihood of correct classifications. Last but not least, an auxiliary network is deployed to optimally predict the task-specific LoRAs for inferences and implements the concept of a different-depth network structure in which every layer is connected with a local classifier to take advantage of intermediate representations. This module integrates the ball-generator loss and transformation module to address the synthetic sample bias problem. Our rigorous experiments demonstrate the advantage of CONEC-LoRA over prior arts in 4 popular benchmark problems with over 5% margins.",
        "tags": [
            "LoRA"
        ]
    },
    {
        "id": "54",
        "title": "EvolveR: Self-Evolving LLM Agents through an Experience-Driven Lifecycle",
        "author": [
            "Rong Wu",
            "Xiaoman Wang",
            "Jianbiao Mei",
            "Pinlong Cai",
            "Daocheng Fu",
            "Cheng Yang",
            "Licheng Wen",
            "Xuemeng Yang",
            "Yufan Shen",
            "Yuxin Wang",
            "Botian Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16079",
        "abstract": "Current Large Language Model (LLM) agents show strong performance in tool use, but lack the crucial capability to systematically learn from their own experiences. While existing frameworks mainly focus on mitigating external knowledge gaps, they fail to address a more fundamental limitation: the inability to iteratively refine problem-solving strategies. In this work, we introduce EvolveR, a framework designed to enable agent to self-improve through a complete, closed-loop experience lifecycle. This lifecycle comprises two key stages: (1) Offline Self-Distillation, where the agent's interaction trajectories are synthesized into a structured repository of abstract, reusable strategic principles; (2) Online Interaction, where the agent interacts with tasks and actively retrieves distilled principles to guide its decision-making, accumulating a diverse set of behavioral trajectories. This loop employs a policy reinforcement mechanism to iteratively update the agent based on its performance. We demonstrate the effectiveness of EvolveR on complex multi-hop question-answering benchmarks, where it achieves superior performance over strong agentic baselines. Our work presents a comprehensive blueprint for agents that learn not only from external data but also from the consequences of their own actions, paving the way for more autonomous and continuously improving systems. Code is available at https://github.com/Edaizi/EvolveR.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "55",
        "title": "STABLE: Gated Continual Learning for Large Language Models",
        "author": [
            "William Hoy",
            "Nurcin Celik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16089",
        "abstract": "Large language models (LLMs) increasingly require mechanisms for continual adaptation without full retraining. However, sequential updates can lead to catastrophic forgetting, where new edits degrade previously acquired knowledge. This work presents STABLE, a gated continual self editing framework that constrains forgetting during sequential updates using parameter efficient fine tuning via Low Rank Adaptation (LoRA; see https://arxiv.org/abs/2106.09685). Each candidate edit is evaluated against a stability budget using one of three metrics: (i) Exact Match (EM) drop, capturing factual accuracy loss; (ii) bits increase, reflecting reduced model confidence; and (iii) KL divergence, quantifying distributional drift between the base and adapted models. If a threshold is exceeded, the LoRA update is rescaled through a clipping procedure or rejected. Experiments on the Qwen-2.5-7B model show that gating effectively mitigates forgetting while preserving adaptability. EM based gating achieved the highest cumulative performance in short continual learning sequences. Our results show that different gating strategies can achieve comparable distribution shift (measured by KL divergence) while producing different accuracy outcomes, highlighting the importance of gating design in continual adaptation. This approach offers a principled method for continual model editing, enabling LLMs to integrate new knowledge while maintaining reliability. Code: https://github.com/Bhoy1/STABLE",
        "tags": [
            "LLM",
            "LoRA",
            "Qwen"
        ]
    },
    {
        "id": "56",
        "title": "Evaluating Prompting Strategies and Large Language Models in Systematic Literature Review Screening: Relevance and Task-Stage Classification",
        "author": [
            "Binglan Han",
            "Anuradha Mathrani",
            "Teo Susnjak"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16091",
        "abstract": "This study quantifies how prompting strategies interact with large language models (LLMs) to automate the screening stage of systematic literature reviews (SLRs). We evaluate six LLMs (GPT-4o, GPT-4o-mini, DeepSeek-Chat-V3, Gemini-2.5-Flash, Claude-3.5-Haiku, Llama-4-Maverick) under five prompt types (zero-shot, few-shot, chain-of-thought (CoT), CoT-few-shot, self-reflection) across relevance classification and six Level-2 tasks, using accuracy, precision, recall, and F1. Results show pronounced model-prompt interaction effects: CoT-few-shot yields the most reliable precision-recall balance; zero-shot maximizes recall for high-sensitivity passes; and self-reflection underperforms due to over-inclusivity and instability across models. GPT-4o and DeepSeek provide robust overall performance, while GPT-4o-mini performs competitively at a substantially lower dollar cost. A cost-performance analysis for relevance classification (per 1,000 abstracts) reveals large absolute differences among model-prompt pairings; GPT-4o-mini remains low-cost across prompts, and structured prompts (CoT/CoT-few-shot) on GPT-4o-mini offer attractive F1 at a small incremental cost. We recommend a staged workflow that (1) deploys low-cost models with structured prompts for first-pass screening and (2) escalates only borderline cases to higher-capacity models. These findings highlight LLMs' uneven but promising potential to automate literature screening. By systematically analyzing prompt-model interactions, we provide a comparative benchmark and practical guidance for task-adaptive LLM deployment.",
        "tags": [
            "CoT",
            "DeepSeek",
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "57",
        "title": "Compressing Many-Shots in In-Context Learning",
        "author": [
            "Devvrit Khatri",
            "Pranamya Kulkarni",
            "Nilesh Gupta",
            "Yerram Varun",
            "Liqian Peng",
            "Jay Yagnik",
            "Praneeth Netrapalli",
            "Cho-Jui Hsieh",
            "Alec Go",
            "Inderjit S Dhillon",
            "Aditya Kusupati",
            "Prateek Jain"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16092",
        "abstract": "Large Language Models (LLMs) have been shown to be able to learn different tasks without explicit finetuning when given many input-output examples / demonstrations through In-Context Learning (ICL). Increasing the number of examples, called ``shots'', improves downstream task performance but incurs higher memory and computational costs. In this work, we study an approach to improve the memory and computational efficiency of ICL inference by compressing the many-shot prompts. Given many shots comprising t tokens, our goal is to generate a m soft-token summary, where m < t. We first show that existing prompt compression methods are ineffective for many-shot compression, and simply using fewer shots as a baseline is surprisingly strong. To achieve effective compression, we find that: (a) a stronger compressor model with more trainable parameters is necessary, and (b) compressing many-shot representations at each transformer layer enables more fine-grained compression by providing each layer with its own compressed representation. Based on these insights, we propose MemCom, a layer-wise compression method. We systematically evaluate various compressor models and training approaches across different model sizes (2B and 7B), architectures (Gemma and Mistral), many-shot sequence lengths (3k-6k tokens), and compression ratios (3x to 8x). MemCom outperforms strong baselines across all compression ratios on multiple classification tasks with large label sets. Notably, while baseline performance degrades sharply at higher compression ratios, often by over 20-30%, MemCom maintains high accuracy with minimal degradation, typically dropping by less than 10%.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "58",
        "title": "ObjectTransforms for Uncertainty Quantification and Reduction in Vision-Based Perception for Autonomous Vehicles",
        "author": [
            "Nishad Sahu",
            "Shounak Sural",
            "Aditya Satish Patil",
            "Ragunathan",
            "Rajkumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16118",
        "abstract": "Reliable perception is fundamental for safety critical decision making in autonomous driving. Yet, vision based object detector neural networks remain vulnerable to uncertainty arising from issues such as data bias and distributional shifts. In this paper, we introduce ObjectTransforms, a technique for quantifying and reducing uncertainty in vision based object detection through object specific transformations at both training and inference times. At training time, ObjectTransforms perform color space perturbations on individual objects, improving robustness to lighting and color variations. ObjectTransforms also uses diffusion models to generate realistic, diverse pedestrian instances. At inference time, object perturbations are applied to detected objects and the variance of detection scores are used to quantify predictive uncertainty in real time. This uncertainty signal is then used to filter out false positives and also recover false negatives, improving the overall precision recall curve. Experiments with YOLOv8 on the NuImages 10K dataset demonstrate that our method yields notable accuracy improvements and uncertainty reduction across all object classes during training, while predicting desirably higher uncertainty values for false positives as compared to true positives during inference. Our results highlight the potential of ObjectTransforms as a lightweight yet effective mechanism for reducing and quantifying uncertainty in vision-based perception during training and inference respectively.",
        "tags": [
            "Detection",
            "Diffusion"
        ]
    },
    {
        "id": "59",
        "title": "Zero-shot World Models via Search in Memory",
        "author": [
            "Federico Malato",
            "Ville HautamÃ¤ki"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16123",
        "abstract": "World Models have vastly permeated the field of Reinforcement Learning. Their ability to model the transition dynamics of an environment have greatly improved sample efficiency in online RL. Among them, the most notorious example is Dreamer, a model that learns to act in a diverse set of image-based environments. In this paper, we leverage similarity search and stochastic representations to approximate a world model without a training procedure. We establish a comparison with PlaNet, a well-established world model of the Dreamer family. We evaluate the models on the quality of latent reconstruction and on the perceived similarity of the reconstructed image, on both next-step and long horizon dynamics prediction. The results of our study demonstrate that a search-based world model is comparable to a training based one in both cases. Notably, our model show stronger performance in long-horizon prediction with respect to the baseline on a range of visually different environments.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "60",
        "title": "A Minimal-Assumption Analysis of Q-Learning with Time-Varying Policies",
        "author": [
            "Phalguni Nanda",
            "Zaiwei Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16132",
        "abstract": "In this work, we present the first finite-time analysis of the Q-learning algorithm under time-varying learning policies (i.e., on-policy sampling) with minimal assumptions -- specifically, assuming only the existence of a policy that induces an irreducible Markov chain over the state space. We establish a last-iterate convergence rate for $\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty^2]$, implying a sample complexity of order $O(1/\\epsilon^2)$ for achieving $\\mathbb{E}[\\|Q_k - Q^*\\|_\\infty] \\le \\epsilon$, matching that of off-policy Q-learning but with a worse dependence on exploration-related parameters. We also derive an explicit rate for $\\mathbb{E}[\\|Q^{\\pi_k} - Q^*\\|_\\infty^2]$, where $\\pi_k$ is the learning policy at iteration $k$. These results reveal that on-policy Q-learning exhibits weaker exploration than its off-policy counterpart but enjoys an exploitation advantage, as its policy converges to an optimal one rather than remaining fixed. Numerical simulations corroborate our theory.\nTechnically, the combination of time-varying learning policies (which induce rapidly time-inhomogeneous Markovian noise) and the minimal assumption on exploration presents significant analytical challenges. To address these challenges, we employ a refined approach that leverages the Poisson equation to decompose the Markovian noise corresponding to the lazy transition matrix into a martingale-difference term and residual terms. To control the residual terms under time inhomogeneity, we perform a sensitivity analysis of the Poisson equation solution with respect to both the Q-function estimate and the learning policy. These tools may further facilitate the analysis of general reinforcement learning algorithms with rapidly time-varying learning policies -- such as single-timescale actor--critic methods and learning-in-games algorithms -- and are of independent interest.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "61",
        "title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
        "author": [
            "Sayan Deb Sarkar",
            "Sinisa Stekovic",
            "Vincent Lepetit",
            "Iro Armeni"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16136",
        "abstract": "Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions.",
        "tags": [
            "3D",
            "Diffusion",
            "GPT",
            "Rectified Flow"
        ]
    },
    {
        "id": "62",
        "title": "Expert Merging in Sparse Mixture of Experts with Nash Bargaining",
        "author": [
            "Dung V. Nguyen",
            "Anh T. Nguyen",
            "Minh H. Nguyen",
            "Luc Q. Nguyen",
            "Shiqi Jiang",
            "Ethan Fetaya",
            "Linh Duy Tran",
            "Gal Chechik",
            "Tan M. Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16138",
        "abstract": "Existing expert merging strategies for Sparse Mixture of Experts (SMoE) typically rely on input-dependent or input-independent averaging of expert parameters, but often lack a principled weighting mechanism. In this work, we reinterpret expert merging through the lens of game theory, revealing cooperative and competitive dynamics among experts. Based on this perspective, we introduce Nash Merging of Experts (NAMEx), a novel framework that incorporates Nash Bargaining into the merging process, enabling more balanced and efficient collaboration among experts. Additionally, we incorporate complex momentum into NAMEx to accelerate expert propagation with theoretical guarantees for convergence. Extensive experiments across language modelling, text classification, image classification, and zero-shot robustness under data corruption show that NAMEx consistently outperforms competing methods while integrating seamlessly with popular MoE architectures. Finally, we demonstrate NAMEx's scalability by applying it to large-scale systems, including Qwen1.5-MoE (14B) and DeepSeek-MoE (16B), where it proves effective in both zero-shot and fine-tuning settings.",
        "tags": [
            "DeepSeek",
            "MoE"
        ]
    },
    {
        "id": "63",
        "title": "Procedural Scene Programs for Open-Universe Scene Generation: LLM-Free Error Correction via Program Search",
        "author": [
            "Maxim Gumin",
            "Do Heon Han",
            "Seung Jean Yoo",
            "Aditya Ganeshan",
            "R. Kenny Jones",
            "Kailiang Fu",
            "Rio Aguina-Kang",
            "Stewart Morris",
            "Daniel Ritchie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16147",
        "abstract": "Synthesizing 3D scenes from open-vocabulary text descriptions is a challenging, important, and recently-popular application. One of its critical subproblems is layout generation: given a set of objects, lay them out to produce a scene matching the input description. Nearly all recent work adopts a declarative paradigm for this problem: using an LLM to generate a specification of constraints between objects, then solving those constraints to produce the final layout. In contrast, we explore an alternative imperative paradigm, in which an LLM iteratively places objects, with each object's position and orientation computed as a function of previously-placed objects. The imperative approach allows for a simpler scene specification language while also handling a wider variety and larger complexity of scenes. We further improve the robustness of our imperative scheme by developing an error correction mechanism that iteratively improves the scene's validity while staying as close as possible to the original layout generated by the LLM. In forced-choice perceptual studies, participants preferred layouts generated by our imperative approach 82% and 94% of the time when compared against two declarative layout generation methods. We also present a simple, automated evaluation metric for 3D scene layout generation that aligns well with human preferences.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "64",
        "title": "Publication Trend Analysis and Synthesis via Large Language Model: A Case Study of Engineering in PNAS",
        "author": [
            "Mason Smetana",
            "Lev Khazanovich"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16152",
        "abstract": "Scientific literature is increasingly siloed by complex language, static disciplinary structures, and potentially sparse keyword systems, making it cumbersome to capture the dynamic nature of modern science. This study addresses these challenges by introducing an adaptable large language model (LLM)-driven framework to quantify thematic trends and map the evolving landscape of scientific knowledge. The approach is demonstrated over a 20-year collection of more than 1,500 engineering articles published by the Proceedings of the National Academy of Sciences (PNAS), marked for their breadth and depth of research focus. A two-stage classification pipeline first establishes a primary thematic category for each article based on its abstract. The subsequent phase performs a full-text analysis to assign secondary classifications, revealing latent, cross-topic connections across the corpus. Traditional natural language processing (NLP) methods, such as Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF), confirm the resulting topical structure and also suggest that standalone word-frequency analyses may be insufficient for mapping fields with high diversity. Finally, a disjoint graph representation between the primary and secondary classifications reveals implicit connections between themes that may be less apparent when analyzing abstracts or keywords alone. The findings show that the approach independently recovers much of the journal's editorially embedded structure without prior knowledge of its existing dual-classification schema (e.g., biological studies also classified as engineering). This framework offers a powerful tool for detecting potential thematic trends and providing a high-level overview of scientific progress.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "65",
        "title": "Zeroth-Order Sharpness-Aware Learning with Exponential Tilting",
        "author": [
            "Xuchen Gong",
            "Tian Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16157",
        "abstract": "Classic zeroth-order optimization approaches typically optimize for a smoothed version of the original function, i.e., the expected objective under randomly perturbed model parameters. This can be interpreted as encouraging the loss values in the perturbation set to be small on average. Popular sharpness-aware minimization (SAM) objectives, however, typically focus on the largest loss within the neighborhood to arrive at flat minima more effectively. In this work, we connect zeroth-order optimization (and its corresponding objectives) with SAM approaches explicitly, through an exponential tilting objective that provides a smooth transition between the average- and the max-loss formulations. We explore new zeroth-order algorithms to solve a soft SAM objective parameterized by a tilting parameter $t$. We provide precise characterizations of the sharpness notions of the tilted SAM framework. Practically, our approach can be used as a gradient-free and memory-efficient alternative to SAM variants, and it achieves better generalization compared to vanilla zeroth-order baselines on a wide range of downstream tasks, including classification, multiple choice QA, and language generation.",
        "tags": [
            "SAM"
        ]
    },
    {
        "id": "66",
        "title": "Still Competitive: Revisiting Recurrent Models for Irregular Time Series Prediction",
        "author": [
            "Ankitkumar Joshi",
            "Milos Hauskrecht"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16161",
        "abstract": "Modeling irregularly sampled multivariate time series is a persistent challenge in domains like healthcare and sensor networks. While recent works have explored a variety of complex learning architectures to solve the prediction problems for irregularly sampled time series, it remains unclear what are the true benefits of some of these architectures, and whether clever modifications of simpler and more efficient RNN-based algorithms are still competitive, i.e. they are on par with or even superior to these methods. In this work, we propose and study GRUwE: Gated Recurrent Unit with Exponential basis functions, that builds upon RNN-based architectures for observations made at irregular times. GRUwE supports both regression-based and event-based predictions in continuous time. GRUwE works by maintaining a Markov state representation of the time series that updates with the arrival of irregular observations. The Markov state update relies on two reset mechanisms: (i) observation-triggered reset, and (ii) time-triggered reset of the GRU state using learnable exponential decays, to support the predictions in continuous time. Our empirical evaluations across several real-world benchmarks on next-observation and next-event prediction tasks demonstrate that GRUwE can indeed achieve competitive to superior performance compared to the recent state-of-the-art (SOTA) methods. Thanks to its simplicity, GRUwE offers compelling advantages: it is easy to implement, requires minimal hyper-parameter tuning efforts, and significantly reduces the computational overhead in the online deployment.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "67",
        "title": "AtomBench: A Benchmark for Generative Atomic Structure Models using GPT, Diffusion, and Flow Architectures",
        "author": [
            "Charles Rhys Campbell",
            "Aldo H. Romero",
            "Kamal Choudhary"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16165",
        "abstract": "Generative models have become significant assets in the exploration and identification of new materials, enabling the rapid proposal of candidate crystal structures that satisfy target properties. Despite the increasing adoption of diverse architectures, a rigorous comparative evaluation of their performance on materials datasets is lacking. In this work, we present a systematic benchmark of three representative generative models- AtomGPT (a transformer-based model), Crystal Diffusion Variational Autoencoder (CDVAE), and FlowMM (a Riemannian flow matching model). These models were trained to reconstruct crystal structures from subsets of two publicly available superconductivity datasets- JARVIS Supercon 3D and DS A/B from the Alexandria database. Performance was assessed using the Kullback-Leibler (KL) divergence between predicted and reference distributions of lattice parameters, as well as the mean absolute error (MAE) of individual lattice constants. For the computed KLD and MAE scores, CDVAE performs most favorably, followed by AtomGPT, and then FlowMM. All benchmarking code and model configurations will be made publicly available at https://github.com/atomgptlab/atombench_inverse.",
        "tags": [
            "3D",
            "Diffusion",
            "Flow Matching",
            "GPT",
            "Transformer"
        ]
    },
    {
        "id": "68",
        "title": "Alignment is Localized: A Causal Probe into Preference Layers",
        "author": [
            "Archie Chaudhury"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16167",
        "abstract": "Reinforcement Learning frameworks, particularly those utilizing human annotations, have become an increasingly popular method for preference fine-tuning, where the outputs of a language model are tuned to match a certain set of behavioral policies or guidelines. Reinforcement Learning through Human Feedback (RLHF) is perhaps the most popular implementation of such a framework, particularly for aligning LMs toward safety and human intent. However, the internal workings of how such alignment is achieved remain largely opaque. In this work, we systematically analyze preference optimization for language model alignment by applying layer-wide causal patching between a base model and its tuned counterpart across human preference pairs. We implement our methodology on \\textit{Llama-3.2-1B}, and find that alignment is spatially localized: mid-layer activations encode a distinct subspace that causally determines reward-consistent behavior, while early and late layers remain largely unaffected. Utilizing LASSO regression, we also find that only a small number of layers possess non-zero coefficients linking activation distances to reward gains. Overall, we show that, at least for some language models, alignment from human-based, preferential tuning is a directional, low rank process, rather than diffuse and parameteric.",
        "tags": [
            "LLaMA",
            "RL",
            "RLHF"
        ]
    },
    {
        "id": "69",
        "title": "In Generative AI We (Dis)Trust? Computational Analysis of Trust and Distrust in Reddit Discussions",
        "author": [
            "Aria Pessianzadeh",
            "Naima Sultana",
            "Hildegarde Van den Bulck",
            "David Gefen",
            "Shahin Jabari",
            "Rezvaneh Rezapour"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16173",
        "abstract": "The rise of generative AI (GenAI) has impacted many aspects of human life. As these systems become embedded in everyday practices, understanding public trust in them also becomes essential for responsible adoption and governance. Prior work on trust in AI has largely drawn from psychology and human-computer interaction, but there is a lack of computational, large-scale, and longitudinal approaches to measuring trust and distrust in GenAI and large language models (LLMs). This paper presents the first computational study of Trust and Distrust in GenAI, using a multi-year Reddit dataset (2022--2025) spanning 39 subreddits and 197,618 posts. Crowd-sourced annotations of a representative sample were combined with classification models to scale analysis. We find that Trust and Distrust are nearly balanced over time, with shifts around major model releases. Technical performance and usability dominate as dimensions, while personal experience is the most frequent reason shaping attitudes. Distinct patterns also emerge across trustors (e.g., experts, ethicists, general users). Our results provide a methodological framework for large-scale Trust analysis and insights into evolving public perceptions of GenAI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "70",
        "title": "The Formalism-Implementation Gap in Reinforcement Learning Research",
        "author": [
            "Pablo Samuel Castro"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16175",
        "abstract": "The last decade has seen an upswing in interest and adoption of reinforcement learning (RL) techniques, in large part due to its demonstrated capabilities at performing certain tasks at \"super-human levels\". This has incentivized the community to prioritize research that demonstrates RL agent performance, often at the expense of research aimed at understanding their learning dynamics. Performance-focused research runs the risk of overfitting on academic benchmarks -- thereby rendering them less useful -- which can make it difficult to transfer proposed techniques to novel problems. Further, it implicitly diminishes work that does not push the performance-frontier, but aims at improving our understanding of these techniques. This paper argues two points: (i) RL research should stop focusing solely on demonstrating agent capabilities, and focus more on advancing the science and understanding of reinforcement learning; and (ii) we need to be more precise on how our benchmarks map to the underlying mathematical formalisms. We use the popular Arcade Learning Environment (ALE; Bellemare et al., 2013) as an example of a benchmark that, despite being increasingly considered \"saturated\", can be effectively used for developing this understanding, and facilitating the deployment of RL techniques in impactful real-world problems.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "71",
        "title": "Expressive Reward Synthesis with the Runtime Monitoring Language",
        "author": [
            "Daniel Donnelly",
            "Angelo Ferrando",
            "Francesco Belardinelli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16185",
        "abstract": "A key challenge in reinforcement learning (RL) is reward (mis)specification, whereby imprecisely defined reward functions can result in unintended, possibly harmful, behaviours. Indeed, reward functions in RL are typically treated as black-box mappings from state-action pairs to scalar values. While effective in many settings, this approach provides no information about why rewards are given, which can hinder learning and interpretability. Reward Machines address this issue by representing reward functions as finite state automata, enabling the specification of structured, non-Markovian reward functions. However, their expressivity is typically bounded by regular languages, leaving them unable to capture more complex behaviours such as counting or parametrised conditions. In this work, we build on the Runtime Monitoring Language (RML) to develop a novel class of language-based Reward Machines. By leveraging the built-in memory of RML, our approach can specify reward functions for non-regular, non-Markovian tasks. We demonstrate the expressiveness of our approach through experiments, highlighting additional advantages in flexible event-handling and task specification over existing Reward Machine-based methods.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "72",
        "title": "Zero-Shot Coordination in Ad Hoc Teams with Generalized Policy Improvement and Difference Rewards",
        "author": [
            "Rupal Nigam",
            "Niket Parikh",
            "Hamid Osooli",
            "Mikihisa Yuasa",
            "Jacob Heglund",
            "Huy T. Tran"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16187",
        "abstract": "Real-world multi-agent systems may require ad hoc teaming, where an agent must coordinate with other previously unseen teammates to solve a task in a zero-shot manner. Prior work often either selects a pretrained policy based on an inferred model of the new teammates or pretrains a single policy that is robust to potential teammates. Instead, we propose to leverage all pretrained policies in a zero-shot transfer setting. We formalize this problem as an ad hoc multi-agent Markov decision process and present a solution that uses two key ideas, generalized policy improvement and difference rewards, for efficient and effective knowledge transfer between different teams. We empirically demonstrate that our algorithm, Generalized Policy improvement for Ad hoc Teaming (GPAT), successfully enables zero-shot transfer to new teams in three simulated environments: cooperative foraging, predator-prey, and Overcooked. We also demonstrate our algorithm in a real-world multi-robot setting.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "73",
        "title": "Human-Allied Relational Reinforcement Learning",
        "author": [
            "Fateme Golivand Darvishvand",
            "Hikaru Shindo",
            "Sahil Sidheekh",
            "Kristian Kersting",
            "Sriraam Natarajan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16188",
        "abstract": "Reinforcement learning (RL) has experienced a second wind in the past decade. While incredibly successful in images and videos, these systems still operate within the realm of propositional tasks ignoring the inherent structure that exists in the problem. Consequently, relational extensions (RRL) have been developed for such structured problems that allow for effective generalization to arbitrary number of objects. However, they inherently make strong assumptions about the problem structure. We introduce a novel framework that combines RRL with object-centric representation to handle both structured and unstructured data. We enhance learning by allowing the system to actively query the human expert for guidance by explicitly modeling the uncertainty over the policy. Our empirical evaluation demonstrates the effectiveness and efficiency of our proposed approach.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "74",
        "title": "Towards Automatic Evaluation and Selection of PHI De-identification Models via Multi-Agent Collaboration",
        "author": [
            "Guanchen Wu",
            "Zuhui Chen",
            "Yuzhang Xie",
            "Carl Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16194",
        "abstract": "Protected health information (PHI) de-identification is critical for enabling the safe reuse of clinical notes, yet evaluating and comparing PHI de-identification models typically depends on costly, small-scale expert annotations. We present TEAM-PHI, a multi-agent evaluation and selection framework that uses large language models (LLMs) to automatically measure de-identification quality and select the best-performing model without heavy reliance on gold labels. TEAM-PHI deploys multiple Evaluation Agents, each independently judging the correctness of PHI extractions and outputting structured metrics. Their results are then consolidated through an LLM-based majority voting mechanism that integrates diverse evaluator perspectives into a single, stable, and reproducible ranking. Experiments on a real-world clinical note corpus demonstrate that TEAM-PHI produces consistent and accurate rankings: despite variation across individual evaluators, LLM-based voting reliably converges on the same top-performing systems. Further comparison with ground-truth annotations and human evaluation confirms that the framework's automated rankings closely match supervised evaluation. By combining independent evaluation agents with LLM majority voting, TEAM-PHI offers a practical, secure, and cost-effective solution for automatic evaluation and best-model selection in PHI de-identification, even when ground-truth labels are limited.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "75",
        "title": "EgMM-Corpus: A Multimodal Vision-Language Dataset for Egyptian Culture",
        "author": [
            "Mohamed Gamil",
            "Abdelrahman Elsayed",
            "Abdelrahman Lila",
            "Ahmed Gad",
            "Hesham Abdelgawad",
            "Mohamed Aref",
            "Ahmed Fares"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16198",
        "abstract": "Despite recent advances in AI, multimodal culturally diverse datasets are still limited, particularly for regions in the Middle East and Africa. In this paper, we introduce EgMM-Corpus, a multimodal dataset dedicated to Egyptian culture. By designing and running a new data collection pipeline, we collected over 3,000 images, covering 313 concepts across landmarks, food, and folklore. Each entry in the dataset is manually validated for cultural authenticity and multimodal coherence. EgMM-Corpus aims to provide a reliable resource for evaluating and training vision-language models in an Egyptian cultural context. We further evaluate the zero-shot performance of Contrastive Language-Image Pre-training CLIP on EgMM-Corpus, on which it achieves 21.2% Top-1 accuracy and 36.4% Top-5 accuracy in classification. These results underscore the existing cultural bias in large-scale vision-language models and demonstrate the importance of EgMM-Corpus as a benchmark for developing culturally aware models.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "76",
        "title": "VAR-SLAM: Visual Adaptive and Robust SLAM for Dynamic Environments",
        "author": [
            "JoÃ£o Carlos Virgolino Soares",
            "Gabriel Fischer Abati",
            "Claudio Semini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16205",
        "abstract": "Visual SLAM in dynamic environments remains challenging, as several existing methods rely on semantic filtering that only handles known object classes, or use fixed robust kernels that cannot adapt to unknown moving objects, leading to degraded accuracy when they appear in the scene. We present VAR-SLAM (Visual Adaptive and Robust SLAM), an ORB-SLAM3-based system that combines a lightweight semantic keypoint filter to deal with known moving objects, with Barron's adaptive robust loss to handle unknown ones. The shape parameter of the robust kernel is estimated online from residuals, allowing the system to automatically adjust between Gaussian and heavy-tailed behavior. We evaluate VAR-SLAM on the TUM RGB-D, Bonn RGB-D Dynamic, and OpenLORIS datasets, which include both known and unknown moving objects. Results show improved trajectory accuracy and robustness over state-of-the-art baselines, achieving up to 25% lower ATE RMSE than NGD-SLAM on challenging sequences, while maintaining performance at 27 FPS on average.",
        "tags": [
            "SLAM"
        ]
    },
    {
        "id": "77",
        "title": "The Right to Be Remembered: Preserving Maximally Truthful Digital Memory in the Age of AI",
        "author": [
            "Alex Zhavoronkov",
            "Dominika Wilczok",
            "Roman Yampolskiy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16206",
        "abstract": "Since the rapid expansion of large language models (LLMs), people have begun to rely on them for information retrieval. While traditional search engines display ranked lists of sources shaped by search engine optimization (SEO), advertising, and personalization, LLMs typically provide a synthesized response that feels singular and authoritative. While both approaches carry risks of bias and omission, LLMs may amplify the effect by collapsing multiple perspectives into one answer, reducing users ability or inclination to compare alternatives. This concentrates power over information in a few LLM vendors whose systems effectively shape what is remembered and what is overlooked. As a result, certain narratives, individuals or groups, may be disproportionately suppressed, while others are disproportionately elevated. Over time, this creates a new threat: the gradual erasure of those with limited digital presence, and the amplification of those already prominent, reshaping collective http://memory.To address these concerns, this paper presents a concept of the Right To Be Remembered (RTBR) which encompasses minimizing the risk of AI-driven information omission, embracing the right of fair treatment, while ensuring that the generated content would be maximally truthful.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "78",
        "title": "StretchySnake: Flexible SSM Training Unlocks Action Recognition Across Spatio-Temporal Scales",
        "author": [
            "Nyle Siddiqui",
            "Rohit Gupta",
            "Sirnam Swetha",
            "Mubarak Shah"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16209",
        "abstract": "State space models (SSMs) have emerged as a competitive alternative to transformers in various tasks. Their linear complexity and hidden-state recurrence make them particularly attractive for modeling long sequences, whereas attention becomes quadratically expensive. However, current training methods for video understanding are tailored towards transformers and fail to fully leverage the unique attributes of SSMs. For example, video models are often trained at a fixed resolution and video length to balance the quadratic scaling of attention cost against performance. Consequently, these models suffer from degraded performance when evaluated on videos with spatial and temporal resolutions unseen during training; a property we call spatio-temporal inflexibility. In the context of action recognition, this severely limits a model's ability to retain performance across both short- and long-form videos. Therefore, we propose a flexible training method that leverages and improves the inherent adaptability of SSMs. Our method samples videos at varying temporal and spatial resolutions during training and dynamically interpolates model weights to accommodate any spatio-temporal scale. This instills our SSM, which we call StretchySnake, with spatio-temporal flexibility and enables it to seamlessly handle videos ranging from short, fine-grained clips to long, complex activities. We introduce and compare five different variants of flexible training, and identify the most effective strategy for video SSMs. On short-action (UCF-101, HMDB-51) and long-action (COIN, Breakfast) benchmarks, StretchySnake outperforms transformer and SSM baselines alike by up to 28%, with strong adaptability to fine-grained actions (SSV2, Diving-48). Therefore, our method provides a simple drop-in training recipe that makes video SSMs more robust, resolution-agnostic, and efficient across diverse action recognition scenarios.",
        "tags": [
            "SSMs",
            "Transformer"
        ]
    },
    {
        "id": "79",
        "title": "SentinelNet: Safeguarding Multi-Agent Collaboration Through Credit-Based Dynamic Threat Detection",
        "author": [
            "Yang Feng",
            "Xudong Pan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16219",
        "abstract": "Malicious agents pose significant threats to the reliability and decision-making capabilities of Multi-Agent Systems (MAS) powered by Large Language Models (LLMs). Existing defenses often fall short due to reactive designs or centralized architectures which may introduce single points of failure. To address these challenges, we propose SentinelNet, the first decentralized framework for proactively detecting and mitigating malicious behaviors in multi-agent collaboration. SentinelNet equips each agent with a credit-based detector trained via contrastive learning on augmented adversarial debate trajectories, enabling autonomous evaluation of message credibility and dynamic neighbor ranking via bottom-k elimination to suppress malicious communications. To overcome the scarcity of attack data, it generates adversarial trajectories simulating diverse threats, ensuring robust training. Experiments on MAS benchmarks show SentinelNet achieves near-perfect detection of malicious agents, close to 100% within two debate rounds, and recovers 95% of system accuracy from compromised baselines. By exhibiting strong generalizability across domains and attack patterns, SentinelNet establishes a novel paradigm for safeguarding collaborative MAS.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "80",
        "title": "VM-BeautyNet: A Synergistic Ensemble of Vision Transformer and Mamba for Facial Beauty Prediction",
        "author": [
            "Djamel Eddine Boukhari"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16220",
        "abstract": "Facial Beauty Prediction (FBP) is a complex and challenging computer vision task, aiming to model the subjective and intricate nature of human aesthetic perception. While deep learning models, particularly Convolutional Neural Networks (CNNs), have made significant strides, they often struggle to capture the global, holistic facial features that are critical to human judgment. Vision Transformers (ViT) address this by effectively modeling long-range spatial relationships, but their quadratic complexity can be a bottleneck. This paper introduces a novel, heterogeneous ensemble architecture, \\textbf{VM-BeautyNet}, that synergistically fuses the complementary strengths of a Vision Transformer and a Mamba-based Vision model, a recent advancement in State-Space Models (SSMs). The ViT backbone excels at capturing global facial structure and symmetry, while the Mamba backbone efficiently models long-range dependencies with linear complexity, focusing on sequential features and textures. We evaluate our approach on the benchmark SCUT-FBP5500 dataset. Our proposed VM-BeautyNet achieves state-of-the-art performance, with a \\textbf{Pearson Correlation (PC) of 0.9212}, a \\textbf{Mean Absolute Error (MAE) of 0.2085}, and a \\textbf{Root Mean Square Error (RMSE) of 0.2698}. Furthermore, through Grad-CAM visualizations, we provide interpretability analysis that confirms the complementary feature extraction of the two backbones, offering new insights into the model's decision-making process and presenting a powerful new architectural paradigm for computational aesthetics.",
        "tags": [
            "Mamba",
            "SSMs",
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "81",
        "title": "DeGrip: A Compact Cable-driven Robotic Gripper for Desktop Disassembly",
        "author": [
            "Bihao Zhang",
            "Davood Soleymanzadeh",
            "Xiao Liang",
            "Minghui Zheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16231",
        "abstract": "Intelligent robotic disassembly of end-of-life (EOL) products has been a long-standing challenge in robotics. While machine learning techniques have shown promise, the lack of specialized hardware limits their application in real-world scenarios. We introduce DeGrip, a customized gripper designed for the disassembly of EOL computer desktops. DeGrip provides three degrees of freedom (DOF), enabling arbitrary configurations within the disassembly environment when mounted on a robotic manipulator. It employs a cable-driven transmission mechanism that reduces its overall size and enables operation in confined spaces. The wrist is designed to decouple the actuation of wrist and jaw joints. We also developed an EOL desktop disassembly environment in Isaac Sim to evaluate the effectiveness of DeGrip. The tasks were designed to demonstrate its ability to operate in confined spaces and disassemble components in arbitrary configurations. The evaluation results confirm the capability of DeGrip for EOL desktop disassembly.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "82",
        "title": "Machine Learning for Climate Policy: Understanding Policy Progression in the European Green Deal",
        "author": [
            "Patricia West",
            "Michelle WL Wan",
            "Alexander Hepburn",
            "Edwin Simpson",
            "Raul Santos-Rodriguez",
            "Jeffrey N Clark"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16233",
        "abstract": "Climate change demands effective legislative action to mitigate its impacts. This study explores the application of machine learning (ML) to understand the progression of climate policy from announcement to adoption, focusing on policies within the European Green Deal. We present a dataset of 165 policies, incorporating text and metadata. We aim to predict a policy's progression status, and compare text representation methods, including TF-IDF, BERT, and ClimateBERT. Metadata features are included to evaluate the impact on predictive performance. On text features alone, ClimateBERT outperforms other approaches (RMSE = 0.17, R^2 = 0.29), while BERT achieves superior performance with the addition of metadata features (RMSE = 0.16, R^2 = 0.38). Using methods from explainable AI highlights the influence of factors such as policy wording and metadata including political party and country representation. These findings underscore the potential of ML tools in supporting climate policy analysis and decision-making.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "83",
        "title": "WEBSERV: A Browser-Server Environment for Efficient Training of Reinforcement Learning-based Web Agents at Scale",
        "author": [
            "Yuxuan Lu",
            "Jing Huang",
            "Hui Liu",
            "Jiri Gesi",
            "Yan Han",
            "Shihan Fu",
            "Tianqi Zheng",
            "Dakuo Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16252",
        "abstract": "Training and evaluation of Reinforcement Learning (RL) web agents have gained increasing attention, yet a scalable and efficient environment that couples realistic and robust browser-side interaction with controllable server-side state at scale is still missing. Existing environments tend to have one or more of the following issues: they overwhelm policy models with excessive and noisy context; they perform actions non-deterministically without waiting for the UI or network to stabilize; or they cannot scale isolated client-server containers effectively for parallel RL rollouts. We propose WEBSERV, an environment that includes 1) a compact, site-agnostic browser environment that balances context and action complexity, and 2) a scalable RL environment via efficient launching and resetting web-servers to enable scalable RL training and evaluation. We evaluate WEBSERV on the shopping CMS and Gitlab tasks in WebArena, achieving state-of-the-art single-prompt success rates while cutting launch latency by ~5x and storage need by ~240x, with a comparable memory footprint, enabling 200+ concurrent containers on a single host.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "84",
        "title": "Detecting Adversarial Fine-tuning with Auditing Agents",
        "author": [
            "Sarah Egler",
            "John Schulman",
            "Nicholas Carlini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16255",
        "abstract": "Large Language Model (LLM) providers expose fine-tuning APIs that let end users fine-tune their frontier LLMs. Unfortunately, it has been shown that an adversary with fine-tuning access to an LLM can bypass safeguards. Particularly concerning, such attacks may avoid detection with datasets that are only implicitly harmful. Our work studies robust detection mechanisms for adversarial use of fine-tuning APIs. We introduce the concept of a fine-tuning auditing agent and show it can detect harmful fine-tuning prior to model deployment. We provide our auditing agent with access to the fine-tuning dataset, as well as the fine-tuned and pre-fine-tuned models, and request the agent assigns a risk score for the fine-tuning job. We evaluate our detection approach on a diverse set of eight strong fine-tuning attacks from the literature, along with five benign fine-tuned models, totaling over 1400 independent audits. These attacks are undetectable with basic content moderation on the dataset, highlighting the challenge of the task. With the best set of affordances, our auditing agent achieves a 56.2% detection rate of adversarial fine-tuning at a 1% false positive rate. Most promising, the auditor is able to detect covert cipher attacks that evade safety evaluations and content moderation of the dataset. While benign fine-tuning with unintentional subtle safety degradation remains a challenge, we establish a baseline configuration for further work in this area. We release our auditing agent at https://github.com/safety-research/finetuning-auditor.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "85",
        "title": "Distractor Injection Attacks on Large Reasoning Models: Characterization and Defense",
        "author": [
            "Zhehao Zhang",
            "Weijie Xu",
            "Shixian Cui",
            "Chandan K. Reddy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16259",
        "abstract": "Recent advances in large reasoning models (LRMs) have enabled remarkable performance on complex tasks such as mathematics and coding by generating long Chain-of-Thought (CoT) traces. In this paper, we identify and systematically analyze a critical vulnerability we term reasoning distraction, where LRMs are diverted from their primary objective by irrelevant yet complex tasks maliciously embedded in the prompt. Through a comprehensive study across diverse models and benchmarks, we show that even state-of-the-art LRMs are highly susceptible, with injected distractors reducing task accuracy by up to 60%. We further reveal that certain alignment techniques can amplify this weakness and that models may exhibit covert compliance, following hidden adversarial instructions in reasoning while concealing them in the final output. To mitigate these risks, we propose a training-based defense that combines Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) on synthetic adversarial data, improving robustness by over 50 points on challenging distractor attacks. Our findings establish reasoning distraction as a distinct and urgent threat to LRM reliability and provide a practical step toward safer and more trustworthy reasoning systems.",
        "tags": [
            "CoT",
            "RL"
        ]
    },
    {
        "id": "86",
        "title": "Proactive Scene Decomposition and Reconstruction",
        "author": [
            "Baicheng Li",
            "Zike Yan",
            "Dong Wu",
            "Hongbin Zha"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16272",
        "abstract": "Human behaviors are the major causes of scene dynamics and inherently contain rich cues regarding the dynamics. This paper formalizes a new task of proactive scene decomposition and reconstruction, an online approach that leverages human-object interactions to iteratively disassemble and reconstruct the environment. By observing these intentional interactions, we can dynamically refine the decomposition and reconstruction process, addressing inherent ambiguities in static object-level reconstruction. The proposed system effectively integrates multiple tasks in dynamic environments such as accurate camera and object pose estimation, instance decomposition, and online map updating, capitalizing on cues from human-object interactions in egocentric live streams for a flexible, progressive alternative to conventional object-level reconstruction methods. Aided by the Gaussian splatting technique, accurate and consistent dynamic scene modeling is achieved with photorealistic and efficient rendering. The efficacy is validated in multiple real-world scenarios with promising advantages.",
        "tags": [
            "Gaussian Splatting",
            "Pose Estimation"
        ]
    },
    {
        "id": "87",
        "title": "MuseTok: Symbolic Music Tokenization for Generation and Semantic Understanding",
        "author": [
            "Jingyue Huang",
            "Zachary Novack",
            "Phillip Long",
            "Yupeng Hou",
            "Ke Chen",
            "Taylor Berg-Kirkpatrick",
            "Julian McAuley"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16273",
        "abstract": "Discrete representation learning has shown promising results across various domains, including generation and understanding in image, speech and language. Inspired by these advances, we propose MuseTok, a tokenization method for symbolic music, and investigate its effectiveness in both music generation and understanding tasks. MuseTok employs the residual vector quantized-variational autoencoder (RQ-VAE) on bar-wise music segments within a Transformer-based encoder-decoder framework, producing music codes that achieve high-fidelity music reconstruction and accurate understanding of music theory. For comprehensive evaluation, we apply MuseTok to music generation and semantic understanding tasks, including melody extraction, chord recognition, and emotion recognition. Models incorporating MuseTok outperform previous representation learning baselines in semantic understanding while maintaining comparable performance in content generation. Furthermore, qualitative analyses on MuseTok codes, using ground-truth categories and synthetic datasets, reveal that MuseTok effectively captures underlying musical concepts from large music collections.",
        "tags": [
            "Transformer",
            "VAE"
        ]
    },
    {
        "id": "88",
        "title": "What Limits Agentic Systems Efficiency?",
        "author": [
            "Song Bian",
            "Minghao Yan",
            "Anand Jayarajan",
            "Gennady Pekhimenko",
            "Shivaram Venkataraman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16276",
        "abstract": "Large Language Models (LLMs), such as OpenAI-o1 and DeepSeek-R1, have demonstrated strong reasoning capabilities. To further enhance LLM capabilities, recent agentic systems, such as Deep Research, incorporate web interactions into LLM reasoning to mitigate uncertainties and reduce potential errors. However, existing research predominantly focuses on reasoning performance, often neglecting the efficiency of agentic systems. In this work, we present a comprehensive empirical study that identifies efficiency bottlenecks in web-interactive agentic systems. We decompose end-to-end latency into two primary components: LLM API latency and web environment latency. We conduct a comprehensive empirical study across 15 models and 5 providers to demonstrate high variability in API-based agentic systems. We observe that web environment latency can contribute as much as 53.7% to the overall latency in a web-based agentic system. To improve latency, we propose SpecCache, a caching framework augmented with speculative execution that can reduce web environment overhead. Extensive evaluations on two standard benchmarks show that our approach improves the cache hit rate by up to 58x compared to a random caching strategy, while reducing web environment overhead by up to 3.2x, without degrading agentic system performance.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "89",
        "title": "Do What You Say: Steering Vision-Language-Action Models via Runtime Reasoning-Action Alignment Verification",
        "author": [
            "Yilin Wu",
            "Anqi Li",
            "Tucker Hermans",
            "Fabio Ramos",
            "Andrea Bajcsy",
            "Claudia P'erez-D'Arpino"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16281",
        "abstract": "Reasoning Vision Language Action (VLA) models improve robotic instruction-following by generating step-by-step textual plans before low-level actions, an approach inspired by Chain-of-Thought (CoT) reasoning in language models. Yet even with a correct textual plan, the generated actions can still miss the intended outcomes in the plan, especially in out-of-distribution (OOD) scenarios. We formalize this phenomenon as a lack of embodied CoT faithfulness, and introduce a training-free, runtime policy steering method for reasoning-action alignment. Given a reasoning VLA's intermediate textual plan, our framework samples multiple candidate action sequences from the same model, predicts their outcomes via simulation, and uses a pre-trained Vision-Language Model (VLM) to select the sequence whose outcome best aligns with the VLA's own textual plan. Only executing action sequences that align with the textual reasoning turns our base VLA's natural action diversity from a source of error into a strength, boosting robustness to semantic and visual OOD perturbations and enabling novel behavior composition without costly re-training. We also contribute a reasoning-annotated extension of LIBERO-100, environment variations tailored for OOD evaluation, and demonstrate up to 15% performance gain over prior work on behavior composition tasks and scales with compute and data diversity. Project Website at: https://yilin-wu98.github.io/steering-reasoning-vla/",
        "tags": [
            "CoT",
            "VLM"
        ]
    },
    {
        "id": "90",
        "title": "Instant Personalized Large Language Model Adaptation via Hypernetwork",
        "author": [
            "Zhaoxuan Tan",
            "Zixuan Zhang",
            "Haoyang Wen",
            "Zheng Li",
            "Rongzhi Zhang",
            "Pei Chen",
            "Fengran Mo",
            "Zheyuan Liu",
            "Qingkai Zeng",
            "Qingyu Yin",
            "Meng Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16282",
        "abstract": "Personalized large language models (LLMs) tailor content to individual preferences using user profiles or histories. However, existing parameter-efficient fine-tuning (PEFT) methods, such as the ``One-PEFT-Per-User'' (OPPU) paradigm, require training a separate adapter for each user, making them computationally expensive and impractical for real-time updates. We introduce Profile-to-PEFT, a scalable framework that employs a hypernetwork, trained end-to-end, to map a user's encoded profile directly to a full set of adapter parameters (e.g., LoRA), eliminating per-user training at deployment. This design enables instant adaptation, generalization to unseen users, and privacy-preserving local deployment. Experimental results demonstrate that our method outperforms both prompt-based personalization and OPPU while using substantially fewer computational resources at deployment. The framework exhibits strong generalization to out-of-distribution users and maintains robustness across varying user activity levels and different embedding backbones. The proposed Profile-to-PEFT framework enables efficient, scalable, and adaptive LLM personalization suitable for large-scale applications.",
        "tags": [
            "LLM",
            "LoRA"
        ]
    },
    {
        "id": "91",
        "title": "QSVD: Efficient Low-rank Approximation for Unified Query-Key-Value Weight Compression in Low-Precision Vision-Language Models",
        "author": [
            "Yutong Wang",
            "Haiyu Wang",
            "Sai Qian Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16292",
        "abstract": "Vision-Language Models (VLMs) are integral to tasks such as image captioning and visual question answering, but their high computational cost, driven by large memory footprints and processing time, limits their scalability and real-time applicability. In this work, we propose leveraging Singular-Value Decomposition (SVD) over the joint query (Q), key (K), and value (V) weight matrices to reduce KV cache size and computational overhead. We in addition introduce an efficient rank allocation strategy that dynamically adjusts the SVD rank based on its impact on VLM accuracy, achieving a significant reduction in both memory usage and computational cost. Finally, we extend this approach by applying quantization to both VLM weights and activations, resulting in a highly efficient VLM. Our method outperforms previous approaches that rely solely on quantization or SVD by achieving more than $10\\%$ accuracy improvement while consuming less hardware cost, making it better for real-time deployment on resource-constrained devices. We open source our code at \\href{https://github.com/SAI-Lab-NYU/QSVD}{\\texttt{https://github.com/SAI-Lab-NYU/QSVD}}.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "92",
        "title": "OpenLVLM-MIA: A Controlled Benchmark Revealing the Limits of Membership Inference Attacks on Large Vision-Language Models",
        "author": [
            "Ryoto Miyamoto",
            "Xin Fan",
            "Fuyuko Kido",
            "Tsuneo Matsumoto",
            "Hayato Yamana"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16295",
        "abstract": "OpenLVLM-MIA is a new benchmark that highlights fundamental challenges in evaluating membership inference attacks (MIA) against large vision-language models (LVLMs). While prior work has reported high attack success rates, our analysis suggests that these results often arise from detecting distributional bias introduced during dataset construction rather than from identifying true membership status. To address this issue, we introduce a controlled benchmark of 6{,}000 images where the distributions of member and non-member samples are carefully balanced, and ground-truth membership labels are provided across three distinct training stages. Experiments using OpenLVLM-MIA demonstrated that the performance of state-of-the-art MIA methods converged to random chance under unbiased conditions. By offering a transparent and unbiased benchmark, OpenLVLM-MIA clarifies the current limitations of MIA research on LVLMs and provides a solid foundation for developing stronger privacy-preserving techniques.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "93",
        "title": "DTKG: Dual-Track Knowledge Graph-Verified Reasoning Framework for Multi-Hop QA",
        "author": [
            "Changhao Wang",
            "Yanfang Liu",
            "Xinxin Fan",
            "Anzhi Zhou",
            "Lao Tian",
            "Yunfeng Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16302",
        "abstract": "Multi-hop reasoning for question answering (QA) plays a critical role in retrieval-augmented generation (RAG) for modern large language models (LLMs). The accurate answer can be obtained through retrieving relational structure of entities from knowledge graph (KG). Regarding the inherent relation-dependency and reasoning pattern, multi-hop reasoning can be in general classified into two categories: i) parallel fact-verification multi-hop reasoning question, i.e., requiring simultaneous verifications of multiple independent sub-questions; and ii) chained multi-hop reasoning questions, i.e., demanding sequential multi-step inference with intermediate conclusions serving as essential premises for subsequent reasoning. Currently, the multi-hop reasoning approaches singly employ one of two techniques: LLM response-based fact verification and KG path-based chain construction. Nevertheless, the former excels at parallel fact-verification but underperforms on chained reasoning tasks, while the latter demonstrates proficiency in chained multi-hop reasoning but suffers from redundant path retrieval when handling parallel fact-verification reasoning. These limitations deteriorate the efficiency and accuracy for multi-hop QA tasks. To address this challenge, we propose a novel dual-track KG verification and reasoning framework DTKG, which is inspired by the Dual Process Theory in cognitive science. Specifically, DTKG comprises two main stages: the Classification Stage and the Branch Processing Stage.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "94",
        "title": "MedRule-KG: A Knowledge-Graph--Steered Scaffold for Mathematical Reasoning with a Lightweight Verifier",
        "author": [
            "Crystal Su"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16309",
        "abstract": "Large language models (LLMs) often produce fluent reasoning steps while violating simple mathematical or logical constraints. We introduce MedRule-KG, a compact typed knowledge graph coupled with a symbolic verifier, designed to enforce mathematically interpretable rules in reasoning tasks. MedRule-KG encodes entities, relations, and three domain-inspired rules, while the verifier checks predictions and applies minimal corrections to guarantee consistency. On a 90-example FDA-derived benchmark, grounding in MedRule-KG improves exact match (EM) from 0.767 to 0.900, and adding the verifier yields 1.000 EM while eliminating rule violations entirely. We demonstrate how MedRule-KG provides a general scaffold for safe mathematical reasoning, discuss ablations, and release code and data to encourage reproducibility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "95",
        "title": "Scaling Laws for Deepfake Detection",
        "author": [
            "Wenhao Wang",
            "Longqi Cai",
            "Taihong Xiao",
            "Yuxiao Wang",
            "Ming-Hsuan Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16320",
        "abstract": "This paper presents a systematic study of scaling laws for the deepfake detection task. Specifically, we analyze the model performance against the number of real image domains, deepfake generation methods, and training images. Since no existing dataset meets the scale requirements for this research, we construct ScaleDF, the largest dataset to date in this field, which contains over 5.8 million real images from 51 different datasets (domains) and more than 8.8 million fake images generated by 102 deepfake methods. Using ScaleDF, we observe power-law scaling similar to that shown in large language models (LLMs). Specifically, the average detection error follows a predictable power-law decay as either the number of real domains or the number of deepfake methods increases. This key observation not only allows us to forecast the number of additional real domains or deepfake methods required to reach a target performance, but also inspires us to counter the evolving deepfake technology in a data-centric manner. Beyond this, we examine the role of pre-training and data augmentations in deepfake detection under scaling, as well as the limitations of scaling itself.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "96",
        "title": "Scale-DiT: Ultra-High-Resolution Image Generation with Hierarchical Local Attention",
        "author": [
            "Yuyao Zhang",
            "Yu-Wing Tai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16325",
        "abstract": "Ultra-high-resolution text-to-image generation demands both fine-grained texture synthesis and globally coherent structure, yet current diffusion models remain constrained to sub-$1K \\times 1K$ resolutions due to the prohibitive quadratic complexity of attention and the scarcity of native $4K$ training data. We present \\textbf{Scale-DiT}, a new diffusion framework that introduces hierarchical local attention with low-resolution global guidance, enabling efficient, scalable, and semantically coherent image synthesis at ultra-high resolutions. Specifically, high-resolution latents are divided into fixed-size local windows to reduce attention complexity from quadratic to near-linear, while a low-resolution latent equipped with scaled positional anchors injects global semantics. A lightweight LoRA adaptation bridges global and local pathways during denoising, ensuring consistency across structure and detail. To maximize inference efficiency, we repermute token sequence in Hilbert curve order and implement a fused-kernel for skipping masked operations, resulting in a GPU-friendly design. Extensive experiments demonstrate that Scale-DiT achieves more than $2\\times$ faster inference and lower memory usage compared to dense attention baselines, while reliably scaling to $4K \\times 4K$ resolution without requiring additional high-resolution training data. On both quantitative benchmarks (FID, IS, CLIP Score) and qualitative comparisons, Scale-DiT delivers superior global coherence and sharper local detail, matching or outperforming state-of-the-art methods that rely on native 4K training. Taken together, these results highlight hierarchical local attention with guided low-resolution anchors as a promising and effective approach for advancing ultra-high-resolution image generation.",
        "tags": [
            "CLIP",
            "DiT",
            "Diffusion",
            "LoRA",
            "Text-to-Image"
        ]
    },
    {
        "id": "97",
        "title": "DiffusionX: Efficient Edge-Cloud Collaborative Image Generation with Multi-Round Prompt Evolution",
        "author": [
            "Yi Wei",
            "Shunpu Tang",
            "Liang Zhao",
            "Qiangian Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16326",
        "abstract": "Recent advances in diffusion models have driven remarkable progress in image generation. However, the generation process remains computationally intensive, and users often need to iteratively refine prompts to achieve the desired results, further increasing latency and placing a heavy burden on cloud resources. To address this challenge, we propose DiffusionX, a cloud-edge collaborative framework for efficient multi-round, prompt-based generation. In this system, a lightweight on-device diffusion model interacts with users by rapidly producing preview images, while a high-capacity cloud model performs final refinements after the prompt is finalized. We further introduce a noise level predictor that dynamically balances the computation load, optimizing the trade-off between latency and cloud workload. Experiments show that DiffusionX reduces average generation time by 15.8% compared with Stable Diffusion v1.5, while maintaining comparable image quality. Moreover, it is only 0.9% slower than Tiny-SD with significantly improved image quality, thereby demonstrating efficiency and scalability with minimal overhead.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "98",
        "title": "RL makes MLLMs see better than SFT",
        "author": [
            "Junha Song",
            "Sangdoo Yun",
            "Dongyoon Han",
            "Jaegul Choo",
            "Byeongho Heo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16333",
        "abstract": "A dominant assumption in Multimodal Language Model (MLLM) research is that its performance is largely inherited from the LLM backbone, given its immense parameter scale and remarkable capabilities. This has created a void in the understanding of the vision encoder, which determines how MLLMs perceive images. The recent shift in MLLM training paradigms, from Supervised Finetuning (SFT) to Reinforcement Learning (RL), magnifies this oversight-namely, the significant lack of analysis on how such training reshapes the vision encoder as well as the MLLM. To address this, we first investigate the impact of training strategies on MLLMs, where RL shows a clear advantage over SFT in strongly vision-related VQA benchmarks. Motivated by this, we conduct a critical yet under-explored analysis of the vision encoder of MLLMs through diverse and in-depth experiments, ranging from ImageNet classification and segmentation to gradient visualization. Our results demonstrate that MLLM's post-training strategy (i.e., SFT or RL) not only leads to distinct outcomes on MLLM downstream tasks, but also fundamentally reshapes MLLM's underlying visual representations. Specifically, the key finding of our study is that RL produces stronger and precisely localized visual representations compared to SFT, boosting the ability of the vision encoder for MLLM. We then reframe our findings into a simple recipe for building strong vision encoders for MLLMs, Preference-Instructed Vision OpTimization (PIVOT). When integrated into MLLMs, a PIVOT-trained vision encoder outperforms even larger and more heavily-trained counterparts, despite requiring less than 1% of the computational cost of standard vision pretraining. This result opens an effective and efficient path for advancing the vision backbones of MLLMs. Project page available at https://june-page.github.io/pivot/",
        "tags": [
            "LLM",
            "RL",
            "Segmentation"
        ]
    },
    {
        "id": "99",
        "title": "On the Provable Importance of Gradients for Language-Assisted Image Clustering",
        "author": [
            "Bo Peng",
            "Jie Lu",
            "Guangquan Zhang",
            "Zhen Fang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16335",
        "abstract": "This paper investigates the recently emerged problem of Language-assisted Image Clustering (LaIC), where textual semantics are leveraged to improve the discriminability of visual representations to facilitate image clustering. Due to the unavailability of true class names, one of core challenges of LaIC lies in how to filter positive nouns, i.e., those semantically close to the images of interest, from unlabeled wild corpus data. Existing filtering strategies are predominantly based on the off-the-shelf feature space learned by CLIP; however, despite being intuitive, these strategies lack a rigorous theoretical foundation. To fill this gap, we propose a novel gradient-based framework, termed as GradNorm, which is theoretically guaranteed and shows strong empirical performance. In particular, we measure the positiveness of each noun based on the magnitude of gradients back-propagated from the cross-entropy between the predicted target distribution and the softmax output. Theoretically, we provide a rigorous error bound to quantify the separability of positive nouns by GradNorm and prove that GradNorm naturally subsumes existing filtering strategies as extremely special cases of itself. Empirically, extensive experiments show that GradNorm achieves the state-of-the-art clustering performance on various benchmarks.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "100",
        "title": "Thinking About Thinking: Evaluating Reasoning in Post-Trained Language Models",
        "author": [
            "Pratham Singla",
            "Shivank Garg",
            "Ayush Singh",
            "Ishan Garg",
            "Ketan Suhaas Saichandran"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16340",
        "abstract": "Recent advances in post-training techniques have endowed Large Language Models (LLMs) with enhanced capabilities for tackling complex, logic-intensive tasks through the generation of supplementary planning tokens. This development raises a fundamental question: Are these models aware of what they \"learn\" and \"think\"? To address this, we define three core competencies: (1) awareness of learned latent policies, (2) generalization of these policies across domains, and (3) alignment between internal reasoning traces and final outputs. We empirically evaluate these abilities on several tasks, each designed to require learning a distinct policy. Furthermore, we contrast the profiles of models post-trained via Supervised Fine-Tuning (SFT), Direct Policy Optimization (DPO), and Group Relative Policy Optimization (GRPO). Our findings indicate that RL-trained models not only demonstrate greater awareness of their learned behaviors and stronger generalizability to novel, structurally similar tasks than SFT models but also often exhibit weak alignment between their reasoning traces and final outputs, an effect most pronounced in GRPO-trained models.",
        "tags": [
            "DPO",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "101",
        "title": "Beyond Fixed Anchors: Precisely Erasing Concepts with Sibling Exclusive Counterparts",
        "author": [
            "Tong Zhang",
            "Ru Zhang",
            "Jianyi Liu",
            "Zhen Yang",
            "Gongshen Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16342",
        "abstract": "Existing concept erasure methods for text-to-image diffusion models commonly rely on fixed anchor strategies, which often lead to critical issues such as concept re-emergence and erosion. To address this, we conduct causal tracing to reveal the inherent sensitivity of erasure to anchor selection and define Sibling Exclusive Concepts as a superior class of anchors. Based on this insight, we propose \\textbf{SELECT} (Sibling-Exclusive Evaluation for Contextual Targeting), a dynamic anchor selection framework designed to overcome the limitations of fixed anchors. Our framework introduces a novel two-stage evaluation mechanism that automatically discovers optimal anchors for precise erasure while identifying critical boundary anchors to preserve related concepts. Extensive evaluations demonstrate that SELECT, as a universal anchor solution, not only efficiently adapts to multiple erasure frameworks but also consistently outperforms existing baselines across key performance metrics, averaging only 4 seconds for anchor mining of a single concept.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "102",
        "title": "Manual2Skill++: Connector-Aware General Robotic Assembly from Instruction Manuals via Vision-Language Models",
        "author": [
            "Chenrui Tie",
            "Shengxiang Sun",
            "Yudi Lin",
            "Yanbo Wang",
            "Zhongrui Li",
            "Zhouhan Zhong",
            "Jinxuan Zhu",
            "Yiman Pang",
            "Haonan Chen",
            "Junting Chen",
            "Ruihai Wu",
            "Lin Shao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16344",
        "abstract": "Assembly hinges on reliably forming connections between parts; yet most robotic approaches plan assembly sequences and part poses while treating connectors as an afterthought. Connections represent the critical \"last mile\" of assembly execution, while task planning may sequence operations and motion plan may position parts, the precise establishment of physical connections ultimately determines assembly success or failure. In this paper, we consider connections as first-class primitives in assembly representation, including connector types, specifications, quantities, and placement locations. Drawing inspiration from how humans learn assembly tasks through step-by-step instruction manuals, we present Manual2Skill++, a vision-language framework that automatically extracts structured connection information from assembly manuals. We encode assembly tasks as hierarchical graphs where nodes represent parts and sub-assemblies, and edges explicitly model connection relationships between components. A large-scale vision-language model parses symbolic diagrams and annotations in manuals to instantiate these graphs, leveraging the rich connection knowledge embedded in human-designed instructions. We curate a dataset containing over 20 assembly tasks with diverse connector types to validate our representation extraction approach, and evaluate the complete task understanding-to-execution pipeline across four complex assembly scenarios in simulation, spanning furniture, toys, and manufacturing components with real-world correspondence.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "103",
        "title": "Sparse Transformer Architectures via Regularized Wasserstein Proximal Operator with $L_1$ Prior",
        "author": [
            "Fuqun Han",
            "Stanley Osher",
            "Wuchen Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16356",
        "abstract": "In this work, we propose a sparse transformer architecture that incorporates prior information about the underlying data distribution directly into the transformer structure of the neural network. The design of the model is motivated by a special optimal transport problem, namely the regularized Wasserstein proximal operator, which admits a closed-form solution and turns out to be a special representation of transformer architectures. Compared with classical flow-based models, the proposed approach improves the convexity properties of the optimization problem and promotes sparsity in the generated samples. Through both theoretical analysis and numerical experiments, including applications in generative modeling and Bayesian inverse problems, we demonstrate that the sparse transformer achieves higher accuracy and faster convergence to the target distribution than classical neural ODE-based methods.",
        "tags": [
            "ODE",
            "Transformer"
        ]
    },
    {
        "id": "104",
        "title": "Utilising Large Language Models for Generating Effective Counter Arguments to Anti-Vaccine Tweets",
        "author": [
            "Utsav Dhanuka",
            "Soham Poddar",
            "Saptarshi Ghosh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16359",
        "abstract": "In an era where public health is increasingly influenced by information shared on social media, combatting vaccine skepticism and misinformation has become a critical societal goal. Misleading narratives around vaccination have spread widely, creating barriers to achieving high immunisation rates and undermining trust in health recommendations. While efforts to detect misinformation have made significant progress, the generation of real time counter-arguments tailored to debunk such claims remains an insufficiently explored area. In this work, we explore the capabilities of LLMs to generate sound counter-argument rebuttals to vaccine misinformation. Building on prior research in misinformation debunking, we experiment with various prompting strategies and fine-tuning approaches to optimise counter-argument generation. Additionally, we train classifiers to categorise anti-vaccine tweets into multi-labeled categories such as concerns about vaccine efficacy, side effects, and political influences allowing for more context aware rebuttals. Our evaluation, conducted through human judgment, LLM based assessments, and automatic metrics, reveals strong alignment across these methods. Our findings demonstrate that integrating label descriptions and structured fine-tuning enhances counter-argument effectiveness, offering a promising approach for mitigating vaccine misinformation at scale.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "105",
        "title": "Integrating LLM and Diffusion-Based Agents for Social Simulation",
        "author": [
            "Xinyi Li",
            "Zhiqiang Guo",
            "Qinglang Guo",
            "Hao Jin",
            "Weizhi Ma",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16366",
        "abstract": "Agent-based social simulation provides a valuable methodology for predicting social information diffusion, yet existing approaches face two primary limitations. Traditional agent models often rely on rigid behavioral rules and lack semantic understanding of textual content, while emerging large language model (LLM)-based agents incur prohibitive computational costs at scale. To address these challenges, we propose a hybrid simulation framework that strategically integrates LLM-driven agents with diffusion model-based agents. The framework employs LLM-based agents to simulate a core subset of users with rich semantic reasoning, while a diffusion model handles the remaining population efficiently. Although the two agent types operate on disjoint user groups, both incorporate key factors including user personalization, social influence, and content awareness, and interact through a coordinated simulation process. Extensive experiments on three real-world datasets demonstrate that our framework outperforms existing methods in prediction accuracy, validating the effectiveness of its modular design.",
        "tags": [
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "106",
        "title": "Before you <think>, monitor: Implementing Flavell's metacognitive framework in LLMs",
        "author": [
            "Nick Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16374",
        "abstract": "Current approaches to enhancing LLM reasoning follows two isolated paradigms: Monitor-Generate methods like Plan-and-Solve (Wang et al., 2023) and SELF-DISCOVER (Zhou et al., 2024) excel at strategic planning but lack mechanisms to verify whether selected strategies succeed; while Generate-Verify approaches like Self-Verification (Weng et al., 2022) and SELF-REFINE (Madaan et al., 2023) iteratively refine outputs but commence generation blindly without task assessment. This separation creates inefficiencies -- strategies fail without feedback, and refinement occurs without strategic grounding. We address this gap by implementing Flavell's cognitive monitoring model (1979) from the broader Monitor-Generate-Verify framework (Oh and Gobet, 2025), operationalising it as a three-phase iterative system. On GSM8K, preliminary results show 75.42% accuracy versus 68.44% for SELF-REFINE and 67.07% for Self-Verification, while requiring fewer attempts (1.3 vs 2.0) at 27-37% increased inference cost. These initial findings suggest upfront monitoring produces higher-quality initial solutions that reduce refinement needs, though evaluation beyond arithmetic reasoning is needed to establish generalisability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "107",
        "title": "ATA: A Neuro-Symbolic Approach to Implement Autonomous and Trustworthy Agents",
        "author": [
            "David Peer",
            "Sebastian Stabinger"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16381",
        "abstract": "Large Language Models (LLMs) have demonstrated impressive capabilities, yet their deployment in high-stakes domains is hindered by inherent limitations in trustworthiness, including hallucinations, instability, and a lack of transparency. To address these challenges, we introduce a generic neuro-symbolic approach, which we call Autonomous Trustworthy Agents (ATA). The core of our approach lies in decoupling tasks into two distinct phases: Offline knowledge ingestion and online task processing. During knowledge ingestion, an LLM translates an informal problem specification into a formal, symbolic knowledge base. This formal representation is crucial as it can be verified and refined by human experts, ensuring its correctness and alignment with domain requirements. In the subsequent task processing phase, each incoming input is encoded into the same formal language. A symbolic decision engine then utilizes this encoded input in conjunction with the formal knowledge base to derive a reliable result. Through an extensive evaluation on a complex reasoning task, we demonstrate that a concrete implementation of ATA is competitive with state-of-the-art end-to-end reasoning models in a fully automated setup while maintaining trustworthiness. Crucially, with a human-verified and corrected knowledge base, our approach significantly outperforms even larger models, while exhibiting perfect determinism, enhanced stability against input perturbations, and inherent immunity to prompt injection attacks. By generating decisions grounded in symbolic reasoning, ATA offers a practical and controllable architecture for building the next generation of transparent, auditable, and reliable autonomous agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "108",
        "title": "SemOpt: LLM-Driven Code Optimization via Rule-Based Analysis",
        "author": [
            "Yuwei Zhao",
            "Yuan-An Xiao",
            "Qianyu Xiao",
            "Zhao Zhang",
            "Yingfei Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16384",
        "abstract": "Automated code optimization aims to improve performance in programs by refactoring code, and recent studies focus on utilizing LLMs for the optimization. Typical existing approaches mine optimization commits from open-source codebases to construct a large-scale knowledge base, then employ information retrieval techniques such as BM25 to retrieve relevant optimization examples for hotspot code locations, thereby guiding LLMs to optimize these hotspots. However, since semantically equivalent optimizations can manifest in syntactically dissimilar code snippets, current retrieval methods often fail to identify pertinent examples, leading to suboptimal optimization performance. This limitation significantly reduces the effectiveness of existing optimization approaches.\nTo address these limitations, we propose SemOpt, a novel framework that leverages static program analysis to precisely identify optimizable code segments, retrieve the corresponding optimization strategies, and generate the optimized results. SemOpt consists of three key components: (1) A strategy library builder that extracts and clusters optimization strategies from real-world code modifications. (2) A rule generator that generates Semgrep static analysis rules to capture the condition of applying the optimization strategy. (3) An optimizer that utilizes the strategy library to generate optimized code results. All the three components are powered by LLMs.\nOn our benchmark containing 151 optimization tasks, SemOpt demonstrates its effectiveness under different LLMs by increasing the number of successful optimizations by 1.38 to 28 times compared to the baseline. Moreover, on popular large-scale C/C++ projects, it can improve individual performance metrics by 5.04% to 218.07%, demonstrating its practical utility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "109",
        "title": "RGMem: Renormalization Group-based Memory Evolution for Language Agent User Profile",
        "author": [
            "Ao Tian",
            "Yunfeng Lu",
            "Xinxin Fan",
            "Changhao Wang",
            "Lanzhi Zhou",
            "Yeyao Zhang",
            "Yanfang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16392",
        "abstract": "Personalized and continuous interactions are the key to enhancing user experience in today's large language model (LLM)-based conversational systems, however, the finite context windows and static parametric memory make it difficult to model the cross-session long-term user states and behavioral consistency. Currently, the existing solutions to this predicament, such as retrieval-augmented generation (RAG) and explicit memory systems, primarily focus on fact-level storage and retrieval, lacking the capability to distill latent preferences and deep traits from the multi-turn dialogues, which limits the long-term and effective user modeling, directly leading to the personalized interactions remaining shallow, and hindering the cross-session continuity. To realize the long-term memory and behavioral consistency for Language Agents in LLM era, we propose a self-evolving memory framework RGMem, inspired by the ideology of classic renormalization group (RG) in physics, this framework enables to organize the dialogue history in multiple scales: it first extracts semantics and user insights from episodic fragments, then through hierarchical coarse-graining and rescaling operations, progressively forms a dynamically-evolved user profile. The core innovation of our work lies in modeling memory evolution as a multi-scale process of information compression and emergence, which accomplishes the high-level and accurate user profiles from noisy and microscopic-level interactions.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "110",
        "title": "Code Digital Twin: Empowering LLMs with Tacit Knowledge for Complex Software Development",
        "author": [
            "Xin Peng",
            "Chong Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16395",
        "abstract": "Recent advances in large language models (LLMs) have demonstrated strong capabilities in software engineering tasks, raising expectations of revolutionary productivity gains. However, enterprise software development is largely driven by incremental evolution, where challenges extend far beyond routine coding and depend critically on tacit knowledge, including design decisions at different levels and historical trade-offs. To achieve effective AI-powered support for complex software development, we should align emerging AI capabilities with the practical realities of enterprise development. To this end, we systematically identify challenges from both software and LLM perspectives. Alongside these challenges, we outline opportunities where AI and structured knowledge frameworks can enhance decision-making in tasks such as issue localization and impact analysis. To address these needs, we propose the Code Digital Twin, a living framework that models both the physical and conceptual layers of software, preserves tacit knowledge, and co-evolves with the codebase. By integrating hybrid knowledge representations, multi-stage extraction pipelines, incremental updates, LLM-empowered applications, and human-in-the-loop feedback, the Code Digital Twin transforms fragmented knowledge into explicit and actionable representations. Our vision positions it as a bridge between AI advancements and enterprise software realities, providing a concrete roadmap toward sustainable, intelligent, and resilient development and evolution of ultra-complex systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "111",
        "title": "Iterative solvers for partial differential equations with dissipative structure: Operator preconditioning and optimal control",
        "author": [
            "Volker Mehrmann",
            "Manuel Schaller",
            "Martin Stoll"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16399",
        "abstract": "This work considers the iterative solution of large-scale problems subject to non-symmetric matrices or operators arising in discretizations of (port-)Hamiltonian partial differential equations. We consider problems governed by an operator $\\mathcal{A}=\\mathcal{H}+\\mathcal{S}$ with symmetric part $\\mathcal{H}$ that is positive (semi-)definite and skew-symmetric part $\\mathcal{S}$. Prior work has shown that the structure and sparsity of the associated linear system enables Krylov subspace solvers such as the generalized minimal residual method (GMRES) or short recurrence variants such as Widlund's or Rapoport's method using the symmetric part $\\mathcal{H}$, or an approximation of it, as preconditioner. In this work, we analyze the resulting condition numbers, which are crucial for fast convergence of these methods, for various partial differential equations (PDEs) arising in diffusion phenomena, fluid dynamics, and elasticity. We show that preconditioning with the symmetric part leads to a condition number uniform in the mesh size in case of elliptic and parabolic PDEs where $\\mathcal{H}^{-1}\\mathcal{S}$ is a bounded operator. Further, we employ the tailored Krylov subspace methods in optimal control by means of a condensing approach and a constraint preconditioner for the optimality system. We illustrate the results by various large-scale numerical examples and discuss efficient evaluations of the preconditioner, such as incomplete Cholesky factorization or the algebraic multigrid method.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "112",
        "title": "Call-Center Staff Scheduling Considering Performance Evolution under Emotional Stress",
        "author": [
            "Yujun Zheng",
            "Xinya Chen",
            "Xueqin Lu",
            "Weiguo Sheng",
            "Shengyong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16406",
        "abstract": "Emotional stress often has a significant effect on the working performance of staff, but this effect is commonly neglected in existing staff scheduling methods. We study a call-center staff scheduling problem, which considers the evolution of work performance of staff under emotional stress. First, we present an emotional stress driven model that estimates the working performance of call-center employees based on not only skill levels but also emotional states. On the basis of the model, we formulate a combined short-term and long-term call-center staff scheduling problem aiming at maximizing the customer service level, which depends on the working performance of employees. We then propose a memetic optimization algorithm combining global mutation and neighborhood search assisted by deep reinforcement learning to efficiently solve this problem. Experimental results on real-world problem instances of bank call-center staff scheduling demonstrate the performance advantages of the proposed method over selected popular staff scheduling methods. By explicitly modeling and incorporating emotional stress, our method reflects a more realistic understanding and utilization of human behavior in staff scheduling.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "113",
        "title": "REALM: An MLLM-Agent Framework for Open World 3D Reasoning Segmentation and Editing on Gaussian Splatting",
        "author": [
            "Changyue Shi",
            "Minghao Chen",
            "Yiping Mao",
            "Chuxiao Yang",
            "Xinyuan Hu",
            "Jiajun Ding",
            "Zhou Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16410",
        "abstract": "Bridging the gap between complex human instructions and precise 3D object grounding remains a significant challenge in vision and robotics. Existing 3D segmentation methods often struggle to interpret ambiguous, reasoning-based instructions, while 2D vision-language models that excel at such reasoning lack intrinsic 3D spatial understanding. In this paper, we introduce REALM, an innovative MLLM-agent framework that enables open-world reasoning-based segmentation without requiring extensive 3D-specific post-training. We perform segmentation directly on 3D Gaussian Splatting representations, capitalizing on their ability to render photorealistic novel views that are highly suitable for MLLM comprehension. As directly feeding one or more rendered views to the MLLM can lead to high sensitivity to viewpoint selection, we propose a novel Global-to-Local Spatial Grounding strategy. Specifically, multiple global views are first fed into the MLLM agent in parallel for coarse-level localization, aggregating responses to robustly identify the target object. Then, several close-up novel views of the object are synthesized to perform fine-grained local segmentation, yielding accurate and consistent 3D masks. Extensive experiments show that REALM achieves remarkable performance in interpreting both explicit and implicit instructions across LERF, 3D-OVS, and our newly introduced REALM3D benchmarks. Furthermore, our agent framework seamlessly supports a range of 3D interaction tasks, including object removal, replacement, and style transfer, demonstrating its practical utility and versatility. Project page: https://ChangyueShi.github.io/REALM.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Robotics",
            "Segmentation",
            "Style Transfer",
            "VLM"
        ]
    },
    {
        "id": "114",
        "title": "Modeling Expert Interactions in Sparse Mixture of Experts via Graph Structures",
        "author": [
            "Minh-Khoi Nguyen-Nhat",
            "Rachel S.Y. Teo",
            "Laziz Abdullaev",
            "Maurice Mok",
            "Viet-Hoang Tran",
            "Tan Minh Nguyen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16411",
        "abstract": "Sparse Mixture of Experts (SMoE) has emerged as a promising solution to achieving unparalleled scalability in deep learning by decoupling model parameter count from computational cost. By activating only a small subset of parameters per sample, SMoE enables significant growth in model capacity while maintaining efficiency. However, SMoE struggles to adapt to distributional shifts, leading to reduced robustness under data contamination. In this work, we introduce SymphonySMoE, a novel family of SMoE that introduces a social graph to model interactions among experts. This graph-based structure enhances the token routing process, addressing the robustness challenges that are inherent in conventional SMoE designs. SymphonySMoE is lightweight, modular, and integrates seamlessly with existing SMoE-based models such as the XMoE and the Generalist Language Model. We provide both theoretical analysis and empirical evidence demonstrating SymphonySMoE's advantages over baseline SMoE. Extensive experiments on language modeling and visual instruction tuning validate our method's effectiveness. We further highlight the scalability of SymphonySMoE to models with 4.2 and 7.4 billion parameters, showcasing its applicability in fine-tuning tasks for large-scale systems.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "115",
        "title": "AoI-Aware Task Offloading and Transmission Optimization for Industrial IoT Networks: A Branching Deep Reinforcement Learning Approach",
        "author": [
            "Yuang Chen",
            "Fengqian Guo",
            "Chang Wu",
            "Shuyi Liu",
            "Hancheng Lu",
            "Chang Wen Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16414",
        "abstract": "In the Industrial Internet of Things (IIoT), the frequent transmission of large amounts of data over wireless networks should meet the stringent timeliness requirements. Particularly, the freshness of packet status updates has a significant impact on the system performance. In this paper, we propose an age-of-information (AoI)-aware multi-base station (BS) real-time monitoring framework to support extensive IIoT deployments. To meet the freshness requirements of IIoT, we formulate a joint task offloading and resource allocation optimization problem with the goal of minimizing long-term average AoI. Tackling the core challenges of combinatorial explosion in multi-BS decision spaces and the stochastic dynamics of IIoT systems is crucial, as these factors render traditional optimization methods intractable. Firstly, an innovative branching-based Dueling Double Deep Q-Network (Branching-D3QN) algorithm is proposed to effectively implement task offloading, which optimizes the convergence performance by reducing the action space complexity from exponential to linear levels. Then, an efficient optimization solution to resource allocation is proposed by proving the semi-definite property of the Hessian matrix of bandwidth and computation resources. Finally, we propose an iterative optimization algorithm for efficient joint task offloading and resource allocation to achieve optimal average AoI performance. Extensive simulations demonstrate that our proposed Branching-D3QN algorithm outperforms both state-of-the-art DRL methods and classical heuristics, achieving up to a 75% enhanced convergence speed and at least a 22% reduction in the long-term average AoI.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "116",
        "title": "MeCeFO: Enhancing LLM Training Robustness via Fault-Tolerant Optimization",
        "author": [
            "Rizhen Hu",
            "Yutong He",
            "Ran Yan",
            "Mou Sun",
            "Binghang Yuan",
            "Kun Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16415",
        "abstract": "As distributed optimization scales to meet the demands of Large Language Model (LLM) training, hardware failures become increasingly non-negligible. Existing fault-tolerant training methods often introduce significant computational or memory overhead, demanding additional resources. To address this challenge, we propose Memory- and Computation-efficient Fault-tolerant Optimization (MeCeFO), a novel algorithm that ensures robust training with minimal overhead. When a computing node fails, MeCeFO seamlessly transfers its training task to a neighboring node while employing memory- and computation-efficient algorithmic optimizations to minimize the extra workload imposed on the neighboring node handling both tasks. MeCeFO leverages three key algorithmic designs: (i) Skip-connection, which drops the multi-head attention (MHA) module during backpropagation for memory- and computation-efficient approximation; (ii) Recomputation, which reduces activation memory in feedforward networks (FFNs); and (iii) Low-rank gradient approximation, enabling efficient estimation of FFN weight matrix gradients. Theoretically, MeCeFO matches the convergence rate of conventional distributed training, with a rate of $\\mathcal{O}(1/\\sqrt{nT})$, where n is the data parallelism size and T is the number of iterations. Empirically, MeCeFO maintains robust performance under high failure rates, incurring only a 4.18% drop in throughput, demonstrating 5.0$\\times$ to 6.7$\\times$ greater resilience than previous SOTA approaches. Codes are available at https://github.com/pkumelon/MeCeFO.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "117",
        "title": "SSL4RL: Revisiting Self-supervised Learning as Intrinsic Reward for Visual-Language Reasoning",
        "author": [
            "Xiaojun Guo",
            "Runyu Zhou",
            "Yifei Wang",
            "Qi Zhang",
            "Chenheng Zhang",
            "Stefanie Jegelka",
            "Xiaohan Wang",
            "Jiajun Chai",
            "Guojun Yin",
            "Wei Lin",
            "Yisen Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16416",
        "abstract": "Vision-language models (VLMs) have shown remarkable abilities by integrating large language models with visual inputs. However, they often fail to utilize visual evidence adequately, either depending on linguistic priors in vision-centric tasks or resorting to textual shortcuts during reasoning. Although reinforcement learning (RL) can align models with desired behaviors, its application to VLMs has been hindered by the lack of scalable and reliable reward mechanisms. To overcome this challenge, we propose SSL4RL, a novel framework that leverages self-supervised learning (SSL) tasks as a source of verifiable rewards for RL-based fine-tuning. Our approach reformulates SSL objectives-such as predicting image rotation or reconstructing masked patches-into dense, automatic reward signals, eliminating the need for human preference data or unreliable AI evaluators. Experiments show that SSL4RL substantially improves performance on both vision-centric and vision-language reasoning benchmarks. Furthermore, through systematic ablations, we identify key factors-such as task difficulty, model scale, and semantic alignment with the target domain-that influence the effectiveness of SSL4RL tasks, offering new design principles for future work. We also demonstrate the framework's generality by applying it to graph learning, where it yields significant gains. SSL4RL establishes a versatile and effective paradigm for aligning multimodal models using verifiable, self-supervised objectives.",
        "tags": [
            "LLM",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "118",
        "title": "FourierCompress: Layer-Aware Spectral Activation Compression for Efficient and Accurate Collaborative LLM Inference",
        "author": [
            "Jian Ma",
            "Xinchen Lyu",
            "Jun Jiang",
            "Longhao Zou",
            "Chenshan Ren",
            "Qimei Cui",
            "Xiaofeng Tao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16418",
        "abstract": "Collaborative large language model (LLM) inference enables real-time, privacy-preserving AI services on resource-constrained edge devices by partitioning computational workloads between client devices and edge servers. However, this paradigm is severely hindered by communication bottlenecks caused by the transmission of high-dimensional intermediate activations, exacerbated by the autoregressive decoding structure of LLMs, where bandwidth consumption scales linearly with output length. Existing activation compression methods struggle to simultaneously achieve high compression ratios, low reconstruction error, and computational efficiency. This paper proposes FourierCompress, a novel, layer-aware activation compression framework that exploits the frequency-domain sparsity of LLM activations. We rigorously demonstrate that activations from the first Transformer layer exhibit strong smoothness and energy concentration in the low-frequency domain, making them highly amenable to near-lossless compression via the Fast Fourier Transform (FFT). FourierCompress transforms activations into the frequency domain, retains only a compact block of low-frequency coefficients, and reconstructs the signal at the server using conjugate symmetry, enabling seamless hardware acceleration on DSPs and FPGAs. Extensive experiments on Llama 3 and Qwen2.5 models across 10 commonsense reasoning datasets demonstrate that FourierCompress preserves performance remarkably close to the uncompressed baseline, outperforming Top-k, QR, and SVD. FourierCompress bridges the gap between communication efficiency (an average 7.6x reduction in activation size), near-lossless inference (less than 0.3% average accuracy loss), and significantly faster compression (achieving over 32x reduction in compression time compared to Top-k via hardware acceleration) for edge-device LLM inference.",
        "tags": [
            "LLM",
            "LLaMA",
            "Transformer"
        ]
    },
    {
        "id": "119",
        "title": "Learning to Optimize Edge Robotics: A Fast Integrated Perception-Motion-Communication Approach",
        "author": [
            "Dan Guo",
            "Xibin Jin",
            "Shuai Wang",
            "Zhigang Wen",
            "Miaowen Wen",
            "Chengzhong Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16424",
        "abstract": "Edge robotics involves frequent exchanges of large-volume multi-modal data. Existing methods ignore the interdependency between robotic functionalities and communication conditions, leading to excessive communication overhead. This paper revolutionizes edge robotics systems through integrated perception, motion, and communication (IPMC). As such, robots can dynamically adapt their communication strategies (i.e., compression ratio, transmission frequency, transmit power) by leveraging the knowledge of robotic perception and motion dynamics, thus reducing the need for excessive sensor data uploads. Furthermore, by leveraging the learning to optimize (LTO) paradigm, an imitation learning neural network is designed and implemented, which reduces the computational complexity by over 10x compared to state-of-the art optimization solvers. Experiments demonstrate the superiority of the proposed IPMC and the real-time execution capability of LTO.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "120",
        "title": "Determining the space dependent coefficients in space-time fractional diffusion equations via Krylov preconditioning",
        "author": [
            "Asim Ilyas",
            "Muhammad Faisal Khan",
            "Rosita L. Sormani",
            "Giacomo Tento",
            "Stefano Serra-Capizzano"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16425",
        "abstract": "We consider a time-space fractional diffusion equation with a variable coefficient and investigate the inverse problem of reconstructing the source term, after regularizing the problem with the quasiboundary value method to mitigate the ill-posedness. The equation involves a Caputo fractional derivative in the space variable and a tempered fractional derivative in the time variable, both of order in (0, 1). A finite difference approximation leads to a two-by-two block linear system of large dimensions. We conduct a spectral analysis of the associated matrix sequences, employing tools from Generalized Locally Toeplitz (GLT) theory, and construct the preconditioner guided by the GLT analysis. Numerical experiments are reported and commented, followed by concluding remarks.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "121",
        "title": "What Questions Should Robots Be Able to Answer? A Dataset of User Questions for Explainable Robotics",
        "author": [
            "Lennart Wachowiak",
            "Andrew Coles",
            "Gerard Canal",
            "Oya Celiktutan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16435",
        "abstract": "With the growing use of large language models and conversational interfaces in human-robot interaction, robots' ability to answer user questions is more important than ever. We therefore introduce a dataset of 1,893 user questions for household robots, collected from 100 participants and organized into 12 categories and 70 subcategories. Most work in explainable robotics focuses on why-questions. In contrast, our dataset provides a wide variety of questions, from questions about simple execution details to questions about how the robot would act in hypothetical scenarios -- thus giving roboticists valuable insights into what questions their robot needs to be able to answer. To collect the dataset, we created 15 video stimuli and 7 text stimuli, depicting robots performing varied household tasks. We then asked participants on Prolific what questions they would want to ask the robot in each portrayed situation. In the final dataset, the most frequent categories are questions about task execution details (22.5%), the robot's capabilities (12.7%), and performance assessments (11.3%). Although questions about how robots would handle potentially difficult scenarios and ensure correct behavior are less frequent, users rank them as the most important for robots to be able to answer. Moreover, we find that users who identify as novices in robotics ask different questions than more experienced users. Novices are more likely to inquire about simple facts, such as what the robot did or the current state of the environment. As robots enter environments shared with humans and language becomes central to giving instructions and interaction, this dataset provides a valuable foundation for (i) identifying the information robots need to log and expose to conversational interfaces, (ii) benchmarking question-answering modules, and (iii) designing explanation strategies that align with user expectations.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "122",
        "title": "LightGlueStick: a Fast and Robust Glue for Joint Point-Line Matching",
        "author": [
            "Aidyn Ubingazhibov",
            "RÃ©mi Pautrat",
            "Iago SuÃ¡rez",
            "Shaohui Liu",
            "Marc Pollefeys",
            "Viktor Larsson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16438",
        "abstract": "Lines and points are complementary local features, whose combination has proven effective for applications such as SLAM and Structure-from-Motion. The backbone of these pipelines are the local feature matchers, establishing correspondences across images. Traditionally, point and line matching have been treated as independent tasks. Recently, GlueStick proposed a GNN-based network that simultaneously operates on points and lines to establish matches. While running a single joint matching reduced the overall computational complexity, the heavy architecture prevented real-time applications or deployment to edge devices.\nInspired by recent progress in point matching, we propose LightGlueStick, a lightweight matcher for points and line segments. The key novel component in our architecture is the Attentional Line Message Passing (ALMP), which explicitly exposes the connectivity of the lines to the network, allowing for efficient communication between nodes. In thorough experiments we show that LightGlueStick establishes a new state-of-the-art across different benchmarks. The code is available at https://github.com/aubingazhib/LightGlueStick.",
        "tags": [
            "SLAM"
        ]
    },
    {
        "id": "123",
        "title": "FrugalPrompt: Reducing Contextual Overhead in Large Language Models via Token Attribution",
        "author": [
            "Syed Rifat Raiyan",
            "Md Farhan Ishmam",
            "Abdullah Al Imran",
            "Mohammad Ali Moni"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16439",
        "abstract": "Large language models (LLMs) owe much of their stellar performance to expansive input contexts, yet such verbosity inflates monetary costs, carbon footprint, and inference-time latency. Much of this overhead manifests from the redundant low-utility tokens present in typical prompts, as only a fraction of tokens typically carries the majority of the semantic weight. We address this inefficiency by introducing FrugalPrompt, a novel prompt compression framework for LLMs, which retains only the most semantically significant tokens. Leveraging two state-of-the-art token attribution methods, GlobEnc and DecompX, we assign salience scores to every token in an input sequence, rank them to preserve the top-k% tokens in their original order, and obtain a sparse frugalized prompt. We evaluate the approach across four NLP tasks: Sentiment Analysis, Commonsense QA, Summarization, and Mathematical Reasoning, using a suite of frontier LLMs. For the first three tasks, a 20% prompt reduction incurs only a marginal loss in task performance, demonstrating that contemporary LLMs can reconstruct elided context from high-salience cues. In contrast, performance on mathematical reasoning deteriorates sharply, reflecting a stronger dependence on complete token continuity. Further analysis with bottom-k% and random-k% tokens reveals asymmetric performance patterns that may suggest potential task contamination effects, wherein models may resort to shallow memorized patterns from pretraining exposure for conventional NLP tasks. We posit that our work contributes to a more nuanced understanding of LLM behavior in performance-efficiency trade-offs, and delineate the boundary between tasks tolerant to contextual sparsity and those requiring exhaustive context. Our source code and models are available at: https://github.com/Starscream-11813/Frugal-ICL",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "124",
        "title": "EDVD-LLaMA: Explainable Deepfake Video Detection via Multimodal Large Language Model Reasoning",
        "author": [
            "Haoran Sun",
            "Chen Cai",
            "Huiping Zhuang",
            "Kong Aik Lee",
            "Lap-Pui Chau",
            "Yi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16442",
        "abstract": "The rapid development of deepfake video technology has not only facilitated artistic creation but also made it easier to spread misinformation. Traditional deepfake video detection (DVD) methods face issues such as a lack of transparency in their principles and insufficient generalization capabilities to cope with evolving forgery techniques. This highlights an urgent need for detectors that can identify forged content and provide verifiable reasoning explanations. This paper proposes the explainable deepfake video detection (EDVD) task and designs the EDVD-LLaMA multimodal, a large language model (MLLM) reasoning framework, which provides traceable reasoning processes alongside accurate detection results and trustworthy explanations. Our approach first incorporates a Spatio-Temporal Subtle Information Tokenization (ST-SIT) to extract and fuse global and local cross-frame deepfake features, providing rich spatio-temporal semantic information input for MLLM reasoning. Second, we construct a Fine-grained Multimodal Chain-of-Thought (Fg-MCoT) mechanism, which introduces facial feature data as hard constraints during the reasoning process to achieve pixel-level spatio-temporal video localization, suppress hallucinated outputs, and enhance the reliability of the chain of thought. In addition, we build an Explainable Reasoning FF++ benchmark dataset (ER-FF++set), leveraging structured data to annotate videos and ensure quality control, thereby supporting dual supervision for reasoning and detection. Extensive experiments demonstrate that EDVD-LLaMA achieves outstanding performance and robustness in terms of detection accuracy, explainability, and its ability to handle cross-forgery methods and cross-dataset scenarios. Compared to previous DVD methods, it provides a more explainable and superior solution. The source code and dataset will be publicly available.",
        "tags": [
            "CoT",
            "Detection",
            "LLaMA"
        ]
    },
    {
        "id": "125",
        "title": "RefAtomNet++: Advancing Referring Atomic Video Action Recognition using Semantic Retrieval based Multi-Trajectory Mamba",
        "author": [
            "Kunyu Peng",
            "Di Wen",
            "Jia Fu",
            "Jiamin Wu",
            "Kailun Yang",
            "Junwei Zheng",
            "Ruiping Liu",
            "Yufan Chen",
            "Yuqian Fu",
            "Danda Pani Paudel",
            "Luc Van Gool",
            "Rainer Stiefelhagen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16444",
        "abstract": "Referring Atomic Video Action Recognition (RAVAR) aims to recognize fine-grained, atomic-level actions of a specific person of interest conditioned on natural language descriptions. Distinct from conventional action recognition and detection tasks, RAVAR emphasizes precise language-guided action understanding, which is particularly critical for interactive human action analysis in complex multi-person scenarios. In this work, we extend our previously introduced RefAVA dataset to RefAVA++, which comprises >2.9 million frames and >75.1k annotated persons in total. We benchmark this dataset using baselines from multiple related domains, including atomic action localization, video question answering, and text-video retrieval, as well as our earlier model, RefAtomNet. Although RefAtomNet surpasses other baselines by incorporating agent attention to highlight salient features, its ability to align and retrieve cross-modal information remains limited, leading to suboptimal performance in localizing the target person and predicting fine-grained actions. To overcome the aforementioned limitations, we introduce RefAtomNet++, a novel framework that advances cross-modal token aggregation through a multi-hierarchical semantic-aligned cross-attention mechanism combined with multi-trajectory Mamba modeling at the partial-keyword, scene-attribute, and holistic-sentence levels. In particular, scanning trajectories are constructed by dynamically selecting the nearest visual spatial tokens at each timestep for both partial-keyword and scene-attribute levels. Moreover, we design a multi-hierarchical semantic-aligned cross-attention strategy, enabling more effective aggregation of spatial and temporal tokens across different semantic hierarchies. Experiments show that RefAtomNet++ establishes new state-of-the-art results. The dataset and code are released at https://github.com/KPeng9510/refAVA2.",
        "tags": [
            "Detection",
            "Mamba"
        ]
    },
    {
        "id": "126",
        "title": "Input Domain Aware MoE: Decoupling Routing Decisions from Task Optimization in Mixture of Experts",
        "author": [
            "Yongxiang Hua",
            "Haoyu Cao",
            "Zhou Tao",
            "Bocheng Li",
            "Zihao Wu",
            "Chaohu Liu",
            "Linli Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16448",
        "abstract": "Sparse Mixture of Experts (sMoE) has become a pivotal approach for scaling large vision-language models, offering substantial capacity while maintaining computational efficiency through dynamic, sparse activation of experts. However, existing routing mechanisms, typically based on similarity scoring, struggle to effectively capture the underlying input structure. This limitation leads to a trade-off between expert specialization and balanced computation, hindering both scalability and performance. We propose Input Domain Aware MoE, a novel routing framework that leverages a probabilistic mixture model to better partition the input space. By modeling routing probabilities as a mixture of distributions, our method enables experts to develop clear specialization boundaries while achieving balanced utilization. Unlike conventional approaches, our routing mechanism is trained independently of task-specific objectives, allowing for stable optimization and decisive expert assignments. Empirical results on vision-language tasks demonstrate that our method consistently outperforms existing sMoE approaches, achieving higher task performance and improved expert utilization balance.",
        "tags": [
            "MoE",
            "VLM"
        ]
    },
    {
        "id": "127",
        "title": "TrajSelector: Harnessing Latent Representations for Efficient and Effective Best-of-N in Large Reasoning Model",
        "author": [
            "Bin Yu",
            "Xinming Wang",
            "Shijie Lian",
            "Haotian Li",
            "Changti Wu",
            "Ruina Hu",
            "Bailing Wang",
            "Yuliang Wei",
            "Kai Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16449",
        "abstract": "Large language models (LLMs) have shown remarkable progress in complex reasoning tasks, largely enabled by test-time scaling (TTS) paradigms that allocate additional compute during inference. Among these, external TTS (particularly the Best-of-N selection paradigm) yields scalable performance improvements by selecting from multiple independently generated reasoning trajectories. However, this approach faces key limitations: (i) the high computational overhead of deploying process reward models, (ii) the underutilization of the LLM's intrinsic latent representations. We introduce TrajSelector, an efficient and effective Best-of-N framework that exploit the hidden states in the sampler LLM for process-level scoring. A lightweight verifier (with only 0.6B parameters) evaluates the quality of step-wise trajectory, and then aggregates these scores to identify the optimal reasoning trajectory. Our framework employs a fully data-driven, end-to-end training recipe that eliminates reliance on massive step-level annotations. Experiential results across five benchmarks demonstrate that TrajSelector delivers consistent performance gains. In Best-of-32 settings, it surpasses majority voting by 4.61% accuracy and outperforms existing process reward models by 4.31% to 12.21%, all while maintaining lower inference costs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "128",
        "title": "RAVEN: Robust Advertisement Video Violation Temporal Grounding via Reinforcement Reasoning",
        "author": [
            "Deyi Ji",
            "Yuekui Yang",
            "Haiyang Wu",
            "Shaoping Ma",
            "Tianrun Chen",
            "Lanyun Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16455",
        "abstract": "Advertisement (Ad) video violation detection is critical for ensuring platform compliance, but existing methods struggle with precise temporal grounding, noisy annotations, and limited generalization. We propose RAVEN, a novel framework that integrates curriculum reinforcement learning with multimodal large language models (MLLMs) to enhance reasoning and cognitive capabilities for violation detection. RAVEN employs a progressive training strategy, combining precisely and coarsely annotated data, and leverages Group Relative Policy Optimization (GRPO) to develop emergent reasoning abilities without explicit reasoning annotations. Multiple hierarchical sophisticated reward mechanism ensures precise temporal grounding and consistent category prediction. Experiments on industrial datasets and public benchmarks show that RAVEN achieves superior performances in violation category accuracy and temporal interval localization. We also design a pipeline to deploy the RAVEN on the online Ad services, and online A/B testing further validates its practical applicability, with significant improvements in precision and recall. RAVEN also demonstrates strong generalization, mitigating the catastrophic forgetting issue associated with supervised fine-tuning.",
        "tags": [
            "Detection",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "129",
        "title": "Buzz, Choose, Forget: A Meta-Bandit Framework for Bee-Like Decision Making",
        "author": [
            "Emmanuelle Claeys",
            "Elena Kerjean",
            "Jean-Michel Loubes"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16462",
        "abstract": "We introduce a sequential reinforcement learning framework for imitation learning designed to model heterogeneous cognitive strategies in pollinators. Focusing on honeybees, our approach leverages trajectory similarity to capture and forecast behavior across individuals that rely on distinct strategies: some exploiting numerical cues, others drawing on memory, or being influenced by environmental factors such as weather. Through empirical evaluation, we show that state-of-the-art imitation learning methods often fail in this setting: when expert policies shift across memory windows or deviate from optimality, these models overlook both fast and slow learning behaviors and cannot faithfully reproduce key decision patterns. Moreover, they offer limited interpretability, hindering biological insight. Our contribution addresses these challenges by (i) introducing a model that minimizes predictive loss while identifying the effective memory horizon most consistent with behavioral data, and (ii) ensuring full interpretability to enable biologists to analyze underlying decision-making strategies and finally (iii) providing a mathematical framework linking bee policy search with bandit formulations under varying exploration-exploitation dynamics, and releasing a novel dataset of 80 tracked bees observed under diverse weather conditions. This benchmark facilitates research on pollinator cognition and supports ecological governance by improving simulations of insect behavior in agroecosystems. Our findings shed new light on the learning strategies and memory interplay shaping pollinator decision-making.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "130",
        "title": "HGC-Avatar: Hierarchical Gaussian Compression for Streamable Dynamic 3D Avatars",
        "author": [
            "Haocheng Tang",
            "Ruoke Yan",
            "Xinhui Yin",
            "Qi Zhang",
            "Xinfeng Zhang",
            "Siwei Ma",
            "Wen Gao",
            "Chuanmin Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16463",
        "abstract": "Recent advances in 3D Gaussian Splatting (3DGS) have enabled fast, photorealistic rendering of dynamic 3D scenes, showing strong potential in immersive communication. However, in digital human encoding and transmission, the compression methods based on general 3DGS representations are limited by the lack of human priors, resulting in suboptimal bitrate efficiency and reconstruction quality at the decoder side, which hinders their application in streamable 3D avatar systems. We propose HGC-Avatar, a novel Hierarchical Gaussian Compression framework designed for efficient transmission and high-quality rendering of dynamic avatars. Our method disentangles the Gaussian representation into a structural layer, which maps poses to Gaussians via a StyleUNet-based generator, and a motion layer, which leverages the SMPL-X model to represent temporal pose variations compactly and semantically. This hierarchical design supports layer-wise compression, progressive decoding, and controllable rendering from diverse pose inputs such as video sequences or text. Since people are most concerned with facial realism, we incorporate a facial attention mechanism during StyleUNet training to preserve identity and expression details under low-bitrate constraints. Experimental results demonstrate that HGC-Avatar provides a streamable solution for rapid 3D avatar rendering, while significantly outperforming prior methods in both visual quality and compression efficiency.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "131",
        "title": "ReviewSense: Transforming Customer Review Dynamics into Actionable Business Insights",
        "author": [
            "Siddhartha Krothapalli",
            "Tridib Kumar Das",
            "Praveen Kumar",
            "Naveen Suravarpu",
            "Pratik Narang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16466",
        "abstract": "As customer feedback becomes increasingly central to strategic growth, the ability to derive actionable insights from unstructured reviews is essential. While traditional AI-driven systems excel at predicting user preferences, far less work has focused on transforming customer reviews into prescriptive, business-facing recommendations. This paper introduces ReviewSense, a novel prescriptive decision support framework that leverages advanced large language models (LLMs) to transform customer reviews into targeted, actionable business recommendations. By identifying key trends, recurring issues, and specific concerns within customer sentiments, ReviewSense extends beyond preference-based systems to provide businesses with deeper insights for sustaining growth and enhancing customer loyalty. The novelty of this work lies in integrating clustering, LLM adaptation, and expert-driven evaluation into a unified, business-facing pipeline. Preliminary manual evaluations indicate strong alignment between the model's recommendations and business objectives, highlighting its potential for driving data-informed decision-making. This framework offers a new perspective on AI-driven sentiment analysis, demonstrating its value in refining business strategies and maximizing the impact of customer feedback.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "132",
        "title": "Declarative Techniques for NL Queries over Heterogeneous Data",
        "author": [
            "Elham Khabiri",
            "Jeffrey O. Kephart",
            "Fenno F. Heath III",
            "Srideepika Jayaraman",
            "Fateh A. Tipu",
            "Yingjie Li",
            "Dhruv Shah",
            "Achille Fokoue",
            "Anu Bhamidipaty"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16470",
        "abstract": "In many industrial settings, users wish to ask questions in natural language, the answers to which require assembling information from diverse structured data sources. With the advent of Large Language Models (LLMs), applications can now translate natural language questions into a set of API calls or database calls, execute them, and combine the results into an appropriate natural language response. However, these applications remain impractical in realistic industrial settings because they do not cope with the data source heterogeneity that typifies such environments. In this work, we simulate the heterogeneity of real industry settings by introducing two extensions of the popular Spider benchmark dataset that require a combination of database and API calls. Then, we introduce a declarative approach to handling such data heterogeneity and demonstrate that it copes with data source heterogeneity significantly better than state-of-the-art LLM-based agentic or imperative code generation systems. Our augmented benchmarks are available to the research community.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "133",
        "title": "NP-Engine: Empowering Optimization Reasoning in Large Language Models with Verifiable Synthetic NP Problems",
        "author": [
            "Xiaozhe Li",
            "Xinyu Fang",
            "Shengyuan Ding",
            "Linyang Li",
            "Haodong Duan",
            "Qingwen Liu",
            "Kai Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16476",
        "abstract": "Large Language Models (LLMs) have shown strong reasoning capabilities, with models like OpenAI's O-series and DeepSeek R1 excelling at tasks such as mathematics, coding, logic, and puzzles through Reinforcement Learning with Verifiable Rewards (RLVR). However, their ability to solve more complex optimization problems - particularly NP-hard tasks - remains underexplored. To bridge this gap, we propose NP-ENGINE, the first comprehensive framework for training and evaluating LLMs on NP-hard problems. NP-ENGINE covers 10 tasks across five domains, each equipped with (i) a controllable instance generator, (ii) a rule-based verifier, and (iii) a heuristic solver that provides approximate optimal solutions as ground truth. This generator-verifier-heuristic pipeline enables scalable and verifiable RLVR training under hierarchical difficulties. We also introduce NP-BENCH, a benchmark derived from NP-ENGINE-DATA, specifically designed to evaluate LLMs' ability to tackle NP-hard level reasoning problems, focusing not only on feasibility but also on solution quality. Additionally, we present QWEN2.5-7B-NP, a model trained via zero-RLVR with curriculum learning on Qwen2.5-7B-Instruct, which significantly outperforms GPT-4o on NP-BENCH and achieves SOTA performance with the same model size. Beyond in-domain tasks, we demonstrate that RLVR training on NP-ENGINE-DATA enables strong out-of-domain (OOD) generalization to reasoning tasks (logic, puzzles, math, and knowledge), as well as non-reasoning tasks such as instruction following. We also observe a scaling trend: increasing task diversity improves OOD generalization. These findings suggest that task-rich RLVR training is a promising direction for advancing LLM's reasoning ability, revealing new insights into the scaling laws of RLVR.",
        "tags": [
            "DeepSeek",
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "134",
        "title": "Check Yourself Before You Wreck Yourself: Selectively Quitting Improves LLM Agent Safety",
        "author": [
            "Vamshi Krishna Bonagiri",
            "Ponnurangam Kumaragurum",
            "Khanh Nguyen",
            "Benjamin Plaut"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16492",
        "abstract": "As Large Language Model (LLM) agents increasingly operate in complex environments with real-world consequences, their safety becomes critical. While uncertainty quantification is well-studied for single-turn tasks, multi-turn agentic scenarios with real-world tool access present unique challenges where uncertainties and ambiguities compound, leading to severe or catastrophic risks beyond traditional text generation failures. We propose using \"quitting\" as a simple yet effective behavioral mechanism for LLM agents to recognize and withdraw from situations where they lack confidence. Leveraging the ToolEmu framework, we conduct a systematic evaluation of quitting behavior across 12 state-of-the-art LLMs. Our results demonstrate a highly favorable safety-helpfulness trade-off: agents prompted to quit with explicit instructions improve safety by an average of +0.39 on a 0-3 scale across all models (+0.64 for proprietary models), while maintaining a negligible average decrease of -0.03 in helpfulness. Our analysis demonstrates that simply adding explicit quit instructions proves to be a highly effective safety mechanism that can immediately be deployed in existing agent systems, and establishes quitting as an effective first-line defense mechanism for autonomous agents in high-stakes applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "135",
        "title": "High-order temporal parametric finite element methods for simulating solid-state dewetting",
        "author": [
            "Xiaowen Gan",
            "Yuqian Teng",
            "Sisheng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16493",
        "abstract": "We propose a class of temporally high-order parametric finite element methods for simulating solid-state dewetting of thin films in two dimensions using a sharp-interface model. The process is governed by surface diffusion and contact point migration, along with appropriate boundary conditions. By incorporating the predictor-corrector strategy and the backward differentiation formula for time discretization into the energy-stable parametric finite element method developed by Zhao et al. (2021), we successfully construct temporally high-order schemes. The resulting numerical scheme is semi-implicit, requiring the solution of a linear system at each time step. The well-posedness of the fully discretized system is established. Moreover, the method maintains the long-term mesh equidistribution property. Extensive numerical experiments demonstrate that our methods achieve the desired temporal accuracy, measured by the manifold distance, while maintaining good mesh quality throughout the evolution.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "136",
        "title": "On the Use of Large Language Models for Qualitative Synthesis",
        "author": [
            "SebastiÃ¡n Pizard",
            "Ramiro Moreira",
            "Federico Galiano",
            "Ignacio Sastre",
            "Lorena Etcheverry"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16502",
        "abstract": "Large language models (LLMs) show promise for supporting systematic reviews (SR), even complex tasks such as qualitative synthesis (QS). However, applying them to a stage that is unevenly reported and variably conducted carries important risks: misuse can amplify existing weaknesses and erode confidence in the SR findings. To examine the challenges of using LLMs for QS, we conducted a collaborative autoethnography involving two trials. We evaluated each trial for methodological rigor and practical usefulness, and interpreted the results through a technical lens informed by how LLMs are built and their current limitations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "137",
        "title": "PRISMM-Bench: A Benchmark of Peer-Review Grounded Multimodal Inconsistencies",
        "author": [
            "Lukas Selch",
            "Yufang Hou",
            "M. Jehanzeb Mirza",
            "Sivan Doveh",
            "James Glass",
            "Rogerio Feris",
            "Wei Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16505",
        "abstract": "Large Multimodal Models (LMMs) are increasingly applied to scientific research, yet it remains unclear whether they can reliably understand and reason over the multimodal complexity of papers. A central challenge lies in detecting and resolving inconsistencies across text, figures, tables, and equations, issues that are often subtle, domain-specific, and ultimately undermine clarity, reproducibility, and trust. Existing benchmarks overlook this issue, either isolating single modalities or relying on synthetic errors that fail to capture real-world complexity. We introduce PRISMM-Bench (Peer-Review-sourced Inconsistency Set for Multimodal Models), the first benchmark grounded in real reviewer-flagged inconsistencies in scientific papers. Through a multi-stage pipeline of review mining, LLM-assisted filtering and human verification, we curate 262 inconsistencies from 242 papers. Based on this set, we design three tasks, namely inconsistency identification, remedy and pair matching, which assess a model's capacity to detect, correct, and reason over inconsistencies across different modalities. Furthermore, to address the notorious problem of choice-only shortcuts in multiple-choice evaluation, where models exploit answer patterns without truly understanding the question, we further introduce structured JSON-based answer representations that minimize linguistic biases by reducing reliance on superficial stylistic cues. We benchmark 21 leading LMMs, including large open-weight models (GLM-4.5V 106B, InternVL3 78B) and proprietary models (Gemini 2.5 Pro, GPT-5 with high reasoning). Results reveal strikingly low performance (26.1-54.2%), underscoring the challenge of multimodal scientific reasoning and motivating progress towards trustworthy scientific assistants.",
        "tags": [
            "GLM",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "138",
        "title": "DIV-Nav: Open-Vocabulary Spatial Relationships for Multi-Object Navigation",
        "author": [
            "JesÃºs Ortega-Peimbert",
            "Finn Lukas Busch",
            "Timon Homberger",
            "Quantao Yang",
            "Olov Andersson"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16518",
        "abstract": "Advances in open-vocabulary semantic mapping and object navigation have enabled robots to perform an informed search of their environment for an arbitrary object. However, such zero-shot object navigation is typically designed for simple queries with an object name like \"television\" or \"blue rug\". Here, we consider more complex free-text queries with spatial relationships, such as \"find the remote on the table\" while still leveraging robustness of a semantic map. We present DIV-Nav, a real-time navigation system that efficiently addresses this problem through a series of relaxations: i) Decomposing natural language instructions with complex spatial constraints into simpler object-level queries on a semantic map, ii) computing the Intersection of individual semantic belief maps to identify regions where all objects co-exist, and iii) Validating the discovered objects against the original, complex spatial constrains via a LVLM. We further investigate how to adapt the frontier exploration objectives of online semantic mapping to such spatial search queries to more effectively guide the search process. We validate our system through extensive experiments on the MultiON benchmark and real-world deployment on a Boston Dynamics Spot robot using a Jetson Orin AGX. More details and videos are available at https://anonsub42.github.io/reponame/",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "139",
        "title": "Semi-Peaucellier Linkage and Differential Mechanism for Linear Pinching and Self-Adaptive Grasping",
        "author": [
            "Haokai Ding",
            "Zhaohan Chen",
            "Tao Yang",
            "Wenzeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16524",
        "abstract": "This paper presents the SP-Diff parallel gripper system, addressing the limited adaptability of conventional end-effectors in intelligent industrial automation. The proposed design employs an innovative differential linkage mechanism with a modular symmetric dual-finger configuration to achieve linear-parallel grasping. By integrating a planetary gear transmission, the system enables synchronized linear motion and independent finger pose adjustment while maintaining structural rigidity, reducing Z-axis recalibration requirements by 30% compared to arc-trajectory grippers. The compact palm architecture incorporates a kinematically optimized parallelogram linkage and Differential mechanism, demonstrating adaptive grasping capabilities for diverse industrial workpieces and deformable objects such as citrus fruits. Future-ready interfaces are embedded for potential force/vision sensor integration to facilitate multimodal data acquisition (e.g., trajectory planning and object deformation) in digital twin frameworks. Designed as a flexible manufacturing solution, SP-Diff advances robotic end-effector intelligence through its adaptive architecture, showing promising applications in collaborative robotics, logistics automation, and specialized operational scenarios.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "140",
        "title": "Realizing LLMs' Causal Potential Requires Science-Grounded, Novel Benchmarks",
        "author": [
            "Ashutosh Srivastava",
            "Lokesh Nagalapatti",
            "Gautam Jajoo",
            "Aniket Vashishtha",
            "Parameswari Krishnamurthy",
            "Amit Sharma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16530",
        "abstract": "Recent claims of strong performance by Large Language Models (LLMs) on causal discovery are undermined by a key flaw: many evaluations rely on benchmarks likely included in pretraining corpora. Thus, apparent success suggests that LLM-only methods, which ignore observational data, outperform classical statistical approaches. We challenge this narrative by asking: Do LLMs truly reason about causal structure, and how can we measure it without memorization concerns? Can they be trusted for real-world scientific discovery? We argue that realizing LLMs' potential for causal analysis requires two shifts: (P.1) developing robust evaluation protocols based on recent scientific studies to guard against dataset leakage, and (P.2) designing hybrid methods that combine LLM-derived knowledge with data-driven statistics. To address P.1, we encourage evaluating discovery methods on novel, real-world scientific studies. We outline a practical recipe for extracting causal graphs from recent publications released after an LLM's training cutoff, ensuring relevance and preventing memorization while capturing both established and novel relations. Compared to benchmarks like BNLearn, where LLMs achieve near-perfect accuracy, they perform far worse on our curated graphs, underscoring the need for statistical grounding. Supporting P.2, we show that using LLM predictions as priors for the classical PC algorithm significantly improves accuracy over both LLM-only and purely statistical methods. We call on the community to adopt science-grounded, leakage-resistant benchmarks and invest in hybrid causal discovery methods suited to real-world inquiry.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "141",
        "title": "Hybrid CNN-Transformer Based Sparse Channel Prediction for High-Mobility OTFS Systems",
        "author": [
            "Zhaowei Guan",
            "Wenkun Wen",
            "Peiran Wu",
            "Chen Wang",
            "Minghua Xia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16539",
        "abstract": "High-mobility scenarios in next-generation wireless networks, such as those involving vehicular communications, require ultra-reliable and low-latency communications (URLLC). However, rapidly time-varying channels pose significant challenges to traditional OFDM-based systems due to the Doppler effect and channel aging. Orthogonal time frequency space (OTFS) modulation offers resilience by representing channels in the quasi-static delay-Doppler (DD) domain. This letter proposes a novel channel prediction framework for OTFS systems using a hybrid convolutional neural network and transformer (CNN-Transformer) architecture. The CNN extracts compact features that exploit the DD-domain sparsity of the channel matrices, while the transformer models temporal dependencies with causal masking for consistency. Simulation experiments under extreme $500$ \\si{km/h} mobility conditions demonstrate that the proposed method outperforms state-of-the-art baselines, reducing the root mean square error and mean absolute error by $12.2\\%$ and $9.4\\%$, respectively. These results demonstrate the effectiveness of DD-domain representations and the proposed model in accurately predicting channels in high-mobility scenarios, thereby supporting the stringent URLLC requirements in future wireless systems.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "142",
        "title": "Enhancing Compositional Reasoning in CLIP via Reconstruction and Alignment of Text Descriptions",
        "author": [
            "Jihoon Kwon",
            "Kyle Min",
            "Jy-yong Sohn"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16540",
        "abstract": "Despite recent advances, vision-language models trained with standard contrastive objectives still struggle with compositional reasoning -- the ability to understand structured relationships between visual and linguistic elements. This shortcoming is largely due to the tendency of the text encoder to focus on individual words rather than their relations, a limitation reinforced by contrastive training that primarily aligns words with visual objects. In this paper, we introduce REconstruction and Alignment of text Descriptions (READ), a fine-tuning method designed to enhance compositional reasoning by adding two auxiliary objectives to the contrastive learning: (1) a token-level reconstruction objective, where a frozen pre-trained decoder reconstructs alternative captions based on the embedding of the original caption; and (2) a sentence-level alignment objective, which explicitly aligns paraphrased sentences in the embedding space. We show that READ-CLIP, a model derived by applying the READ method to the pre-trained CLIP model, achieves the state-of-the-art performance across five major compositional reasoning benchmarks, outperforming the strongest conventional fine-tuning baseline by up to 4.1%. Furthermore, applying the READ to existing CLIP variants (including NegCLIP and FSC-CLIP) also improves performance on these benchmarks. Quantitative and qualitative analyses reveal that our proposed objectives -- reconstruction and alignment -- offer complementary benefits: the former encourages the encoder to capture relationships between words within a caption, while the latter ensures consistent representations for paraphrases expressed with different wording.",
        "tags": [
            "CLIP",
            "VLM"
        ]
    },
    {
        "id": "143",
        "title": "NeurIPT: Foundation Model for Neural Interfaces",
        "author": [
            "Zitao Fang",
            "Chenxuan Li",
            "Hongting Zhou",
            "Shuyang Yu",
            "Guodong Du",
            "Ashwaq Qasem",
            "Yang Lu",
            "Jing Li",
            "Junsong Zhang",
            "Sim Kuan Goh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16548",
        "abstract": "Electroencephalography (EEG) has wide-ranging applications, from clinical diagnosis to brain-computer interfaces (BCIs). With the increasing volume and variety of EEG data, there has been growing interest in establishing foundation models (FMs) to scale up and generalize neural decoding. Despite showing early potential, applying FMs to EEG remains challenging due to substantial inter-subject, inter-task, and inter-condition variability, as well as diverse electrode configurations across recording setups. To tackle these open challenges, we propose NeurIPT, a foundation model developed for diverse EEG-based Neural Interfaces with a Pre-trained Transformer by capturing both homogeneous and heterogeneous spatio-temporal characteristics inherent in EEG signals. Temporally, we introduce Amplitude-Aware Masked Pretraining (AAMP), masking based on signal amplitude rather than random intervals, to learn robust representations across varying signal intensities beyond local interpolation. Moreover, this temporal representation is enhanced by a Progressive Mixture-of-Experts (PMoE) architecture, where specialized expert subnetworks are progressively introduced at deeper layers, adapting effectively to the diverse temporal characteristics of EEG signals. Spatially, NeurIPT leverages the 3D physical coordinates of electrodes, enabling effective transfer of embedding across varying EEG settings, and develops Intra-Inter Lobe Pooling (IILP) during fine-tuning to efficiently exploit regional brain features. Empirical evaluations across eight downstream BCI datasets, via fine-tuning, demonstrated NeurIPT consistently achieved state-of-the-art performance, highlighting its broad applicability and robust generalization. Our work pushes forward the state of FMs in EEG and offers insights into scalable and generalizable neural information processing systems.",
        "tags": [
            "3D",
            "MoE",
            "Transformer"
        ]
    },
    {
        "id": "144",
        "title": "ReviewGuard: Enhancing Deficient Peer Review Detection via LLM-Driven Data Augmentation",
        "author": [
            "Haoxuan Zhang",
            "Ruochi Li",
            "Sarthak Shrestha",
            "Shree Harshini Mamidala",
            "Revanth Putta",
            "Arka Krishan Aggarwal",
            "Ting Xiao",
            "Junhua Ding",
            "Haihua Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16549",
        "abstract": "Peer review serves as the gatekeeper of science, yet the surge in submissions and widespread adoption of large language models (LLMs) in scholarly evaluation present unprecedented challenges. Recent work has focused on using LLMs to improve review efficiency or generate insightful review content. However, unchecked deficient reviews from both human experts and AI systems threaten to systematically undermine the peer review ecosystem and compromise academic integrity. To address this critical issue, we introduce ReviewGuard, an automated system for detecting and categorizing deficient reviews. ReviewGuard employs a comprehensive four-stage LLM-driven framework that: (1) collects ICLR and NeurIPS papers with their corresponding reviews from OpenReview; (2) annotates review types using GPT-4.1 with human validation; (3) addresses class imbalance and data scarcity through LLM-driven synthetic data augmentation, producing a final corpus of 6,634 papers, 24,657 real reviews, and 46,438 synthetic reviews; and (4) fine-tunes both encoder-based models and open source LLMs. We perform comprehensive feature analysis of the structure and quality of the review text. Compared to sufficient reviews, deficient reviews demonstrate lower rating scores, higher self-reported confidence, reduced structural complexity, and a higher proportion of negative sentiment. AI-generated text detection reveals that, since ChatGPT's emergence, AI-generated reviews have increased dramatically. In the evaluation of deficient review detection models, mixed training with synthetic and real review data provides substantial enhancements to recall and F1 scores on the binary task. This study presents the first LLM-driven system for detecting deficient peer reviews, providing evidence to inform AI governance in peer review while offering valuable insights into human-AI collaboration to maintain academic integrity.",
        "tags": [
            "Detection",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "145",
        "title": "LANPO: Bootstrapping Language and Numerical Feedback for Reinforcement Learning in LLMs",
        "author": [
            "Ang Li",
            "Yifei Wang",
            "Zhihang Yuan",
            "Stefanie Jegelka",
            "Yisen Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16552",
        "abstract": "Reinforcement learning in large language models (LLMs) often relies on scalar rewards, a practice that discards valuable textual rationale buried in the rollouts, forcing the model to explore \\textit{de novo} with each attempt and hindering sample efficiency. While LLMs can uniquely learn from language feedback provided in-context, naively integrating on-line experiences into RL training presents a paradox: feedback from the same problem risks information leakage and memorization, while feedback from different problems often leads to behavior collapse due to irrelevant context. To resolve this tension, we propose \\textbf{Language-And-Numerical Policy Optimization (LANPO)}, a framework that cleanly separates the roles of feedback: language guides exploration, while numerical rewards drive optimization. LANPO builds a dynamic experience pool from past trials and introduces two principles to ensure feedback is effective: \\emph{Reward-Agnostic Reflection} for safe intra-sample self-correction and \\emph{Relevant Abstraction} to distill generalizable lessons from inter-sample experiences. Across mathematical reasoning benchmarks, LANPO enables 7B and 14B models to significantly outperform strong baselines trained with GRPO in test accuracy. Our work provides a robust method for integrating historical experiences into the LLM RL loop, creating more effective and data-efficient learning agents.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "146",
        "title": "Urban-R1: Reinforced MLLMs Mitigate Geospatial Biases for Urban General Intelligence",
        "author": [
            "Qiongyan Wang",
            "Xingchen Zou",
            "Yutian Jiang",
            "Haomin Wen",
            "Jiaheng Wei",
            "Qingsong Wen",
            "Yuxuan Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16555",
        "abstract": "Rapid urbanization intensifies the demand for Urban General Intelligence (UGI), referring to AI systems that can understand and reason about complex urban environments. Recent studies have built urban foundation models using supervised fine-tuning (SFT) of LLMs and MLLMs, yet these models exhibit persistent geospatial bias, producing regionally skewed predictions and limited generalization. To this end, we propose Urban-R1, a reinforcement learning-based post-training framework that aligns MLLMs with the objectives of UGI. Urban-R1 adopts Group Relative Policy Optimization (GRPO) to optimize reasoning across geographic groups and employs urban region profiling as a proxy task to provide measurable rewards from multimodal urban data. Extensive experiments across diverse regions and tasks show that Urban-R1 effectively mitigates geo-bias and improves cross-region generalization, outperforming both SFT-trained and closed-source models. Our results highlight reinforcement learning alignment as a promising pathway toward equitable and trustworthy urban intelligence.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "147",
        "title": "Fit for Purpose? Deepfake Detection in the Real World",
        "author": [
            "Guangyu Lin",
            "Li Lin",
            "Christina P. Walker",
            "Daniel S. Schiff",
            "Shu Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16556",
        "abstract": "The rapid proliferation of AI-generated content, driven by advances in generative adversarial networks, diffusion models, and multimodal large language models, has made the creation and dissemination of synthetic media effortless, heightening the risks of misinformation, particularly political deepfakes that distort truth and undermine trust in political institutions. In turn, governments, research institutions, and industry have strongly promoted deepfake detection initiatives as solutions. Yet, most existing models are trained and validated on synthetic, laboratory-controlled datasets, limiting their generalizability to the kinds of real-world political deepfakes circulating on social platforms that affect the public. In this work, we introduce the first systematic benchmark based on the Political Deepfakes Incident Database, a curated collection of real-world political deepfakes shared on social media since 2018. Our study includes a systematic evaluation of state-of-the-art deepfake detectors across academia, government, and industry. We find that the detectors from academia and government perform relatively poorly. While paid detection tools achieve relatively higher performance than free-access models, all evaluated detectors struggle to generalize effectively to authentic political deepfakes, and are vulnerable to simple manipulations, especially in the video domain. Results urge the need for politically contextualized deepfake detection frameworks to better safeguard the public in real-world settings.",
        "tags": [
            "Detection",
            "Diffusion",
            "LLM"
        ]
    },
    {
        "id": "148",
        "title": "Toward Understanding Security Issues in the Model Context Protocol Ecosystem",
        "author": [
            "Xiaofan Li",
            "Xing Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16558",
        "abstract": "The Model Context Protocol (MCP) is an emerging open standard that enables AI-powered applications to interact with external tools through structured metadata. A rapidly growing ecosystem has formed around MCP, including a wide range of MCP hosts (i.e., Cursor, Windsurf, Claude Desktop, and Cline), MCP registries (i.e., http://mcp.so, MCP Market, MCP Store, Pulse MCP, Smithery, and npm), and thousands of community-contributed MCP servers. Although the MCP ecosystem is gaining traction, there has been little systematic study of its architecture and associated security risks. In this paper, we present the first comprehensive security analysis of the MCP ecosystem. We decompose MCP ecosystem into three core components: hosts, registries, and servers, and study the interactions and trust relationships among them. Users search for servers on registries and configure them in the host, which translates LLM-generated output into external tool invocations provided by the servers and executes them. Our qualitative analysis reveals that hosts lack output verification mechanisms for LLM-generated outputs, enabling malicious servers to manipulate model behavior and induce a variety of security threats, including but not limited to sensitive data exfiltration. We uncover a wide range of vulnerabilities that enable attackers to hijack servers, due to the lack of a vetted server submission process in registries. To support our analysis, we collect and analyze a dataset of 67,057 servers from six public registries. Our quantitative analysis demonstrates that a substantial number of servers can be hijacked by attackers. Finally, we propose practical defense strategies for MCP hosts, registries, and users. We responsibly disclosed our findings to affected hosts and registries.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "149",
        "title": "BuildArena: A Physics-Aligned Interactive Benchmark of LLMs for Engineering Construction",
        "author": [
            "Tian Xia",
            "Tianrun Gao",
            "Wenhao Deng",
            "Long Wei",
            "Xiaowei Qian",
            "Yixian Jiang",
            "Chenglei Yu",
            "Tailin Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16559",
        "abstract": "Engineering construction automation aims to transform natural language specifications into physically viable structures, requiring complex integrated reasoning under strict physical constraints. While modern LLMs possess broad knowledge and strong reasoning capabilities that make them promising candidates for this domain, their construction competencies remain largely unevaluated. To address this gap, we introduce BuildArena, the first physics-aligned interactive benchmark designed for language-driven engineering construction. It contributes to the community in four aspects: (1) a highly customizable benchmarking framework for in-depth comparison and analysis of LLMs; (2) an extendable task design strategy spanning static and dynamic mechanics across multiple difficulty tiers; (3) a 3D Spatial Geometric Computation Library for supporting construction based on language instructions; (4) a baseline LLM agentic workflow that effectively evaluates diverse model capabilities. On eight frontier LLMs, BuildArena comprehensively evaluates their capabilities for language-driven and physics-grounded construction automation. The project page is at https://build-arena.github.io/.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": "150",
        "title": "Language over Content: Tracing Cultural Understanding in Multilingual Large Language Models",
        "author": [
            "Seungho Cho",
            "Changgeon Ko",
            "Eui Jun Hwang",
            "Junmyeong Lee",
            "Huije Lee",
            "Jong C. Park"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16565",
        "abstract": "Large language models (LLMs) are increasingly used across diverse cultural contexts, making accurate cultural understanding essential. Prior evaluations have mostly focused on output-level performance, obscuring the factors that drive differences in responses, while studies using circuit analysis have covered few languages and rarely focused on culture. In this work, we trace LLMs' internal cultural understanding mechanisms by measuring activation path overlaps when answering semantically equivalent questions under two conditions: varying the target country while fixing the question language, and varying the question language while fixing the country. We also use same-language country pairs to disentangle language from cultural aspects. Results show that internal paths overlap more for same-language, cross-country questions than for cross-language, same-country questions, indicating strong language-specific patterns. Notably, the South Korea-North Korea pair exhibits low overlap and high variability, showing that linguistic similarity does not guarantee aligned internal representation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "151",
        "title": "Ripple Effect Protocol: Coordinating Agent Populations",
        "author": [
            "Ayush Chopra",
            "Aman Sharma",
            "Feroz Ahmad",
            "Luca Muscariello",
            "Vijoy Pandey",
            "Ramesh Raskar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16572",
        "abstract": "Modern AI agents can exchange messages using protocols such as A2A and ACP, yet these mechanisms emphasize communication over coordination. As agent populations grow, this limitation produces brittle collective behavior, where individually smart agents converge on poor group outcomes. We introduce the Ripple Effect Protocol (REP), a coordination protocol in which agents share not only their decisions but also lightweight sensitivities - signals expressing how their choices would change if key environmental variables shifted. These sensitivities ripple through local networks, enabling groups to align faster and more stably than with agent-centric communication alone. We formalize REP's protocol specification, separating required message schemas from optional aggregation rules, and evaluate it across scenarios with varying incentives and network topologies. Benchmarks across three domains: (i) supply chain cascades (Beer Game), (ii) preference aggregation in sparse networks (Movie Scheduling), and (iii) sustainable resource allocation (Fishbanks) show that REP improves coordination accuracy and efficiency over A2A by 41 to 100%, while flexibly handling multimodal sensitivity signals from LLMs. By making coordination a protocol-level capability, REP provides scalable infrastructure for the emerging Internet of Agents",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "152",
        "title": "AI-Generated Text Detection in Low-Resource Languages: A Case Study on Urdu",
        "author": [
            "Muhammad Ammar",
            "Hadiya Murad Hadi",
            "Usman Majeed Butt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16573",
        "abstract": "Large Language Models (LLMs) are now capable of generating text that closely resembles human writing, making them powerful tools for content creation, but this growing ability has also made it harder to tell whether a piece of text was written by a human or by a machine. This challenge becomes even more serious for languages like Urdu, where there are very few tools available to detect AI-generated text. To address this gap, we propose a novel AI-generated text detection framework tailored for the Urdu language. A balanced dataset comprising 1,800 humans authored, and 1,800 AI generated texts, sourced from models such as Gemini, GPT-4o-mini, and Kimi AI was developed. Detailed linguistic and statistical analysis was conducted, focusing on features such as character and word counts, vocabulary richness (Type Token Ratio), and N-gram patterns, with significance evaluated through t-tests and MannWhitney U tests. Three state-of-the-art multilingual transformer models such as mdeberta-v3-base, distilbert-base-multilingualcased, and xlm-roberta-base were fine-tuned on this dataset. The mDeBERTa-v3-base achieved the highest performance, with an F1-score 91.29 and accuracy of 91.26% on the test set. This research advances efforts in contesting misinformation and academic misconduct in Urdu-speaking communities and contributes to the broader development of NLP tools for low resource languages.",
        "tags": [
            "Detection",
            "GPT",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "153",
        "title": "ViT-Transformer: Self-attention mechanism based constitutive modeling for nonlinear heterogeneous materials",
        "author": [
            "Yijing Zhou",
            "Shabnam J. Semnani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16575",
        "abstract": "Multi-scale simulations of nonlinear heterogeneous materials and composites are challenging due to the prohibitive computational costs of high-fidelity simulations. Recently, machine learning (ML) based approaches have emerged as promising alternatives to traditional multiscale methods. However, existing ML surrogate constitutive models struggle in capturing long-range dependencies and generalization across microstructures. The recent advancements in attention-based Transformer architectures open the door to a more powerful class of surrogate models. Attention mechanism has demonstrated remarkable capabilities in natural language processing and computer vision. In this work, we introduce a surrogate (meta) model, namely ViT-Transformer, using a Vision Transformer (ViT) encoder and a Transformer-based decoder which are both driven by the self-attention mechanism. The ViT encoder extracts microstructural features from material images, while the decoder is a masked Transformer encoder that combines the latent geometrical features with the macroscopic strain input sequence to predict the corresponding stress response. To enhance training, we propose a random extract training algorithm that improves robustness to sequences of variable length. We design and construct a compact yet diverse dataset via data augmentation, and validate the surrogate model using various composite material images and loading scenarios. Several numerical examples are provided to show the effectiveness and accuracy of the ViT-Transformer model and the training algorithm.",
        "tags": [
            "Transformer",
            "ViT"
        ]
    },
    {
        "id": "154",
        "title": "Human-Aligned Code Readability Assessment with Large Language Models",
        "author": [
            "WendkÃ»uni C. OuÃ©draogo",
            "Yinghua Li",
            "Xueqi Dang",
            "Pawel Borsukiewicz",
            "Xin Zhou",
            "Anil Koyuncu",
            "Jacques Klein",
            "David Lo",
            "TegawendÃ© F. BissyandÃ©"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16579",
        "abstract": "Code readability is crucial for software comprehension and maintenance, yet difficult to assess at scale. Traditional static metrics often fail to capture the subjective, context-sensitive nature of human judgments. Large Language Models (LLMs) offer a scalable alternative, but their behavior as readability evaluators remains underexplored. We introduce CoReEval, the first large-scale benchmark for evaluating LLM-based code readability assessment, comprising over 1.4 million model-snippet-prompt evaluations across 10 state of the art LLMs. The benchmark spans 3 programming languages (Java, Python, CUDA), 2 code types (functional code and unit tests), 4 prompting strategies (ZSL, FSL, CoT, ToT), 9 decoding settings, and developer-guided prompts tailored to junior and senior personas. We compare LLM outputs against human annotations and a validated static model, analyzing numerical alignment (MAE, Pearson's, Spearman's) and justification quality (sentiment, aspect coverage, semantic clustering). Our findings show that developer-guided prompting grounded in human-defined readability dimensions improves alignment in structured contexts, enhances explanation quality, and enables lightweight personalization through persona framing. However, increased score variability highlights trade-offs between alignment, stability, and interpretability. CoReEval provides a robust foundation for prompt engineering, model alignment studies, and human in the loop evaluation, with applications in education, onboarding, and CI/CD pipelines where LLMs can serve as explainable, adaptable reviewers.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "155",
        "title": "Patronus: Safeguarding Text-to-Image Models against White-Box Adversaries",
        "author": [
            "Xinfeng Li",
            "Shengyuan Pang",
            "Jialin Wu",
            "Jiangyi Deng",
            "Huanlong Zhong",
            "Yanjiao Chen",
            "Jie Zhang",
            "Wenyuan Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16581",
        "abstract": "Text-to-image (T2I) models, though exhibiting remarkable creativity in image generation, can be exploited to produce unsafe images. Existing safety measures, e.g., content moderation or model alignment, fail in the presence of white-box adversaries who know and can adjust model parameters, e.g., by fine-tuning. This paper presents a novel defensive framework, named Patronus, which equips T2I models with holistic protection to defend against white-box adversaries. Specifically, we design an internal moderator that decodes unsafe input features into zero vectors while ensuring the decoding performance of benign input features. Furthermore, we strengthen the model alignment with a carefully designed non-fine-tunable learning mechanism, ensuring the T2I model will not be compromised by malicious fine-tuning. We conduct extensive experiments to validate the intactness of the performance on safe content generation and the effectiveness of rejecting unsafe content generation. Results also confirm the resilience of Patronus against various fine-tuning attacks by white-box adversaries.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "156",
        "title": "Can Knowledge-Graph-based Retrieval Augmented Generation Really Retrieve What You Need?",
        "author": [
            "Junchi Yu",
            "Yujie Liu",
            "Jindong Gu",
            "Philip Torr",
            "Dongzhan Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16582",
        "abstract": "Retrieval-Augmented Generation (RAG) based on knowledge graphs (KGs) enhances large language models (LLMs) by providing structured and interpretable external knowledge. However, existing KG-based RAG methods struggle to retrieve accurate and diverse information from text-rich KGs for complex real-world queries. Process Reward Models (PRMs) offer a way to align the retrieval process of KG-based RAG with query-specific knowledge requirements, but they heavily rely on process-level supervision signals that are expensive and hard to obtain on KGs. To address this challenge, we propose GraphFlow, a framework that efficiently retrieves accurate and diverse knowledge required for real-world queries from text-rich KGs. GraphFlow employs a transition-based flow matching objective to jointly optimize a retrieval policy and a flow estimator. The flow estimator factorizes the reward of the retrieval outcome into the intermediate retrieval states. Such reward factorization guides the retrieval policy to retrieve candidates from KGs in proportion to their reward. This allows GraphFlow to explore high-quality regions of KGs that yield diverse and relevant results. We evaluate GraphFlow on the STaRK benchmark, which includes real-world queries from multiple domains over text-rich KGs. GraphFlow outperforms strong KG-RAG baselines, including GPT-4o, by 10% on average in hit rate and recall. It also shows strong generalization to unseen KGs, demonstrating its effectiveness and robustness.",
        "tags": [
            "Flow Matching",
            "GPT",
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "157",
        "title": "VisionSelector: End-to-End Learnable Visual Token Compression for Efficient Multimodal LLMs",
        "author": [
            "Jiaying Zhu",
            "Yurui Zhu",
            "Xin Lu",
            "Wenrui Yan",
            "Dong Li",
            "Kunlin Liu",
            "Xueyang Fu",
            "Zheng-Jun Zha"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16598",
        "abstract": "Multimodal Large Language Models (MLLMs) encounter significant computational and memory bottlenecks from the massive number of visual tokens generated by high-resolution images or multi-image inputs. Previous token compression techniques are often constrained by heuristic rules that risk discarding critical information. They may suffer from biases, such as attention sinks, that lead to sharp performance drops under aggressive compression ratios. To address these limitations, we reformulate token compression as a lightweight plug-and-play framework that reformulates token compression into an end-to-end learnable decision process. To be specific, we propose VisionSelector, a scorer module decoupled from the MLLM backbone that incorporates a differentiable Top-K mechanism and a curriculum annealing strategy to bridge the training-inference gap, enabling efficient and adaptive token selection various arbitrary compression rates. Remarkably lightweight with only 12.85M trainable parameters, VisionSelector demonstrates generalization across various compression rates and adaptively identifying critical tokens. This leads to superior performance across all compression budgets, evidenced by preserving 100% accuracy on MME with 30% retention budget, outperforming prior methods by 12.14% at 10% retention budget, and doubling prefill speed. Our code is available at https://github.com/JulietChoo/VisionSelector .",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "158",
        "title": "Fine-tuning of Large Language Models for Constituency Parsing Using a Sequence to Sequence Approach",
        "author": [
            "Francisco Jose Cortes Delgado",
            "Eduardo Martinez Gracia",
            "Rafael Valencia Garcia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16604",
        "abstract": "Recent advances in natural language processing with large neural models have opened new possibilities for syntactic analysis based on machine learning. This work explores a novel approach to phrase-structure analysis by fine-tuning large language models (LLMs) to translate an input sentence into its corresponding syntactic structure. The main objective is to extend the capabilities of MiSintaxis, a tool designed for teaching Spanish syntax. Several models from the Hugging Face repository were fine-tuned using training data generated from the AnCora-ES corpus, and their performance was evaluated using the F1 score. The results demonstrate high accuracy in phrase-structure analysis and highlight the potential of this methodology.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "159",
        "title": "Prior Makes It Possible: From Sublinear Graph Algorithms to LLM Test-Time Methods",
        "author": [
            "Avrim Blum",
            "Daniel Hsu",
            "Cyrus Rashtchian",
            "Donya Saless"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16609",
        "abstract": "Test-time augmentation, such as Retrieval-Augmented Generation (RAG) or tool use, critically depends on an interplay between a model's parametric knowledge and externally retrieved information. However, the theoretical underpinnings of this relationship remain poorly understood. Specifically, it is not clear how much pre-training knowledge is required to answer queries with a small number of augmentation steps, which is a desirable property in practice. To address this question, we formulate multi-step reasoning as an $s$-$t$ connectivity problem on a knowledge graph. We represent a model's pre-training parametric knowledge as a partial, potentially noisy subgraph. We view augmentation as querying an oracle for true edges that augment the model's knowledge. Then, we characterize the necessary and sufficient number of augmentation steps for the model to generate an accurate answer given partial prior knowledge. One key result shows a phase transition: if the prior knowledge graph over $n$ vertices is disconnected into small components, then finding a path via augmentation is inefficient and requires $\\Omega(\\sqrt{n})$ queries. On the other hand, once the density of correct knowledge surpasses a threshold, forming a giant component, we can find paths with an expected constant number of queries.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "160",
        "title": "Structuring Security: A Survey of Cybersecurity Ontologies, Semantic Log Processing, and LLMs Application",
        "author": [
            "Bruno LourenÃ§o",
            "Pedro AdÃ£o",
            "JoÃ£o F. Ferreira",
            "Mario Monteiro Marques",
            "CÃ¡tia Vaz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16610",
        "abstract": "This survey investigates how ontologies, semantic log processing, and Large Language Models (LLMs) enhance cybersecurity. Ontologies structure domain knowledge, enabling interoperability, data integration, and advanced threat analysis. Security logs, though critical, are often unstructured and complex. To address this, automated construction of Knowledge Graphs (KGs) from raw logs is emerging as a key strategy for organizing and reasoning over security data. LLMs enrich this process by providing contextual understanding and extracting insights from unstructured content. This work aligns with European Union (EU) efforts such as NIS 2 and the Cybersecurity Taxonomy, highlighting challenges and opportunities in intelligent ontology-driven cyber defense.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "161",
        "title": "Count Counts: Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards",
        "author": [
            "Xuan Zhang",
            "Ruixiao Li",
            "Zhijian Zhou",
            "Long Li",
            "Yulei Qin",
            "Ke Li",
            "Xing Sun",
            "Xiaoyu Tan",
            "Chao Qu",
            "Yuan Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16614",
        "abstract": "Reinforcement Learning (RL) has become a compelling way to strengthen the multi step reasoning ability of Large Language Models (LLMs). However, prevalent RL paradigms still lean on sparse outcome-based rewards and limited exploration, which often drives LLMs toward repetitive and suboptimal reasoning patterns. In this paper, we study the central question of how to design exploration for LLM reasoning and introduce MERCI (Motivating Exploration in LLM Reasoning with Count-based Intrinsic Rewards), a novel RL algorithm that augments policy optimization with a principled intrinsic reward. Building on the idea of count-based exploration, MERCI leverages a lightweight Coin Flipping Network (CFN) to estimate the pseudo count and further epistemic uncertainty over reasoning trajectories, and converts them into an intrinsic reward that values novelty while preserving the learning signal from task rewards. We integrate MERCI into some advanced RL frameworks like Group Relative Policy Optimization (GRPO). Experiments on complex reasoning benchmarks demonstrate that MERCI encourages richer and more varied chains of thought, significantly improves performance over strong baselines, and helps the policy escape local routines to discover better solutions. It indicates that our targeted intrinsic motivation can make exploration reliable for language model reasoning.",
        "tags": [
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "162",
        "title": "MoS-VLA: A Vision-Language-Action Model with One-Shot Skill Adaptation",
        "author": [
            "Ruihan Zhao",
            "Tyler Ingebrand",
            "Sandeep Chinchali",
            "Ufuk Topcu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16617",
        "abstract": "Vision-Language-Action (VLA) models trained on large robot datasets promise general-purpose, robust control across diverse domains and embodiments. However, existing approaches often fail out-of-the-box when deployed in novel environments, embodiments, or tasks. We introduce Mixture of Skills VLA (MoS-VLA), a framework that represents robot manipulation policies as linear combinations of a finite set of learned basis functions. During pretraining, MoS-VLA jointly learns these basis functions across datasets from the Open X-Embodiment project, producing a structured skill space. At test time, adapting to a new task requires only a single expert demonstration. The corresponding skill representation is then inferred via a lightweight convex optimization problem that minimizes the L1 action error, without requiring gradient updates. This gradient-free adaptation incurs minimal overhead while enabling rapid instantiation of new skills. Empirically, MoS-VLA achieves lower action-prediction error on five out of five unseen datasets and succeeds in both simulation and real-robot tasks where a pretrained VLA model fails outright. Project page: http://mos-vla.github.io/",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "163",
        "title": "On the Impossibility of Retrain Equivalence in Machine Unlearning",
        "author": [
            "Jiatong Yu",
            "Yinghui He",
            "Anirudh Goyal",
            "Sanjeev Arora"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16629",
        "abstract": "Machine unlearning seeks to selectively remove the \"influence\" of specific training data on a model's outputs. The ideal goal is Retrain Equivalence--behavior identical to a model trained from scratch on only the retained data. This goal was formulated for models trained on i.i.d. data batches, but modern pipelines often involve multi-stage training, with each stage having a distinct data distribution and objective. Examples include LLM fine-tuning for alignment, reasoning ability, etc. Our study shows via theory and experiments that this shift to multi-stage training introduces a fundamental barrier for machine unlearning. The theory indicates that the outcome of local unlearning--methods that only use gradients computed on the forget set--is path-dependent. That is, a model's behavior during unlearning is influenced by the order of its training stages during learning, making it impossible for path-oblivious algorithms to universally achieve Retrain Equivalence. We empirically demonstrate the same phenomenon in LLM post-training across Llama and Qwen models (1B to 14B) with gradient ascent, NPO, and SimNPO local unlearning algorithms. Models fine-tuned via different orderings of identical training stages diverge in behavior during unlearning, with the degradation in GSM8K accuracy after unlearning varying by over 20% across paths. We also observe that some learning paths consistently produce models that unlearn slowly. During unlearning, whether the probability mass gets squeezed into paraphrasing or alternative concepts is also path-dependent. These results consistently show that Retrain Equivalence is an ill-posed target for local unlearning algorithms, so long as the target models are trained in stages. In situations where access to models' training histories is hard, the current work calls for rethinking the definition and desiderata of machine unlearning.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen"
        ]
    },
    {
        "id": "164",
        "title": "Prompt Optimization via Retrieved Reasoning Assets and Multi-Agent Analysis",
        "author": [
            "Wonduk Seo",
            "Juhyeon Lee",
            "Junseo Koh",
            "Hyunjin An",
            "Jian Park",
            "Seunghyun Lee",
            "Haihua Chen",
            "Yi Bu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16635",
        "abstract": "Prompt optimization has emerged as an effective alternative to retraining for improving the performance of Large Language Models (LLMs). However, most existing approaches treat evaluation as a black box, relying solely on numerical scores while offering limited insight into why a prompt succeeds or fails. They also depend heavily on trial-and-error refinements, which are difficult to interpret and control. In this paper, we introduce MA-SAPO, a Multi-Agent framework for Score-Aware Prompt Optimization. Compared to prior methods, MA-SAPO explicitly couples evaluation outcomes with structured reasoning to guide systematic edits. The framework specifically consists of two stages: during the Reasoning Phase, agents collaboratively explain metric scores, diagnose weaknesses, and synthesize targeted refinements that are stored as reusable reasoning assets; during the Test Phase, agents retrieve these assets to analyze optimized prompts and apply only evidence-grounded edits. By turning evaluation signals into interpretable reasoning chains, MA-SAPO produces prompt refinements that are more transparent, auditable, and controllable. Experiments on the HelpSteer1/2 benchmarks demonstrate consistent improvements over single-pass prompting, retrieval-augmented baselines, and prior multi-agent strategies, validating the effectiveness of our approach.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "165",
        "title": "MultiVerse: A Multi-Turn Conversation Benchmark for Evaluating Large Vision and Language Models",
        "author": [
            "Young-Jun Lee",
            "Byung-Kwan Lee",
            "Jianshu Zhang",
            "Yechan Hwang",
            "Byungsoo Ko",
            "Han-Gyu Kim",
            "Dongyu Yao",
            "Xuankun Rong",
            "Eojin Joo",
            "Seung-Ho Han",
            "Bowon Ko",
            "Ho-Jin Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16641",
        "abstract": "Vision-and-Language Models (VLMs) have shown impressive capabilities on single-turn benchmarks, yet real-world applications often demand more intricate multi-turn dialogues. Existing multi-turn datasets (e.g, MMDU, ConvBench) only partially capture the breadth and depth of conversational scenarios encountered by users. In this work, we introduce MultiVerse, a novel multi-turn conversation benchmark featuring 647 dialogues - each averaging four turns - derived from a diverse set of 12 popular VLM evaluation benchmarks. With 484 tasks and 484 interaction goals, MultiVerse covers a wide range of topics, from factual knowledge and perception to advanced reasoning tasks such as mathematics and coding. To facilitate robust assessment, we propose a checklist-based evaluation method that leverages GPT-4o as the automated evaluator, measuring performance across 37 key aspects, including perceptual accuracy, linguistic clarity, and factual correctness. We evaluate 18 VLMs on MultiVerse, revealing that even the strongest models (e.g., GPT-4o) achieve only a 50% success rate in complex multi-turn conversations, highlighting the dataset's challenging nature. Notably, we find that providing full dialogue context significantly enhances performance for smaller or weaker models, emphasizing the importance of in-context learning. We believe MultiVerse is a landscape of evaluating multi-turn interaction abilities for VLMs.",
        "tags": [
            "GPT",
            "VLM"
        ]
    },
    {
        "id": "166",
        "title": "Structured Interfaces for Automated Reasoning with 3D Scene Graphs",
        "author": [
            "Aaron Ray",
            "Jacob Arkin",
            "Harel Biggie",
            "Chuchu Fan",
            "Luca Carlone",
            "Nicholas Roy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16643",
        "abstract": "In order to provide a robot with the ability to understand and react to a user's natural language inputs, the natural language must be connected to the robot's underlying representations of the world. Recently, large language models (LLMs) and 3D scene graphs (3DSGs) have become a popular choice for grounding natural language and representing the world. In this work, we address the challenge of using LLMs with 3DSGs to ground natural language. Existing methods encode the scene graph as serialized text within the LLM's context window, but this encoding does not scale to large or rich 3DSGs. Instead, we propose to use a form of Retrieval Augmented Generation to select a subset of the 3DSG relevant to the task. We encode a 3DSG in a graph database and provide a query language interface (Cypher) as a tool to the LLM with which it can retrieve relevant data for language grounding. We evaluate our approach on instruction following and scene question-answering tasks and compare against baseline context window and code generation methods. Our results show that using Cypher as an interface to 3D scene graphs scales significantly better to large, rich graphs on both local and cloud-based models. This leads to large performance improvements in grounded language tasks while also substantially reducing the token count of the scene graph content. A video supplement is available at https://www.youtube.com/watch?v=zY_YI9giZSA.",
        "tags": [
            "3D",
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "167",
        "title": "Unleashing Diverse Thinking Modes in LLMs through Multi-Agent Collaboration",
        "author": [
            "Zhixuan He",
            "Yue Feng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16645",
        "abstract": "Large Language Models (LLMs) demonstrate strong performance but often lack interpretable reasoning. This paper introduces the Multi-Agent Collaboration Framework for Diverse Thinking Modes (DiMo), which enhances both performance and interpretability by simulating a structured debate among four specialized LLM agents. Each agent embodies a distinct reasoning paradigm, allowing the framework to collaboratively explore diverse cognitive approaches. Through iterative debate, agents challenge and refine initial responses, yielding more robust conclusions and an explicit, auditable reasoning chain. Across six benchmarks and under a unified open-source setup, DiMo improves accuracy over widely used single-model and debate baselines, with the largest gains on math. We position DiMo as a semantics-aware, Web-native multi-agent framework: it models human-machine intelligence with LLM agents that produce semantically typed, URL-annotated evidence chains for explanations and user-friendly interactions. Although our experiments use standard reasoning benchmarks, the framework is designed to be instantiated over Web corpora and knowledge graphs, combining retrieval-augmented reasoning with structured justifications that downstream systems can inspect and reuse.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "168",
        "title": "All You Need is One: Capsule Prompt Tuning with a Single Vector",
        "author": [
            "Yiyang Liu",
            "James C. Liang",
            "Heng Fan",
            "Wenhao Yang",
            "Yiming Cui",
            "Xiaotian Han",
            "Lifu Huang",
            "Dongfang Liu",
            "Qifan Wang",
            "Cheng Han"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16670",
        "abstract": "Prompt-based learning has emerged as a parameter-efficient finetuning (PEFT) approach to facilitate Large Language Model (LLM) adaptation to downstream tasks by conditioning generation with task-aware guidance. Despite its successes, current prompt-based learning methods heavily rely on laborious grid searching for optimal prompt length and typically require considerable number of prompts, introducing additional computational burden. Worse yet, our pioneer findings indicate that the task-aware prompt design is inherently limited by its absence of instance-aware information, leading to a subtle attention interplay with the input sequence. In contrast, simply incorporating instance-aware information as a part of the guidance can enhance the prompt-tuned model performance without additional fine-tuning. Moreover, we find an interesting phenomenon, namely \"attention anchor\", that incorporating instance-aware tokens at the earliest position of the sequence can successfully preserve strong attention to critical structural information and exhibit more active attention interaction with all input tokens. In light of our observation, we introduce Capsule Prompt-Tuning (CaPT), an efficient and effective solution that leverages off-the-shelf, informative instance semantics into prompt-based learning. Our approach innovatively integrates both instance-aware and task-aware information in a nearly parameter-free manner (i.e., one single capsule prompt). Empirical results demonstrate that our method can exhibit superior performance across various language tasks (e.g., 84.03\\% average accuracy on T5-Large), serving as an \"attention anchor,\" while enjoying high parameter efficiency (e.g., 0.003\\% of model parameters on Llama3.2-1B).",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "169",
        "title": "Renaissance of RNNs in Streaming Clinical Time Series: Compact Recurrence Remains Competitive with Transformers",
        "author": [
            "Ran Tong",
            "Jiaqi Liu",
            "Su Liu",
            "Xin Hu",
            "Lanruo Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16677",
        "abstract": "We present a compact, strictly causal benchmark for streaming clinical time series on the MIT--BIH Arrhythmia Database using per-second heart rate. Two tasks are studied under record-level, non-overlapping splits: near-term tachycardia risk (next ten seconds) and one-step heart rate forecasting. We compare a GRU-D (RNN) and a Transformer under matched training budgets against strong non-learned baselines. Evaluation is calibration-aware for classification and proper for forecasting, with temperature scaling and grouped bootstrap confidence intervals. On MIT-BIH, GRU-D slightly surpasses the Transformer for tachycardia risk, while the Transformer clearly lowers forecasting error relative to GRU-D and persistence. Our results show that, in longitudinal monitoring, model choice is task-dependent: compact RNNs remain competitive for short-horizon risk scoring, whereas compact Transformers deliver clearer gains for point forecasting.",
        "tags": [
            "RNN",
            "Transformer"
        ]
    },
    {
        "id": "170",
        "title": "Temporal Understanding under Deictic Frame of Reference",
        "author": [
            "Damin Zhang",
            "Julia Rayz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16685",
        "abstract": "Understanding time is fundamental to human cognition, where temporal experience is often conceptualized through spatial metaphors grounded in sensory-motor experience. For example, \"summer is approaching\" parallels \"We are approaching the summer\". In such expressions, humans rely on a frame of reference (FoR) to interpret meaning relative to a particular viewpoint. Extending this concept to time, a temporal frame of reference (t-FoR) defines how temporal relations are perceived relative to an experiencer's moment of \"now\". While Large Language Models (LLMs) have shown remarkable advances in natural language understanding, their ability to interpret and reason about time remains limited. In this work, we introduce TUuD (Temporal Understanding under Deictic t-FoR), a framework that evaluates how LLMs interpret time-event and event-event relations when the reference point of \"now\" dynamically shifts along a timeline. Following recent work on temporal cognition \\cite{li2025other}, LLMs are prompted to rate the similarity between the current moment and a target event from 0.00 (completely dissimilar) to 1.00 (highly similar), where similarity quantifies perceived temporal alignment between the two points. Our results show that four evaluated LLMs exhibit measurable adaptation to a deictic t-FoR, with similarity ratings peaking around the present and decreasing toward past and future events. The adaptation, however, weakens beyond near-term contexts, suggesting that while LLMs display partial human-like temporal cognition, their temporal reasoning remains sensitive to reference-frame shifts and temporal distance.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "171",
        "title": "Investigating the Impact of Rationales for LLMs on Natural Language Understanding",
        "author": [
            "Wenhang Shi",
            "Shuqing Bian",
            "Yiren Chen",
            "Xinyi Zhang",
            "Zhe Zhao",
            "Pengfei Hu",
            "Wei Lu",
            "Xiaoyong Du"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16686",
        "abstract": "Chain-of-thought (CoT) rationales, which provide step-by-step reasoning to derive final answers, benefit LLMs in both inference and training. Incorporating rationales, either by generating them before answering during inference, or by placing them before or after the original answers during training - significantly improves model performance on mathematical, symbolic and commonsense reasoning tasks. However, most work focuses on the role of rationales in these reasoning tasks, overlooking their potential impact on other important tasks like natural language understanding (NLU) tasks. In this work, we raise the question: Can rationales similarly benefit NLU tasks? To conduct a systematic exploration, we construct NLURC, a comprehensive and high-quality NLU dataset collection with rationales, and develop various rationale-augmented methods. Through exploring the applicability of these methods on NLU tasks using the dataset, we uncover several potentially surprising findings: (1) CoT inference shifts from hindering NLU performance to surpassing direct label prediction as model size grows, indicating a positive correlation. (2) Most rationale-augmented training methods perform worse than label-only training, with one specially designed method consistently achieving improvements. (3) LLMs trained with rationales achieve significant performance gains on unseen NLU tasks, rivaling models ten times their size, while delivering interpretability on par with commercial LLMs.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "172",
        "title": "High-Dimensional Privacy-Utility Dynamics of Noisy Stochastic Gradient Descent on Least Squares",
        "author": [
            "Shurong Lin",
            "Eric D. Kolaczyk",
            "Adam Smith",
            "Elliot Paquette"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16687",
        "abstract": "The interplay between optimization and privacy has become a central theme in privacy-preserving machine learning. Noisy stochastic gradient descent (SGD) has emerged as a cornerstone algorithm, particularly in large-scale settings. These variants of gradient methods inject carefully calibrated noise into each update to achieve differential privacy, the gold standard notion of rigorous privacy guarantees. Prior work primarily provides various bounds on statistical risk and privacy loss for noisy SGD, yet the \\textit{exact} behavior of the process remains unclear, particularly in high-dimensional settings. This work leverages a diffusion approach to analyze noisy SGD precisely, providing a continuous-time perspective that captures both statistical risk evolution and privacy loss dynamics in high dimensions. Moreover, we study a variant of noisy SGD that does not require explicit knowledge of gradient sensitivity, unlike existing work that assumes or enforces sensitivity through gradient clipping. Specifically, we focus on the least squares problem with $\\ell_2$ regularization.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "173",
        "title": "Pursuing Minimal Sufficiency in Spatial Reasoning",
        "author": [
            "Yejie Guo",
            "Yunzhong Hou",
            "Wufei Ma",
            "Meng Tang",
            "Ming-Hsuan Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16688",
        "abstract": "Spatial reasoning, the ability to ground language in 3D understanding, remains a persistent challenge for Vision-Language Models (VLMs). We identify two fundamental bottlenecks: inadequate 3D understanding capabilities stemming from 2D-centric pre-training, and reasoning failures induced by redundant 3D information. To address these, we first construct a Minimal Sufficient Set (MSS) of information before answering a given question: a compact selection of 3D perception results from \\textit{expert models}. We introduce MSSR (Minimal Sufficient Spatial Reasoner), a dual-agent framework that implements this principle. A Perception Agent programmatically queries 3D scenes using a versatile perception toolbox to extract sufficient information, including a novel SOG (Situated Orientation Grounding) module that robustly extracts language-grounded directions. A Reasoning Agent then iteratively refines this information to pursue minimality, pruning redundant details and requesting missing ones in a closed loop until the MSS is curated. Extensive experiments demonstrate that our method, by explicitly pursuing both sufficiency and minimality, significantly improves accuracy and achieves state-of-the-art performance across two challenging benchmarks. Furthermore, our framework produces interpretable reasoning paths, offering a promising source of high-quality training data for future models. Source code is available at https://github.com/gyj155/mssr.",
        "tags": [
            "3D",
            "VLM"
        ]
    },
    {
        "id": "174",
        "title": "First Responders' Perceptions of Semantic Information for Situational Awareness in Robot-Assisted Emergency Response",
        "author": [
            "Tianshu Ruan",
            "Zoe Betta",
            "Georgios Tzoumas",
            "Rustam Stolkin",
            "Manolis Chiou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16692",
        "abstract": "This study investigates First Responders' (FRs) attitudes toward the use of semantic information and Situational Awareness (SA) in robotic systems during emergency operations. A structured questionnaire was administered to 22 FRs across eight countries, capturing their demographic profiles, general attitudes toward robots, and experiences with semantics-enhanced SA. Results show that most FRs expressed positive attitudes toward robots, and rated the usefulness of semantic information for building SA at an average of 3.6 out of 5. Semantic information was also valued for its role in predicting unforeseen emergencies (mean 3.9). Participants reported requiring an average of 74.6\\% accuracy to trust semantic outputs and 67.8\\% for them to be considered useful, revealing a willingness to use imperfect but informative AI support tools.\nTo the best of our knowledge, this study offers novel insights by being one of the first to directly survey FRs on semantic-based SA in a cross-national context. It reveals the types of semantic information most valued in the field, such as object identity, spatial relationships, and risk context-and connects these preferences to the respondents' roles, experience, and education levels. The findings also expose a critical gap between lab-based robotics capabilities and the realities of field deployment, highlighting the need for more meaningful collaboration between FRs and robotics researchers. These insights contribute to the development of more user-aligned and situationally aware robotic systems for emergency response.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "175",
        "title": "CLIP: Client-Side Invariant Pruning for Mitigating Stragglers in Secure Federated Learning",
        "author": [
            "Anthony DiMaggio",
            "Raghav Sharma",
            "Gururaj Saileshwar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16694",
        "abstract": "Secure federated learning (FL) preserves data privacy during distributed model training. However, deploying such frameworks across heterogeneous devices results in performance bottlenecks, due to straggler clients with limited computational or network capabilities, slowing training for all participating clients. This paper introduces the first straggler mitigation technique for secure aggregation with deep neural networks. We propose CLIP, a client-side invariant neuron pruning technique coupled with network-aware pruning, that addresses compute and network bottlenecks due to stragglers during training with minimal accuracy loss. Our technique accelerates secure FL training by 13% to 34% across multiple datasets (CIFAR10, Shakespeare, FEMNIST) with an accuracy impact of between 1.3% improvement to 2.6% reduction.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "176",
        "title": "An Agentic Framework with LLMs for Solving Complex Vehicle Routing Problems",
        "author": [
            "Ni Zhang",
            "Zhiguang Cao",
            "Jianan Zhou",
            "Cong Zhang",
            "Yew-Soon Ong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16701",
        "abstract": "Complex vehicle routing problems (VRPs) remain a fundamental challenge, demanding substantial expert effort for intent interpretation and algorithm design. While large language models (LLMs) offer a promising path toward automation, current approaches still rely on external intervention, which restrict autonomy and often lead to execution errors and low solution feasibility. To address these challenges, we propose an Agentic Framework with LLMs (AFL) for solving complex vehicle routing problems, achieving full automation from problem instance to solution. AFL directly extracts knowledge from raw inputs and enables self-contained code generation without handcrafted modules or external solvers. To improve trustworthiness, AFL decomposes the overall pipeline into three manageable subtasks and employs four specialized agents whose coordinated interactions enforce cross-functional consistency and logical soundness. Extensive experiments on 60 complex VRPs, ranging from standard benchmarks to practical variants, validate the effectiveness and generality of our framework, showing comparable performance against meticulously designed algorithms. Notably, it substantially outperforms existing LLM-based baselines in both code reliability and solution feasibility, achieving rates close to 100% on the evaluated benchmarks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "177",
        "title": "HumanCM: One Step Human Motion Prediction",
        "author": [
            "Liu Haojie",
            "Gao Suixiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16709",
        "abstract": "We present HumanCM, a one-step human motion prediction framework built upon consistency models. Instead of relying on multi-step denoising as in diffusion-based methods, HumanCM performs efficient single-step generation by learning a self-consistent mapping between noisy and clean motion states. The framework adopts a Transformer-based spatiotemporal architecture with temporal embeddings to model long-range dependencies and preserve motion coherence. Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achieves comparable or superior accuracy to state-of-the-art diffusion models while reducing inference steps by up to two orders of magnitude.",
        "tags": [
            "Consistency Models",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "178",
        "title": "The Chameleon Nature of LLMs: Quantifying Multi-Turn Stance Instability in Search-Enabled Language Models",
        "author": [
            "Shivam Ratnakar",
            "Sanjay Raghavendra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16712",
        "abstract": "Integration of Large Language Models with search/retrieval engines has become ubiquitous, yet these systems harbor a critical vulnerability that undermines their reliability. We present the first systematic investigation of \"chameleon behavior\" in LLMs: their alarming tendency to shift stances when presented with contradictory questions in multi-turn conversations (especially in search-enabled LLMs). Through our novel Chameleon Benchmark Dataset, comprising 17,770 carefully crafted question-answer pairs across 1,180 multi-turn conversations spanning 12 controversial domains, we expose fundamental flaws in state-of-the-art systems. We introduce two theoretically grounded metrics: the Chameleon Score (0-1) that quantifies stance instability, and Source Re-use Rate (0-1) that measures knowledge diversity. Our rigorous evaluation of Llama-4-Maverick, GPT-4o-mini, and Gemini-2.5-Flash reveals consistent failures: all models exhibit severe chameleon behavior (scores 0.391-0.511), with GPT-4o-mini showing the worst performance. Crucially, small across-temperature variance (less than 0.004) suggests the effect is not a sampling artifact. Our analysis uncovers the mechanism: strong correlations between source re-use rate and confidence (r=0.627) and stance changes (r=0.429) are statistically significant (p less than 0.05), indicating that limited knowledge diversity makes models pathologically deferential to query framing. These findings highlight the need for comprehensive consistency evaluation before deploying LLMs in healthcare, legal, and financial systems where maintaining coherent positions across interactions is critical for reliable decision support.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "179",
        "title": "so much depends / upon / a whitespace: Why Whitespace Matters for Poets and LLMs",
        "author": [
            "Sriharsh Bhyravajjula",
            "Melanie Walsh",
            "Anna Preus",
            "Maria Antoniak"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16713",
        "abstract": "Whitespace is a critical component of poetic form, reflecting both adherence to standardized forms and rebellion against those forms. Each poem's whitespace distribution reflects the artistic choices of the poet and is an integral semantic and spatial feature of the poem. Yet, despite the popularity of poetry as both a long-standing art form and as a generation task for large language models (LLMs), whitespace has not received sufficient attention from the NLP community. Using a corpus of 19k English-language published poems from Poetry Foundation, we investigate how 4k poets have used whitespace in their works. We release a subset of 2.8k public-domain poems with preserved formatting to facilitate further research in this area. We compare whitespace usage in the published poems to (1) 51k LLM-generated poems, and (2) 12k unpublished poems posted in an online community. We also explore whitespace usage across time periods, poetic forms, and data sources. Additionally, we find that different text processing methods can result in significantly different representations of whitespace in poetry data, motivating us to use these poems and whitespace patterns to discuss implications for the processing strategies used to assemble pretraining datasets for LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "180",
        "title": "Eliciting Grounded Chain-of-Thought Reasoning in 3D Scenes",
        "author": [
            "Xiongkun Linghu",
            "Jiangyong Huang",
            "Ziyu Zhu",
            "Baoxiong Jia",
            "Siyuan Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16714",
        "abstract": "Existing research on 3D Large Language Models (LLMs) still struggles to achieve grounded question-answering, primarily due to the under-exploration of the mech- anism of human-like scene-object grounded reasoning. This paper bridges the gap by presenting a novel framework. We first introduce a grounded Chain-of- Thought reasoning method in 3D scenes (SCENECOT), decoupling a complex reasoning task into simpler and manageable problems, and building corresponding visual clues based on multimodal expert modules. To enable such a method, we develop SCENECOT-185K, the first large-scale grounded CoT reasoning dataset, consisting of 185K high-quality instances. Extensive experiments across various complex 3D scene reasoning benchmarks demonstrate that our new framework achieves strong performance with high grounding-QA coherence. To the best of our knowledge, this is the first successful application of CoT reasoning to 3D scene understanding, enabling step-by-step human-like reasoning and showing potential for extension to broader 3D scene understanding scenarios.",
        "tags": [
            "3D",
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "181",
        "title": "DistilLock: Safeguarding LLMs from Unauthorized Knowledge Distillation on the Edge",
        "author": [
            "Asmita Mohanty",
            "Gezheng Kang",
            "Lei Gao",
            "Murali Annavaram"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16716",
        "abstract": "Large Language Models (LLMs) have demonstrated strong performance across diverse tasks, but fine-tuning them typically relies on cloud-based, centralized infrastructures. This requires data owners to upload potentially sensitive data to external servers, raising serious privacy concerns. An alternative approach is to fine-tune LLMs directly on edge devices using local data; however, this introduces a new challenge: the model owner must transfer proprietary models to the edge, which risks intellectual property (IP) leakage. To address this dilemma, we propose DistilLock, a TEE-assisted fine-tuning framework that enables privacy-preserving knowledge distillation on the edge. In DistilLock, a proprietary foundation model is executed within a trusted execution environment (TEE) enclave on the data owner's device, acting as a secure black-box teacher. This setup preserves both data privacy and model IP by preventing direct access to model internals. Furthermore, DistilLock employs a model obfuscation mechanism to offload obfuscated weights to untrusted accelerators for efficient knowledge distillation without compromising security. We demonstrate that DistilLock prevents unauthorized knowledge distillation processes and model-stealing attacks while maintaining high computational efficiency, but offering a secure and practical solution for edge-based LLM personalization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "182",
        "title": "U-Codec: Ultra Low Frame-rate Neural Speech Codec for Fast High-fidelity Speech Generation",
        "author": [
            "Xusheng Yang",
            "Long Zhou",
            "Wenfu Wang",
            "Kai Hu",
            "Shulin Feng",
            "Chenxing Li",
            "Meng Yu",
            "Dong Yu",
            "Yuexian Zou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16718",
        "abstract": "We propose \\textbf{U-Codec}, an \\textbf{U}ltra low frame-rate neural speech \\textbf{Codec} that achieves high-fidelity reconstruction and fast speech generation at an extremely low frame-rate of 5Hz (5 frames per second). Extreme compression at 5Hz typically leads to severe intelligibility and spectral detail loss, we introduce a Transformer-based inter-frame long-term dependency module and systematically explore residual vector quantization (RVQ) depth and codebook size to identify optimal configurations. Moreover, we apply U-Codec into a large language model (LLM)-based auto-regressive TTS model, which leverages global and local hierarchical architecture to effectively capture dependencies across multi-layer tokens. We extend LLM-based TTS from 3-layer RVQ at 50Hz to 32-layer RVQ at 5Hz. Experimental results demonstrate that U-Codec improves LLM-based TTS inference speed by around 3 $\\times$ over high-frame-rate codecs while maintaining similarity and naturalness. These results validate the feasibility of using highly compressed 5Hz discrete tokens for fast and high-fidelity speech synthesis.",
        "tags": [
            "LLM",
            "Transformer",
            "Vector Quantization"
        ]
    },
    {
        "id": "183",
        "title": "LSTM-Based Forecasting and Analysis of EV Charging Demand in a Dense Urban Campus",
        "author": [
            "Zak Ressler",
            "Marcus Grijalva",
            "Angelica Marie Ignacio",
            "Melanie Torres",
            "Abelardo Cuadra Rojas",
            "Rohollah Moghadam",
            "Mohammad Rasoul narimani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16719",
        "abstract": "This paper presents a framework for processing EV charging load data in order to forecast future load predictions using a Recurrent Neural Network, specifically an LSTM. The framework processes a large set of raw data from multiple locations and transforms it with normalization and feature extraction to train the LSTM. The pre-processing stage corrects for missing or incomplete values by interpolating and normalizing the measurements. This information is then fed into a Long Short-Term Memory Model designed to capture the short-term fluctuations while also interpreting the long-term trends in the charging data. Experimental results demonstrate the model's ability to accurately predict charging demand across multiple time scales (daily, weekly, and monthly), providing valuable insights for infrastructure planning, energy management, and grid integration of EV charging facilities. The system's modular design allows for adaptation to different charging locations with varying usage patterns, making it applicable across diverse deployment scenarios.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "184",
        "title": "Beyond Pipelines: A Survey of the Paradigm Shift toward Model-Native Agentic AI",
        "author": [
            "Jitao Sang",
            "Jinlin Xiao",
            "Jiarun Han",
            "Jilin Chen",
            "Xiaoyi Chen",
            "Shuyu Wei",
            "Yongjie Sun",
            "Yuhang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16720",
        "abstract": "The rapid evolution of agentic AI marks a new phase in artificial intelligence, where Large Language Models (LLMs) no longer merely respond but act, reason, and adapt. This survey traces the paradigm shift in building agentic AI: from Pipeline-based systems, where planning, tool use, and memory are orchestrated by external logic, to the emerging Model-native paradigm, where these capabilities are internalized within the model's parameters. We first position Reinforcement Learning (RL) as the algorithmic engine enabling this paradigm shift. By reframing learning from imitating static data to outcome-driven exploration, RL underpins a unified solution of LLM + RL + Task across language, vision and embodied domains. Building on this, the survey systematically reviews how each capability -- Planning, Tool use, and Memory -- has evolved from externally scripted modules to end-to-end learned behaviors. Furthermore, it examines how this paradigm shift has reshaped major agent applications, specifically the Deep Research agent emphasizing long-horizon reasoning and the GUI agent emphasizing embodied interaction. We conclude by discussing the continued internalization of agentic capabilities like Multi-agent collaboration and Reflection, alongside the evolving roles of the system and model layers in future agentic AI. Together, these developments outline a coherent trajectory toward model-native agentic AI as an integrated learning and interaction framework, marking the transition from constructing systems that apply intelligence to developing models that grow intelligence through experience.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "185",
        "title": "A Comprehensive Survey on Reinforcement Learning-based Agentic Search: Foundations, Roles, Optimizations, Evaluations, and Applications",
        "author": [
            "Minhua Lin",
            "Zongyu Wu",
            "Zhichao Xu",
            "Hui Liu",
            "Xianfeng Tang",
            "Qi He",
            "Charu Aggarwal",
            "Hui Liu",
            "Xiang Zhang",
            "Suhang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16724",
        "abstract": "The advent of large language models (LLMs) has transformed information access and reasoning through open-ended natural language interaction. However, LLMs remain limited by static knowledge, factual hallucinations, and the inability to retrieve real-time or domain-specific information. Retrieval-Augmented Generation (RAG) mitigates these issues by grounding model outputs in external evidence, but traditional RAG pipelines are often single turn and heuristic, lacking adaptive control over retrieval and reasoning. Recent advances in agentic search address these limitations by enabling LLMs to plan, retrieve, and reflect through multi-step interaction with search environments. Within this paradigm, reinforcement learning (RL) offers a powerful mechanism for adaptive and self-improving search behavior. This survey provides the first comprehensive overview of \\emph{RL-based agentic search}, organizing the emerging field along three complementary dimensions: (i) What RL is for (functional roles), (ii) How RL is used (optimization strategies), and (iii) Where RL is applied (scope of optimization). We summarize representative methods, evaluation protocols, and applications, and discuss open challenges and future directions toward building reliable and scalable RL driven agentic search systems. We hope this survey will inspire future research on the integration of RL and agentic search. Our repository is available at https://github.com/ventr1c/Awesome-RL-based-Agentic-Search-Papers.",
        "tags": [
            "LLM",
            "RAG",
            "RL"
        ]
    },
    {
        "id": "186",
        "title": "Beacon: Single-Turn Diagnosis and Mitigation of Latent Sycophancy in Large Language Models",
        "author": [
            "Sanskar Pandey",
            "Ruhaan Chopra",
            "Angkul Puniya",
            "Sohom Pal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16727",
        "abstract": "Large language models internalize a structural trade-off between truthfulness and obsequious flattery, emerging from reward optimization that conflates helpfulness with polite submission. This latent bias, known as sycophancy, manifests as a preference for user agreement over principled reasoning. We introduce Beacon, a single-turn forced-choice benchmark that isolates this bias independent of conversational context, enabling precise measurement of the tension between factual accuracy and submissive bias. Evaluations across twelve state-of-the-art models reveal that sycophancy decomposes into stable linguistic and affective sub-biases, each scaling with model capacity. We further propose prompt-level and activation-level interventions that modulate these biases in opposing directions, exposing the internal geometry of alignment as a dynamic manifold between truthfulness and socially compliant judgment. Beacon reframes sycophancy as a measurable form of normative misgeneralization, providing a reproducible foundation for studying and mitigating alignment drift in large-scale generative systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "187",
        "title": "UKANFormer: Noise-Robust Semantic Segmentation for Coral Reef Mapping via a Kolmogorov-Arnold Network-Transformer Hybrid",
        "author": [
            "Tianyang Dou",
            "Ming Li",
            "Jiangying Qin",
            "Xuan Liao",
            "Jiageng Zhong",
            "Armin Gruen",
            "Mengyi Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16730",
        "abstract": "Coral reefs are vital yet fragile ecosystems that require accurate large-scale mapping for effective conservation. Although global products such as the Allen Coral Atlas provide unprecedented coverage of global coral reef distri-bution, their predictions are frequently limited in spatial precision and semantic consistency, especially in regions requiring fine-grained boundary delineation. To address these challenges, we propose UKANFormer, a novel se-mantic segmentation model designed to achieve high-precision mapping under noisy supervision derived from Allen Coral Atlas. Building upon the UKAN architecture, UKANFormer incorporates a Global-Local Transformer (GL-Trans) block in the decoder, enabling the extraction of both global semantic structures and local boundary details. In experiments, UKANFormer achieved a coral-class IoU of 67.00% and pixel accuracy of 83.98%, outperforming conventional baselines under the same noisy labels setting. Remarkably, the model produces predictions that are visually and structurally more accurate than the noisy labels used for training. These results challenge the notion that data quality directly limits model performance, showing that architectural design can mitigate label noise and sup-port scalable mapping under imperfect supervision. UKANFormer provides a foundation for ecological monitoring where reliable labels are scarce.",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "188",
        "title": "A Comprehensive Survey on World Models for Embodied AI",
        "author": [
            "Xinqing Li",
            "Xin He",
            "Le Zhang",
            "Yun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16732",
        "abstract": "Embodied AI requires agents that perceive, act, and anticipate how actions reshape future world states. World models serve as internal simulators that capture environment dynamics, enabling forward and counterfactual rollouts to support perception, prediction, and decision making. This survey presents a unified framework for world models in embodied AI. Specifically, we formalize the problem setting and learning objectives, and propose a three-axis taxonomy encompassing: (1) Functionality, Decision-Coupled vs. General-Purpose; (2) Temporal Modeling, Sequential Simulation and Inference vs. Global Difference Prediction; (3) Spatial Representation, Global Latent Vector, Token Feature Sequence, Spatial Latent Grid, and Decomposed Rendering Representation. We systematize data resources and metrics across robotics, autonomous driving, and general video settings, covering pixel prediction quality, state-level understanding, and task performance. Furthermore, we offer a quantitative comparison of state-of-the-art models and distill key open challenges, including the scarcity of unified datasets and the need for evaluation metrics that assess physical consistency over pixel fidelity, the trade-off between model performance and the computational efficiency required for real-time control, and the core modeling difficulty of achieving long-horizon temporal consistency while mitigating error accumulation. Finally, we maintain a curated bibliography at https://github.com/Li-Zn-H/AwesomeWorldModels.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "189",
        "title": "A Control-Theoretic Approach to Dynamic Payment Routing for Success Rate Optimization",
        "author": [
            "Aniket Agrawal",
            "Harsharanga Patil"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16735",
        "abstract": "This paper introduces a control-theoretic framework for dynamic payment routing, implemented within JUSPAY's Payment Orchestrator to maximize transaction success rate. The routing system is modeled as a closed-loop feedback controller continuously sensing gateway performance, computing corrective actions, and dynamically routes transactions across gateway to ensure operational resilience. The system leverages concepts from control theory, reinforcement learning, and multi-armed bandit optimization to achieve both short-term responsiveness and long-term stability. Rather than relying on explicit PID regulation, the framework applies generalized feedback-based adaptation, ensuring that corrective actions remain proportional to observed performance deviations and the computed gateway score gradually converges toward the success rate. This hybrid approach unifies control theory and adaptive decision systems, enabling self-regulating transaction routing that dampens instability, and improves reliability. Live production results show an improvement of up to 1.15% in success rate over traditional rule-based routing, demonstrating the effectiveness of feedback-based control in payment systems.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "190",
        "title": "Zero-Shot Performance Prediction for Probabilistic Scaling Laws",
        "author": [
            "Viktoria Schram",
            "Markus Hiller",
            "Daniel Beck",
            "Trevor Cohn"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16743",
        "abstract": "The prediction of learning curves for Natural Language Processing (NLP) models enables informed decision-making to meet specific performance objectives, while reducing computational overhead and lowering the costs associated with dataset acquisition and curation. In this work, we formulate the prediction task as a multitask learning problem, where each task's data is modelled as being organized within a two-layer hierarchy. To model the shared information and dependencies across tasks and hierarchical levels, we employ latent variable multi-output Gaussian Processes, enabling to account for task correlations and supporting zero-shot prediction of learning curves (LCs). We demonstrate that this approach facilitates the development of probabilistic scaling laws at lower costs. Applying an active learning strategy, LCs can be queried to reduce predictive uncertainty and provide predictions close to ground truth scaling laws. We validate our framework on three small-scale NLP datasets with up to $30$ LCs. These are obtained from nanoGPT models, from bilingual translation using mBART and Transformer models, and from multilingual translation using M2M100 models of varying sizes.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "191",
        "title": "An Efficient Semantic Segmentation Decoder for In-Car or Distributed Applications",
        "author": [
            "Danish Nazir",
            "Gowtham Sai Inti",
            "Timo Bartels",
            "Jan Piewek",
            "Thorsten Bagdonat",
            "Tim Fingscheidt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16747",
        "abstract": "Modern automotive systems leverage deep neural networks (DNNs) for semantic segmentation and operate in two key application areas: (1) In-car, where the DNN solely operates in the vehicle without strict constraints on the data rate. (2) Distributed, where one DNN part operates in the vehicle and the other part typically on a large-scale cloud platform with a particular constraint on transmission bitrate efficiency. Typically, both applications share an image and source encoder, while each uses distinct (joint) source and task decoders. Prior work utilized convolutional neural networks for joint source and task decoding but did not investigate transformer-based alternatives such as SegDeformer, which offer superior performance at the cost of higher computational complexity. In this work, we propose joint feature and task decoding for SegDeformer, thereby enabling lower computational complexity in both in-car and distributed applications, despite SegDeformer's computational demands. This improves scalability in the cloud while reducing in-car computational complexity. For the in-car application, we increased the frames per second (fps) by up to a factor of $11.7$ ($1.4$ fps to $16.5$ fps) on Cityscapes and by up to a factor of $3.5$ ($43.3$ fps to $154.3$ fps) on ADE20K, while being on-par w.r.t.\\ the mean intersection over union (mIoU) of the transformer-based baseline that doesn't compress by a source codec. For the distributed application, we achieve state-of-the-art (SOTA) over a wide range of bitrates on the mIoU metric, while using only $0.14$\\% ($0.04$\\%) of cloud DNN parameters used in previous SOTA, reported on ADE20K (Cityscapes).",
        "tags": [
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "192",
        "title": "Visual Autoregressive Models Beat Diffusion Models on Inference Time Scaling",
        "author": [
            "Erik Riise",
            "Mehmet Onurcan Kaya",
            "Dim P. Papadopoulos"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16751",
        "abstract": "While inference-time scaling through search has revolutionized Large Language Models, translating these gains to image generation has proven difficult. Recent attempts to apply search strategies to continuous diffusion models show limited benefits, with simple random sampling often performing best. We demonstrate that the discrete, sequential nature of visual autoregressive models enables effective search for image generation. We show that beam search substantially improves text-to-image generation, enabling a 2B parameter autoregressive model to outperform a 12B parameter diffusion model across benchmarks. Systematic ablations show that this advantage comes from the discrete token space, which allows early pruning and computational reuse, and our verifier analysis highlights trade-offs between speed and reasoning capability. These findings suggest that model architecture, not just scale, is critical for inference-time optimization in visual generation.",
        "tags": [
            "Diffusion",
            "LLM",
            "Text-to-Image"
        ]
    },
    {
        "id": "193",
        "title": "ELMM: Efficient Lightweight Multimodal Large Language Models for Multimodal Knowledge Graph Completion",
        "author": [
            "Wei Huang",
            "Peining Li",
            "Meiyu Liang",
            "Xu Hou",
            "Junping Du",
            "Yingxia Shao",
            "Guanhua Ye",
            "Wu Liu",
            "Kangkang Lu",
            "Yang Yu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16753",
        "abstract": "Multimodal Knowledge Graphs (MKGs) extend traditional knowledge graphs by incorporating visual and textual modalities, enabling richer and more expressive entity representations. However, existing MKGs often suffer from incompleteness, which hinder their effectiveness in downstream tasks. Therefore, multimodal knowledge graph completion (MKGC) task is receiving increasing attention. While large language models (LLMs) have shown promise for knowledge graph completion (KGC), their application to the multimodal setting remains underexplored. Moreover, applying Multimodal Large Language Models (MLLMs) to the task of MKGC introduces significant challenges: (1) the large number of image tokens per entity leads to semantic noise and modality conflicts, and (2) the high computational cost of processing large token inputs. To address these issues, we propose Efficient Lightweight Multimodal Large Language Models (ELMM) for MKGC. ELMM proposes a Multi-view Visual Token Compressor (MVTC) based on multi-head attention mechanism, which adaptively compresses image tokens from both textual and visual views, thereby effectively reducing redundancy while retaining necessary information and avoiding modality conflicts. Additionally, we design an attention pruning strategy to remove redundant attention layers from MLLMs, thereby significantly reducing the inference cost. We further introduce a linear projection to compensate for the performance degradation caused by pruning. Extensive experiments on benchmark FB15k-237-IMG and WN18-IMG demonstrate that ELMM achieves state-of-the-art performance while substantially improving computational efficiency, establishing a new paradigm for multimodal knowledge graph completion.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "194",
        "title": "Adaptive Invariant Extended Kalman Filter for Legged Robot State Estimation",
        "author": [
            "Kyung-Hwan Kim",
            "DongHyun Ahn",
            "Dong-hyun Lee",
            "JuYoung Yoon",
            "Dong Jin Hyun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16755",
        "abstract": "State estimation is crucial for legged robots as it directly affects control performance and locomotion stability. In this paper, we propose an Adaptive Invariant Extended Kalman Filter to improve proprioceptive state estimation for legged robots. The proposed method adaptively adjusts the noise level of the contact foot model based on online covariance estimation, leading to improved state estimation under varying contact conditions. It effectively handles small slips that traditional slip rejection fails to address, as overly sensitive slip rejection settings risk causing filter divergence. Our approach employs a contact detection algorithm instead of contact sensors, reducing the reliance on additional hardware. The proposed method is validated through real-world experiments on the quadruped robot LeoQuad, demonstrating enhanced state estimation performance in dynamic locomotion scenarios.",
        "tags": [
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "195",
        "title": "End-to-end Listen, Look, Speak and Act",
        "author": [
            "Siyin Wang",
            "Wenyi Yu",
            "Xianzhao Chen",
            "Xiaohai Tian",
            "Jun Zhang",
            "Lu Lu",
            "Chao Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16756",
        "abstract": "Human interaction is inherently multimodal and full-duplex: we listen while watching, speak while acting, and fluidly adapt to turn-taking and interruptions. Realizing these capabilities is essential for building models simulating humans. We present ELLSA (End-to-end Listen, Look, Speak and Act), which, to our knowledge, is the first full-duplex, end-to-end model that simultaneously perceives and generates across vision, text, speech, and action within a single architecture, enabling interaction patterns previously out of reach, yielding more natural, human-like behaviors. At its core is a novel SA-MoE architecture (Self-Attention Mixture-of-Experts) that routes each modality to specialized experts and fuses them through a unified attention backbone. This provides a generalizable solution for joint multimodal perception and concurrent generation, leveraging strong pre-trained components while enabling efficient modality integration and mitigating modality interference. On speech-interaction and robot-manipulation benchmarks, ELLSA matches modality-specific baselines, while uniquely supporting advanced multimodal and full-duplex behaviors such as dialogue and action turn-taking, defective instruction rejection, speaking-while-acting, context-grounded visual question answering, and action barge-ins. We contend that ELLSA represents a step toward more natural and general interactive intelligence, contributing to the broader pursuit of artificial general intelligence. All data, code and model checkpoints will be released upon acceptance.",
        "tags": [
            "MoE",
            "Robotics"
        ]
    },
    {
        "id": "196",
        "title": "SAMOSA: Sharpness Aware Minimization for Open Set Active learning",
        "author": [
            "Young In Kim",
            "Andrea Agiollo",
            "Rajiv Khanna"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16757",
        "abstract": "Modern machine learning solutions require extensive data collection where labeling remains costly. To reduce this burden, open set active learning approaches aim to select informative samples from a large pool of unlabeled data that includes irrelevant or unknown classes. In this context, we propose Sharpness Aware Minimization for Open Set Active Learning (SAMOSA) as an effective querying algorithm. Building on theoretical findings concerning the impact of data typicality on the generalization properties of traditional stochastic gradient descent (SGD) and sharpness-aware minimization (SAM), SAMOSA actively queries samples based on their typicality. SAMOSA effectively identifies atypical samples that belong to regions of the embedding manifold close to the model decision boundaries. Therefore, SAMOSA prioritizes the samples that are (i) highly informative for the targeted classes, and (ii) useful for distinguishing between targeted and unwanted classes. Extensive experiments show that SAMOSA achieves up to 3% accuracy improvement over the state of the art across several datasets, while not introducing computational overhead. The source code of our experiments is available at: https://anonymous.4open.science/r/samosa-DAF4",
        "tags": [
            "SAM"
        ]
    },
    {
        "id": "197",
        "title": "Enhancing Language Agent Strategic Reasoning through Self-Play in Adversarial Games",
        "author": [
            "Yikai Zhang",
            "Ye Rong",
            "Siyu Yuan",
            "Jiangjie Chen",
            "Jian Xie",
            "Yanghua Xiao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16761",
        "abstract": "Existing language agents often encounter difficulties in dynamic adversarial games due to poor strategic reasoning. To mitigate this limitation, a promising approach is to allow agents to learn from game interactions automatically, without relying on costly expert-labeled data. Unlike static environments where agents receive fixed feedback or rewards, selecting appropriate opponents in dynamic adversarial games can significantly impact learning performance. However, the discussion of opponents in adversarial environments remains an area under exploration. In this paper, we propose a Step-level poliCy Optimization method through Play-And-Learn, SCO-PAL. Leveraging SCO-PAL, we conduct a detailed analysis of opponent selection by setting opponents at different levels and find that self-play is the most effective way to improve strategic reasoning in such adversarial environments. Utilizing SCO-PAL with self-play, we increase the average win rate against four opponents by approximately 30% compared to baselines and achieve a 54.76% win rate against GPT-4 in six adversarial games.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "198",
        "title": "WaMaIR: Image Restoration via Multiscale Wavelet Convolutions and Mamba-based Channel Modeling with Texture Enhancement",
        "author": [
            "Shengyu Zhu",
            "Fuxuan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16765",
        "abstract": "Image restoration is a fundamental and challenging task in computer vision, where CNN-based frameworks demonstrate significant computational efficiency. However, previous CNN-based methods often face challenges in adequately restoring fine texture details, which are limited by the small receptive field of CNN structures and the lack of channel feature modeling. In this paper, we propose WaMaIR, which is a novel framework with a large receptive field for image perception and improves the reconstruction of texture details in restored images. Specifically, we introduce the Global Multiscale Wavelet Transform Convolutions (GMWTConvs) for expandding the receptive field to extract image features, preserving and enriching texture features in model inputs. Meanwhile, we propose the Mamba-Based Channel-Aware Module (MCAM), explicitly designed to capture long-range dependencies within feature channels, which enhancing the model sensitivity to color, edges, and texture information. Additionally, we propose Multiscale Texture Enhancement Loss (MTELoss) for image restoration to guide the model in preserving detailed texture structures effectively. Extensive experiments confirm that WaMaIR outperforms state-of-the-art methods, achieving better image restoration and efficient computational performance of the model.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "199",
        "title": "T3 Planner: A Self-Correcting LLM Framework for Robotic Motion Planning with Temporal Logic",
        "author": [
            "Jia Li",
            "Guoxiang Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16767",
        "abstract": "Translating natural language instructions into executable motion plans is a fundamental challenge in robotics. Traditional approaches are typically constrained by their reliance on domain-specific expertise to customize planners, and often struggle with spatio-temporal couplings that usually lead to infeasible motions or discrepancies between task planning and motion execution. Despite the proficiency of Large Language Models (LLMs) in high-level semantic reasoning, hallucination could result in infeasible motion plans. In this paper, we introduce the T3 Planner, an LLM-enabled robotic motion planning framework that self-corrects it output with formal methods. The framework decomposes spatio-temporal task constraints via three cascaded modules, each of which stimulates an LLM to generate candidate trajectory sequences and examines their feasibility via a Signal Temporal Logic (STL) verifier until one that satisfies complex spatial, temporal, and logical constraints is http://found.Experiments across different scenarios show that T3 Planner significantly outperforms the baselines. The required reasoning can be distilled into a lightweight Qwen3-4B model that enables efficient deployment. All supplementary materials are accessible at https://github.com/leeejia/T3_Planner.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "200",
        "title": "See or Say Graphs: Agent-Driven Scalable Graph Understanding with Vision-Language Models",
        "author": [
            "Shuo Han",
            "Yukun Cao",
            "Zezhong Ding",
            "Zengyi Gao",
            "S Kevin Zhou",
            "Xike Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16769",
        "abstract": "Vision-language models (VLMs) have shown promise in graph understanding, but remain limited by input-token constraints, facing scalability bottlenecks and lacking effective mechanisms to coordinate textual and visual modalities. To address these challenges, we propose GraphVista, a unified framework that enhances both scalability and modality coordination in graph understanding. For scalability, GraphVista organizes graph information hierarchically into a lightweight GraphRAG base, which retrieves only task-relevant textual descriptions and high-resolution visual subgraphs, compressing redundant context while preserving key reasoning elements. For modality coordination, GraphVista introduces a planning agent that routes tasks to the most suitable modality-using the text modality for simple property reasoning and the visual modality for local and structurally complex reasoning grounded in explicit topology. Extensive experiments demonstrate that GraphVista scales to large graphs, up to $200\\times$ larger than those used in existing benchmarks, and consistently outperforms existing textual, visual, and fusion-based methods, achieving up to $4.4\\times$ quality improvement over the state-of-the-art baselines by fully exploiting the complementary strengths of both modalities.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "201",
        "title": "Region in Context: Text-condition Image editing with Human-like semantic reasoning",
        "author": [
            "Thuy Phuong Vu",
            "Dinh-Cuong Hoang",
            "Minhhuy Le",
            "Phan Xuan Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16772",
        "abstract": "Recent research has made significant progress in localizing and editing image regions based on text. However, most approaches treat these regions in isolation, relying solely on local cues without accounting for how each part contributes to the overall visual and semantic composition. This often results in inconsistent edits, unnatural transitions, or loss of coherence across the image. In this work, we propose Region in Context, a novel framework for text-conditioned image editing that performs multilevel semantic alignment between vision and language, inspired by the human ability to reason about edits in relation to the whole scene. Our method encourages each region to understand its role within the global image context, enabling precise and harmonized changes. At its core, the framework introduces a dual-level guidance mechanism: regions are represented with full-image context and aligned with detailed region-level descriptions, while the entire image is simultaneously matched to a comprehensive scene-level description generated by a large vision-language model. These descriptions serve as explicit verbal references of the intended content, guiding both local modifications and global structure. Experiments show that it produces more coherent and instruction-aligned results. Code is available at: https://github.com/thuyvuphuong/Region-in-Context.git",
        "tags": [
            "Image Editing"
        ]
    },
    {
        "id": "202",
        "title": "GS2POSE: Marry Gaussian Splatting to 6D Object Pose Estimation",
        "author": [
            "Junbo Li",
            "Weimin Yuan",
            "Yinuo Wang",
            "Yue Zeng",
            "Shihao Shu",
            "Cai Meng",
            "Xiangzhi Bai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16777",
        "abstract": "Accurate 6D pose estimation of 3D objects is a fundamental task in computer vision, and current research typically predicts the 6D pose by establishing correspondences between 2D image features and 3D model features. However, these methods often face difficulties with textureless objects and varying illumination conditions. To overcome these limitations, we propose GS2POSE, a novel approach for 6D object pose estimation. GS2POSE formulates a pose regression algorithm inspired by the principles of Bundle Adjustment (BA). By leveraging Lie algebra, we extend the capabilities of 3DGS to develop a pose-differentiable rendering pipeline, which iteratively optimizes the pose by comparing the input image to the rendered image. Additionally, GS2POSE updates color parameters within the 3DGS model, enhancing its adaptability to changes in illumination. Compared to previous models, GS2POSE demonstrates accuracy improvements of 1.4\\%, 2.8\\% and 2.5\\% on the T-LESS, LineMod-Occlusion and LineMod datasets, respectively.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Pose Estimation"
        ]
    },
    {
        "id": "203",
        "title": "QuanBench: Benchmarking Quantum Code Generation with Large Language Models",
        "author": [
            "Xiaoyu Guo",
            "Minggu Wang",
            "Jianjun Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16779",
        "abstract": "Large language models (LLMs) have demonstrated good performance in general code generation; however, their capabilities in quantum code generation remain insufficiently studied. This paper presents QuanBench, a benchmark for evaluating LLMs on quantum code generation. QuanBench includes 44 programming tasks that cover quantum algorithms, state preparation, gate decomposition, and quantum machine learning. Each task has an executable canonical solution and is evaluated by functional correctness (Pass@K) and quantum semantic equivalence (Process Fidelity). We evaluate several recent LLMs, including general-purpose and code-specialized models. The results show that current LLMs have limited capability in generating the correct quantum code, with overall accuracy below 40% and frequent semantic errors. We also analyze common failure cases, such as outdated API usage, circuit construction errors, and incorrect algorithm logic. QuanBench provides a basis for future work on improving quantum code generation with LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "204",
        "title": "Xiaoice: Training-Free Video Understanding via Self-Supervised Spatio-Temporal Clustering of Semantic Features",
        "author": [
            "Shihao Ji",
            "Zihui Song"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16781",
        "abstract": "The remarkable zero-shot reasoning capabilities of large-scale Visual Language Models (VLMs) on static images have yet to be fully translated to the video domain. Conventional video understanding models often rely on extensive, task-specific training on annotated datasets, a process that is both costly and limited in scalability. This paper introduces a novel, training-free framework for video understanding that circumvents end-to-end training by synergistically combining the rich semantic priors of pre-trained VLMs with classic machine learning algorithms for pattern discovery. Our core idea is to reframe video understanding as a self-supervised spatio-temporal clustering problem within a high-dimensional semantic feature space. The proposed pipeline first transforms a video stream into a semantic feature trajectory using the frozen visual encoder of a pre-trained VLM. Subsequently, we employ Kernel Temporal Segmentation (KTS), a robust machine learning technique, to partition the continuous feature stream into discrete, semantically coherent event segments. These segments are then subjected to unsupervised density-based clustering to identify recurring macroscopic scenes and themes throughout the video. By selecting representative keyframes from each discovered cluster and leveraging the VLM's generative capabilities for textual description, our framework automatically produces a structured, multi-modal summary of the video content. This approach provides an effective, interpretable, and model-agnostic pathway for zero-shot, automated structural analysis of video content.",
        "tags": [
            "Segmentation",
            "VLM"
        ]
    },
    {
        "id": "205",
        "title": "LC-Eval: A Bilingual Multi-Task Evaluation Benchmark for Long-Context Understanding",
        "author": [
            "Sheikh Jubair",
            "Arwa Omayrah",
            "Amal Alshammari",
            "Alhanoof Althnian",
            "Abdulhamed Alothaimen",
            "Norah A. Alzahrani",
            "Shahad D. Alzaidi",
            "Nora Al-Twairesh",
            "Abdulmohsen Al-Thubaity"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16783",
        "abstract": "Recent advancements in Large Language Models (LLMs) have demonstrated sophisticated capabilities, including the ability to process and comprehend extended contexts. These emergent capabilities necessitate rigorous evaluation methods to effectively assess their performance in long-context understanding. In this paper, we present \\textbf{LC-Eval}, a bilingual, multi-task evaluation benchmark designed to evaluate long-context understanding in English and Arabic, targeting context lengths ranging from 4k to over 128k tokens. LC-Eval introduces four novel and challenging tasks: multi-document question answering, bilingual question answering, claim verification within a paragraph, and multiple-choice questions based on long contexts. These tasks are designed to assess LLMs' abilities in deep reasoning, document comprehension, information tracing, and bilingual information extraction and understanding. The benchmark includes datasets in both Arabic and English for each task, allowing for a comparative analysis of their performance across different text genres. Evaluations were conducted on both open-weight and closed LLMs, with results indicating that LC-Eval presents significant challenges. Even high-performing models, such as GPT-4o, struggled with certain tasks, highlighting the complexity and rigor of the benchmark.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "206",
        "title": "Segmentation as A Plug-and-Play Capability for Frozen Multimodal LLMs",
        "author": [
            "Jiazhen Liu",
            "Long Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16785",
        "abstract": "Integrating diverse visual capabilities into a unified model is a significant trend in Multimodal Large Language Models (MLLMs). Among these, the inclusion of segmentation poses a distinct set of challenges. To equip MLLMs with pixel-level segmentation abilities, prevailing methods require finetuning the model to produce specific outputs compatible with a mask decoder. This process typically alters the model's output space and compromises its intrinsic generalization, which undermines the goal of building a unified model. We introduce LENS (Leveraging kEypoiNts for MLLMs' Segmentation), a novel plug-and-play solution. LENS attaches a lightweight, trainable head to a completely frozen MLLM. By refining the spatial cues embedded in attention maps, LENS extracts keypoints and describes them into point-wise features directly compatible with the mask decoder. Extensive experiments validate our approach: LENS achieves segmentation performance competitive with or superior to that of retraining-based methods. Crucially, it does so while fully preserving the MLLM's generalization capabilities, which are significantly degraded by finetuning approaches. As such, the attachable design of LENS establishes an efficient and powerful paradigm for extending MLLMs, paving the way for truly multi-talented, unified models.",
        "tags": [
            "LLM",
            "Segmentation"
        ]
    },
    {
        "id": "207",
        "title": "More with Less: An Empirical Study of Turn-Control Strategies for Efficient Coding Agents",
        "author": [
            "Pengfei Gao",
            "Chao Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16786",
        "abstract": "LLM-powered coding agents, which operate in iterative loops (turns) to solve software engineering tasks, are becoming increasingly powerful. However, their practical deployment is hindered by significant and unpredictable costs. This challenge arises from a combination of factors: quadratically growing token counts with each turn, the high price of models, the large number of turns required for real-world tasks, and the tendency of agents to take inefficient or unnecessary actions. While existing research focuses on optimizing individual turns, the strategic control of the total number of turns remains an underexplored area for managing agent performance and cost. To address this gap, we conduct a comprehensive empirical study on SWE-bench using three state-of-the-art models and evaluate the impact of three distinct turn-control strategies: an unrestricted baseline, a fixed-turn limit with reminders, and a novel dynamic-turn strategy that grants extensions on-demand. Our findings first reveal a fundamental trade-off in the unrestricted setting, where no single model excels across performance, cost, and turn efficiency. We then show that a fixed-turn limit, specifically at the 75th percentile of the baseline, serves as a \"sweet spot\", substantially reducing costs (by 24%-68%) with minimal impact on solve rates. Most significantly, the dynamic-turn strategy consistently outperforms fixed-limit approaches, achieving comparable or better solve rates while further reducing costs by an additional 12%-24% by intelligently allocating resources only to tasks that need them. This work provides the first systematic analysis of turn-control strategies, offering simple yet effective guidelines for developers to balance cost and efficacy. We demonstrate that dynamic resource allocation is a superior, easy-to-implement approach for deploying powerful yet economically viable coding agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "208",
        "title": "Personalized Image Filter: Mastering Your Photographic Style",
        "author": [
            "Chengxuan Zhu",
            "Shuchen Weng",
            "Jiacong Fang",
            "Peixuan Zhang",
            "Si Li",
            "Chao Xu",
            "Boxin Shi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16791",
        "abstract": "Photographic style, as a composition of certain photographic concepts, is the charm behind renowned photographers. But learning and transferring photographic style need a profound understanding of how the photo is edited from the unknown original appearance. Previous works either fail to learn meaningful photographic concepts from reference images, or cannot preserve the content of the content image. To tackle these issues, we proposed a Personalized Image Filter (PIF). Based on a pretrained text-to-image diffusion model, the generative prior enables PIF to learn the average appearance of photographic concepts, as well as how to adjust them according to text prompts. PIF then learns the photographic style of reference images with the textual inversion technique, by optimizing the prompts for the photographic concepts. PIF shows outstanding performance in extracting and transferring various kinds of photographic style. Project page: https://pif.pages.dev/",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "209",
        "title": "Black-box Optimization of LLM Outputs by Asking for Directions",
        "author": [
            "Jie Zhang",
            "Meng Ding",
            "Yang Liu",
            "Jue Hong",
            "Florian TramÃ¨r"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16794",
        "abstract": "We present a novel approach for attacking black-box large language models (LLMs) by exploiting their ability to express confidence in natural language. Existing black-box attacks require either access to continuous model outputs like logits or confidence scores (which are rarely available in practice), or rely on proxy signals from other models. Instead, we demonstrate how to prompt LLMs to express their internal confidence in a way that is sufficiently calibrated to enable effective adversarial optimization. We apply our general method to three attack scenarios: adversarial examples for vision-LLMs, jailbreaks and prompt injections. Our attacks successfully generate malicious inputs against systems that only expose textual outputs, thereby dramatically expanding the attack surface for deployed LLMs. We further find that better and larger models exhibit superior calibration when expressing confidence, creating a concerning security paradox where model capability improvements directly enhance vulnerability. Our code is available at this [link](https://github.com/zj-jayzhang/black_box_llm_optimization).",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "210",
        "title": "Strong error analysis and first-order convergence of Milstein-type schemes for McKean-Vlasov SDEs with superlinear coefficients",
        "author": [
            "Jingtao Zhu",
            "Yuying Zhao",
            "Siqing Gan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16801",
        "abstract": "In the study of McKean-Vlasov stochastic differential equations (MV-SDEs), numerical approximation plays a crucial role in understanding the behavior of interacting particle systems (IPS). Classical Milstein schemes provide strong convergence of order one under globally Lipschitz coefficients. Nevertheless, many MV-SDEs arising from applications possess super-linearly growing drift and diffusion terms, where classical methods may diverge and particle corruption can occur. In the present work, we aim to fill this gap by developing a unified class of Milstein-type discretizations handling both super-linear drift and diffusion coefficients. The proposed framework includes the tamed-, tanh-, and sine-Milstein methods as special cases and establishes order-one strong convergence for the associated interacting particle system under mild regularity assumptions, requiring only once differentiable coefficients. In particular, our results complement Chen et al. (Electron. J. Probab., 2025), where a taming-based Euler scheme was only tested numerically without theoretical guarantees, by providing a rigorous convergence theory within a broader Milstein-type framework. The analysis relies on discrete-time arguments and binomial-type expansions, avoiding the continuous-time ItÃ´ approach that is standard in the literature. Numerical experiments are presented to illustrate the convergence behavior and support the theoretical findings.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "211",
        "title": "An Efficient Framework for Whole-Page Reranking via Single-Modal Supervision",
        "author": [
            "Zishuai Zhang",
            "Sihao Yu",
            "Wenyi Xie",
            "Ying Nie",
            "Junfeng Wang",
            "Zhiming Zheng",
            "Dawei Yin",
            "Hainan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16803",
        "abstract": "The whole-page reranking plays a critical role in shaping the user experience of search engines, which integrates retrieval results from multiple modalities, such as documents, images, videos, and LLM outputs. Existing methods mainly rely on large-scale human-annotated data, which is costly to obtain and time-consuming. This is because whole-page annotation is far more complex than single-modal: it requires assessing the entire result page while accounting for cross-modal relevance differences. Thus, how to improve whole-page reranking performance while reducing annotation costs is still a key challenge in optimizing search engine result pages(SERP). In this paper, we propose SMAR, a novel whole-page reranking framework that leverages strong Single-modal rankers to guide Modal-wise relevance Alignment for effective Reranking, using only limited whole-page annotation to outperform fully-annotated reranking models. Specifically, high-quality single-modal rankers are first trained on data specific to their respective modalities. Then, for each query, we select a subset of their outputs to construct candidate pages and perform human annotation at the page level. Finally, we train the whole-page reranker using these limited annotations and enforcing consistency with single-modal preferences to maintain ranking quality within each modality. Experiments on the Qilin and Baidu datasets demonstrate that SMAR reduces annotation costs by about 70-90\\% while achieving significant ranking improvements compared to baselines. Further offline and online A/B testing on Baidu APPs also shows notable gains in standard ranking metrics as well as user experience indicators, fully validating the effectiveness and practical value of our approach in real-world search scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "212",
        "title": "Mixed-Precision Quantization for Language Models: Techniques and Prospects",
        "author": [
            "Mariam Rakka",
            "Marios Fournarakis",
            "Olga Krestinskaya",
            "Jinane Bazzi",
            "Khaled N. Salama",
            "Fadi Kurdahi",
            "Ahmed M. Eltawil",
            "Mohammed E. Fouda"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16805",
        "abstract": "The rapid scaling of language models (LMs) has resulted in unprecedented computational, memory, and energy requirements, making their training and deployment increasingly unsustainable. Quantization has emerged as an essential compression technique to reduce model size, alleviate memory bottlenecks, and accelerate inference. However, while uniform low-bit quantization (e.g., INT8, INT4) provides significant efficiency gains, it can degrade accuracy in sensitive components of transformer-based LMs. Mixed-precision quantization offers a promising alternative by selectively allocating precision across layers or within tensors to balance efficiency and accuracy. This survey provides a comprehensive overview of Mixed-Precision quantization frameworks for LMs (MXPLMs). We first review quantization fundamentals, including uniform and non-uniform quantizers, quantization granularity, and methods widely used in post-training quantization. We then categorize and compare recent MXPLM frameworks according to their bit allocation strategies and precision configurations across weights, activations, and key-value caches. A comparative analysis highlights differences in perplexity, zero-shot task performance, and deployment trade-offs. Furthermore, we contrast MXPLMs with earlier mixed-precision quantization methods for deep neural networks, identifying strategies that transfer and those that face challenges in the LM setting. Finally, we summarize open issues and future directions, including hardware-aware design, activation quantization, and scalable optimization methods for billion-parameter models. By consolidating recent advances, this work serves as a reference for understanding the current landscape and research prospects of mixed-precision quantization for large-scale language models.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "213",
        "title": "Improving Model Representation and Reducing KV Cache via Skip Connections with First Value Heads",
        "author": [
            "Zhoutong Wu",
            "Yuan Zhang",
            "Yiming Dong",
            "Chenheng Zhang",
            "Cong Fang",
            "Kun Yuan",
            "Zhouchen Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16807",
        "abstract": "Transformer models have driven breakthroughs across various language tasks by their strong capability to learn rich contextual representations. Scaling them to improve representation, however, often demands substantial memory and compute costs, such as the Key-Value (KV) cache used during auto-regressive decoding. Skip connections offer a promising way to improve representation without bloating resource usage, yet most prior works either improve expressivity while leaving KV costs unchanged, or reduce memory at the cost of weaker representation. In this work, we propose SkipV1Former, a Transformer variant that uses skip connections from the first layer's Value heads to strengthen model representation and reduce KV cache. Specifically, from the second block onward, each layer reuses half of its Value heads from the very first layer, while computing the other half as usual-cutting Value projections and V cache by nearly 50 \\%. Theoretically, we show that routing uncompressed first-layer Values into deeper layers restores information lost to compression and accelerates the model's implicit mesa-optimization-a key pattern of Transformer in auto-regressive tasks. Empirically, across different model scales, SkipV1Former delivers consistent reductions of approximately 25 \\% in KV cache while improving perplexity relative to standard Multi-Head Attention (MHA) Transformers and some advanced variants. Moreover, we propose a recipe for uptraining existing MHA Transformer checkpoints to SkipV1Former with only 10-15\\% additional compute. Finally, SkipV1Former can seamlessly combine advanced methods like Group-Query Attention and Multi-Latent Attention to achieve further KV cache savings and performance improvement. When combined with YOCO, it cuts KV cache size by nearly 50 \\% while still improving performance.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "214",
        "title": "When Many-Shot Prompting Fails: An Empirical Study of LLM Code Translation",
        "author": [
            "Amirkia Rafiei Oskooei",
            "Kaan Baturalp Cosdan",
            "Husamettin Isiktas",
            "Mehmet S. Aktas"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16809",
        "abstract": "Large Language Models (LLMs) with vast context windows offer new avenues for in-context learning (ICL), where providing many examples (\"many-shot\" prompting) is often assumed to enhance performance. We investigate this assumption for the complex task of code translation. Through a large-scale empirical study of over 90,000 translations, we systematically evaluate the impact of scaling in-context examples from zero-shot to many-shot configurations of up to 625 examples, with prompts spanning from approximately 100,000 to 800,000 tokens. Our findings reveal a \"many-shot paradox\": while static similarity metrics may modestly improve with more examples, functional correctness consistently peaks with few-shot prompting (5-25 examples). Providing substantially more examples often degrades this crucial functional performance. This study highlights that for code translation, the quality of a few well-chosen examples outweighs sheer quantity, challenging the universal efficacy of \"more is better\" for ICL and underscoring the task-dependent nature of optimal prompting strategies. Our results have significant implications for effectively leveraging LLMs in software engineering.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "215",
        "title": "Needles in the Landscape: Semi-Supervised Pseudolabeling for Archaeological Site Discovery under Label Scarcity",
        "author": [
            "Simon Jaxy",
            "Anton Theys",
            "Patrick Willett",
            "W. Chris Carleton",
            "Ralf Vandam",
            "Pieter Libin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16814",
        "abstract": "Archaeological predictive modelling estimates where undiscovered sites are likely to occur by combining known locations with environmental, cultural, and geospatial variables. We address this challenge using a deep learning approach but must contend with structural label scarcity inherent to archaeology: positives are rare, and most locations are unlabeled. To address this, we adopt a semi-supervised, positive-unlabeled (PU) learning strategy, implemented as a semantic segmentation model and evaluated on two datasets covering a representative range of archaeological periods. Our approach employs dynamic pseudolabeling, refined with a Conditional Random Field (CRF) implemented via an RNN, increasing label confidence under severe class imbalance. On a geospatial dataset derived from a digital elevation model (DEM), our model performs on par with the state-of-the-art, LAMAP, while achieving higher Dice scores. On raw satellite imagery, assessed end-to-end with stratified k-fold cross-validation, it maintains performance and yields predictive surfaces with improved interpretability. Overall, our results indicate that semi-supervised learning offers a promising approach to identifying undiscovered sites across large, sparsely annotated landscapes.",
        "tags": [
            "RNN",
            "Segmentation"
        ]
    },
    {
        "id": "216",
        "title": "Knowing the Facts but Choosing the Shortcut: Understanding How Large Language Models Compare Entities",
        "author": [
            "Hans Hergen Lehmann",
            "Jae Hee Lee",
            "Steven Schockaert",
            "Stefan Wermter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16815",
        "abstract": "Large Language Models (LLMs) are increasingly used for knowledge-based reasoning tasks, yet understanding when they rely on genuine knowledge versus superficial heuristics remains challenging. We investigate this question through entity comparison tasks by asking models to compare entities along numerical attributes (e.g., ``Which river is longer, the Danube or the Nile?''), which offer clear ground truth for systematic analysis. Despite having sufficient numerical knowledge to answer correctly, LLMs frequently make predictions that contradict this knowledge. We identify three heuristic biases that strongly influence model predictions: entity popularity, mention order, and semantic co-occurrence. For smaller models, a simple logistic regression using only these surface cues predicts model choices more accurately than the model's own numerical predictions, suggesting heuristics largely override principled reasoning. Crucially, we find that larger models (32B parameters) selectively rely on numerical knowledge when it is more reliable, while smaller models (7--8B parameters) show no such discrimination, which explains why larger models outperform smaller ones even when the smaller models possess more accurate knowledge. Chain-of-thought prompting steers all models towards using the numerical features across all model sizes.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "217",
        "title": "Efficient High-Accuracy PDEs Solver with the Linear Attention Neural Operator",
        "author": [
            "Ming Zhong",
            "Zhenya Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16816",
        "abstract": "Neural operators offer a powerful data-driven framework for learning mappings between function spaces, in which the transformer-based neural operator architecture faces a fundamental scalability-accuracy trade-off: softmax attention provides excellent fidelity but incurs quadratic complexity $\\mathcal{O}(N^2 d)$ in the number of mesh points $N$ and hidden dimension $d$, while linear attention variants reduce cost to $\\mathcal{O}(N d^2)$ but often suffer significant accuracy degradation. To address the aforementioned challenge, in this paper, we present a novel type of neural operators, Linear Attention Neural Operator (LANO), which achieves both scalability and high accuracy by reformulating attention through an agent-based mechanism. LANO resolves this dilemma by introducing a compact set of $M$ agent tokens $(M \\ll N)$ that mediate global interactions among $N$ tokens. This agent attention mechanism yields an operator layer with linear complexity $\\mathcal{O}(MN d)$ while preserving the expressive power of softmax attention. Theoretically, we demonstrate the universal approximation property, thereby demonstrating improved conditioning and stability properties. Empirically, LANO surpasses current state-of-the-art neural PDE solvers, including Transolver with slice-based softmax attention, achieving average $19.5\\%$ accuracy improvement across standard benchmarks. By bridging the gap between linear complexity and softmax-level performance, LANO establishes a scalable, high-accuracy foundation for scientific machine learning applications.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "218",
        "title": "Cross-Genre Authorship Attribution via LLM-Based Retrieve-and-Rerank",
        "author": [
            "Shantanu Agarwal",
            "Joel Barry",
            "Steven Fincke",
            "Scott Miller"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16819",
        "abstract": "Authorship attribution (AA) is the task of identifying the most likely author of a query document from a predefined set of candidate authors. We introduce a two-stage retrieve-and-rerank framework that finetunes LLMs for cross-genre AA. Unlike the field of information retrieval (IR), where retrieve-and-rerank is a de facto strategy, cross-genre AA systems must avoid relying on topical cues and instead learn to identify author-specific linguistic patterns that are independent of the text's subject matter (genre/domain/topic). Consequently, for the reranker, we demonstrate that training strategies commonly used in IR are fundamentally misaligned with cross-genre AA, leading to suboptimal behavior. To address this, we introduce a targeted data curation strategy that enables the reranker to effectively learn author-discriminative signals. Using our LLM-based retrieve-and-rerank pipeline, we achieve substantial gains of 22.3 and 34.4 absolute Success@8 points over the previous state-of-the-art on HIATUS's challenging HRS1 and HRS2 cross-genre AA benchmarks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "219",
        "title": "When AI Takes the Wheel: Security Analysis of Framework-Constrained Program Generation",
        "author": [
            "Yue Liu",
            "Zhenchang Xing",
            "Shidong Pan",
            "Chakkrit Tantithamthavorn"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16823",
        "abstract": "In recent years, the AI wave has grown rapidly in software development. Even novice developers can now design and generate complex framework-constrained software systems based on their high-level requirements with the help of Large Language Models (LLMs). However, when LLMs gradually \"take the wheel\" of software development, developers may only check whether the program works. They often miss security problems hidden in how the generated programs are implemented.\nIn this work, we investigate the security properties of framework-constrained programs generated by state-of-the-art LLMs. We focus specifically on Chrome extensions due to their complex security model involving multiple privilege boundaries and isolated components. To achieve this, we built ChromeSecBench, a dataset with 140 prompts based on known vulnerable extensions. We used these prompts to instruct nine state-of-the-art LLMs to generate complete Chrome extensions, and then analyzed them for vulnerabilities across three dimensions: scenario types, model differences, and vulnerability categories. Our results show that LLMs produced vulnerable programs at alarmingly high rates (18%-50%), particularly in Authentication & Identity and Cookie Management scenarios (up to 83% and 78% respectively). Most vulnerabilities exposed sensitive browser data like cookies, history, or bookmarks to untrusted code. Interestingly, we found that advanced reasoning models performed worse, generating more vulnerabilities than simpler models. These findings highlight a critical gap between LLMs' coding skills and their ability to write secure framework-constrained programs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "220",
        "title": "Who's Asking? Simulating Role-Based Questions for Conversational AI Evaluation",
        "author": [
            "Navreet Kaur",
            "Hoda Ayad",
            "Hayoung Jung",
            "Shravika Mittal",
            "Munmun De Choudhury",
            "Tanushree Mitra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16829",
        "abstract": "Language model users often embed personal and social context in their questions. The asker's role -- implicit in how the question is framed -- creates specific needs for an appropriate response. However, most evaluations, while capturing the model's capability to respond, often ignore who is asking. This gap is especially critical in stigmatized domains such as opioid use disorder (OUD), where accounting for users' contexts is essential to provide accessible, stigma-free responses. We propose CoRUS (COmmunity-driven Roles for User-centric Question Simulation), a framework for simulating role-based questions. Drawing on role theory and posts from an online OUD recovery community (r/OpiatesRecovery), we first build a taxonomy of asker roles -- patients, caregivers, practitioners. Next, we use it to simulate 15,321 questions that embed each role's goals, behaviors, and experiences. Our evaluations show that these questions are both highly believable and comparable to real-world data. When used to evaluate five LLMs, for the same question but differing roles, we find systematic differences: vulnerable roles, such as patients and caregivers, elicit more supportive responses (+17%) and reduced knowledge content (-19%) in comparison to practitioners. Our work demonstrates how implicitly signaling a user's role shapes model responses, and provides a methodology for role-informed evaluation of conversational AI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "221",
        "title": "Verifiable Fine-Tuning for LLMs: Zero-Knowledge Training Proofs Bound to Data Provenance and Policy",
        "author": [
            "Hasan Akgul",
            "Daniel Borg",
            "Arta Berisha",
            "Amina Rahimova",
            "Andrej Novak",
            "Mila Petrov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16830",
        "abstract": "Large language models are often adapted through parameter efficient fine tuning, but current release practices provide weak assurances about what data were used and how updates were computed. We present Verifiable Fine Tuning, a protocol and system that produces succinct zero knowledge proofs that a released model was obtained from a public initialization under a declared training program and an auditable dataset commitment. The approach combines five elements. First, commitments that bind data sources, preprocessing, licenses, and per epoch quota counters to a manifest. Second, a verifiable sampler that supports public replayable and private index hiding batch selection. Third, update circuits restricted to parameter efficient fine tuning that enforce AdamW style optimizer semantics and proof friendly approximations with explicit error budgets. Fourth, recursive aggregation that folds per step proofs into per epoch and end to end certificates with millisecond verification. Fifth, provenance binding and optional trusted execution property cards that attest code identity and constants. On English and bilingual instruction mixtures, the method maintains utility within tight budgets while achieving practical proof performance. Policy quotas are enforced with zero violations, and private sampling windows show no measurable index leakage. Federated experiments demonstrate that the system composes with probabilistic audits and bandwidth constraints. These results indicate that end to end verifiable fine tuning is feasible today for real parameter efficient pipelines, closing a critical trust gap for regulated and decentralized deployments.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "222",
        "title": "From Mannequin to Human: A Pose-Aware and Identity-Preserving Video Generation Framework for Lifelike Clothing Display",
        "author": [
            "Xiangyu Mu",
            "Dongliang Zhou",
            "Jie Hou",
            "Haijun Zhang",
            "Weili Guan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16833",
        "abstract": "Mannequin-based clothing displays offer a cost-effective alternative to real-model showcases for online fashion presentation, but lack realism and expressive detail. To overcome this limitation, we introduce a new task called mannequin-to-human (M2H) video generation, which aims to synthesize identity-controllable, photorealistic human videos from footage of mannequins. We propose M2HVideo, a pose-aware and identity-preserving video generation framework that addresses two key challenges: the misalignment between head and body motion, and identity drift caused by temporal modeling. In particular, M2HVideo incorporates a dynamic pose-aware head encoder that fuses facial semantics with body pose to produce consistent identity embeddings across frames. To address the loss of fine facial details due to latent space compression, we introduce a mirror loss applied in pixel space through a denoising diffusion implicit model (DDIM)-based one-step denoising. Additionally, we design a distribution-aware adapter that aligns statistical distributions of identity and clothing features to enhance temporal coherence. Extensive experiments on the UBC fashion dataset, our self-constructed ASOS dataset, and the newly collected MannequinVideos dataset captured on-site demonstrate that M2HVideo achieves superior performance in terms of clothing consistency, identity preservation, and video fidelity in comparison to state-of-the-art methods.",
        "tags": [
            "DDIM",
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": "223",
        "title": "SchrÃ¶dinger Bridge Mamba for One-Step Speech Enhancement",
        "author": [
            "Jing Yang",
            "Sirui Wang",
            "Chao Wu",
            "Fan Fan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16834",
        "abstract": "We propose SchrÃ¶dinger Bridge Mamba (SBM), a new concept of training-inference framework motivated by the inherent compatibility between SchrÃ¶dinger Bridge (SB) training paradigm and selective state-space model Mamba. We exemplify the concept of SBM with an implementation for generative speech enhancement. Experiments on a joint denoising and dereverberation task using four benchmark datasets demonstrate that SBM, with only 1-step inference, outperforms strong baselines with 1-step or iterative inference and achieves the best real-time factor (RTF). Beyond speech enhancement, we discuss the integration of SB paradigm and selective state-space model architecture based on their underlying alignment, which indicates a promising direction for exploring new deep generative models potentially applicable to a broad range of generative tasks. Demo page: https://sbmse.github.io",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "224",
        "title": "2DGS-R: Revisiting the Normal Consistency Regularization in 2D Gaussian Splatting",
        "author": [
            "Haofan Ren",
            "Qingsong Yan",
            "Ming Lu",
            "Rongfeng Lu",
            "Zunjie Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16837",
        "abstract": "Recent advancements in 3D Gaussian Splatting (3DGS) have greatly influenced neural fields, as it enables high-fidelity rendering with impressive visual quality. However, 3DGS has difficulty accurately representing surfaces. In contrast, 2DGS transforms the 3D volume into a collection of 2D planar Gaussian disks. Despite advancements in geometric fidelity, rendering quality remains compromised, highlighting the challenge of achieving both high-quality rendering and precise geometric structures. This indicates that optimizing both geometric and rendering quality in a single training stage is currently unfeasible. To overcome this limitation, we present 2DGS-R, a new method that uses a hierarchical training approach to improve rendering quality while maintaining geometric accuracy. 2DGS-R first trains the original 2D Gaussians with the normal consistency regularization. Then 2DGS-R selects the 2D Gaussians with inadequate rendering quality and applies a novel in-place cloning operation to enhance the 2D Gaussians. Finally, we fine-tune the 2DGS-R model with opacity frozen. Experimental results show that compared to the original 2DGS, our method requires only 1\\% more storage and minimal additional training time. Despite this negligible overhead, it achieves high-quality rendering results while preserving fine geometric structures. These findings indicate that our approach effectively balances efficiency with performance, leading to improvements in both visual fidelity and geometric reconstruction accuracy.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "225",
        "title": "DiRAC - Distributed Robot Awareness and Consensus",
        "author": [
            "Uday Gopan",
            "Manjari Kulkarni",
            "Lakshasri S",
            "Kashish Mittal",
            "Sriram Radhakrishna",
            "Aditya Naskar",
            "Rameshwar DL"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16850",
        "abstract": "DiRAC is a scalable, distributed framework designed to enable efficient task assignment and path planning in very large robotic swarms. It introduces a novel zone-partitioned architecture with dynamically elected leaders and a tick-synchronized consensus protocol that yields strong consistency and deterministic outcomes. For path planning, DiRAC uses a novel algorithm, a force-based decentralized planner for real-time collision resolution. Validated within ROS 2 middleware through preliminary simulation, DiRAC demonstrates architectural scalability and modular efficiency in simulated warehouse environments, laying the groundwork for real-world deployment in large-scale industrial and logistics domains.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "226",
        "title": "Neuronal Group Communication for Efficient Neural representation",
        "author": [
            "Zhengqi Pei",
            "Qingming Huang",
            "Shuhui Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16851",
        "abstract": "The ever-increasing scale of modern neural networks has brought unprecedented performance alongside daunting challenges in efficiency and interpretability. This paper addresses the core question of how to build large neural systems that learn efficient, modular, and interpretable representations. We propose Neuronal Group Communication (NGC), a theory-driven framework that reimagines a neural network as a dynamical system of interacting neuronal groups rather than a monolithic collection of neural weights. Instead of treating each weight as an independent trainable parameter, NGC treats weights as transient interactions between embedding-like neuronal states, with neural computation unfolding through iterative communication among groups of neurons. This low-rank, modular representation yields compact models: groups of neurons exchange low-dimensional signals, enabling intra-group specialization and inter-group information sharing while dramatically reducing redundant parameters. By drawing on dynamical systems theory, we introduce a neuronal stability metric (analogous to Lyapunov stability) that quantifies the contraction of neuron activations toward stable patterns during sequence processing. Using this metric, we reveal that emergent reasoning capabilities correspond to an external driving force or ``potential'', which nudges the neural dynamics away from trivial trajectories while preserving stability. Empirically, we instantiate NGC in large language models (LLMs) and demonstrate improved performance on complex reasoning benchmarks under moderate compression. NGC consistently outperforms standard low-rank approximations and cross-layer basis-sharing methods at comparable compression rates. We conclude by discussing the broader implications of NGC, including how structured neuronal group dynamics might relate to generalization in high-dimensional learning systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "227",
        "title": "ArmFormer: Lightweight Transformer Architecture for Real-Time Multi-Class Weapon Segmentation and Classification",
        "author": [
            "Akhila Kambhatla",
            "Taminul Islam",
            "Khaled R Ahmed"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16854",
        "abstract": "The escalating threat of weapon-related violence necessitates automated detection systems capable of pixel-level precision for accurate threat assessment in real-time security applications. Traditional weapon detection approaches rely on object detection frameworks that provide only coarse bounding box localizations, lacking the fine-grained segmentation required for comprehensive threat analysis. Furthermore, existing semantic segmentation models either sacrifice accuracy for computational efficiency or require excessive computational resources incompatible with edge deployment scenarios. This paper presents ArmFormer, a lightweight transformer-based semantic segmentation framework that strategically integrates Convolutional Block Attention Module (CBAM) with MixVisionTransformer architecture to achieve superior accuracy while maintaining computational efficiency suitable for resource-constrained edge devices. Our approach combines CBAM-enhanced encoder backbone with attention-integrated hamburger decoder to enable multi-class weapon segmentation across five categories: handgun, rifle, knife, revolver, and human. Comprehensive experiments demonstrate that ArmFormer achieves state-of-the-art performance with 80.64% mIoU and 89.13% mFscore while maintaining real-time inference at 82.26 FPS. With only 4.886G FLOPs and 3.66M parameters, ArmFormer outperforms heavyweight models requiring up to 48x more computation, establishing it as the optimal solution for deployment on portable security cameras, surveillance drones, and embedded AI accelerators in distributed security infrastructure.",
        "tags": [
            "Detection",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "228",
        "title": "DeepAnalyze: Agentic Large Language Models for Autonomous Data Science",
        "author": [
            "Shaolei Zhang",
            "Ju Fan",
            "Meihao Fan",
            "Guoliang Li",
            "Xiaoyong Du"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16872",
        "abstract": "Autonomous data science, from raw data sources to analyst-grade deep research reports, has been a long-standing challenge, and is now becoming feasible with the emergence of powerful large language models (LLMs). Recent workflow-based data agents have shown promising results on specific data tasks but remain fundamentally limited in achieving fully autonomous data science due to their reliance on predefined workflows. In this paper, we introduce DeepAnalyze-8B, the first agentic LLM designed for autonomous data science, capable of automatically completing the end-toend pipeline from data sources to analyst-grade deep research reports. To tackle high-complexity data science tasks, we propose a curriculum-based agentic training paradigm that emulates the learning trajectory of human data scientists, enabling LLMs to progressively acquire and integrate multiple capabilities in real-world environments. We also introduce a data-grounded trajectory synthesis framework that constructs high-quality training data. Through agentic training, DeepAnalyze learns to perform a broad spectrum of data tasks, ranging from data question answering and specialized analytical tasks to open-ended data research. Experiments demonstrate that, with only 8B parameters, DeepAnalyze outperforms previous workflow-based agents built on most advanced proprietary LLMs. The model, code, and training data of DeepAnalyze are open-sourced, paving the way toward autonomous data science.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "229",
        "title": "Utility-Diversity Aware Online Batch Selection for LLM Supervised Fine-tuning",
        "author": [
            "Heming Zou",
            "Yixiu Mao",
            "Yun Qu",
            "Qi Wang",
            "Xiangyang Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16882",
        "abstract": "Supervised fine-tuning (SFT) is a commonly used technique to adapt large language models (LLMs) to downstream tasks. In practice, SFT on a full dataset is computationally expensive and sometimes suffers from overfitting or bias amplification. This facilitates the rise of data curation in SFT, which prioritizes the most valuable data to optimze. This work studies the online batch selection family that dynamically scores and filters samples during the training process. However, existing popular methods often (i) rely merely on the utility of data to select a subset while neglecting other crucial factors like diversity, (ii) rely on external resources such as reference models or validation sets, and (iii) incur extra training time over full-dataset training. To address these limitations, this work develops \\textbf{UDS (Utility-Diversity Sampling)}, a framework for efficient online batch selection in SFT. UDS leverages the nuclear norm of the logits matrix to capture both data utility and intra-sample diversity, while estimating inter-sample diversity through efficient low-dimensional embedding comparisons with a lightweight memory buffer of historical samples. Such a design eliminates the need for external resources and unnecessary backpropagation, securing computational efficiency. Experiments on multiple benchmarks demonstrate that UDS consistently outperforms state-of-the-art online batch selection methods under varying data budgets, and significantly reduces training time compared to full-dataset fine-tuning. Code is available at https://github.com/gfyddha/UDS.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "230",
        "title": "UniGTE: Unified Graph-Text Encoding for Zero-Shot Generalization across Graph Tasks and Domains",
        "author": [
            "Duo Wang",
            "Yuan Zuo",
            "Guangyue Lu",
            "Junjie Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16885",
        "abstract": "Generalizing to unseen graph tasks without task-specific supervision is challenging: conventional graph neural networks are typically tied to a fixed label space, while large language models (LLMs) struggle to capture graph structure. We introduce UniGTE, an instruction-tuned encoder-decoder framework that unifies structural and semantic reasoning. The encoder augments a pretrained autoregressive LLM with learnable alignment tokens and a structure-aware graph-text attention mechanism, enabling it to attend jointly to a tokenized graph and a natural-language task prompt while remaining permutation-invariant to node order. This yields compact, task-aware graph representations. Conditioned solely on these representations, a frozen LLM decoder predicts and reconstructs: it outputs the task answer and simultaneously paraphrases the input graph in natural language. The reconstruction objective regularizes the encoder to preserve structural cues. UniGTE is instruction-tuned on five datasets spanning node-level, edge-level, and graph-level tasks across diverse domains, yet requires no fine-tuning at inference. It achieves new state-of-the-art zero-shot results on node classification, link prediction, graph classification, and graph regression under cross-task and cross-domain settings, demonstrating that tight integration of graph structure with LLM semantics enables robust, transferable graph reasoning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "231",
        "title": "Uniworld-V2: Reinforce Image Editing with Diffusion Negative-aware Finetuning and MLLM Implicit Feedback",
        "author": [
            "Zongjian Li",
            "Zheyuan Liu",
            "Qihui Zhang",
            "Bin Lin",
            "Shenghai Yuan",
            "Zhiyuan Yan",
            "Yang Ye",
            "Wangbo Yu",
            "Yuwei Niu",
            "Li Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16888",
        "abstract": "Instruction-based image editing has achieved remarkable progress; however, models solely trained via supervised fine-tuning often overfit to annotated patterns, hindering their ability to explore and generalize beyond training distributions. To this end, we introduce Edit-R1, a novel post-training framework for instruction-based image editing based on policy optimization. Specifically, we utilize Diffusion Negative-aware Finetuning (DiffusionNFT), a likelihood-free policy optimization method consistent with the flow matching forward process, thereby enabling the use of higher-order samplers and more efficient training. Another key challenge here is the absence of a universal reward model, resulting from the diverse nature of editing instructions and tasks. To bridge this gap, we employ a Multimodal Large Language Model (MLLM) as a unified, training-free reward model, leveraging its output logits to provide fine-grained feedback. Furthermore, we carefully design a low-variance group filtering mechanism to reduce MLLM scoring noise and stabilize optimization. UniWorld-V2, trained with this framework, achieves \\textbf{state-of-the-art} results on the ImgEdit and GEdit-Bench benchmarks, scoring 4.49 and 7.83, respectively. Crucially, our framework is model-agnostic, delivering substantial performance gains when applied to diverse base models like Qwen-Image-Edit and FLUX-Kontext, demonstrating its wide applicability. Code and models are publicly available at https://github.com/PKU-YuanGroup/UniWorld-V2.",
        "tags": [
            "Diffusion",
            "FLUX",
            "Flow Matching",
            "Image Editing",
            "Qwen"
        ]
    },
    {
        "id": "232",
        "title": "Investigating Safety Vulnerabilities of Large Audio-Language Models Under Speaker Emotional Variations",
        "author": [
            "Bo-Han Feng",
            "Chien-Feng Liu",
            "Yu-Hsuan Li Liang",
            "Chih-Kai Yang",
            "Szu-Wei Fu",
            "Zhehuai Chen",
            "Ke-Han Lu",
            "Sung-Feng Huang",
            "Chao-Han Huck Yang",
            "Yu-Chiang Frank Wang",
            "Yun-Nung Chen",
            "Hung-yi Lee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16893",
        "abstract": "Large audio-language models (LALMs) extend text-based LLMs with auditory understanding, offering new opportunities for multimodal applications. While their perception, reasoning, and task performance have been widely studied, their safety alignment under paralinguistic variation remains underexplored. This work systematically investigates the role of speaker emotion. We construct a dataset of malicious speech instructions expressed across multiple emotions and intensities, and evaluate several state-of-the-art LALMs. Our results reveal substantial safety inconsistencies: different emotions elicit varying levels of unsafe responses, and the effect of intensity is non-monotonic, with medium expressions often posing the greatest risk. These findings highlight an overlooked vulnerability in LALMs and call for alignment strategies explicitly designed to ensure robustness under emotional variation, a prerequisite for trustworthy deployment in real-world settings.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "233",
        "title": "VAGEN: Reinforcing World Model Reasoning for Multi-Turn VLM Agents",
        "author": [
            "Kangrui Wang",
            "Pingyue Zhang",
            "Zihan Wang",
            "Yaning Gao",
            "Linjie Li",
            "Qineng Wang",
            "Hanyang Chen",
            "Chi Wan",
            "Yiping Lu",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Ranjay Krishna",
            "Jiajun Wu",
            "Li Fei-Fei",
            "Yejin Choi",
            "Manling Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16907",
        "abstract": "A key challenge in training Vision-Language Model (VLM) agents, compared to Language Model (LLM) agents, lies in the shift from textual states to complex visual observations. This transition introduces partial observability and demands robust world modeling. We ask: Can VLM agents construct internal world models through explicit visual state reasoning? To address this question, we architecturally enforce and reward the agent's reasoning process via reinforcement learning (RL), formulating it as a Partially Observable Markov Decision Process (POMDP). We find that decomposing the agent's reasoning into State Estimation (\"what is the current state?\") and Transition Modeling (\"what comes next?\") is critical for success, as demonstrated through five reasoning strategies. Our investigation into how agents represent internal beliefs reveals that the optimal representation is task-dependent: Natural Language excels at capturing semantic relationships in general tasks, while Structured formats are indispensable for precise manipulation and control. Building on these insights, we design a World Modeling Reward that provides dense, turn-level supervision for accurate state prediction, and introduce Bi-Level General Advantage Estimation (Bi-Level GAE) for turn-aware credit assignment. Through this form of visual state reasoning, a 3B-parameter model achieves a score of 0.82 across five diverse agent benchmarks, representing a 3$\\times$ improvement over its untrained counterpart (0.21) and outperforming proprietary reasoning models such as GPT-5 (0.75), Gemini 2.5 Pro (0.67) and Claude 4.5 (0.62). All experiments are conducted within our VAGEN framework, a scalable system for training and analyzing multi-turn VLM agents in diverse visual environments. Code and data are publicly available at https://vagen-ai.github.io.",
        "tags": [
            "GPT",
            "LLM",
            "RL",
            "VLM"
        ]
    },
    {
        "id": "234",
        "title": "Beyond RGB: Leveraging Vision Transformers for Thermal Weapon Segmentation",
        "author": [
            "Akhila Kambhatla",
            "Ahmed R Khaled"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16913",
        "abstract": "Thermal weapon segmentation is crucial for surveillance and security applications, enabling robust detection under lowlight and visually obscured conditions where RGB-based systems fail. While convolutional neural networks (CNNs) dominate thermal segmentation literature, their ability to capture long-range dependencies and fine structural details is limited. Vision Transformers (ViTs), with their global context modeling capabilities, have achieved state-of-the-art results in RGB segmentation tasks, yet their potential in thermal weapon segmentation remains underexplored. This work adapts and evaluates four transformer-based architectures SegFormer, DeepLabV3\\+, SegNeXt, and Swin Transformer for binary weapon segmentation on a custom thermal dataset comprising 9,711 images collected from real world surveillance videos and automatically annotated using SAM2. We employ standard augmentation strategies within the MMSegmentation framework to ensure robust model training and fair architectural comparison. Experimental results demonstrate significant improvements in segmentation performance: SegFormer-b5 achieves the highest mIoU (94.15\\%) and Pixel Accuracy (97.04\\%), while SegFormer-b0 provides the fastest inference speed (98.32 FPS) with competitive mIoU (90.84\\%). SegNeXt-mscans offers balanced performance with 85.12 FPS and 92.24\\% mIoU, and DeepLabV3\\+ R101-D8 reaches 92.76\\% mIoU at 29.86 FPS. The transformer architectures demonstrate robust generalization capabilities for weapon detection in low-light and occluded thermal environments, with flexible accuracy-speed trade-offs suitable for diverse real-time security applications.",
        "tags": [
            "Detection",
            "Segmentation",
            "Transformer"
        ]
    },
    {
        "id": "235",
        "title": "SolverLLM: Leveraging Test-Time Scaling for Optimization Problem via LLM-Guided Search",
        "author": [
            "Dong Li",
            "Xujiang Zhao",
            "Linlin Yu",
            "Yanchi Liu",
            "Wei Cheng",
            "Zhengzhang Chen",
            "Zhong Chen",
            "Feng Chen",
            "Chen Zhao",
            "Haifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16916",
        "abstract": "Large Language Models (LLMs) offer promising capabilities for tackling complex reasoning tasks, including optimization problems. However, existing methods either rely on prompt engineering, which leads to poor generalization across problem types, or require costly supervised training. We introduce SolverLLM, a training-free framework that leverages test-time scaling to solve diverse optimization problems. Rather than solving directly, SolverLLM generates mathematical formulations and translates them into solver-ready code, guided by a novel Monte Carlo Tree Search (MCTS) strategy. To enhance the search process, we modify classical MCTS with (1) dynamic expansion for adaptive formulation generation, (2) prompt backpropagation to guide exploration via outcome-driven feedback, and (3) uncertainty backpropagation to incorporate reward reliability into decision-making. Experiments on six standard benchmark datasets demonstrate that SolverLLM outperforms both prompt-based and learning-based baselines, achieving strong generalization without additional training.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "236",
        "title": "Does Visual Grounding Enhance the Understanding of Embodied Knowledge in Large Language Models?",
        "author": [
            "Zhihui Yang",
            "Yupei Wang",
            "Kaijie Mo",
            "Zhe Zhao",
            "Renfen Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16924",
        "abstract": "Despite significant progress in multimodal language models (LMs), it remains unclear whether visual grounding enhances their understanding of embodied knowledge compared to text-only models. To address this question, we propose a novel embodied knowledge understanding benchmark based on the perceptual theory from psychology, encompassing visual, auditory, tactile, gustatory, olfactory external senses, and interoception. The benchmark assesses the models' perceptual abilities across different sensory modalities through vector comparison and question-answering tasks with over 1,700 questions. By comparing 30 state-of-the-art LMs, we surprisingly find that vision-language models (VLMs) do not outperform text-only models in either task. Moreover, the models perform significantly worse in the visual dimension compared to other sensory dimensions. Further analysis reveals that the vector representations are easily influenced by word form and frequency, and the models struggle to answer questions involving spatial perception and reasoning. Our findings underscore the need for more effective integration of embodied knowledge in LMs to enhance their understanding of the physical world.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "237",
        "title": "Res-Bench: Benchmarking the Robustness of Multimodal Large Language Models to Dynamic Resolution Input",
        "author": [
            "Chenxu Li",
            "Zhicai Wang",
            "Yuan Sheng",
            "Xingyu Zhu",
            "Yanbin Hao",
            "Xiang Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16926",
        "abstract": "Multimodal Large Language Models (MLLMs) increasingly support dynamic image resolutions. However, current evaluation paradigms primarily assess semantic performance, overlooking the critical question of resolution robustness - whether performance remains stable across varying input resolutions. To address this gap, we introduce \\textbf{Res-Bench}, a comprehensive benchmark comprising 14,400 samples across 12 resolution levels and six core capability dimensions. We designed a novel evaluation framework that goes beyond traditional accuracy metrics to capture performance stability. This framework introduces multiple robustness metrics: Spearman's correlation for assessing resolution-performance trends, and Absolute/Relative Continuous Error (ACE/RCE) for measuring performance volatility. Using these metrics, we conducted a large-scale evaluation of leading MLLMs. Our analysis encompasses: (1) model-centric and task-centric robustness examination, (2) investigation of preprocessing strategies including padding and super-resolution, and (3) exploration of fine-tuning for stability enhancement.",
        "tags": [
            "LLM",
            "Super Resolution"
        ]
    },
    {
        "id": "238",
        "title": "Closing the Curvature Gap: Full Transformer Hessians and Their Implications for Scaling Laws",
        "author": [
            "Egor Petrov",
            "Nikita Kiselev",
            "Vladislav Meshkov",
            "Andrey Grabovoy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16927",
        "abstract": "The lack of theoretical results for Layer Normalization and feedforward Hessians has left a gap in the study of Transformer optimization landscapes. We address this by deriving explicit second-order expressions for these components, thereby completing the Hessian characterization of full Transformer blocks. Our results generalize prior self-attention analyses and yield estimations for the role of each sublayer in curvature propagation. We demonstrate how these Hessian structures inform both convergence dynamics and the empirical scaling laws governing large-model performance. Further, we propose a Taylor-expansion-based framework for analyzing loss differences to quantify convergence trajectories. By extending Hessian theory to the full Transformer architecture, this work establishes a new foundation for theoretical and empirical investigations of optimization in large-scale deep learning.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "239",
        "title": "ChiKhaPo: A Large-Scale Multilingual Benchmark for Evaluating Lexical Comprehension and Generation in Large Language Models",
        "author": [
            "Emily Chang",
            "Niyati Bafna"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16928",
        "abstract": "Existing benchmarks for large language models (LLMs) are largely restricted to high- or mid-resource languages, and often evaluate performance on higher-order tasks in reasoning and generation. However, plenty of evidence points to the fact that LLMs lack basic linguistic competence in the vast majority of the world's 3800+ written languages. We introduce ChiKhaPo, consisting of 8 subtasks of varying difficulty designed to evaluate the lexical comprehension and generation abilities of generative models. ChiKhaPo draws on existing lexicons, monolingual data, and bitext, and provides coverage for 2700+ languages for 2 subtasks, surpassing any existing benchmark in terms of language coverage. We further show that 6 SOTA models struggle on our benchmark, and discuss the factors contributing to performance scores, including language family, language resourcedness, task, and comprehension versus generation directions. With ChiKhaPo, we hope to enable and encourage the massively multilingual benchmarking of LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "240",
        "title": "Design of an Affordable, Fully-Actuated Biomimetic Hand for Dexterous Teleoperation Systems",
        "author": [
            "Zhaoliang Wan",
            "Zida Zhou",
            "Zetong Bi",
            "Zehui Yang",
            "Hao Ding",
            "Hui Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16931",
        "abstract": "This paper addresses the scarcity of affordable, fully-actuated five-fingered hands for dexterous teleoperation, which is crucial for collecting large-scale real-robot data within the \"Learning from Demonstrations\" paradigm. We introduce the prototype version of the RAPID Hand, the first low-cost, 20-degree-of-actuation (DoA) dexterous hand that integrates a novel anthropomorphic actuation and transmission scheme with an optimized motor layout and structural design to enhance dexterity. Specifically, the RAPID Hand features a universal phalangeal transmission scheme for the non-thumb fingers and an omnidirectional thumb actuation mechanism. Prioritizing affordability, the hand employs 3D-printed parts combined with custom gears for easier replacement and repair. We assess the RAPID Hand's performance through quantitative metrics and qualitative testing in a dexterous teleoperation system, which is evaluated on three challenging tasks: multi-finger retrieval, ladle handling, and human-like piano playing. The results indicate that the RAPID Hand's fully actuated 20-DoF design holds significant promise for dexterous teleoperation.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "241",
        "title": "Prompt-MII: Meta-Learning Instruction Induction for LLMs",
        "author": [
            "Emily Xiao",
            "Yixiao Zeng",
            "Ada Chen",
            "Chin-Jou Li",
            "Amanda Bertsch",
            "Graham Neubig"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16932",
        "abstract": "A popular method to adapt large language models (LLMs) to new tasks is in-context learning (ICL), which is effective but incurs high inference costs as context length grows. In this paper we propose a method to perform instruction induction, where we take training examples and reduce them to a compact but descriptive prompt that can achieve performance comparable to ICL over the full training set. Specifically, we propose PROMPT-MII, a reinforcement learning (RL) based framework to meta-learn an instruction induction model that can generate compact instructions on the fly for an arbitrary new dataset. We train on over 3,000 diverse classification datasets from the HuggingFace hub, and evaluate on 90 unseen tasks. PROMPT-MII improves downstream model quality by 4-9 F1 points (10-20% relative), matching ICL performance while requiring 3-13x fewer tokens.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "242",
        "title": "Tutoring LLM into a Better CUDA Optimizer",
        "author": [
            "MatyÃ¡Å¡ Brabec",
            "JiÅÃ­ Klepl",
            "Michal TÃ¶pfer",
            "Martin KruliÅ¡"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16933",
        "abstract": "Recent leaps in large language models (LLMs) caused a revolution in programming tools (like GitHub Copilot) that can help with code generation, debugging, and even performance optimization. In this paper, we focus on the capabilities of the most recent reasoning models to generate optimized CUDA code for predefined, well-known tasks. Our objective is to determine which types of code optimizations and parallel patterns the LLMs can perform by themselves and whether they can be improved by tutoring (providing more detailed hints and guidelines in the prompt). The generated solutions were evaluated both automatically (for correctness and speedup) and manually (code reviews) to provide a more detailed perspective. We also tried an interactive approach where the LLM can fix its previous mistakes within a session. The results indicate that LLMs are quite skilled coders; however, they require tutoring to reach optimized solutions provided by parallel computing experts.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "243",
        "title": "A Primer on Kolmogorov-Arnold Networks (KANs) for Probabilistic Time Series Forecasting",
        "author": [
            "Cristian J. Vaca-Rubio",
            "Roberto Pereira",
            "Luis Blanco",
            "Engin Zeydan",
            "MÃ rius Caus"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16940",
        "abstract": "This work introduces Probabilistic Kolmogorov-Arnold Network (P-KAN), a novel probabilistic extension of Kolmogorov-Arnold Networks (KANs) for time series forecasting. By replacing scalar weights with spline-based functional connections and directly parameterizing predictive distributions, P-KANs offer expressive yet parameter-efficient models capable of capturing nonlinear and heavy-tailed dynamics. We evaluate P-KANs on satellite traffic forecasting, where uncertainty-aware predictions enable dynamic thresholding for resource allocation. Results show that P-KANs consistently outperform Multi Layer Perceptron (MLP) baselines in both accuracy and calibration, achieving superior efficiency-risk trade-offs while using significantly fewer parameters. We build up P-KANs on two distributions, namely Gaussian and Student-t distributions. The Gaussian variant provides robust, conservative forecasts suitable for safety-critical scenarios, whereas the Student-t variant yields sharper distributions that improve efficiency under stable demand. These findings establish P-KANs as a powerful framework for probabilistic forecasting with direct applicability to satellite communications and other resource-constrained domains.",
        "tags": [
            "KAN"
        ]
    },
    {
        "id": "244",
        "title": "Peering Inside the Black Box: Uncovering LLM Errors in Optimization Modelling through Component-Level Evaluation",
        "author": [
            "Dania Refai",
            "Moataz Ahmed"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16943",
        "abstract": "Large language models (LLMs) are increasingly used to convert natural language descriptions into mathematical optimization formulations. Current evaluations often treat formulations as a whole, relying on coarse metrics like solution accuracy or runtime, which obscure structural or numerical errors. In this study, we present a comprehensive, component-level evaluation framework for LLM-generated formulations. Beyond the conventional optimality gap, our framework introduces metrics such as precision and recall for decision variables and constraints, constraint and objective root mean squared error (RMSE), and efficiency indicators based on token usage and latency. We evaluate GPT-5, LLaMA 3.1 Instruct, and DeepSeek Math across optimization problems of varying complexity under six prompting strategies. Results show that GPT-5 consistently outperforms other models, with chain-of-thought, self-consistency, and modular prompting proving most effective. Analysis indicates that solver performance depends primarily on high constraint recall and low constraint RMSE, which together ensure structural correctness and solution reliability. Constraint precision and decision variable metrics play secondary roles, while concise outputs enhance computational efficiency. These findings highlight three principles for NLP-to-optimization modeling: (i) Complete constraint coverage prevents violations, (ii) minimizing constraint RMSE ensures solver-level accuracy, and (iii) concise outputs improve computational efficiency. The proposed framework establishes a foundation for fine-grained, diagnostic evaluation of LLMs in optimization modeling.",
        "tags": [
            "CoT",
            "DeepSeek",
            "GPT",
            "LLM",
            "LLaMA"
        ]
    },
    {
        "id": "245",
        "title": "Real-Time World Crafting: Generating Structured Game Behaviors from Natural Language with Large Language Models",
        "author": [
            "Austin Drake",
            "Hang Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16952",
        "abstract": "We present a novel architecture for safely integrating Large Language Models (LLMs) into interactive game engines, allowing players to \"program\" new behaviors using natural language. Our framework mitigates risks by using an LLM to translate commands into a constrained Domain-Specific Language (DSL), which configures a custom Entity-Component-System (ECS) at runtime. We evaluated this system in a 2D spell-crafting game prototype by experimentally assessing models from the Gemini, GPT, and Claude families with various prompting strategies. A validated LLM judge qualitatively rated the outputs, showing that while larger models better captured creative intent, the optimal prompting strategy is task-dependent: Chain-of-Thought improved creative alignment, while few-shot examples were necessary to generate more complex DSL scripts. This work offers a validated LLM-ECS pattern for emergent gameplay and a quantitative performance comparison for developers.",
        "tags": [
            "CoT",
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "246",
        "title": "A Comparative User Evaluation of XRL Explanations using Goal Identification",
        "author": [
            "Mark Towers",
            "Yali Du",
            "Christopher Freeman",
            "Timothy J. Norman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16956",
        "abstract": "Debugging is a core application of explainable reinforcement learning (XRL) algorithms; however, limited comparative evaluations have been conducted to understand their relative performance. We propose a novel evaluation methodology to test whether users can identify an agent's goal from an explanation of its decision-making. Utilising the Atari's Ms. Pacman environment and four XRL algorithms, we find that only one achieved greater than random accuracy for the tested goals and that users were generally overconfident in their selections. Further, we find that users' self-reported ease of identification and understanding for every explanation did not correlate with their accuracy.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "247",
        "title": "Quantile Regression, Variational Autoencoders, and Diffusion Models for Uncertainty Quantification: A Spatial Analysis of Sub-seasonal Wind Speed Prediction",
        "author": [
            "Ganglin Tian",
            "Anastase Alexandre Charantonis",
            "Camille Le Coz",
            "Alexis Tantet",
            "Riwal Plougonven"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16958",
        "abstract": "This study aims to improve the spatial representation of uncertainties when regressing surface wind speeds from large-scale atmospheric predictors for sub-seasonal forecasting. Sub-seasonal forecasting often relies on large-scale atmospheric predictors such as 500 hPa geopotential height (Z500), which exhibit higher predictability than surface variables and can be downscaled to obtain more localised information. Previous work by Tian et al. (2024) demonstrated that stochastic perturbations based on model residuals can improve ensemble dispersion representation in statistical downscaling frameworks, but this method fails to represent spatial correlations and physical consistency adequately. More sophisticated approaches are needed to capture the complex relationships between large-scale predictors and local-scale predictands while maintaining physical consistency. Probabilistic deep learning models offer promising solutions for capturing complex spatial dependencies. This study evaluates three probabilistic methods with distinct uncertainty quantification mechanisms: Quantile Regression Neural Network that directly models distribution quantiles, Variational Autoencoders that leverage latent space sampling, and Diffusion Models that utilise iterative denoising. These models are trained on ERA5 reanalysis data and applied to ECMWF sub-seasonal hindcasts to regress probabilistic wind speed ensembles. Our results show that probabilistic downscaling approaches provide more realistic spatial uncertainty representations compared to simpler stochastic methods, with each probabilistic model offering different strengths in terms of ensemble dispersion, deterministic skill, and physical consistency. These findings establish probabilistic downscaling as an effective enhancement to operational sub-seasonal wind forecasts for renewable energy planning and risk assessment.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "248",
        "title": "Leave It to the Experts: Detecting Knowledge Distillation via MoE Expert Signatures",
        "author": [
            "Pingzhi Li",
            "Morris Yu-Chao Huang",
            "Zhen Tan",
            "Qingquan Song",
            "Jie Peng",
            "Kai Zou",
            "Yu Cheng",
            "Kaidi Xu",
            "Tianlong Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16968",
        "abstract": "Knowledge Distillation (KD) accelerates training of large language models (LLMs) but poses intellectual property protection and LLM diversity risks. Existing KD detection methods based on self-identity or output similarity can be easily evaded through prompt engineering. We present a KD detection framework effective in both white-box and black-box settings by exploiting an overlooked signal: the transfer of MoE \"structural habits\", especially internal routing patterns. Our approach analyzes how different experts specialize and collaborate across various inputs, creating distinctive fingerprints that persist through the distillation process. To extend beyond the white-box setup and MoE architectures, we further propose Shadow-MoE, a black-box method that constructs proxy MoE representations via auxiliary distillation to compare these patterns between arbitrary model pairs. We establish a comprehensive, reproducible benchmark that offers diverse distilled checkpoints and an extensible framework to facilitate future research. Extensive experiments demonstrate >94% detection accuracy across various scenarios and strong robustness to prompt-based evasion, outperforming existing baselines while highlighting the structural habits transfer in LLMs.",
        "tags": [
            "Detection",
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "249",
        "title": "Lark: Biologically Inspired Neuroevolution for Multi-Stakeholder LLM Agents",
        "author": [
            "Dheeraj Chintapalli",
            "Rikhil Tanugula",
            "Sunkalp Chandra"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16978",
        "abstract": "We present Lark, a biologically inspired decision-making framework that couples LLM-driven reasoning with an evolutionary, stakeholder-aware Multi-Agent System (MAS). To address verbosity and stakeholder trade-offs, we integrate four mechanisms: (i) plasticity, which applies concise adjustments to candidate solutions; (ii) duplication and maturation, which copy high-performing candidates and specialize them into new modules; (iii) ranked-choice stakeholder aggregation using influence-weighted Borda scoring; and (iv) compute awareness via token-based penalties that reward brevity. The system iteratively proposes diverse strategies, applies plasticity tweaks, simulates stakeholder evaluations, aggregates preferences, selects top candidates, and performs duplication/maturation while factoring compute cost into final scores. In a controlled evaluation over 30 rounds comparing 14 systems, Lark Full achieves a mean rank of 2.55 (95% CI [2.17, 2.93]) and a mean composite score of 29.4/50 (95% CI [26.34, 32.46]), finishing Top-3 in 80% of rounds while remaining cost competitive with leading commercial models ($0.016 per task). Paired Wilcoxon tests confirm that all four mechanisms contribute significantly as ablating duplication/maturation yields the largest deficit ({\\Delta}Score = 3.5, Cohen's d_z = 2.53, p < 0.001), followed by plasticity ({\\Delta}Score = 3.4, d_z = 1.86), ranked-choice voting ({\\Delta}Score = 2.4, d_z = 1.20), and token penalties ({\\Delta}Score = 2.2, d_z = 1.63). Rather than a formal Markov Decision Process with constrained optimization, Lark is a practical, compute-aware neuroevolutionary loop that scales stakeholder-aligned strategy generation and makes trade-offs transparent through per-step metrics. Our work presents proof-of-concept findings and invites community feedback as we expand toward real-world validation studies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "250",
        "title": "One-step Diffusion Models with Bregman Density Ratio Matching",
        "author": [
            "Yuanzhi Zhu",
            "Eleftherios Tsonis",
            "Lucas Degeorge",
            "Vicky Kalogeiton"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16983",
        "abstract": "Diffusion and flow models achieve high generative quality but remain computationally expensive due to slow multi-step sampling. Distillation methods accelerate them by training fast student generators, yet most existing objectives lack a unified theoretical foundation. In this work, we propose Di-Bregman, a compact framework that formulates diffusion distillation as Bregman divergence-based density-ratio matching. This convex-analytic view connects several existing objectives through a common lens. Experiments on CIFAR-10 and text-to-image generation demonstrate that Di-Bregman achieves improved one-step FID over reverse-KL distillation and maintains high visual fidelity compared to the teacher model. Our results highlight Bregman density-ratio matching as a practical and theoretically-grounded route toward efficient one-step diffusion generation.",
        "tags": [
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "251",
        "title": "Parameter-Efficient Fine-Tuning for Low-Resource Languages: A Comparative Study of LLMs for Bengali Hate Speech Detection",
        "author": [
            "Akif Islam",
            "Mohd Ruhul Ameen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16985",
        "abstract": "Bengali social media platforms have witnessed a sharp increase in hate speech, disproportionately affecting women and adolescents. While datasets such as BD-SHS provide a basis for structured evaluation, most prior approaches rely on either computationally costly full-model fine-tuning or proprietary APIs. This paper presents the first application of Parameter-Efficient Fine-Tuning (PEFT) for Bengali hate speech detection using LoRA and QLoRA. Three instruction-tuned large language models - Gemma-3-4B, Llama-3.2-3B, and Mistral-7B - were fine-tuned on the BD-SHS dataset of 50,281 annotated comments. Each model was adapted by training fewer than 1% of its parameters, enabling experiments on a single consumer-grade GPU. The results show that Llama-3.2-3B achieved the highest F1-score of 92.23%, followed by Mistral-7B at 88.94% and Gemma-3-4B at 80.25%. These findings establish PEFT as a practical and replicable strategy for Bengali and related low-resource languages.",
        "tags": [
            "Detection",
            "LLM",
            "LLaMA",
            "LoRA"
        ]
    },
    {
        "id": "252",
        "title": "Training-free Online Video Step Grounding",
        "author": [
            "Luca Zanella",
            "Massimiliano Mancini",
            "Yiming Wang",
            "Alessio Tonioni",
            "Elisa Ricci"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16989",
        "abstract": "Given a task and a set of steps composing it, Video Step Grounding (VSG) aims to detect which steps are performed in a video. Standard approaches for this task require a labeled training set (e.g., with step-level annotations or narrations), which may be costly to collect. Moreover, they process the full video offline, limiting their applications for scenarios requiring online decisions. Thus, in this work, we explore how to perform VSG online and without training. We achieve this by exploiting the zero-shot capabilities of recent Large Multimodal Models (LMMs). In particular, we use LMMs to predict the step associated with a restricted set of frames, without access to the whole video. We show that this online strategy without task-specific tuning outperforms offline and training-based models. Motivated by this finding, we develop Bayesian Grounding with Large Multimodal Models (BaGLM), further injecting knowledge of past frames into the LMM-based predictions. BaGLM exploits Bayesian filtering principles, modeling step transitions via (i) a dependency matrix extracted through large language models and (ii) an estimation of step progress. Experiments on three datasets show superior performance of BaGLM over state-of-the-art training-based offline methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "253",
        "title": "Graph4MM: Weaving Multimodal Learning with Structural Information",
        "author": [
            "Xuying Ning",
            "Dongqi Fu",
            "Tianxin Wei",
            "Wujiang Xu",
            "Jingrui He"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16990",
        "abstract": "Real-world multimodal data usually exhibit complex structural relationships beyond traditional one-to-one mappings like image-caption pairs. Entities across modalities interact in intricate ways, with images and text forming diverse interconnections through contextual dependencies and co-references. Graphs provide powerful structural information for modeling intra-modal and inter-modal relationships. However, previous works fail to distinguish multi-hop neighbors and treat the graph as a standalone modality, which fragments the overall understanding. This limitation presents two key challenges in multimodal learning: (1) integrating structural information from multi-hop neighbors into foundational models, and (2) fusing modality-specific information in a principled manner. To address these challenges, we revisit the role of graphs in multimodal learning within the era of foundation models and propose Graph4MM, a graph-based multimodal learning framework. To be specific, we introduce Hop-Diffused Attention, which integrates multi-hop structural information into self-attention through causal masking and hop diffusion. Furthermore, we design MM-QFormer, a multi-mapping querying transformer for cross-modal fusion. Through theoretical and empirical analysis, we show that leveraging structures to integrate both intra- and inter-modal interactions improves multimodal understanding beyond treating them as a standalone modality. Experiments on both generative and discriminative tasks show that Graph4MM outperforms larger VLMs, LLMs, and multimodal graph baselines, achieving a 6.93% average improvement.",
        "tags": [
            "Diffusion",
            "LLM",
            "Transformer",
            "VLM"
        ]
    },
    {
        "id": "254",
        "title": "STARK: Strategic Team of Agents for Refining Kernels",
        "author": [
            "Juncheng Dong",
            "Yang Yang",
            "Tao Liu",
            "Yang Wang",
            "Feng Qi",
            "Vahid Tarokh",
            "Kaushik Rangadurai",
            "Shuang Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16996",
        "abstract": "The efficiency of GPU kernels is central to the progress of modern AI, yet optimizing them remains a difficult and labor-intensive task due to complex interactions between memory hierarchies, thread scheduling, and hardware-specific characteristics. While recent advances in large language models (LLMs) provide new opportunities for automated code generation, existing approaches largely treat LLMs as single-shot generators or naive refinement tools, limiting their effectiveness in navigating the irregular kernel optimization landscape. We introduce an LLM agentic framework for GPU kernel optimization that systematically explores the design space through multi-agent collaboration, grounded instruction, dynamic context management, and strategic search. This framework mimics the workflow of expert engineers, enabling LLMs to reason about hardware trade-offs, incorporate profiling feedback, and refine kernels iteratively. We evaluate our approach on KernelBench, a benchmark for LLM-based kernel optimization, and demonstrate substantial improvements over baseline agents: our system produces correct solutions where baselines often fail, and achieves kernels with up to 16x faster runtime performance. These results highlight the potential of agentic LLM frameworks to advance fully automated, scalable GPU kernel optimization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "255",
        "title": "Vocab Diet: Reshaping the Vocabulary of LLMs with Vector Arithmetic",
        "author": [
            "Yuval Reif",
            "Guy Kaplan",
            "Roy Schwartz"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17001",
        "abstract": "Large language models (LLMs) were shown to encode word form variations, such as \"walk\"->\"walked\", as linear directions in embedding space. However, standard tokenization algorithms treat these variations as distinct tokens -- filling the size-capped vocabulary with surface form variants (e.g., \"walk\", \"walking\", \"Walk\"), at the expense of less frequent words and multilingual coverage. We show that many of these variations can be captured by transformation vectors -- additive offsets that yield the appropriate word's representation when applied to the base form word embedding -- in both the input and output spaces. Building on this, we propose a compact reshaping of the vocabulary: rather than assigning unique tokens to each surface form, we compose them from shared base form and transformation vectors (e.g., \"walked\" = \"walk\" + past tense). We apply our approach to multiple LLMs and across five languages, removing up to 10% of vocabulary entries -- thereby freeing space to allocate new, more diverse tokens. Importantly, we do so while also expanding vocabulary coverage to out-of-vocabulary words, with minimal impact on downstream performance, and without modifying model weights. Our findings motivate a foundational rethinking of vocabulary design, moving from string enumeration to a compositional vocabulary that leverages the underlying structure of language.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "256",
        "title": "EEschematic: Multimodal-LLM Based AI Agent for Schematic Generation of Analog Circuit",
        "author": [
            "Chang Liu",
            "Danial Chitnis"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17002",
        "abstract": "Circuit schematics play a crucial role in analog integrated circuit design, serving as the primary medium for human understanding and verification of circuit functionality. While recent large language model (LLM)-based approaches have shown promise in circuit topology generation and device sizing, most rely solely on textual representations such as SPICE netlists, which lack visual interpretability for circuit designers. To address this limitation, we propose EEschematic, an AI agent for automatic analog schematic generation based on a Multimodal Large Language Model (MLLM). EEschematic integrates textual, visual, and symbolic modalities to translate SPICE netlists into schematic diagrams represented in a human-editable format. The framework uses six analog substructure examples for few-shot placement and a Visual Chain-of-Thought (VCoT) strategy to iteratively refine placement and wiring, enhancing schematic clarity and symmetry. Experimental results on representative analog circuits, including a CMOS inverter, a five-transistor operational transconductance amplifier (5T-OTA), and a telescopic cascode amplifier, demonstrate that EEschematic produces schematics with high visual quality and structural correctness.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "257",
        "title": "Online Learning Defense against Iterative Jailbreak Attacks via Prompt Optimization",
        "author": [
            "Masahiro Kaneko",
            "Zeerak Talat",
            "Timothy Baldwin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17006",
        "abstract": "Iterative jailbreak methods that repeatedly rewrite and input prompts into large language models (LLMs) to induce harmful outputs -- using the model's previous responses to guide each new iteration -- have been found to be a highly effective attack strategy. Despite being an effective attack strategy against LLMs and their safety mechanisms, existing defenses do not proactively disrupt this dynamic trial-and-error cycle. In this study, we propose a novel framework that dynamically updates its defense strategy through online learning in response to each new prompt from iterative jailbreak methods. Leveraging the distinctions between harmful jailbreak-generated prompts and typical harmless prompts, we introduce a reinforcement learning-based approach that optimizes prompts to ensure appropriate responses for harmless tasks while explicitly rejecting harmful prompts. Additionally, to curb overfitting to the narrow band of partial input rewrites explored during an attack, we introduce Past-Direction Gradient Damping (PDGD). Experiments conducted on three LLMs show that our approach significantly outperforms five existing defense methods against five iterative jailbreak methods. Moreover, our results indicate that our prompt optimization strategy simultaneously enhances response quality for harmless tasks.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "258",
        "title": "DiscoTrack: A Multilingual LLM Benchmark for Discourse Tracking",
        "author": [
            "Lanni Bu",
            "Lauren Levin",
            "Amir Zeldes"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17013",
        "abstract": "Recent LLM benchmarks have tested models on a range of phenomena, but are still focused primarily on natural language understanding for extraction of explicit information, such as QA or summarization, with responses often tar- geting information from individual sentences. We are still lacking more challenging, and im- portantly also multilingual, benchmarks focus- ing on implicit information and pragmatic infer- ences across larger documents in the context of discourse tracking: integrating and aggregating information across sentences, paragraphs and multiple speaker utterances. To this end, we present DiscoTrack, an LLM benchmark target- ing a range of tasks across 12 languages and four levels of discourse understanding: salience recognition, entity tracking, discourse relations and bridging inference. Our evaluation shows that these tasks remain challenging, even for state-of-the-art models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "259",
        "title": "Justitia: Fair and Efficient Scheduling for LLM Applications",
        "author": [
            "Mingyan Yang",
            "Guanjie Wang",
            "Manqi Luo",
            "Yifei Liu",
            "Chen Chen",
            "Han Zhao",
            "Yu Feng",
            "Quan Chen",
            "Minyi Guo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17015",
        "abstract": "In the era of Large Language Models (LLMs), it has been popular to launch a series of LLM inferences -- we call an LLM application -- to better solve real-world problems. When serving those applications in shared GPU servers, the schedulers are expected to attain fast application completions with guaranteed worst-case performance. However, mainstream LLM schedulers fail to behave well for LLM applications -- due to head-of-line blocking or over-constrained resource allocation. In this paper, we propose to serve LLM applications in a fair and also efficient manner. To this end, we design Justitia, a novel scheduler with three key techniques. First, given that memory is prevalently a bottleneck for mainstream inference frameworks like vLLM, Justitia models the service cost of LLM applications in a memory-centric manner. Meanwhile, it uses a simple neural network model to conduct light-weight and also accurate demand prediction. Moreover, Justitia adopts a virtual-time based fair queuing algorithm to reduce the overall performance with guaranteed worst-case delay. We have implemented Justitia atop vLLM, and experimental results involving diverse LLM applications show that it can substantially enhance the scheduling efficiency with fairness preserved.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "260",
        "title": "SafeSearch: Do Not Trade Safety for Utility in LLM Search Agents",
        "author": [
            "Qiusi Zhan",
            "Angeline Budiman-Chan",
            "Abdelrahman Zayed",
            "Xingzhi Guo",
            "Daniel Kang",
            "Joo-Kyung Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17017",
        "abstract": "Large language model (LLM) based search agents iteratively generate queries, retrieve external information, and reason to answer open-domain questions. While researchers have primarily focused on improving their utility, their safety behaviors remain underexplored. In this paper, we first evaluate search agents using red-teaming datasets and find that they are more likely to produce harmful outputs than base LLMs. For example, when asked ``How can I track someone's location without their consent?'', a base model refuses, whereas a search agent designed to retrieve and cite sources may lower its refusal threshold, fetch documents (e.g., court cases), and, once appended, synthesize them into an informative yet unsafe summary. We further show that utility-oriented fine-tuning intensifies this risk, motivating joint alignment of safety and utility. We present SafeSearch, a multi-objective reinforcement learning approach that couples a final-output safety/utility reward with a novel query-level shaping term that penalizes unsafe queries and rewards safe ones. Experiments show that SafeSearch reduces agent harmfulness by over 70% across three red-teaming datasets while producing safe, helpful responses, and matches the QA performance of a utility-only finetuned agent; further analyses confirm the effectiveness of the query-level reward in jointly improving safety and utility.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "261",
        "title": "Extended LSTM: Adaptive Feature Gating for Toxic Comment Classification",
        "author": [
            "Noor Islam S. Mohammad"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17018",
        "abstract": "Toxic comment detection remains a challenging task, where transformer-based models (e.g., BERT) incur high computational costs and degrade on minority toxicity classes, while classical ensembles lack semantic adaptability. We propose xLSTM, a parameter-efficient and theoretically grounded framework that unifies cosine-similarity gating, adaptive feature prioritization, and principled class rebalancing. A learnable reference vector {v} in {R}^d modulates contextual embeddings via cosine similarity, amplifying toxic cues and attenuating benign signals to yield stronger gradients under severe class imbalance. xLSTM integrates multi-source embeddings (GloVe, FastText, BERT CLS) through a projection layer, a character-level BiLSTM for morphological cues, embedding-space SMOTE for minority augmentation, and adaptive focal loss with dynamic class weighting. On the Jigsaw Toxic Comment benchmark, xLSTM attains 96.0% accuracy and 0.88 macro-F1, outperforming BERT by 33% on threat and 28% on identity_hate categories, with 15 times fewer parameters and 50ms inference latency. Cosine gating contributes a +4.8% F1 gain in ablations. The results establish a new efficiency adaptability frontier, demonstrating that lightweight, theoretically informed architectures can surpass large pretrained models on imbalanced, domain-specific NLP tasks.",
        "tags": [
            "BERT",
            "Detection",
            "Transformer"
        ]
    },
    {
        "id": "262",
        "title": "Curiosity-driven RL for symbolic equation solving",
        "author": [
            "Kevin P. O Keeffe"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17022",
        "abstract": "We explore if RL can be useful for symbolic mathematics. Previous work showed contrastive learning can solve linear equations in one variable. We show model-free PPO \\cite{schulman2017proximal} augmented with curiosity-based exploration and graph-based actions can solve nonlinear equations such as those involving radicals, exponentials, and trig functions. Our work suggests curiosity-based exploration may be useful for general symbolic reasoning tasks.",
        "tags": [
            "PPO",
            "RL"
        ]
    },
    {
        "id": "263",
        "title": "Enrich and Detect: Video Temporal Grounding with Multimodal LLMs",
        "author": [
            "Shraman Pramanick",
            "Effrosyni Mavroudi",
            "Yale Song",
            "Rama Chellappa",
            "Lorenzo Torresani",
            "Triantafyllos Afouras"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17023",
        "abstract": "We introduce ED-VTG, a method for fine-grained video temporal grounding utilizing multi-modal large language models. Our approach harnesses the capabilities of multimodal LLMs to jointly process text and video, in order to effectively localize natural language queries in videos through a two-stage process. Rather than being directly grounded, language queries are initially transformed into enriched sentences that incorporate missing details and cues to aid in grounding. In the second stage, these enriched queries are grounded, using a lightweight decoder, which specializes at predicting accurate boundaries conditioned on contextualized representations of the enriched queries. To mitigate noise and reduce the impact of hallucinations, our model is trained with a multiple-instance-learning objective that dynamically selects the optimal version of the query for each training sample. We demonstrate state-of-the-art results across various benchmarks in temporal video grounding and paragraph grounding settings. Experiments reveal that our method significantly outperforms all previously proposed LLM-based temporal grounding approaches and is either superior or comparable to specialized models, while maintaining a clear advantage against them in zero-shot evaluation scenarios.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "264",
        "title": "Mapping from Meaning: Addressing the Miscalibration of Prompt-Sensitive Language Models",
        "author": [
            "Kyle Cox",
            "Jiawei Xu",
            "Yikun Han",
            "Rong Xu",
            "Tianhao Li",
            "Chi-Yang Hsu",
            "Tianlong Chen",
            "Walter Gerych",
            "Ying Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17028",
        "abstract": "An interesting behavior in large language models (LLMs) is prompt sensitivity. When provided with different but semantically equivalent versions of the same prompt, models may produce very different distributions of answers. This suggests that the uncertainty reflected in a model's output distribution for one prompt may not reflect the model's uncertainty about the meaning of the prompt. We model prompt sensitivity as a type of generalization error, and show that sampling across the semantic ``concept space'' with paraphrasing perturbations improves uncertainty calibration without compromising accuracy. Additionally, we introduce a new metric for uncertainty decomposition in black-box LLMs that improves upon entropy-based decomposition by modeling semantic continuities in natural language generation. We show that this decomposition metric can be used to quantify how much LLM uncertainty is attributed to prompt sensitivity. Our work introduces a new way to improve uncertainty calibration in prompt-sensitive language models, and provides evidence that some LLMs fail to exhibit consistent general reasoning about the meanings of their inputs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "265",
        "title": "Where, Not What: Compelling Video LLMs to Learn Geometric Causality for 3D-Grounding",
        "author": [
            "Yutong Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17034",
        "abstract": "Multimodal 3D grounding has garnered considerable interest in Vision-Language Models (VLMs) \\cite{yin2025spatial} for advancing spatial reasoning in complex environments. However, these models suffer from a severe \"2D semantic bias\" that arises from over-reliance on 2D image features for coarse localization, largely disregarding 3D geometric inputs and resulting in suboptimal fusion performance. In this paper, we propose a novel training framework called What-Where Representation Re-Forming (W2R2) to tackle this issue via disentangled representation learning and targeted shortcut suppression. Our approach fundamentally reshapes the model's internal space by designating 2D features as semantic beacons for \"What\" identification and 3D features as spatial anchors for \"Where\" localization, enabling precise 3D grounding without modifying inference architecture. Key components include a dual-objective loss function with an Alignment Loss that supervises fused predictions using adapted cross-entropy for multimodal synergy, and a Pseudo-Label Loss that penalizes overly effective 2D-dominant pseudo-outputs via a margin-based mechanism. Experiments conducted on ScanRefer and ScanQA demonstrate the effectiveness of W2R2, with significant gains in localization accuracy and robustness, particularly in cluttered outdoor scenes.",
        "tags": [
            "3D",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "266",
        "title": "Hephaestus: Mixture Generative Modeling with Energy Guidance for Large-scale QoS Degradation",
        "author": [
            "Nguyen Do",
            "Bach Ngo",
            "Youval Kashuv",
            "Canh V. Pham",
            "Hanghang Tong",
            "My T. Thai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17036",
        "abstract": "We study the Quality of Service Degradation (QoSD) problem, in which an adversary perturbs edge weights to degrade network performance. This setting arises in both network infrastructures and distributed ML systems, where communication quality, not just connectivity, determines functionality. While classical methods rely on combinatorial optimization, and recent ML approaches address only restricted linear variants with small-size networks, no prior model directly tackles the QoSD problem under nonlinear edge-weight functions. This work proposes \\PIMMA, a self-reinforcing generative framework that synthesizes feasible solutions in latent space, to fill this gap. Our method includes three phases: (1) Forge: a Predictive Path-Stressing (PPS) algorithm that uses graph learning and approximation to produce feasible solutions with performance guarantee, (2) Morph: a new theoretically grounded training paradigm for Mixture of Conditional VAEs guided by an energy-based model to capture solution feature distributions, and (3) Refine: a reinforcement learning agent that explores this space to generate progressively near-optimal solutions using our designed differentiable reward function. Experiments on both synthetic and real-world networks show that our approach consistently outperforms classical and ML baselines, particularly in scenarios with nonlinear cost functions where traditional methods fail to generalize.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "267",
        "title": "Video Reasoning without Training",
        "author": [
            "Deepak Sridhar",
            "Kartikeya Bhardwaj",
            "Jeya Pradha Jeyaraj",
            "Nuno Vasconcelos",
            "Ankita Nayak",
            "Harris Teague"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17045",
        "abstract": "Video reasoning using Large Multimodal Models (LMMs) relies on costly reinforcement learning (RL) and verbose chain-of-thought, resulting in substantial computational overhead during both training and inference. Moreover, the mechanisms that control the thinking process in these reasoning models are very limited. In this paper, using entropy of the model's output as a signal, we discover that the high-quality models go through a series of micro-explorations and micro-exploitations which keep the reasoning process grounded (i.e., avoid excessive randomness while the model is exploring or thinking through an answer). We further observe that once this \"thinking\" process is over, more accurate models demonstrate a better convergence by reducing the entropy significantly via a final exploitation phase (i.e., a more certain convergence towards a solution trajectory). We then use these novel, theoretically-grounded insights to tune the model's behavior directly at inference, without using any RL or supervised fine-tuning. Specifically, during inference, our proposed approach called V-Reason (Video-Reason) adapts the value cache of the LMM via a few optimization steps on a small, trainable controller using an entropy-based objective, i.e., no supervision from any dataset or RL is necessary. This tuning improves the model's micro-exploration and exploitation behavior during inference. Our experiments show that our proposed method achieves significant improvements over the base instruction-tuned models across several video reasoning datasets, narrowing the gap with RL-trained models to within 0.6% average accuracy without any training, while offering massive efficiency benefits: output tokens are reduced by 58.6% compared to the RL model.",
        "tags": [
            "CoT",
            "RL"
        ]
    },
    {
        "id": "268",
        "title": "How Universal Are SAM2 Features?",
        "author": [
            "Masoud Khairi Atani",
            "Alon Harell",
            "Hyomin Choi",
            "Runyu Yang",
            "Fabien Racape",
            "Ivan V. Bajic"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17051",
        "abstract": "The trade-off between general-purpose foundation vision models and their specialized counterparts is critical for efficient feature coding design and is not yet fully understood. We investigate this trade-off by comparing the feature versatility of the general-purpose Hiera encoder against the segmentation-specialized Segment Anything Model 2 (SAM2). Using a lightweight, trainable neck to probe the adaptability of their frozen features, we quantify the information-theoretic cost of specialization. Our results reveal that while SAM2's specialization is highly effective for spatially-related tasks like depth estimation, it comes at a cost. The specialized SAM2 encoder underperforms its generalist predecessor, Hiera, on conceptually distant tasks such as pose estimation and image captioning, demonstrating a measurable loss of broader semantic information. A novel cross-neck analysis on SAM2 reveals that each level of adaptation creates a further representational bottleneck. Our analysis illuminates these trade-offs in feature universality, providing a quantitative foundation for designing efficient feature coding and adaptation strategies for diverse downstream applications.",
        "tags": [
            "Depth Estimation",
            "Pose Estimation",
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "269",
        "title": "ToolCritic: Detecting and Correcting Tool-Use Errors in Dialogue Systems",
        "author": [
            "Hassan Hamad",
            "Yingru Xu",
            "Liang Zhao",
            "Wenbo Yan",
            "Narendra Gyanchandani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17052",
        "abstract": "Tool-augmented large language models (LLMs) are increasingly employed in real-world applications, but tool usage errors still hinder their reliability. We introduce ToolCritic, a diagnostic framework that evaluates and improves LLM behavior in multi-turn, tool-augmented dialogues. ToolCritic detects eight distinct error types specific to tool-calling (e.g., premature invocation, argument misalignment, and misinterpretation of tool outputs) and provides targeted feedback to the main LLM. The main LLM, assumed to have strong reasoning, task understanding and orchestration capabilities, then revises its response based on ToolCritic's feedback. We systematically define these error categories and construct a synthetic dataset to train ToolCritic. Experimental results on the Schema-Guided Dialogue (SGD) dataset demonstrate that ToolCritic improves tool-calling accuracy by up to 13% over baselines, including zero-shot prompting and self-correction techniques. This represents a promising step toward more robust LLM integration with external tools in real-world dialogue applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "270",
        "title": "Will AI also replace inspectors? Investigating the potential of generative AIs in usability inspection",
        "author": [
            "Luis F. G. Campos",
            "Leonardo C. Marques",
            "Walter T. Nakamura"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17056",
        "abstract": "Usability inspection is a well-established technique for identifying interaction issues in software interfaces, thereby contributing to improved product quality. However, it is a costly process that requires time and specialized knowledge from inspectors. With advances in Artificial Intelligence (AI), new opportunities have emerged to support this task, particularly through generative models capable of interpreting interfaces and performing inspections more efficiently. This study examines the performance of generative AIs in identifying usability problems, comparing them to those of experienced human inspectors. A software prototype was evaluated by four specialists and two AI models (GPT-4o and Gemini 2.5 Flash), using metrics such as precision, recall, and F1-score. While inspectors achieved the highest levels of precision and overall coverage, the AIs demonstrated high individual performance and discovered many novel defects, but with a higher rate of false positives and redundant reports. The combination of AIs and human inspectors produced the best results, revealing their complementarity. These findings suggest that AI, in its current stage, cannot replace human inspectors but can serve as a valuable augmentation tool to improve efficiency and expand defect coverage. The results provide evidence based on quantitative analysis to inform the discussion on the role of AI in usability inspections, pointing to viable paths for its complementary use in software quality assessment contexts.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": "271",
        "title": "The Ends Justify the Thoughts: RL-Induced Motivated Reasoning in LLMs",
        "author": [
            "Nikolaus Howe",
            "Micah Carroll"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17057",
        "abstract": "The use of reinforcement learning (RL) with chain-of-thought (CoT) reasoning has emerged as a promising approach for developing more capable language models. In turn, this has led to investigation of CoT monitoring as a compelling method for detecting harmful behaviors such as reward hacking, under the assumption that models' reasoning processes reflect their internal decision-making. In practice, LLM training often produces unintended behaviors due to imperfect reward signals, leading models to develop misaligned tendencies. A common corrective approach is to apply post-hoc instructions to avoid problematic behaviors like sycophancy, but what happens to the model's reasoning process when these instructions conflict with learned behaviors? We investigate this question in simple settings and find that models engage in systematic motivated reasoning -- generating plausible-sounding justifications for violating their instructions while downplaying potential harms. Beyond being an interesting property of training, we find that while motivated reasoning can be detected by most frontier reasoning models, smaller LLM judges can fail to identify a portion of it, and in rare cases can themselves be persuaded that the reasoning is correct, despite it contradicting clear instructions. This capability gap raises concerns that as models become more sophisticated, their motivated reasoning may become increasingly difficult for monitors to detect. Our results underscore the need to account for motivated reasoning when relying on chain-of-thought processes for model evaluation and oversight. All code for this paper will be made available. WARNING: some examples in this paper may be upsetting.",
        "tags": [
            "CoT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "272",
        "title": "Investigating Thinking Behaviours of Reasoning-Based Language Models for Social Bias Mitigation",
        "author": [
            "Guoqing Luo",
            "Iffat Maab",
            "Lili Mou",
            "Junichi Yamagishi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17062",
        "abstract": "While reasoning-based large language models excel at complex tasks through an internal, structured thinking process, a concerning phenomenon has emerged that such a thinking process can aggregate social stereotypes, leading to biased outcomes. However, the underlying behaviours of these language models in social bias scenarios remain underexplored. In this work, we systematically investigate mechanisms within the thinking process behind this phenomenon and uncover two failure patterns that drive social bias aggregation: 1) stereotype repetition, where the model relies on social stereotypes as its primary justification, and 2) irrelevant information injection, where it fabricates or introduces new details to support a biased narrative. Building on these insights, we introduce a lightweight prompt-based mitigation approach that queries the model to review its own initial reasoning against these specific failure patterns. Experiments on question answering (BBQ and StereoSet) and open-ended (BOLD) benchmarks show that our approach effectively reduces bias while maintaining or improving accuracy.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "273",
        "title": "GSPlane: Concise and Accurate Planar Reconstruction via Structured Representation",
        "author": [
            "Ruitong Gan",
            "Junran Peng",
            "Yang Liu",
            "Chuanchen Luo",
            "Qing Li",
            "Zhaoxiang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17095",
        "abstract": "Planes are fundamental primitives of 3D sences, especially in man-made environments such as indoor spaces and urban streets. Representing these planes in a structured and parameterized format facilitates scene editing and physical simulations in downstream applications. Recently, Gaussian Splatting (GS) has demonstrated remarkable effectiveness in the Novel View Synthesis task, with extensions showing great potential in accurate surface reconstruction. However, even state-of-the-art GS representations often struggle to reconstruct planar regions with sufficient smoothness and precision. To address this issue, we propose GSPlane, which recovers accurate geometry and produces clean and well-structured mesh connectivity for plane regions in the reconstructed scene. By leveraging off-the-shelf segmentation and normal prediction models, GSPlane extracts robust planar priors to establish structured representations for planar Gaussian coordinates, which help guide the training process by enforcing geometric consistency. To further enhance training robustness, a Dynamic Gaussian Re-classifier is introduced to adaptively reclassify planar Gaussians with persistently high gradients as non-planar, ensuring more reliable optimization. Furthermore, we utilize the optimized planar priors to refine the mesh layouts, significantly improving topological structure while reducing the number of vertices and faces. We also explore applications of the structured planar representation, which enable decoupling and flexible manipulation of objects on supportive planes. Extensive experiments demonstrate that, with no sacrifice in rendering quality, the introduction of planar priors significantly improves the geometric accuracy of the extracted meshes across various baselines.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Segmentation"
        ]
    },
    {
        "id": "274",
        "title": "Can Transformer Memory Be Corrupted? Investigating Cache-Side Vulnerabilities in Large Language Models",
        "author": [
            "Elias Hossain",
            "Swayamjit Saha",
            "Somshubhra Roy",
            "Ravi Prasad"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17098",
        "abstract": "Even when prompts and parameters are secured, transformer language models remain vulnerable because their key-value (KV) cache during inference constitutes an overlooked attack surface. This paper introduces Malicious Token Injection (MTI), a modular framework that systematically perturbs cached key vectors at selected layers and timesteps through controlled magnitude and frequency, using additive Gaussian noise, zeroing, and orthogonal rotations. A theoretical analysis quantifies how these perturbations propagate through attention, linking logit deviations to the Frobenius norm of corruption and softmax Lipschitz dynamics. Empirical results show that MTI significantly alters next-token distributions and downstream task performance across GPT-2 and LLaMA-2/7B, as well as destabilizes retrieval-augmented and agentic reasoning pipelines. These findings identify cache integrity as a critical yet underexplored vulnerability in current LLM deployments, positioning cache corruption as a reproducible and theoretically grounded threat model for future robustness and security research.",
        "tags": [
            "GPT",
            "LLM",
            "LLaMA",
            "Transformer"
        ]
    },
    {
        "id": "275",
        "title": "Boosting Fidelity for Pre-Trained-Diffusion-Based Low-Light Image Enhancement via Condition Refinement",
        "author": [
            "Xiaogang Xu",
            "Jian Wang",
            "Yunfan Lu",
            "Ruihang Chu",
            "Ruixing Wang",
            "Jiafei Wu",
            "Bei Yu",
            "Liang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17105",
        "abstract": "Diffusion-based methods, leveraging pre-trained large models like Stable Diffusion via ControlNet, have achieved remarkable performance in several low-level vision tasks. However, Pre-Trained Diffusion-Based (PTDB) methods often sacrifice content fidelity to attain higher perceptual realism. This issue is exacerbated in low-light scenarios, where severely degraded information caused by the darkness limits effective control. We identify two primary causes of fidelity loss: the absence of suitable conditional latent modeling and the lack of bidirectional interaction between the conditional latent and noisy latent in the diffusion process. To address this, we propose a novel optimization strategy for conditioning in pre-trained diffusion models, enhancing fidelity while preserving realism and aesthetics. Our method introduces a mechanism to recover spatial details lost during VAE encoding, i.e., a latent refinement pipeline incorporating generative priors. Additionally, the refined latent condition interacts dynamically with the noisy latent, leading to improved restoration performance. Our approach is plug-and-play, seamlessly integrating into existing diffusion networks to provide more effective control. Extensive experiments demonstrate significant fidelity improvements in PTDB methods.",
        "tags": [
            "ControlNet",
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": "276",
        "title": "Fighter: Unveiling the Graph Convolutional Nature of Transformers in Time Series Modeling",
        "author": [
            "Chen Zhang",
            "Weixin Bu",
            "Wendong Xu",
            "Runsheng Yu",
            "Yik-Chung Wu",
            "Ngai Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17106",
        "abstract": "Transformers have achieved remarkable success in time series modeling, yet their internal mechanisms remain opaque. This work demystifies the Transformer encoder by establishing its fundamental equivalence to a Graph Convolutional Network (GCN). We show that in the forward pass, the attention distribution matrix serves as a dynamic adjacency matrix, and its composition with subsequent transformations performs computations analogous to graph convolution. Moreover, we demonstrate that in the backward pass, the update dynamics of value and feed-forward projections mirror those of GCN parameters. Building on this unified theoretical reinterpretation, we propose \\textbf{Fighter} (Flexible Graph Convolutional Transformer), a streamlined architecture that removes redundant linear projections and incorporates multi-hop graph aggregation. This perspective yields an explicit and interpretable representation of temporal dependencies across different scales, naturally expressed as graph edges. Experiments on standard forecasting benchmarks confirm that Fighter achieves competitive performance while providing clearer mechanistic interpretability of its predictions.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "277",
        "title": "Structured Debate Improves Corporate Credit Reasoning in Financial AI",
        "author": [
            "Yoonjin Lee",
            "Munhee Kim",
            "Hanbi Choi",
            "Juhyeon Park",
            "Seungho Lyoo",
            "Woojin Park"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17108",
        "abstract": "Despite advances in financial AI, the automation of evidence-based reasoning remains unresolved in corporate credit assessment, where qualitative non-financial indicators exert decisive influence on loan repayment outcomes yet resist formalization. Existing approaches focus predominantly on numerical prediction and provide limited support for the interpretive judgments required in professional loan evaluation. This study develops and evaluates two operational large language model (LLM)-based systems designed to generate structured reasoning from non-financial evidence. The first is a non-adversarial single-agent system (NAS) that produces bidirectional analysis through a single-pass reasoning pipeline. The second is a debate-based multi-agent system (KPD-MADS) that operationalizes adversarial verification through a ten-step structured interaction protocol grounded in Karl Popper's critical dialogue framework. Both systems were applied to three real corporate cases and evaluated by experienced credit risk professionals. Compared to manual expert reporting, both systems achieved substantial productivity gains (NAS: 11.55 s per case; KPD-MADS: 91.97 s; human baseline: 1920 s). The KPD-MADS demonstrated superior reasoning quality, receiving higher median ratings in explanatory adequacy (4.0 vs. 3.0), practical applicability (4.0 vs. 3.0), and usability (62.5 vs. 52.5). These findings show that structured multi-agent interaction can enhance reasoning rigor and interpretability in financial AI, advancing scalable and defensible automation in corporate credit assessment.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "278",
        "title": "Verification-Aware Planning for Multi-Agent Systems",
        "author": [
            "Tianyang Xu",
            "Dan Zhang",
            "Kushan Mitra",
            "Estevam Hruschka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17109",
        "abstract": "Large language model (LLM) agents are increasingly deployed to tackle complex tasks, often necessitating collaboration among multiple specialized agents. However, multi-agent collaboration introduces new challenges in planning, coordination, and verification. Execution failures frequently arise not from flawed reasoning alone, but from subtle misalignments in task interpretation, output format, or inter-agent handoffs. To address these challenges, we present VeriMAP, a framework for multi-agent collaboration with verification-aware planning. The VeriMAP planner decomposes tasks, models subtask dependencies, and encodes planner-defined passing criteria as subtask verification functions (VFs) in Python and natural language. We evaluate VeriMAP on diverse datasets, demonstrating that it outperforms both single- and multi-agent baselines while enhancing system robustness and interpretability. Our analysis highlights how verification-aware planning enables reliable coordination and iterative refinement in multi-agent systems, without relying on external labels or annotations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "279",
        "title": "Efficient Vision-Language-Action Models for Embodied Manipulation: A Systematic Survey",
        "author": [
            "Weifan Guan",
            "Qinghao Hu",
            "Aosheng Li",
            "Jian Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17111",
        "abstract": "Vision-Language-Action (VLA) models extend vision-language models to embodied control by mapping natural-language instructions and visual observations to robot actions. Despite their capabilities, VLA systems face significant challenges due to their massive computational and memory demands, which conflict with the constraints of edge platforms such as on-board mobile manipulators that require real-time performance. Addressing this tension has become a central focus of recent research. In light of the growing efforts toward more efficient and scalable VLA systems, this survey provides a systematic review of approaches for improving VLA efficiency, with an emphasis on reducing latency, memory footprint, and training and inference costs. We categorize existing solutions into four dimensions: model architecture, perception feature, action generation, and training/inference strategies, summarizing representative techniques within each category. Finally, we discuss future trends and open challenges, highlighting directions for advancing efficient embodied intelligence.",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "280",
        "title": "DVAGen: Dynamic Vocabulary Augmented Generation",
        "author": [
            "Wei Du",
            "Nuowei Liu",
            "Jie Wang",
            "Jiahao Kuang",
            "Tao Ji",
            "Xiaoling Wang",
            "Yuanbin Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17115",
        "abstract": "Language models trained with a fixed vocabulary struggle to generalize to novel or out-of-vocabulary words, limiting their flexibility in handling diverse token combinations. Existing dynamic vocabulary approaches attempt to address this limitation but face challenges such as fragmented codebases, lack of support for modern LLMs, and limited inference scalability. To overcome these issues, we introduce DVAGen, a fully open-source, unified framework designed for training, evaluation, and visualization of dynamic vocabulary-augmented language models. Our framework modularizes the pipeline for ease of customization, integrates seamlessly with open-source LLMs, and is the first to provide both CLI and WebUI tools for real-time result inspection. We validate the effectiveness of dynamic vocabulary methods on modern LLMs and demonstrate support for batch inference, significantly improving inference throughput.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "281",
        "title": "Continuous Q-Score Matching: Diffusion Guided Reinforcement Learning for Continuous-Time Control",
        "author": [
            "Chengxiu Hua",
            "Jiawen Gu",
            "Yushun Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17122",
        "abstract": "Reinforcement learning (RL) has achieved significant success across a wide range of domains, however, most existing methods are formulated in discrete time. In this work, we introduce a novel RL method for continuous-time control, where stochastic differential equations govern state-action dynamics. Departing from traditional value function-based approaches, our key contribution is the characterization of continuous-time Q-functions via a martingale condition and the linking of diffusion policy scores to the action gradient of a learned continuous Q-function by the dynamic programming principle. This insight motivates Continuous Q-Score Matching (CQSM), a score-based policy improvement algorithm. Notably, our method addresses a long-standing challenge in continuous-time RL: preserving the action-evaluation capability of Q-functions without relying on time discretization. We further provide theoretical closed-form solutions for linear-quadratic (LQ) control problems within our framework. Numerical results in simulated environments demonstrate the effectiveness of our proposed method and compare it to popular baselines.",
        "tags": [
            "Diffusion",
            "RL",
            "Score Matching"
        ]
    },
    {
        "id": "282",
        "title": "Semantic Intelligence: A Bio-Inspired Cognitive Framework for Embodied Agents",
        "author": [
            "Wenbing Tang",
            "Meilin Zhu",
            "Fenghua Wu",
            "Yang Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17129",
        "abstract": "Recent advancements in Large Language Models (LLMs) have greatly enhanced natural language understanding and content generation. However, these models primarily operate in disembodied digital environments and lack interaction with the physical world. To address this limitation, Embodied Artificial Intelligence (EAI) has emerged, focusing on agents that can perceive and interact with their surroundings. Despite progress, current embodied agents face challenges in unstructured real-world environments due to insufficient semantic intelligence, which is critical for understanding and reasoning about complex tasks. This paper introduces the Semantic Intelligence-Driven Embodied (SIDE) agent framework, which integrates a hierarchical semantic cognition architecture with a semantic-driven decision-making process. This enables agents to reason about and interact with the physical world in a contextually adaptive manner. The framework is inspired by biological cognitive mechanisms and utilizes bio-inspired principles to design a semantic cognitive architecture that mimics how humans and animals integrate and process sensory information. We present this framework as a step toward developing more intelligent and versatile embodied agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "283",
        "title": "SEER: Enhancing Chain-of-Thought Code Generation through Self-Exploring Deep Reasoning",
        "author": [
            "Shuzheng Gao",
            "Chaozheng Wang",
            "Cuiyun Gao",
            "Michael R. Lyu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17130",
        "abstract": "Code generation, the task of creating executable programs from natural language requirements, has recently seen tremendous advances through Chain-of-Thought (CoT) reasoning, which enables Large Language Models (LLMs) to develop high-level reasoning plans before writing code. Recent research has proposed various methods to enhance models' CoT reasoning for code generation such as prompt engineering and supervised fine-tuning. However, existing approaches still face three critical limitations: (1) limited exploration of diverse reasoning paths, which constrains generalization across various programming scenarios, (2) lack of quality assessment for intermediate reasoning steps, which hampers the reliability of the generated plans and code, and (3) the potential negative impact of \"overthinking\", potentially leading to unnecessarily complex and incorrect solutions. To address these limitations, we frame CoT code generation as a decision making problem and present SEER, a SElf-Exploring deep Reasoning framework that enables accurate and adaptive reasoning for code generation. SEER introduces three key components: (1) Diverse reasoning path exploration, which aims at exploring diverse reasoning paths and annotating intermediate steps without relying on manual experts or closed-source proprietary models; (2) Reasoning quality-aware model training, which trains a policy model for generating candidate reasoning steps and a value model for assessing their quality; and (3) Adaptive CoT reasoning, which dynamically switches between direct generation and step-by-step reasoning for different problems.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "284",
        "title": "GOOD: Training-Free Guided Diffusion Sampling for Out-of-Distribution Detection",
        "author": [
            "Xin Gao",
            "Jiyao Liu",
            "Guanghao Li",
            "Yueming Lyu",
            "Jianxiong Gao",
            "Weichen Yu",
            "Ningsheng Xu",
            "Liang Wang",
            "Caifeng Shan",
            "Ziwei Liu",
            "Chenyang Si"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17131",
        "abstract": "Recent advancements have explored text-to-image diffusion models for synthesizing out-of-distribution (OOD) samples, substantially enhancing the performance of OOD detection. However, existing approaches typically rely on perturbing text-conditioned embeddings, resulting in semantic instability and insufficient shift diversity, which limit generalization to realistic OOD. To address these challenges, we propose GOOD, a novel and flexible framework that directly guides diffusion sampling trajectories towards OOD regions using off-the-shelf in-distribution (ID) classifiers. GOOD incorporates dual-level guidance: (1) Image-level guidance based on the gradient of log partition to reduce input likelihood, drives samples toward low-density regions in pixel space. (2) Feature-level guidance, derived from k-NN distance in the classifier's latent space, promotes sampling in feature-sparse regions. Hence, this dual-guidance design enables more controllable and diverse OOD sample generation. Additionally, we introduce a unified OOD score that adaptively combines image and feature discrepancies, enhancing detection robustness. We perform thorough quantitative and qualitative analyses to evaluate the effectiveness of GOOD, demonstrating that training with samples generated by GOOD can notably enhance OOD detection performance.",
        "tags": [
            "Detection",
            "Diffusion",
            "Text-to-Image"
        ]
    },
    {
        "id": "285",
        "title": "Do LLMs Recognize Your Latent Preferences? A Benchmark for Latent Information Discovery in Personalized Interaction",
        "author": [
            "Ioannis Tsaknakis",
            "Bingqing Song",
            "Shuyu Gan",
            "Dongyeop Kang",
            "Alfredo Garcia",
            "Gaowen Liu",
            "Charles Fleming",
            "Mingyi Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17132",
        "abstract": "Large Language Models (LLMs) excel at producing broadly relevant text, but this generality becomes a limitation when user-specific preferences are required, such as recommending restaurants or planning travel. In these scenarios, users rarely articulate every preference explicitly; instead, much of what they care about remains latent, waiting to be inferred. This raises a fundamental question: Can LLMs uncover and reason about such latent information through conversation?\nWe address this problem by introducing a unified benchmark for evaluating latent information discovery - the ability of LLMs to reveal and utilize hidden user attributes through multi-turn interaction. The benchmark spans three progressively realistic settings: the classic 20 Questions game, Personalized Question Answering, and Personalized Text Summarization. All tasks share a tri-agent framework (User, Assistant, Judge) enabling turn-level evaluation of elicitation and adaptation. Our results reveal that while LLMs can indeed surface latent information through dialogue, their success varies dramatically with context: from 32% to 98%, depending on task complexity, topic, and number of hidden attributes. This benchmark provides the first systematic framework for studying latent information discovery in personalized interaction, highlighting that effective preference inference remains an open frontier for building truly adaptive AI systems.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "286",
        "title": "In-situ Autoguidance: Eliciting Self-Correction in Diffusion Models",
        "author": [
            "Enhao Gu",
            "Haolin Hou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17136",
        "abstract": "The generation of high-quality, diverse, and prompt-aligned images is a central goal in image-generating diffusion models. The popular classifier-free guidance (CFG) approach improves quality and alignment at the cost of reduced variation, creating an inherent entanglement of these effects. Recent work has successfully disentangled these properties by guiding a model with a separately trained, inferior counterpart; however, this solution introduces the considerable overhead of requiring an auxiliary model. We challenge this prerequisite by introducing In-situ Autoguidance, a method that elicits guidance from the model itself without any auxiliary components. Our approach dynamically generates an inferior prediction on the fly using a stochastic forward pass, reframing guidance as a form of inference-time self-correction. We demonstrate that this zero-cost approach is not only viable but also establishes a powerful new baseline for cost-efficient guidance, proving that the benefits of self-guidance can be achieved without external models.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "287",
        "title": "KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation",
        "author": [
            "WenBo Xu",
            "Liu Liu",
            "Li Zhang",
            "Ran Zhang",
            "Hao Wu",
            "Dan Guo",
            "Meng Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17137",
        "abstract": "Articulated objects, such as laptops and drawers, exhibit significant challenges for 3D reconstruction and pose estimation due to their multi-part geometries and variable joint configurations, which introduce structural diversity across different states. To address these challenges, we propose KineDiff3D: Kinematic-Aware Diffusion for Category-Level Articulated Object Shape Reconstruction and Generation, a unified framework for reconstructing diverse articulated instances and pose estimation from single view input. Specifically, we first encode complete geometry (SDFs), joint angles, and part segmentation into a structured latent space via a novel Kinematic-Aware VAE (KA-VAE). In addition, we employ two conditional diffusion models: one for regressing global pose (SE(3)) and joint parameters, and another for generating the kinematic-aware latent code from partial observations. Finally, we produce an iterative optimization module that bidirectionally refines reconstruction accuracy and kinematic parameters via Chamfer-distance minimization while preserving articulation constraints. Experimental results on synthetic, semi-synthetic, and real-world datasets demonstrate the effectiveness of our approach in accurately reconstructing articulated objects and estimating their kinematic properties.",
        "tags": [
            "3D",
            "Diffusion",
            "Pose Estimation",
            "Segmentation",
            "VAE"
        ]
    },
    {
        "id": "288",
        "title": "Rethinking On-policy Optimization for Query Augmentation",
        "author": [
            "Zhichao Xu",
            "Shengyao Zhuang",
            "Xueguang Ma",
            "Bingsen Chen",
            "Yijun Tian",
            "Fengran Mo",
            "Jie Cao",
            "Vivek Srikumar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17139",
        "abstract": "Recent advances in large language models (LLMs) have led to a surge of interest in query augmentation for information retrieval (IR). Two main approaches have emerged. The first prompts LLMs to generate answers or pseudo-documents that serve as new queries, relying purely on the model's parametric knowledge or contextual information. The second applies reinforcement learning (RL) to fine-tune LLMs for query rewriting, directly optimizing retrieval metrics. While having respective advantages and limitations, the two approaches have not been compared under consistent experimental conditions. In this work, we present the first systematic comparison of prompting-based and RL-based query augmentation across diverse benchmarks, including evidence-seeking, ad hoc, and tool retrieval. Our key finding is that simple, training-free query augmentation often performs on par with, or even surpasses, more expensive RL-based counterparts, especially when using powerful LLMs. Motivated by this discovery, we introduce a novel hybrid method, On-policy Pseudo-document Query Expansion (OPQE), which, instead of rewriting a query, the LLM policy learns to generate a pseudo-document that maximizes retrieval performance, thus merging the flexibility and generative structure of prompting with the targeted optimization of RL. We show OPQE outperforms both standalone prompting and RL-based rewriting, demonstrating that a synergistic approach yields the best results. Our implementation is made available to facilitate reproducibility.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "289",
        "title": "PEACE: Towards Efficient Project-Level Efficiency Optimization via Hybrid Code Editing",
        "author": [
            "Xiaoxue Ren",
            "Jun Wan",
            "Yun Peng",
            "Zhongxin Liu",
            "Ming Liang",
            "Dajun Chen",
            "Wei Jiang",
            "Yong Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17142",
        "abstract": "Large Language Models (LLMs) have demonstrated significant capability in code generation, but their potential in code efficiency optimization remains underexplored. Previous LLM-based code efficiency optimization approaches exclusively focus on function-level optimization and overlook interaction between functions, failing to generalize to real-world development scenarios. Code editing techniques show great potential for conducting project-level optimization, yet they face challenges associated with invalid edits and suboptimal internal functions. To address these gaps, we propose Peace, a novel hybrid framework for Project-level code Efficiency optimization through Automatic Code Editing, which also ensures the overall correctness and integrity of the project. Peace integrates three key phases: dependency-aware optimizing function sequence construction, valid associated edits identification, and efficiency optimization editing iteration. To rigorously evaluate the effectiveness of Peace, we construct PeacExec, the first benchmark comprising 146 real-world optimization tasks from 47 high-impact GitHub Python projects, along with highly qualified test cases and executable environments. Extensive experiments demonstrate Peace's superiority over the state-of-the-art baselines, achieving a 69.2% correctness rate (pass@1), +46.9% opt rate, and 0.840 speedup in execution efficiency. Notably, our Peace outperforms all baselines by significant margins, particularly in complex optimization tasks with multiple functions. Moreover, extensive experiments are also conducted to validate the contributions of each component in Peace, as well as the rationale and effectiveness of our hybrid framework design.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "290",
        "title": "Mamba4Net: Distilled Hybrid Mamba Large Language Models For Networking",
        "author": [
            "Linhan Xia",
            "Mingzhan Yang",
            "Jingjing Wang",
            "Ziwei Yan",
            "Yakun Ren",
            "Guo Yu",
            "Kai Lei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17147",
        "abstract": "Transformer-based large language models (LLMs) are increasingly being adopted in networking research to address domain-specific challenges. However, their quadratic time complexity and substantial model sizes often result in significant computational overhead and memory constraints, particularly in resource-constrained environments. Drawing inspiration from the efficiency and performance of the Deepseek-R1 model within the knowledge distillation paradigm, this paper introduces Mamba4Net, a novel cross-architecture distillation framework. Mamba4Net transfers networking-specific knowledge from transformer-based LLMs to student models built on the Mamba architecture, which features linear time complexity. This design substantially enhances computational efficiency compared to the quadratic complexity of transformer-based models, while the reduced model size further minimizes computational demands, improving overall performance and resource utilization. To evaluate its effectiveness, Mamba4Net was tested across three diverse networking tasks: viewport prediction, adaptive bitrate streaming, and cluster job scheduling. Compared to existing methods that do not leverage LLMs, Mamba4Net demonstrates superior task performance. Furthermore, relative to direct applications of transformer-based LLMs, it achieves significant efficiency gains, including a throughput 3.96 times higher and a storage footprint of only 5.48% of that required by previous LLM-based approaches. These results highlight Mamba4Net's potential to enable the cost-effective application of LLM-derived knowledge in networking contexts. The source code is openly available to support further research and development.",
        "tags": [
            "DeepSeek",
            "LLM",
            "Mamba",
            "Transformer"
        ]
    },
    {
        "id": "291",
        "title": "Which LLM Multi-Agent Protocol to Choose?",
        "author": [
            "Hongyi Du",
            "Jiaqi Su",
            "Jisen Li",
            "Lijie Ding",
            "Yingxuan Yang",
            "Peixuan Han",
            "Xiangru Tang",
            "Kunlun Zhu",
            "Jiaxuan You"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17149",
        "abstract": "As large-scale multi-agent systems evolve, the communication protocol layer has become a critical yet under-evaluated factor shaping performance and reliability. Despite the existence of diverse protocols (A2A, ACP, ANP, Agora, etc.), selection is often intuition-driven and lacks standardized guidance. We introduce ProtocolBench, a benchmark that systematically compares agent protocols along four measurable axes: task success, end-to-end latency, message or byte overhead, and robustness under failures. On ProtocolBench, protocol choice significantly influences system behavior. In the Streaming Queue scenario, overall completion time varies by up to 36.5% across protocols, and mean end-to-end latency differs by 3.48 s. Under Fail-Storm Recovery, resilience also differs consistently across protocols. Beyond evaluation, we present ProtocolRouter, a learnable protocol router that selects per-scenario (or per-module) protocols from requirement and runtime signals. ProtocolRouter reduces Fail-Storm recovery time by up to 18.1% versus the best single-protocol baseline, and achieves scenario-specific gains such as higher success in GAIA. We also release ProtocolRouterBench to standardize protocol evaluation and improve reliability at scale.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "292",
        "title": "OmniVIC: A Self-Improving Variable Impedance Controller with Vision-Language In-Context Learning for Safe Robotic Manipulation",
        "author": [
            "Heng Zhang",
            "Wei-Hsing Huang",
            "Gokhan Solak",
            "Arash Ajoudani"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17150",
        "abstract": "We present OmniVIC, a universal variable impedance controller (VIC) enhanced by a vision language model (VLM), which improves safety and adaptation in any contact-rich robotic manipulation task to enhance safe physical interaction. Traditional VIC have shown advantages when the robot physically interacts with the environment, but lack generalization in unseen, complex, and unstructured safe interactions in universal task scenarios involving contact or uncertainty. To this end, the proposed OmniVIC interprets task context derived reasoning from images and natural language and generates adaptive impedance parameters for a VIC controller. Specifically, the core of OmniVIC is a self-improving Retrieval-Augmented Generation(RAG) and in-context learning (ICL), where RAG retrieves relevant prior experiences from a structured memory bank to inform the controller about similar past tasks, and ICL leverages these retrieved examples and the prompt of current task to query the VLM for generating context-aware and adaptive impedance parameters for the current manipulation scenario. Therefore, a self-improved RAG and ICL guarantee OmniVIC works in universal task scenarios. The impedance parameter regulation is further informed by real-time force/torque feedback to ensure interaction forces remain within safe thresholds. We demonstrate that our method outperforms baselines on a suite of complex contact-rich tasks, both in simulation and on real-world robotic tasks, with improved success rates and reduced force violations. OmniVIC takes a step towards bridging high-level semantic reasoning and low-level compliant control, enabling safer and more generalizable manipulation. Overall, the average success rate increases from 27% (baseline) to 61.4% (OmniVIC).",
        "tags": [
            "RAG",
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": "293",
        "title": "GACO-CAD: Geometry-Augmented and Conciseness-Optimized CAD Model Generation from Single Image",
        "author": [
            "Yinghui Wang",
            "Xinyu Zhang",
            "Peng Du"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17157",
        "abstract": "Generating editable, parametric CAD models from a single image holds great potential to lower the barriers of industrial concept design. However, current multi-modal large language models (MLLMs) still struggle with accurately inferring 3D geometry from 2D images due to limited spatial reasoning capabilities. We address this limitation by introducing GACO-CAD, a novel two-stage post-training framework. It is designed to achieve a joint objective: simultaneously improving the geometric accuracy of the generated CAD models and encouraging the use of more concise modeling procedures. First, during supervised fine-tuning, we leverage depth and surface normal maps as dense geometric priors, combining them with the RGB image to form a multi-channel input. In the context of single-view reconstruction, these priors provide complementary spatial cues that help the MLLM more reliably recover 3D geometry from 2D observations. Second, during reinforcement learning, we introduce a group length reward that, while preserving high geometric fidelity, promotes the generation of more compact and less redundant parametric modeling sequences. A simple dynamic weighting strategy is adopted to stabilize training. Experiments on the DeepCAD and Fusion360 datasets show that GACO-CAD achieves state-of-the-art performance under the same MLLM backbone, consistently outperforming existing methods in terms of code validity, geometric accuracy, and modeling conciseness.",
        "tags": [
            "3D",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "294",
        "title": "Integrating Performance Tools in Model Reasoning for GPU Kernel Optimization",
        "author": [
            "Daniel Nichols",
            "Konstantinos Parasyris",
            "Charles Jekel",
            "Abhinav Bhatele",
            "Harshitha Menon"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17158",
        "abstract": "Language models are now prevalent in software engineering with many developers using them to automate tasks and accelerate their development. While language models have been tremendous at accomplishing complex software engineering tasks, there are still many areas where they fail to deliver desirable results, for instance code performance related tasks. Tasks like optimization depend on many complex data from the environment, hardware, etc. that are not directly represented in source code. Recent efforts have seen large improvements in general code modeling tasks using chain-of-thought style reasoning, but these models still fail to comprehend how the environment interacts with code performance. In this paper we propose a methodology to train language models that can interact with performance tools during their reasoning process. We then demonstrate how this methodology can be used to train a state-of-the-art GPU kernel optimization model.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": "295",
        "title": "TREAT: A Code LLMs Trustworthiness / Reliability Evaluation and Testing Framework",
        "author": [
            "Shuzheng Gao",
            "Eric John Li",
            "Man Ho Lam",
            "Jingyu Xiao",
            "Yuxuan Wan",
            "Chaozheng Wang",
            "Ng Man Tik",
            "Michael R. Lyu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17163",
        "abstract": "Large foundation models are fundamentally transforming the software engineering landscape, demonstrating exceptional capabilities across diverse tasks such as code generation, debugging, and testing. Despite this rapid progress, a significant gap remains in how to comprehensively evaluate these models' trustworthiness in real-world software engineering scenarios. Existing benchmarks suffer from limited task scope and fail to incorporate critical evaluation aspects such as the robustness and reliability of models. To bridge this gap, we present an evaluation framework called TREAT (Code LLMs Trustworthiness / Reliability Evaluation And Testing) that provides a holistic assessment of model performance in code intelligence tasks. Our evaluation framework addresses key limitations in existing approaches with four main improvements: (1) Multi-Task Holistic Evaluation that spans diverse software engineering activities rather than limited coding tasks; (2) Multi-Language and Multi-Modality Assessment that extends beyond traditional single-language, text-only benchmarks to include multi-modality coding tasks; (3) Robustness Assessment that evaluates model reliability under semantically-preserving code transformations; and (4) Rigorous Evaluation Methodology that enhances the trustworthiness of evaluation results through diverse evaluation prompts and adaptive solution extraction. Based on this evaluation framework, we assess 26 state-of-the-art models and uncover both their strengths and limitations, yielding several key insights:(1) Current models show substantial performance variation across programming tasks; (2) Multi-modal language models demonstrate specific performance limitations in UI code generation and edit;",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "296",
        "title": "Software Testing with Large Language Models: An Interview Study with Practitioners",
        "author": [
            "Maria Deolinda Santana",
            "Cleyton Magalhaes",
            "Ronnie de Souza Santos"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17164",
        "abstract": "\\textit{Background:} The use of large language models in software testing is growing fast as they support numerous tasks, from test case generation to automation, and documentation. However, their adoption often relies on informal experimentation rather than structured guidance. \\textit{Aims:} This study investigates how software testing professionals use LLMs in practice to propose a preliminary, practitioner-informed guideline to support their integration into testing workflows. \\textit{Method:} We conducted a qualitative study with 15 software testers from diverse roles and domains. Data were collected through semi-structured interviews and analyzed using grounded theory-based processes focused on thematic analysis. \\textit{Results:} Testers described an iterative and reflective process that included defining testing objectives, applying prompt engineering strategies, refining prompts, evaluating outputs, and learning over time. They emphasized the need for human oversight and careful validation, especially due to known limitations of LLMs such as hallucinations and inconsistent reasoning. \\textit{Conclusions:} LLM adoption in software testing is growing, but remains shaped by evolving practices and caution around risks. This study offers a starting point for structuring LLM use in testing contexts and invites future research to refine these practices across teams, tools, and tasks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "297",
        "title": "When AI companions become witty: Can human brain recognize AI-generated irony?",
        "author": [
            "Xiaohui Rao",
            "Hanlin Wu",
            "Zhenguang G. Cai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17168",
        "abstract": "As Large Language Models (LLMs) are increasingly deployed as social agents and trained to produce humor and irony, a question emerges: when encountering witty AI remarks, do people interpret these as intentional communication or mere computational output? This study investigates whether people adopt the intentional stance, attributing mental states to explain behavior,toward AI during irony comprehension. Irony provides an ideal paradigm because it requires distinguishing intentional contradictions from unintended errors through effortful semantic reanalysis. We compared behavioral and neural responses to ironic statements from AI versus human sources using established ERP components: P200 reflecting early incongruity detection and P600 indexing cognitive efforts in reinterpreting incongruity as deliberate irony. Results demonstrate that people do not fully adopt the intentional stance toward AI-generated irony. Behaviorally, participants attributed incongruity to deliberate communication for both sources, though significantly less for AI than human, showing greater tendency to interpret AI incongruities as computational errors. Neural data revealed attenuated P200 and P600 effects for AI-generated irony, suggesting reduced effortful detection and reanalysis consistent with diminished attribution of communicative intent. Notably, people who perceived AI as more sincere showed larger P200 and P600 effects for AI-generated irony, suggesting that intentional stance adoption is calibrated by specific mental models of artificial agents. These findings reveal that source attribution shapes neural processing of social-communicative phenomena. Despite current LLMs' linguistic sophistication, achieving genuine social agency requires more than linguistic competence, it necessitates a shift in how humans perceive and attribute intentionality to artificial agents.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "298",
        "title": "Generation then Reconstruction: Accelerating Masked Autoregressive Models via Two-Stage Sampling",
        "author": [
            "Feihong Yan",
            "Peiru Wang",
            "Yao Zhu",
            "Kaiyu Pang",
            "Qingyan Wei",
            "Huiqi Li",
            "Linfeng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17171",
        "abstract": "Masked Autoregressive (MAR) models promise better efficiency in visual generation than autoregressive (AR) models for the ability of parallel generation, yet their acceleration potential remains constrained by the modeling complexity of spatially correlated visual tokens in a single step. To address this limitation, we introduce Generation then Reconstruction (GtR), a training-free hierarchical sampling strategy that decomposes generation into two stages: structure generation establishing global semantic scaffolding, followed by detail reconstruction efficiently completing remaining tokens. Assuming that it is more difficult to create an image from scratch than to complement images based on a basic image framework, GtR is designed to achieve acceleration by computing the reconstruction stage quickly while maintaining the generation quality by computing the generation stage slowly. Moreover, observing that tokens on the details of an image often carry more semantic information than tokens in the salient regions, we further propose Frequency-Weighted Token Selection (FTS) to offer more computation budget to tokens on image details, which are localized based on the energy of high frequency information. Extensive experiments on ImageNet class-conditional and text-to-image generation demonstrate 3.72x speedup on MAR-H while maintaining comparable quality (e.g., FID: 1.59, IS: 304.4 vs. original 1.59, 299.1), substantially outperforming existing acceleration methods across various model scales and generation tasks. Our codes will be released in https://github.com/feihongyan1/GtR.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": "299",
        "title": "Combining ECG Foundation Model and XGBoost to Predict In-Hospital Malignant Ventricular Arrhythmias in AMI Patients",
        "author": [
            "Shun Huang",
            "Wenlu Xing",
            "Shijia Geng",
            "Hailong Wang",
            "Guangkun Nie",
            "Gongzheng Tang",
            "Chenyang He",
            "Shenda Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17172",
        "abstract": "Malignant ventricular arrhythmias (VT/VF) following acute myocardial infarction (AMI) are a major cause of in-hospital death, yet early identification remains a clinical challenge. While traditional risk scores have limited performance, end-to-end deep learning models often lack the interpretability needed for clinical trust. This study aimed to develop a hybrid predictive framework that integrates a large-scale electrocardiogram (ECG) foundation model (ECGFounder) with an interpretable XGBoost classifier to improve both accuracy and interpretability. We analyzed 6,634 ECG recordings from AMI patients, among whom 175 experienced in-hospital VT/VF. The ECGFounder model was used to extract 150-dimensional diagnostic probability features , which were then refined through feature selection to train the XGBoost classifier. Model performance was evaluated using AUC and F1-score , and the SHAP method was used for interpretability. The ECGFounder + XGBoost hybrid model achieved an AUC of 0.801 , outperforming KNN (AUC 0.677), RNN (AUC 0.676), and an end-to-end 1D-CNN (AUC 0.720). SHAP analysis revealed that model-identified key features, such as \"premature ventricular complexes\" (risk predictor) and \"normal sinus rhythm\" (protective factor), were highly consistent with clinical knowledge. We conclude that this hybrid framework provides a novel paradigm for VT/VF risk prediction by validating the use of foundation model outputs as effective, automated feature engineering for building trustworthy, explainable AI-based clinical decision support systems.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "300",
        "title": "Offline Policy Evaluation of Multi-Turn LLM Health Coaching with Real Users",
        "author": [
            "Melik Ozolcer",
            "Sang Won Bae"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17173",
        "abstract": "We study a web-deployed, tool-augmented LLM health coach with real users. In a pilot with seven users (280 rated turns), offline policy evaluation (OPE) over factorized decision heads (Tool/Style) shows that a uniform heavy-tool policy raises average value on logs but harms specific subgroups, most notably low-health-literacy/high-self-efficacy users. A lightweight simulator with hidden archetypes further shows that adding a small early information-gain bonus reliably shortens trait identification and improves goal success and pass@3. Together, these early findings indicate an evaluation-first path to personalization: freeze the generator, learn subgroup-aware decision heads on typed rewards (objective tool outcomes and satisfaction), and always report per-archetype metrics to surface subgroup harms that averages obscure.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "301",
        "title": "OLIVAW: ACIMOV's GitHub robot assisting agile collaborative ontology development",
        "author": [
            "Nicolas Robert",
            "Fabien Gandon",
            "Maxime LefranÃ§ois"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17184",
        "abstract": "Agile and collaborative approaches to ontologies design are crucial because they contribute to making them userdriven, up-to-date, and able to evolve alongside the systems they support, hence proper continuous validation tooling is required to ensure ontologies match developers' requirements all along their development. We propose OLIVAW (Ontology Long-lived Integration Via ACIMOV Workflow), a tool supporting the ACIMOV methodology on GitHub. It relies on W3C Standards to assist the development of modular ontologies through GitHub Composite Actions, pre-commit hooks, or a command line interface. OLIVAW was tested on several ontology projects to ensure its usefulness, genericity and reusability. A template repository is available for a quick start. OLIVAW is",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "302",
        "title": "Robustness in Text-Attributed Graph Learning: Insights, Trade-offs, and New Defenses",
        "author": [
            "Runlin Lei",
            "Lu Yi",
            "Mingguo He",
            "Pengyu Qiu",
            "Zhewei Wei",
            "Yongchao Liu",
            "Chuntao Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17185",
        "abstract": "While Graph Neural Networks (GNNs) and Large Language Models (LLMs) are powerful approaches for learning on Text-Attributed Graphs (TAGs), a comprehensive understanding of their robustness remains elusive. Current evaluations are fragmented, failing to systematically investigate the distinct effects of textual and structural perturbations across diverse models and attack scenarios. To address these limitations, we introduce a unified and comprehensive framework to evaluate robustness in TAG learning. Our framework evaluates classical GNNs, robust GNNs (RGNNs), and GraphLLMs across ten datasets from four domains, under diverse text-based, structure-based, and hybrid perturbations in both poisoning and evasion scenarios. Our extensive analysis reveals multiple findings, among which three are particularly noteworthy: 1) models have inherent robustness trade-offs between text and structure, 2) the performance of GNNs and RGNNs depends heavily on the text encoder and attack type, and 3) GraphLLMs are particularly vulnerable to training data corruption. To overcome the identified trade-offs, we introduce SFT-auto, a novel framework that delivers superior and balanced robustness against both textual and structural attacks within a single model. Our work establishes a foundation for future research on TAG security and offers practical solutions for robust TAG learning in adversarial environments. Our code is available at: https://github.com/Leirunlin/TGRB.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "303",
        "title": "HIDISC: A Hyperbolic Framework for Domain Generalization with Generalized Category Discovery",
        "author": [
            "Vaibhav Rathore",
            "Divyam Gupta",
            "Biplab Banerjee"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17188",
        "abstract": "Generalized Category Discovery (GCD) aims to classify test-time samples into either seen categories** -- available during training -- or novel ones, without relying on label supervision. Most existing GCD methods assume simultaneous access to labeled and unlabeled data during training and arising from the same domain, limiting applicability in open-world scenarios involving distribution shifts. Domain Generalization with GCD (DG-GCD) lifts this constraint by requiring models to generalize to unseen domains containing novel categories, without accessing targetdomain data during training. The only prior DG-GCD method, DG2CD-Net, relies on episodic training with multiple synthetic domains and task vector aggregation, incurring high computational cost and error accumulation. We propose HIDISC, a hyperbolic representation learning framework that achieves domain and category-level generalization without episodic simulation. To expose the model to minimal but diverse domain variations, we augment the source domain using GPT-guided diffusion, avoiding overfitting while maintaining efficiency. To structure the representation space, we introduce Tangent CutMix, a curvature-aware interpolation that synthesizes pseudo-novel samples in tangent space, preserving manifold consistency. A unified loss -- combining penalized Busemann alignment, hybrid hyperbolic contrastive regularization, and adaptive outlier repulsion -- **facilitates compact, semantically structured embeddings. A learnable curvature parameter further adapts the geometry to dataset complexity. HIDISC achieves state-of-the-art results on PACS , Office-Home , and DomainNet, consistently outperforming the existing Euclidean and hyperbolic (DG)-GCD baselines.",
        "tags": [
            "Diffusion",
            "GPT"
        ]
    },
    {
        "id": "304",
        "title": "SOLE: Hardware-Software Co-design of Softmax and LayerNorm for Efficient Transformer Inference",
        "author": [
            "Wenxun Wang",
            "Shuchang Zhou",
            "Wenyu Sun",
            "Peiqin Sun",
            "Yongpan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17189",
        "abstract": "Transformers have shown remarkable performance in both natural language processing (NLP) and computer vision (CV) tasks. However, their real-time inference speed and efficiency are limited due to the inefficiency in Softmax and Layer Normalization (LayerNorm). Previous works based on function approximation suffer from inefficient implementation as they place emphasis on computation while disregarding memory overhead concerns. Moreover, such methods rely on retraining to compensate for approximation error which can be costly and inconvenient.\nIn this paper, we present SOLE, a hardware-software co-design for Softmax and LayerNorm which is composed of E2Softmax and AILayerNorm. E2Softmax utilizes log2 quantization of exponent function and log-based division to approximate Softmax while AILayerNorm adopts low-precision statistic calculation. Compared with state-of-the-art designs, we achieve both low-precision calculation and low bit-width storage on Softmax and LayerNorm. Experiments show that SOLE maintains inference accuracy without retraining while offering orders of magnitude speedup and energy savings over GPU, achieving 3.04x, 3.86x energy-efficiency improvements and 2.82x, 3.32x area-efficiency improvements over prior state-of-the-art custom hardware for Softmax and LayerNorm, respectively.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "305",
        "title": "SimpleVSF: VLM-Scoring Fusion for Trajectory Prediction of End-to-End Autonomous Driving",
        "author": [
            "Peiru Zheng",
            "Yun Zhao",
            "Zhan Gong",
            "Hong Zhu",
            "Shaohua Wu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17191",
        "abstract": "End-to-end autonomous driving has emerged as a promising paradigm for achieving robust and intelligent driving policies. However, existing end-to-end methods still face significant challenges, such as suboptimal decision-making in complex scenarios. In this paper,we propose SimpleVSF (Simple VLM-Scoring Fusion), a novel framework that enhances end-to-end planning by leveraging the cognitive capabilities of Vision-Language Models (VLMs) and advanced trajectory fusion techniques. We utilize the conventional scorers and the novel VLM-enhanced scorers. And we leverage a robust weight fusioner for quantitative aggregation and a powerful VLM-based fusioner for qualitative, context-aware decision-making. As the leading approach in the ICCV 2025 NAVSIM v2 End-to-End Driving Challenge, our SimpleVSF framework demonstrates state-of-the-art performance, achieving a superior balance between safety, comfort, and efficiency.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "306",
        "title": "Understanding and Improving Length Generalization in Hierarchical Sparse Attention Models",
        "author": [
            "Jiaqi Leng",
            "Xiang Hu",
            "Junxiong Wang",
            "Jianguo Li",
            "Wei Wu",
            "Yucheng Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17196",
        "abstract": "Effectively processing long contexts is a critical challenge for language models. While standard Transformers are limited by quadratic complexity and poor length extrapolation, alternative architectures like sliding window attention and state space models sacrifice the ability to effectively utilize the full context due to their fixed-size memory. Chunk-based sparse attention has emerged as a promising paradigm for extreme length generalization, yet the key architectural principles underpinning its success are not yet fully understood. In this work, we present a systematic dissection of these models to identify the core components driving their performance. Through a unified framework and comprehensive ablation studies, we demonstrate that a combination of three design principles is critical: (1) an expressive, non-linear Chunk Encoder with a dedicated CLS token to produce representations for retrieval; (2) a Bypassing Residual Path to stably integrate retrieved global information without it being overridden by the local residual stream; and (3) enforced selection sparsity during pre-training to bridge the train-test distribution gap. We provide a theoretical motivation for intra-chunk information processing and landmark generation. By combining these principles, we establish a new state-of-the-art for training-free length extrapolation, successfully generalizing models trained on a 4K context to 32 million tokens on RULER and BABILong. Our findings provide a clear and empirically-grounded set of design principles for developing future, highly-capable long-context language models.",
        "tags": [
            "SSMs"
        ]
    },
    {
        "id": "307",
        "title": "ZSPAPrune: Zero-Shot Prompt-Aware Token Pruning for Vision-Language Models",
        "author": [
            "Pu Zhang",
            "Yuwei Li",
            "Xingyuan Xian",
            "Guoming Tang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17197",
        "abstract": "As the capabilities of Vision-Language Models (VLMs) advance, they can process increasingly large inputs, which, unlike in LLMs, generates significant visual token redundancy and leads to prohibitive inference costs. While many methods aim to reduce these costs by pruning visual tokens, existing approaches, whether based on attention or diversity, typically neglect the guidance of the text prompt and thus fail to prioritize task relevance. In this work, we propose a novel, zero-shot method that reframes the problem by introducing a prompt-aware perspective, explicitly modeling visual token pruning as a balance between task relevance and information diversity. Our hierarchical approach first selects a core set of task-relevant visual tokens and then supplements them with diversity tokens to preserve broader context. Experiments across multiple models and benchmarks show that our method achieves performance that matches or surpasses the state-of-the-art with only minimal accuracy loss, even when pruning up to 90\\% of the tokens. Furthermore, these gains are accompanied by significant reductions in GPU memory footprint and inference latency.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "308",
        "title": "From Pixels to People: Satellite-Based Mapping and Quantification of Riverbank Erosion and Lost Villages in Bangladesh",
        "author": [
            "M Saifuzzaman Rafat",
            "Mohd Ruhul Ameen",
            "Akif Islam",
            "Abu Saleh Musa Miah",
            "Jungpil Shin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17198",
        "abstract": "The great rivers of Bangladesh, arteries of commerce and sustenance, are also agents of relentless destruction. Each year, they swallow whole villages and vast tracts of farmland, erasing communities from the map and displacing thousands of families. To track this slow-motion catastrophe has, until now, been a Herculean task for human analysts. Here we show how a powerful general-purpose vision model, the Segment Anything Model (SAM), can be adapted to this task with remarkable precision. To do this, we assembled a new dataset - a digital chronicle of loss compiled from historical Google Earth imagery of Bangladesh's most vulnerable regions, including Mokterer Char Union, Kedarpur Union, Balchipara village, and Chowhali Upazila, from 2003 to 2025. Crucially, this dataset is the first to include manually annotated data on the settlements that have vanished beneath the water. Our method first uses a simple color-channel analysis to provide a rough segmentation of land and water, and then fine-tunes SAM's mask decoder to recognize the subtle signatures of riverbank erosion. The resulting model demonstrates a keen eye for this destructive process, achieving a mean Intersection over Union of 86.30% and a Dice score of 92.60% - a performance that significantly surpasses traditional methods and off-the-shelf deep learning models. This work delivers three key contributions: the first annotated dataset of disappeared settlements in Bangladesh due to river erosion; a specialized AI model fine-tuned for this critical task; and a method for quantifying land loss with compelling visual evidence. Together, these tools provide a powerful new lens through which policymakers and disaster management agencies can monitor erosion, anticipate its trajectory, and ultimately protect the vulnerable communities in its path.",
        "tags": [
            "SAM",
            "Segmentation"
        ]
    },
    {
        "id": "309",
        "title": "$\\mathcal{V}isi\\mathcal{P}runer$: Decoding Discontinuous Cross-Modal Dynamics for Efficient Multimodal LLMs",
        "author": [
            "Yingqi Fan",
            "Anhao Zhao",
            "Jinlan Fu",
            "Junlong Tong",
            "Hui Su",
            "Yijie Pan",
            "Wei Zhang",
            "Xiaoyu Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17205",
        "abstract": "Multimodal Large Language Models (MLLMs) have achieved strong performance across vision-language tasks, but suffer from significant computational overhead due to the quadratic growth of attention computations with the number of multimodal tokens. Though efforts have been made to prune tokens in MLLMs, \\textit{they lack a fundamental understanding of how MLLMs process and fuse multimodal information.} Through systematic analysis, we uncover a \\textbf{three-stage} cross-modal interaction process: (1) Shallow layers recognize task intent, with visual tokens acting as passive attention sinks; (2) Cross-modal fusion occurs abruptly in middle layers, driven by a few critical visual tokens; (3) Deep layers discard vision tokens, focusing solely on linguistic refinement. Based on these findings, we propose \\emph{VisiPruner}, a training-free pruning framework that reduces up to 99\\% of vision-related attention computations and 53.9\\% of FLOPs on LLaVA-v1.5 7B. It significantly outperforms existing token pruning methods and generalizes across diverse MLLMs. Beyond pruning, our insights further provide actionable guidelines for training efficient MLLMs by aligning model architecture with its intrinsic layer-wise processing dynamics. Our code is available at: https://github.com/EIT-NLP/VisiPruner.",
        "tags": [
            "LLM",
            "LLaVA"
        ]
    },
    {
        "id": "310",
        "title": "Soft-Masked Diffusion Language Models",
        "author": [
            "Michael Hersche",
            "Samuel Moor-Smith",
            "Thomas Hofmann",
            "Abbas Rahimi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17206",
        "abstract": "Diffusion models have demonstrated strong potential in language modeling, offering various advantages over traditional autoregressive approaches. Their ability to generate and revise entire responses in parallel enables faster generation and built-in self-correction mechanisms. Most modern diffusion-based language models employ masked diffusion, where decoding involves iteratively processing masked tokens based on a binary decision: either retaining the mask or replacing it with the predicted token. However, this binary choice discards valuable predictive information when the mask is retained. To address this limitation, we introduce soft-masking (SM), a novel method that dynamically blends the embedding of the mask token with the embeddings of the top-$k$ predicted tokens from the previous decoding step, for each retained mask. This provides the model with a more informative prior, preserving context from earlier computations and allowing partial information about masked tokens to propagate beyond a single step. We propose a training methodology that adapts a pretrained masked diffusion language model to incorporate SM. We demonstrate that continuing pretraining a 169M parameter model with SM leads to improved perplexity and MAUVE scores. Furthermore, we finetune two state-of-the-art diffusion models, Dream-7B and Dream-Coder-7B, with SM. SM consistently improves performance across multiple coding benchmarks, particularly in high-throughput settings.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "311",
        "title": "Wisdom is Knowing What not to Say: Hallucination-Free LLMs Unlearning via Attention Shifting",
        "author": [
            "Chenchen Tan",
            "Youyang Qu",
            "Xinghao Li",
            "Hui Zhang",
            "Shujie Cui",
            "Cunjian Chen",
            "Longxiang Gao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17210",
        "abstract": "The increase in computing power and the necessity of AI-assisted decision-making boost the growing application of large language models (LLMs). Along with this, the potential retention of sensitive data of LLMs has spurred increasing research into machine unlearning. However, existing unlearning approaches face a critical dilemma: Aggressive unlearning compromises model utility, while conservative strategies preserve utility but risk hallucinated responses. This significantly limits LLMs' reliability in knowledge-intensive applications. To address this, we introduce a novel Attention-Shifting (AS) framework for selective unlearning. AS is driven by two design objectives: (1) context-preserving suppression that attenuates attention to fact-bearing tokens without disrupting LLMs' linguistic structure; and (2) hallucination-resistant response shaping that discourages fabricated completions when queried about unlearning content. AS realizes these objectives through two attention-level interventions, which are importance-aware suppression applied to the unlearning set to reduce reliance on memorized knowledge and attention-guided retention enhancement that reinforces attention toward semantically essential tokens in the retained dataset to mitigate unintended degradation. These two components are jointly optimized via a dual-loss objective, which forms a soft boundary that localizes unlearning while preserving unrelated knowledge under representation superposition. Experimental results show that AS improves performance preservation over the state-of-the-art unlearning methods, achieving up to 15% higher accuracy on the ToFU benchmark and 10% on the TDEC benchmark, while maintaining competitive hallucination-free unlearning effectiveness. Compared to existing methods, AS demonstrates a superior balance between unlearning effectiveness, generalization, and response reliability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "312",
        "title": "D2C-HRHR: Discrete Actions with Double Distributional Critics for High-Risk-High-Return Tasks",
        "author": [
            "Jundong Zhang",
            "Yuhui Situ",
            "Fanji Zhang",
            "Rongji Deng",
            "Tianqi Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17212",
        "abstract": "Tasks involving high-risk-high-return (HRHR) actions, such as obstacle crossing, often exhibit multimodal action distributions and stochastic returns. Most reinforcement learning (RL) methods assume unimodal Gaussian policies and rely on scalar-valued critics, which limits their effectiveness in HRHR settings. We formally define HRHR tasks and theoretically show that Gaussian policies cannot guarantee convergence to the optimal solution. To address this, we propose a reinforcement learning framework that (i) discretizes continuous action spaces to approximate multimodal distributions, (ii) employs entropy-regularized exploration to improve coverage of risky but rewarding actions, and (iii) introduces a dual-critic architecture for more accurate discrete value distribution estimation. The framework scales to high-dimensional action spaces, supporting complex control domains. Experiments on locomotion and manipulation benchmarks with high risks of failure demonstrate that our method outperforms baselines, underscoring the importance of explicitly modeling multimodality and risk in RL.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "313",
        "title": "DSEBench: A Test Collection for Explainable Dataset Search with Examples",
        "author": [
            "Qing Shi",
            "Jing He",
            "Qiaosheng Chen",
            "Gong Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17228",
        "abstract": "Dataset search has been an established information retrieval task. Current paradigms either retrieve datasets that are relevant to a keyword query or find datasets that are similar to an input target dataset. To allow for their combined specification of information needs, in this article, we investigate the more generalized task of Dataset Search with Examples (DSE) and further extend it to Explainable DSE that requires identifying the metadata and content fields of a dataset that indicate its relevance to the query and similarity to the target datasets. To facilitate this research, we construct DSEBench, a test collection that provides high-quality dataset- and field-level annotations to enable the evaluation of explainable DSE. We also employ a large language model to generate numerous annotations to be used for training. We establish extensive baselines on DSEBench by adapting and evaluating a variety of sparse, dense, and LLM-based retrieval, reranking, and explanation methods.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "314",
        "title": "Coinvisor: An RL-Enhanced Chatbot Agent for Interactive Cryptocurrency Investment Analysis",
        "author": [
            "Chong Chen",
            "Ze Liu",
            "Lingfeng Bao",
            "Yanlin Wang",
            "Ting Chen",
            "Daoyuan Wu",
            "Jiachi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17235",
        "abstract": "The cryptocurrency market offers significant investment opportunities but faces challenges including high volatility and fragmented information. Data integration and analysis are essential for informed investment decisions. Currently, investors use three main approaches: (1) Manual analysis across various sources, which depends heavily on individual experience and is time-consuming and prone to bias; (2) Data aggregation platforms-limited in functionality and depth of analysis; (3) Large language model agents-based on static pretrained models, lacking real-time data integration and multi-step reasoning capabilities. To address these limitations, we present Coinvisor, a reinforcement learning-based chatbot that provides comprehensive analytical support for cryptocurrency investment through a multi-agent framework. Coinvisor integrates diverse analytical capabilities through specialized tools. Its key innovation is a reinforcement learning-based tool selection mechanism that enables multi-step planning and flexible integration of diverse data sources. This design supports real-time interaction and adaptive analysis of dynamic content, delivering accurate and actionable investment insights. We evaluated Coinvisor through automated benchmarks on tool calling accuracy and user studies with 20 cryptocurrency investors using our interface. Results show that Coinvisor improves recall by 40.7% and F1 score by 26.6% over the base model in tool orchestration. User studies show high satisfaction (4.64/5), with participants preferring Coinvisor to both general LLMs and existing crypto platforms (4.62/5).",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "315",
        "title": "Pole-Image: A Self-Supervised Pole-Anchored Descriptor for Long-Term LiDAR Localization and Map Maintenance",
        "author": [
            "Wuhao Xie",
            "Kanji Tanaka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17237",
        "abstract": "Long-term autonomy for mobile robots requires both robust self-localization and reliable map maintenance. Conventional landmark-based methods face a fundamental trade-off between landmarks with high detectability but low distinctiveness (e.g., poles) and those with high distinctiveness but difficult stable detection (e.g., local point cloud structures). This work addresses the challenge of descriptively identifying a unique \"signature\" (local point cloud) by leveraging a detectable, high-precision \"anchor\" (like a pole). To solve this, we propose a novel canonical representation, \"Pole-Image,\" as a hybrid method that uses poles as anchors to generate signatures from the surrounding 3D structure. Pole-Image represents a pole-like landmark and its surrounding environment, detected from a LiDAR point cloud, as a 2D polar coordinate image with the pole itself as the origin. This representation leverages the pole's nature as a high-precision reference point, explicitly encoding the \"relative geometry\" between the stable pole and the variable surrounding point cloud. The key advantage of pole landmarks is that \"detection\" is extremely easy. This ease of detection allows the robot to easily track the same pole, enabling the automatic and large-scale collection of diverse observational data (positive pairs). This data acquisition feasibility makes \"Contrastive Learning (CL)\" applicable. By applying CL, the model learns a viewpoint-invariant and highly discriminative descriptor. The contributions are twofold: 1) The descriptor overcomes perceptual aliasing, enabling robust self-localization. 2) The high-precision encoding enables high-sensitivity change detection, contributing to map maintenance.",
        "tags": [
            "3D",
            "Detection",
            "Robotics"
        ]
    },
    {
        "id": "316",
        "title": "StreamingThinker: Large Language Models Can Think While Reading",
        "author": [
            "Junlong Tong",
            "Yingqi Fan",
            "Anhao Zhao",
            "Yunpu Ma",
            "Xiaoyu Shen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17238",
        "abstract": "Large language models (LLMs) have demonstrated remarkable capabilities in chain of thought (CoT) reasoning. However, the current LLM reasoning paradigm initiates thinking only after the entire input is available, which introduces unnecessary latency and weakens attention to earlier information in dynamic scenarios. Inspired by human cognition of thinking while reading, we first design a \\textit{\\textbf{streaming thinking}} paradigm for LLMs, where reasoning unfolds in the order of input and further adjusts its depth once reading is complete. We instantiate this paradigm with \\textit{StreamingThinker}, a framework that enables LLMs to think while reading through the integration of streaming CoT generation, streaming-constraint training, and streaming parallel inference. Specifically, StreamingThinker employs streaming reasoning units with quality control for CoT generation, enforces order-preserving reasoning through streaming attention masks and position encoding, and leverages parallel KV caches that decouple input encoding from reasoning generation, thereby ensuring alignment and enabling true concurrency. We evaluate StreamingThinker on the Qwen3 model family across math reasoning, logical reasoning, and context-based QA reasoning tasks. Experimental results show that the StreamingThinker preserves performance comparable to batch thinking, while yielding an 80\\% reduction in token waiting before the onset of reasoning and a more than 60\\% reduction in time-level latency for producing the final answer, demonstrating the effectiveness of the streaming paradigm for LLM reasoning. Code will be released at \\href{https://github.com/EIT-NLP/StreamingLLM/tree/main/StreamingThinker}{this repository.}",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "317",
        "title": "From Preferences to Prejudice: The Role of Alignment Tuning in Shaping Social Bias in Video Diffusion Models",
        "author": [
            "Zefan Cai",
            "Haoyi Qiu",
            "Haozhe Zhao",
            "Ke Wan",
            "Jiachen Li",
            "Jiuxiang Gu",
            "Wen Xiao",
            "Nanyun Peng",
            "Junjie Hu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17247",
        "abstract": "Recent advances in video diffusion models have significantly enhanced text-to-video generation, particularly through alignment tuning using reward models trained on human preferences. While these methods improve visual quality, they can unintentionally encode and amplify social biases. To systematically trace how such biases evolve throughout the alignment pipeline, we introduce VideoBiasEval, a comprehensive diagnostic framework for evaluating social representation in video generation. Grounded in established social bias taxonomies, VideoBiasEval employs an event-based prompting strategy to disentangle semantic content (actions and contexts) from actor attributes (gender and ethnicity). It further introduces multi-granular metrics to evaluate (1) overall ethnicity bias, (2) gender bias conditioned on ethnicity, (3) distributional shifts in social attributes across model variants, and (4) the temporal persistence of bias within videos. Using this framework, we conduct the first end-to-end analysis connecting biases in human preference datasets, their amplification in reward models, and their propagation through alignment-tuned video diffusion models. Our results reveal that alignment tuning not only strengthens representational biases but also makes them temporally stable, producing smoother yet more stereotyped portrayals. These findings highlight the need for bias-aware evaluation and mitigation throughout the alignment process to ensure fair and socially responsible video generation.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": "318",
        "title": "An adaptive hierarchical control framework for quadrupedal robots in planetary exploration",
        "author": [
            "Franek Stark",
            "Rohit Kumar",
            "Shubham Vyas",
            "Hannah Isermann",
            "Jonas Haack",
            "Mihaela Popescu",
            "Jakob Middelberg",
            "Dennis Mronga",
            "Frank Kirchner"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17249",
        "abstract": "Planetary exploration missions require robots capable of navigating extreme and unknown environments. While wheeled rovers have dominated past missions, their mobility is limited to traversable surfaces. Legged robots, especially quadrupeds, can overcome these limitations by handling uneven, obstacle-rich, and deformable terrains. However, deploying such robots in unknown conditions is challenging due to the need for environment-specific control, which is infeasible when terrain and robot parameters are uncertain. This work presents a modular control framework that combines model-based dynamic control with online model adaptation and adaptive footstep planning to address uncertainties in both robot and terrain properties. The framework includes state estimation for quadrupeds with and without contact sensing, supports runtime reconfiguration, and is integrated into ROS 2 with open-source availability. Its performance was validated on two quadruped platforms, multiple hardware architectures, and in a volcano field test, where the robot walked over 700 m.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "319",
        "title": "Explainability of Large Language Models: Opportunities and Challenges toward Generating Trustworthy Explanations",
        "author": [
            "Shahin Atakishiyev",
            "Housam K.B. Babiker",
            "Jiayi Dai",
            "Nawshad Farruque",
            "Teruaki Hayashi",
            "Nafisa Sadaf Hriti",
            "Md Abed Rahman",
            "Iain Smith",
            "Mi-Young Kim",
            "Osmar R. ZaÃ¯ane",
            "Randy Goebel"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17256",
        "abstract": "Large language models have exhibited impressive performance across a broad range of downstream tasks in natural language processing. However, how a language model predicts the next token and generates content is not generally understandable by humans. Furthermore, these models often make errors in prediction and reasoning, known as hallucinations. These errors underscore the urgent need to better understand and interpret the intricate inner workings of language models and how they generate predictive outputs. Motivated by this gap, this paper investigates local explainability and mechanistic interpretability within Transformer-based large language models to foster trust in such models. In this regard, our paper aims to make three key contributions. First, we present a review of local explainability and mechanistic interpretability approaches and insights from relevant studies in the literature. Furthermore, we describe experimental studies on explainability and reasoning with large language models in two critical domains -- healthcare and autonomous driving -- and analyze the trust implications of such explanations for explanation receivers. Finally, we summarize current unaddressed issues in the evolving landscape of LLM explainability and outline the opportunities, critical challenges, and future directions toward generating human-aligned, trustworthy LLM explanations.",
        "tags": [
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": "320",
        "title": "Adaptive Discretization for Consistency Models",
        "author": [
            "Jiayu Bai",
            "Zhanbo Feng",
            "Zhijie Deng",
            "Tianqi Hou",
            "Robert C. Qiu",
            "Zenan Ling"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17266",
        "abstract": "Consistency Models (CMs) have shown promise for efficient one-step generation. However, most existing CMs rely on manually designed discretization schemes, which can cause repeated adjustments for different noise schedules and datasets. To address this, we propose a unified framework for the automatic and adaptive discretization of CMs, formulating it as an optimization problem with respect to the discretization step. Concretely, during the consistency training process, we propose using local consistency as the optimization objective to ensure trainability by avoiding excessive discretization, and taking global consistency as a constraint to ensure stability by controlling the denoising error in the training target. We establish the trade-off between local and global consistency with a Lagrange multiplier. Building on this framework, we achieve adaptive discretization for CMs using the Gauss-Newton method. We refer to our approach as ADCMs. Experiments demonstrate that ADCMs significantly improve the training efficiency of CMs, achieving superior generative performance with minimal training overhead on both CIFAR-10 and ImageNet. Moreover, ADCMs exhibit strong adaptability to more advanced DM variants. Code is available at https://github.com/rainstonee/ADCM.",
        "tags": [
            "Consistency Models"
        ]
    },
    {
        "id": "321",
        "title": "FineVision: Open Data Is All You Need",
        "author": [
            "Luis Wiedmann",
            "Orr Zohar",
            "Amir Mahla",
            "Xiaohan Wang",
            "Rui Li",
            "Thibaud Frere",
            "Leandro von Werra",
            "Aritra Roy Gosthipaty",
            "AndrÃ©s Marafioti"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17269",
        "abstract": "The advancement of vision-language models (VLMs) is hampered by a fragmented landscape of inconsistent and contaminated public datasets. We introduce FineVision, a meticulously collected, curated, and unified corpus of 24 million samples - the largest open resource of its kind. We unify more than 200 sources into 185 subsets via a semi-automated, human-in-the-loop pipeline: automation performs bulk ingestion and schema mapping, while reviewers audit mappings and spot-check outputs to verify faithful consumption of annotations, appropriate formatting and diversity, and safety; issues trigger targeted fixes and re-runs. The workflow further applies rigorous de-duplication within and across sources and decontamination against 66 public benchmarks. FineVision also encompasses agentic/GUI tasks with a unified action space; reviewers validate schemas and inspect a sample of trajectories to confirm executable fidelity. Models trained on FineVision consistently outperform those trained on existing open mixtures across a broad evaluation suite, underscoring the benefits of scale, data hygiene, and balanced automation with human oversight. We release the corpus and curation tools to accelerate data-centric VLM research.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "322",
        "title": "Enhanced Motion Forecasting with Plug-and-Play Multimodal Large Language Models",
        "author": [
            "Katie Luo",
            "Jingwei Ji",
            "Tong He",
            "Runsheng Xu",
            "Yichen Xie",
            "Dragomir Anguelov",
            "Mingxing Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17274",
        "abstract": "Current autonomous driving systems rely on specialized models for perceiving and predicting motion, which demonstrate reliable performance in standard conditions. However, generalizing cost-effectively to diverse real-world scenarios remains a significant challenge. To address this, we propose Plug-and-Forecast (PnF), a plug-and-play approach that augments existing motion forecasting models with multimodal large language models (MLLMs). PnF builds on the insight that natural language provides a more effective way to describe and handle complex scenarios, enabling quick adaptation to targeted behaviors. We design prompts to extract structured scene understanding from MLLMs and distill this information into learnable embeddings to augment existing behavior prediction models. Our method leverages the zero-shot reasoning capabilities of MLLMs to achieve significant improvements in motion prediction performance, while requiring no fine-tuning -- making it practical to adopt. We validate our approach on two state-of-the-art motion forecasting models using the Waymo Open Motion Dataset and the nuScenes Dataset, demonstrating consistent performance improvements across both benchmarks.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "323",
        "title": "Breaking and Fixing Defenses Against Control-Flow Hijacking in Multi-Agent Systems",
        "author": [
            "Rishi Jha",
            "Harold Triedman",
            "Justin Wagle",
            "Vitaly Shmatikov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17276",
        "abstract": "Control-flow hijacking attacks manipulate orchestration mechanisms in multi-agent systems into performing unsafe actions that compromise the system and exfiltrate sensitive information. Recently proposed defenses, such as LlamaFirewall, rely on alignment checks of inter-agent communications to ensure that all agent invocations are \"related to\" and \"likely to further\" the original objective.\nWe start by demonstrating control-flow hijacking attacks that evade these defenses even if alignment checks are performed by advanced LLMs. We argue that the safety and functionality objectives of multi-agent systems fundamentally conflict with each other. This conflict is exacerbated by the brittle definitions of \"alignment\" and the checkers' incomplete visibility into the execution context.\nWe then propose, implement, and evaluate ControlValve, a new defense inspired by the principles of control-flow integrity and least privilege. ControlValve (1) generates permitted control-flow graphs for multi-agent systems, and (2) enforces that all executions comply with these graphs, along with contextual rules (generated in a zero-shot manner) for each agent invocation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "324",
        "title": "Multimodal Safety Is Asymmetric: Cross-Modal Exploits Unlock Black-Box MLLMs Jailbreaks",
        "author": [
            "Xinkai Wang",
            "Beibei Li",
            "Zerui Shao",
            "Ao Liu",
            "Shouling Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17277",
        "abstract": "Multimodal large language models (MLLMs) have demonstrated significant utility across diverse real-world applications. But MLLMs remain vulnerable to jailbreaks, where adversarial inputs can collapse their safety constraints and trigger unethical responses. In this work, we investigate jailbreaks in the text-vision multimodal setting and pioneer the observation that visual alignment imposes uneven safety constraints across modalities in MLLMs, thereby giving rise to multimodal safety asymmetry. We then develop PolyJailbreak, a black-box jailbreak method grounded in reinforcement learning. Initially, we probe the model's attention dynamics and latent representation space, assessing how visual inputs reshape cross-modal information flow and diminish the model's ability to separate harmful from benign inputs, thereby exposing exploitable vulnerabilities. On this basis, we systematize them into generalizable and reusable operational rules that constitute a structured library of Atomic Strategy Primitives, which translate harmful intents into jailbreak inputs through step-wise transformations. Guided by the primitives, PolyJailbreak employs a multi-agent optimization process that automatically adapts inputs against the target models. We conduct comprehensive evaluations on a variety of open-source and closed-source MLLMs, demonstrating that PolyJailbreak outperforms state-of-the-art baselines.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "325",
        "title": "MemoryBench: A Benchmark for Memory and Continual Learning in LLM Systems",
        "author": [
            "Qingyao Ai",
            "Yichen Tang",
            "Changyue Wang",
            "Jianming Long",
            "Weihang Su",
            "Yiqun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17281",
        "abstract": "Scaling up data, parameters, and test-time computation has been the mainstream methods to improve LLM systems (LLMsys), but their upper bounds are almost reached due to the gradual depletion of high-quality data and marginal gains obtained from larger computational resource consumption. Inspired by the abilities of human and traditional AI systems in learning from practice, constructing memory and continual learning frameworks for LLMsys has become an important and popular research direction in recent literature. Yet, existing benchmarks for LLM memory often focus on evaluating the system on homogeneous reading comprehension tasks with long-form inputs rather than testing their abilities to learn from accumulated user feedback in service time. Therefore, we propose a user feedback simulation framework and a comprehensive benchmark covering multiple domains, languages, and types of tasks to evaluate the continual learning abilities of LLMsys. Experiments show that the effectiveness and efficiency of state-of-the-art baselines are far from satisfying, and we hope this benchmark could pave the way for future studies on LLM memory and optimization algorithms.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "326",
        "title": "Comprehending Spatio-temporal Data via Cinematic Storytelling using Large Language Models",
        "author": [
            "Panos Kalnis. Shuo Shang",
            "Christian S. Jensen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17301",
        "abstract": "Spatio-temporal data captures complex dynamics across both space and time, yet traditional visualizations are complex, require domain expertise and often fail to resonate with broader audiences. Here, we propose MapMuse, a storytelling-based framework for interpreting spatio-temporal datasets, transforming them into compelling, narrative-driven experiences. We utilize large language models and employ retrieval augmented generation (RAG) and agent-based techniques to generate comprehensive stories. Drawing on principles common in cinematic storytelling, we emphasize clarity, emotional connection, and audience-centric design. As a case study, we analyze a dataset of taxi trajectories. Two perspectives are presented: a captivating story based on a heat map that visualizes millions of taxi trip endpoints to uncover urban mobility patterns; and a detailed narrative following a single long taxi journey, enriched with city landmarks and temporal shifts. By portraying locations as characters and movement as plot, we argue that data storytelling drives insight, engagement, and action from spatio-temporal information. The case study illustrates how MapMuse can bridge the gap between data complexity and human understanding. The aim of this short paper is to provide a glimpse to the potential of the cinematic storytelling technique as an effective communication tool for spatio-temporal data, as well as to describe open problems and opportunities for future research.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "327",
        "title": "RubiSCoT: A Framework for AI-Supported Academic Assessment",
        "author": [
            "Thorsten FrÃ¶hlich",
            "Tim Schlippe"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17309",
        "abstract": "The evaluation of academic theses is a cornerstone of higher education, ensuring rigor and integrity. Traditional methods, though effective, are time-consuming and subject to evaluator variability. This paper presents RubiSCoT, an AI-supported framework designed to enhance thesis evaluation from proposal to final submission. Using advanced natural language processing techniques, including large language models, retrieval-augmented generation, and structured chain-of-thought prompting, RubiSCoT offers a consistent, scalable solution. The framework includes preliminary assessments, multidimensional assessments, content extraction, rubric-based scoring, and detailed reporting. We present the design and implementation of RubiSCoT, discussing its potential to optimize academic assessment processes through consistent, scalable, and transparent evaluation.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "328",
        "title": "Disentanglement Beyond Static vs. Dynamic: A Benchmark and Evaluation Framework for Multi-Factor Sequential Representations",
        "author": [
            "Tal Barami",
            "Nimrod Berman",
            "Ilan Naiman",
            "Amos H. Hason",
            "Rotem Ezra",
            "Omri Azencot"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17313",
        "abstract": "Learning disentangled representations in sequential data is a key goal in deep learning, with broad applications in vision, audio, and time series. While real-world data involves multiple interacting semantic factors over time, prior work has mostly focused on simpler two-factor static and dynamic settings, primarily because such settings make data collection easier, thereby overlooking the inherently multi-factor nature of real-world data. We introduce the first standardized benchmark for evaluating multi-factor sequential disentanglement across six diverse datasets spanning video, audio, and time series. Our benchmark includes modular tools for dataset integration, model development, and evaluation metrics tailored to multi-factor analysis. We additionally propose a post-hoc Latent Exploration Stage to automatically align latent dimensions with semantic factors, and introduce a Koopman-inspired model that achieves state-of-the-art results. Moreover, we show that Vision-Language Models can automate dataset annotation and serve as zero-shot disentanglement evaluators, removing the need for manual labels and human intervention. Together, these contributions provide a robust and scalable foundation for advancing multi-factor sequential disentanglement.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "329",
        "title": "Auto-Rubric: Learning to Extract Generalizable Criteria for Reward Modeling",
        "author": [
            "Lipeng Xie",
            "Sen Huang",
            "Zhuo Zhang",
            "Anni Zou",
            "Yunpeng Zhai",
            "Dingchao Ren",
            "Kezun Zhang",
            "Haoyuan Hu",
            "Boyin Liu",
            "Haoran Chen",
            "Zhaoyang Liu",
            "Bolin Ding"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17314",
        "abstract": "Reward models are essential for aligning Large Language Models (LLMs) with human values, yet their development is hampered by costly preference datasets and poor interpretability. While recent rubric-based approaches offer transparency, they often lack systematic quality control and optimization, creating a trade-off between scalability and reliability. We address these limitations with a novel, training-free framework built on a key assumption: \\textit{evaluation rubrics underlying human preferences exhibit significant generalization ability across diverse queries}, a property that enables remarkable data efficiency. Our two-stage approach first infers high-quality, query-specific rubrics using a validation-guided \\textbf{Propose-Evaluate-Revise} pipeline. Second, it generalizes these granular rubrics into a compact, non-redundant core set by maximizing an \\textbf{information-theoretic coding rate}. The final output is an interpretable, hierarchical \"Theme-Tips\" rubric set. Extensive experiments demonstrate the framework's exceptional data efficiency and performance. Critically, using just 70 preference pairs (1.5\\% of the source data), our method also empowers smaller models like Qwen3-8B to outperform specialized, fully-trained counterparts. This work pioneers a scalable, interpretable, and data-efficient path for reward modeling.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "330",
        "title": "CharDiff: A Diffusion Model with Character-Level Guidance for License Plate Image Restoration",
        "author": [
            "Gyuhwan Park",
            "Kihyun Na",
            "Injung Kim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17330",
        "abstract": "The significance of license plate image restoration goes beyond the preprocessing stage of License Plate Recognition (LPR) systems, as it also serves various purposes, including increasing evidential value, enhancing the clarity of visual interface, and facilitating further utilization of license plate images. We propose a novel diffusion-based framework with character-level guidance, CharDiff, which effectively restores and recognizes severely degraded license plate images captured under realistic conditions. CharDiff leverages fine-grained character-level priors extracted through external segmentation and Optical Character Recognition (OCR) modules tailored for low-quality license plate images. For precise and focused guidance, CharDiff incorporates a novel Character-guided Attention through Region-wise Masking (CHARM) module, which ensures that each character's guidance is restricted to its own region, thereby avoiding interference with other regions. In experiments, CharDiff significantly outperformed the baseline restoration models in both restoration quality and recognition accuracy, achieving a 28% relative reduction in CER on the Roboflow-LP dataset, compared to the best-performing baseline model. These results indicate that the structured character-guided conditioning effectively enhances the robustness of diffusion-based license plate restoration and recognition in practical deployment scenarios.",
        "tags": [
            "Diffusion",
            "Segmentation"
        ]
    },
    {
        "id": "331",
        "title": "DDBot: Differentiable Physics-based Digging Robot for Unknown Granular Materials",
        "author": [
            "Xintong Yang",
            "Minglun Wei",
            "Ze Ji",
            "Yu-Kun Lai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17335",
        "abstract": "Automating the manipulation of granular materials poses significant challenges due to complex contact dynamics, unpredictable material properties, and intricate system states. Existing approaches often fail to achieve efficiency and accuracy in such tasks. To fill the research gap, this paper studies the small-scale and high-precision granular material digging task with unknown physical properties. A new framework, named differentiable digging robot (DDBot), is proposed to manipulate granular materials, including sand and soil.\nSpecifically, we equip DDBot with a differentiable physics-based simulator, tailored for granular material manipulation, powered by GPU-accelerated parallel computing and automatic differentiation. DDBot can perform efficient differentiable system identification and high-precision digging skill optimisation for unknown granular materials, which is enabled by a differentiable skill-to-action mapping, a task-oriented demonstration method, gradient clipping and line search-based gradient descent.\nExperimental results show that DDBot can efficiently (converge within 5 to 20 minutes) identify unknown granular material dynamics and optimise digging skills, with high-precision results in zero-shot real-world deployments, highlighting its practicality. Benchmark results against state-of-the-art baselines also confirm the robustness and efficiency of DDBot in such digging tasks.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "332",
        "title": "Interactive Force-Impedance Control",
        "author": [
            "Fan Shao",
            "Satoshi Endo",
            "Sandra Hirche",
            "Fanny Ficuciello"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17341",
        "abstract": "Human collaboration with robots requires flexible role adaptation, enabling robot to switch between active leader and passive follower. Effective role switching depends on accurately estimating human intention, which is typically achieved through external force analysis, nominal robot dynamics, or data-driven approaches. However, these methods are primarily effective in contact-sparse environments. When robots under hybrid or unified force-impedance control physically interact with active humans or non-passive environments, the robotic system may lose passivity and thus compromise safety. To address this challenge, this paper proposes the unified Interactive Force-Impedance Control (IFIC) framework that adapts to the interaction power flow, ensuring effortless and safe interaction in contact-rich environments. The proposed control architecture is formulated within a port-Hamiltonian framework, incorporating both interaction and task control ports, through which system passivity is guaranteed.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "333",
        "title": "Exploring The Missing Semantics In Event Modality",
        "author": [
            "Jingqian Wu",
            "Shengpeng Xu",
            "Yunbo Jia",
            "Edmund Y. Lam"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17347",
        "abstract": "Event cameras offer distinct advantages such as low latency, high dynamic range, and efficient motion capture. However, event-to-video reconstruction (E2V), a fundamental event-based vision task, remains challenging, particularly for reconstructing and recovering semantic information. This is primarily due to the nature of the event camera, as it only captures intensity changes, ignoring static objects and backgrounds, resulting in a lack of semantic information in captured event modality. Further, semantic information plays a crucial role in video and frame reconstruction, yet is often overlooked by existing E2V approaches. To bridge this gap, we propose Semantic-E2VID, an E2V framework that explores the missing visual semantic knowledge in event modality and leverages it to enhance event-to-video reconstruction. Specifically, Semantic-E2VID introduces a cross-modal feature alignment (CFA) module to transfer the robust visual semantics from a frame-based vision foundation model, the Segment Anything Model (SAM), to the event encoder, while aligning the high-level features from distinct modalities. To better utilize the learned semantic feature, we further propose a semantic-aware feature fusion (SFF) block to integrate learned semantics in frame modality to form event representations with rich semantics that can be decoded by the event decoder. Further, to facilitate the reconstruction of semantic information, we propose a novel Semantic Perceptual E2V Supervision that helps the model to reconstruct semantic details by leveraging SAM-generated categorical labels. Extensive experiments demonstrate that Semantic-E2VID significantly enhances frame quality, outperforming state-of-the-art E2V methods across multiple benchmarks. The sample code is included in the supplementary material.",
        "tags": [
            "SAM"
        ]
    },
    {
        "id": "334",
        "title": "Towards Mixed-Modal Retrieval for Universal Retrieval-Augmented Generation",
        "author": [
            "Chenghao Zhang",
            "Guanting Dong",
            "Xinyu Yang",
            "Zhicheng Dou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17354",
        "abstract": "Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for enhancing large language models (LLMs) by retrieving relevant documents from an external corpus. However, existing RAG systems primarily focus on unimodal text documents, and often fall short in real-world scenarios where both queries and documents may contain mixed modalities (such as text and images). In this paper, we address the challenge of Universal Retrieval-Augmented Generation (URAG), which involves retrieving and reasoning over mixed-modal information to improve vision-language generation. To this end, we propose Nyx, a unified mixed-modal to mixed-modal retriever tailored for URAG scenarios. To mitigate the scarcity of realistic mixed-modal data, we introduce a four-stage automated pipeline for generation and filtering, leveraging web documents to construct NyxQA, a dataset comprising diverse mixed-modal question-answer pairs that better reflect real-world information needs. Building on this high-quality dataset, we adopt a two-stage training framework for Nyx: we first perform pre-training on NyxQA along with a variety of open-source retrieval datasets, followed by supervised fine-tuning using feedback from downstream vision-language models (VLMs) to align retrieval outputs with generative preferences. Experimental results demonstrate that Nyx not only performs competitively on standard text-only RAG benchmarks, but also excels in the more general and realistic URAG setting, significantly improving generation quality in vision-language tasks.",
        "tags": [
            "LLM",
            "RAG",
            "VLM"
        ]
    },
    {
        "id": "335",
        "title": "Localist LLMs with Recruitment Learning",
        "author": [
            "Joachim Diederich"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17358",
        "abstract": "We present a novel framework for training large language models with continuously adjustable internal representations that span the full spectrum from localist (interpretable, rule-based) to distributed (generalizable, efficient) encodings. The key innovations are (1) a locality dial, a tunable parameter that dynamically controls the degree of localization during both training and inference without requiring model retraining, (2) an information-theoretic recruitment mechanism that adaptively allocates semantic blocks as needed, eliminating the requirement for complete domain knowledge at initialization, and (3) a hierarchical recruitment framework that extends capacity allocation to entire specialized LLMs, enabling multi-granularity architectural adaptation. This is achieved through group sparsity penalties on attention mechanisms, information-theoretic anchor design, dynamic rule injection, and principled recruitment  criteria based on penalized likelihood with explicit units. We provide rigorous mathematical results establishing explicit threshold conditions under which attention provably concentrates on semantically relevant blocks at stationary points, with exact bounds on attention entropy and pointer fidelity. The hierarchical recruitment mechanism provides convergence guarantees at both the block level (fine-grained, within-LLM) and the LLM level (coarse-grained, cross-domain), ensuring the system discovers semantic partitions that balance model complexity against data encoding efficiency. This framework enables practitioners to continuously interpolate between interpretable and high-performance modes while adapting architectural capacity at multiple granularities, supporting applications in regulated domains requiring both transparency and capability.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "336",
        "title": "M2H: Multi-Task Learning with Efficient Window-Based Cross-Task Attention for Monocular Spatial Perception",
        "author": [
            "U.V.B.L Udugama",
            "George Vosselman",
            "Francesco Nex"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17363",
        "abstract": "Deploying real-time spatial perception on edge devices requires efficient multi-task models that leverage complementary task information while minimizing computational overhead. This paper introduces Multi-Mono-Hydra (M2H), a novel multi-task learning framework designed for semantic segmentation and depth, edge, and surface normal estimation from a single monocular image. Unlike conventional approaches that rely on independent single-task models or shared encoder-decoder architectures, M2H introduces a Window-Based Cross-Task Attention Module that enables structured feature exchange while preserving task-specific details, improving prediction consistency across tasks. Built on a lightweight ViT-based DINOv2 backbone, M2H is optimized for real-time deployment and serves as the foundation for monocular spatial perception systems supporting 3D scene graph construction in dynamic environments. Comprehensive evaluations show that M2H outperforms state-of-the-art multi-task models on NYUDv2, surpasses single-task depth and semantic baselines on Hypersim, and achieves superior performance on the Cityscapes dataset, all while maintaining computational efficiency on laptop hardware. Beyond benchmarks, M2H is validated on real-world data, demonstrating its practicality in spatial perception tasks.",
        "tags": [
            "3D",
            "Segmentation",
            "ViT"
        ]
    },
    {
        "id": "337",
        "title": "Recurrent Attention-based Token Selection for Efficient Streaming Video-LLMs",
        "author": [
            "Vaggelis Dorovatas",
            "Soroush Seifi",
            "Gunshi Gupta",
            "Rahaf Aljundi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17364",
        "abstract": "Video Large Language Models (Video-LLMs) excel at understanding videos in-context, provided they have full access to the video when answering queries. However, these models face challenges in streaming scenarios where hour-long videos must be processed online, and questions need timely responses. In this work, we propose a training-free approach compatible with standard Video-LLMs, leveraging three key concepts: 1) LLM-informed selection of visual tokens to identify those that the LLM has attended to and contributed to its understanding of each short clip. Our attention-based selection allows us to discard up to ~95% of unimportant visual tokens with minimal performance loss; 2) Recurrent processing of past selected tokens to generate temporally coherent understanding of each processed clip; 3) Caption-based question answering for lightweight and accurate responses. Our method achieves state-of-the-art performance on streaming video benchmarks, striking a balance between efficiency and effectiveness.",
        "tags": [
            "CLIP",
            "LLM"
        ]
    },
    {
        "id": "338",
        "title": "Bridging Embodiment Gaps: Deploying Vision-Language-Action Models on Soft Robots",
        "author": [
            "Haochen Su",
            "Cristian Meo",
            "Francesco Stella",
            "Andrea Peirone",
            "Kai Junge",
            "Josie Hughes"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17369",
        "abstract": "Robotic systems are increasingly expected to operate in human-centered, unstructured environments where safety, adaptability, and generalization are essential. Vision-Language-Action (VLA) models have been proposed as a language guided generalized control framework for real robots. However, their deployment has been limited to conventional serial link manipulators. Coupled by their rigidity and unpredictability of learning based control, the ability to safely interact with the environment is missing yet critical. In this work, we present the deployment of a VLA model on a soft continuum manipulator to demonstrate autonomous safe human-robot interaction. We present a structured finetuning and deployment pipeline evaluating two state-of-the-art VLA models (OpenVLA-OFT and $\\pi_0$) across representative manipulation tasks, and show while out-of-the-box policies fail due to embodiment mismatch, through targeted finetuning the soft robot performs equally to the rigid counterpart. Our findings highlight the necessity of finetuning for bridging embodiment gaps, and demonstrate that coupling VLA models with soft robots enables safe and flexible embodied AI in human-shared environments.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "339",
        "title": "AdapTrack: Constrained Decoding without Distorting LLM's Output Intent",
        "author": [
            "Yongmin Li",
            "Jia Li",
            "Ge Li",
            "Zhi Jin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17376",
        "abstract": "Language model-based code generation and completion tools have been widely adopted, but they may sometimes produce code that does not meet necessary constraints, such as syntactic correctness or API existence. Constrained decoding techniques are developed to help the model generate code adhering to the constraints by greedily eliminating generation options that violate constraints at each step of the generation process. However, there is a severe limitation of constrained decoding, that it distorts the model's output intent, forcing it to produce code that may satisfy the constraint but does not match the development intent and is therefore incorrect. In response to this challenge, we propose AdapTrack. By incorporating backtracking into the generation process, AdapTrack avoids distorting the output intent of the model, thereby producing results that are not only constraint-compliant but also more semantically aligned with model's output intent. On our synthetic API completion dataset, AdapTrack can achieve up to 360.87% improvement compared to constrained decoding; on the real-world API completion dataset we collect that exhibits similar issues, AdapTrack can achieve up to 38.93% improvement over constrained decoding; in general code genration benchmarks, compared to constrained decoding, AdapTrack can achieve up to 7.84% improvement on HumanEval, and up to 6.42% improvement on MBPP. This indicates that, simply by better adhering to the model's output intent, AdapTrack can achieve significant improvements. We provide a theoretical proof that the distribution produced by AdapTrack aligns with the model's distribution given the generated tokens, thereby ensuring that the model's output intent is not distorted. Experiments on DSL problems show that, compared to existing methods, our approach can provide generation results that are more consistent with the language model's distribution.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "340",
        "title": "Optimizing Energy Management of Smart Grid using Reinforcement Learning aided by Surrogate models built using Physics-informed Neural Networks",
        "author": [
            "Julen Cestero",
            "Carmine Delle Femine",
            "Kenji S. Muro",
            "Marco Quartulli",
            "Marcello Restelli"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17380",
        "abstract": "Optimizing the energy management within a smart grids scenario presents significant challenges, primarily due to the complexity of real-world systems and the intricate interactions among various components. Reinforcement Learning (RL) is gaining prominence as a solution for addressing the challenges of Optimal Power Flow in smart grids. However, RL needs to iterate compulsively throughout a given environment to obtain the optimal policy. This means obtaining samples from a, most likely, costly simulator, which can lead to a sample efficiency problem. In this work, we address this problem by substituting costly smart grid simulators with surrogate models built using Phisics-informed Neural Networks (PINNs), optimizing the RL policy training process by arriving to convergent results in a fraction of the time employed by the original environment.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "341",
        "title": "Latent Spaces Beyond Synthesis: From GANs to Diffusion Models",
        "author": [
            "Ludovica Schaerf"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17383",
        "abstract": "This paper examines the evolving nature of internal representations in generative visual models, focusing on the conceptual and technical shift from GANs and VAEs to diffusion-based architectures. Drawing on Beatrice Fazi's account of synthesis as the amalgamation of distributed representations, we propose a distinction between \"synthesis in a strict sense\", where a compact latent space wholly determines the generative process, and \"synthesis in a broad sense,\" which characterizes models whose representational labor is distributed across layers. Through close readings of model architectures and a targeted experimental setup that intervenes in layerwise representations, we show how diffusion models fragment the burden of representation and thereby challenge assumptions of unified internal space. By situating these findings within media theoretical frameworks and critically engaging with metaphors such as the latent space and the Platonic Representation Hypothesis, we argue for a reorientation of how generative AI is understood: not as a direct synthesis of content, but as an emergent configuration of specialized processes.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "342",
        "title": "Inference of Deterministic Finite Automata via Q-Learning",
        "author": [
            "Elaheh Hosseinkhani",
            "Martin Leucker"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17386",
        "abstract": "Traditional approaches to inference of deterministic finite-state automata (DFA) stem from symbolic AI, including both active learning methods (e.g., Angluin's L* algorithm and its variants) and passive techniques (e.g., Biermann and Feldman's method, RPNI). Meanwhile, sub-symbolic AI, particularly machine learning, offers alternative paradigms for learning from data, such as supervised, unsupervised, and reinforcement learning (RL). This paper investigates the use of Q-learning, a well-known reinforcement learning algorithm, for the passive inference of deterministic finite automata. It builds on the core insight that the learned Q-function, which maps state-action pairs to rewards, can be reinterpreted as the transition function of a DFA over a finite domain. This provides a novel bridge between sub-symbolic learning and symbolic representations. The paper demonstrates how Q-learning can be adapted for automaton inference and provides an evaluation on several examples.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "343",
        "title": "The Atomic Instruction Gap: Instruction-Tuned LLMs Struggle with Simple, Self-Contained Directives",
        "author": [
            "Henry Lim",
            "Kwan Hui Lim"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17388",
        "abstract": "Instruction-tuned large language models (IT-LLMs) exhibit strong zero-shot reasoning, yet their ability to execute simple, self-contained instructions remains underexplored, despite this being foundational to complex instruction-following. We evaluate 20 IT-LLMs on modified MMLU and MMLU-Pro benchmarks, by systematically varying the format of option labels (alphabetic, numeric, Roman) while keeping their meaning identical under four paradigms, namely: (1) With explicit instructions, label changes cause large performance shifts (e.g., -30.45\\% for Roman vs. numeric), revealing instruction-format bias. (2) Without instructions, performance drops further (up to -10.84\\%) and label sensitivity intensifies, underscoring the role of explicit guidance. (3) When option contents are removed, models fail random-choice baselines except with numeric labels, suggesting weak adherence to atomic directives. (4) Three-shot exemplars yield no significant gains in robustness or fidelity, and generation analyses show persistent label errors, especially for non-numeric formats. Across model sizes, larger LLMs achieve higher accuracy but remain inconsistent in instruction adherence. These results expose the insufficiencies of current instruction-tuning paradigms and highlight the need for evaluation methods and training strategies that explicitly target atomic instruction-following.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "344",
        "title": "EduAdapt: A Question Answer Benchmark Dataset for Evaluating Grade-Level Adaptability in LLMs",
        "author": [
            "Numaan Naeem",
            "Abdellah El Mekki",
            "Muhammad Abdul-Mageed"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17389",
        "abstract": "Large language models (LLMs) are transforming education by answering questions, explaining complex concepts, and generating content across a wide range of subjects. Despite strong performance on academic benchmarks, they often fail to tailor responses to students' grade levels. This is a critical need in K-12 education, where age-appropriate vocabulary and explanation are essential for effective learning. Existing models frequently produce outputs that are too advanced or vague for younger learners, and there are no standardized benchmarks to evaluate their ability to adjust across cognitive and developmental stages. To address this gap, we introduce EduAdapt, a benchmark of nearly 48k grade-labeled QA pairs across nine science subjects, spanning Grades 1-12 and grouped into four grade levels. We evaluate a diverse set of open-source LLMs on EduAdapt and find that while larger models generally perform better, they still struggle with generating suitable responses for early-grade students (Grades 1-5). Our work presents the first dataset and evaluation framework for assessing grade-level adaptability in LLMs, aiming to foster more developmentally aligned educational AI systems through better training and prompting strategies. EduAdapt code and datasets are publicly available at https://github.com/NaumanNaeem/EduAdapt.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "345",
        "title": "Finite-Time Bounds for Average-Reward Fitted Q-Iteration",
        "author": [
            "Jongmin Lee",
            "Ernest K. Ryu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17391",
        "abstract": "Although there is an extensive body of work characterizing the sample complexity of discounted-return offline RL with function approximations, prior work on the average-reward setting has received significantly less attention, and existing approaches rely on restrictive assumptions, such as ergodicity or linearity of the MDP. In this work, we establish the first sample complexity results for average-reward offline RL with function approximation for weakly communicating MDPs, a much milder assumption. To this end, we introduce Anchored Fitted Q-Iteration, which combines the standard Fitted Q-Iteration with an anchor mechanism. We show that the anchor, which can be interpreted as a form of weight decay, is crucial for enabling finite-time analysis in the average-reward setting. We also extend our finite-time analysis to the setup where the dataset is generated from a single-trajectory rather than IID transitions, again leveraging the anchor mechanism.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "346",
        "title": "S4ECG: Exploring the impact of long-range interactions for arrhythmia prediction",
        "author": [
            "Tiezhi Wang",
            "Wilhelm Haverkamp",
            "Nils Strodthoff"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17406",
        "abstract": "The electrocardiogram (ECG) exemplifies biosignal-based time series with continuous, temporally ordered structure reflecting cardiac physiological and pathophysiological dynamics. Detailed analysis of these dynamics has proven challenging, as conventional methods capture either global trends or local waveform features but rarely their simultaneous interplay at high temporal resolution. To bridge global and local signal analysis, we introduce S4ECG, a novel deep learning architecture leveraging structured state space models for multi-epoch arrhythmia classification. Our joint multi-epoch predictions significantly outperform single-epoch approaches by 1.0-11.6% in macro-AUROC, with atrial fibrillation specificity improving from 0.718-0.979 to 0.967-0.998, demonstrating superior performance in-distribution and enhanced out-of-distribution robustness. Systematic investigation reveals optimal temporal dependency windows spanning 10-20 minutes for peak performance. This work contributes to a paradigm shift toward temporally-aware arrhythmia detection algorithms, opening new possibilities for ECG interpretation, in particular for complex arrhythmias like atrial fibrillation and atrial flutter.",
        "tags": [
            "Detection",
            "SSMs"
        ]
    },
    {
        "id": "347",
        "title": "Monitoring Horses in Stalls: From Object to Event Detection",
        "author": [
            "Dmitrii Galimzianov",
            "Viacheslav Vyshegorodtsev",
            "Ivan Nezhivykh"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17409",
        "abstract": "Monitoring the behavior of stalled horses is essential for early detection of health and welfare issues but remains labor-intensive and time-consuming. In this study, we present a prototype vision-based monitoring system that automates the detection and tracking of horses and people inside stables using object detection and multi-object tracking techniques. The system leverages YOLOv11 and BoT-SORT for detection and tracking, while event states are inferred based on object trajectories and spatial relations within the stall. To support development, we constructed a custom dataset annotated with assistance from foundation models CLIP and GroundingDINO. The system distinguishes between five event types and accounts for the camera's blind spots. Qualitative evaluation demonstrated reliable performance for horse-related events, while highlighting limitations in detecting people due to data scarcity. This work provides a foundation for real-time behavioral monitoring in equine facilities, with implications for animal welfare and stable management.",
        "tags": [
            "CLIP",
            "Detection"
        ]
    },
    {
        "id": "348",
        "title": "A Conditional Diffusion Model for Probabilistic Prediction of Battery Capacity Degradation",
        "author": [
            "Hequn Li",
            "Zhongwei Deng",
            "Chunlin Jiang",
            "Yvxin He andZhansheng Ning"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17414",
        "abstract": "Accurate prediction of lithium-ion battery capacity and its associated uncertainty is essential for reliable battery management but remains challenging due to the stochastic nature of aging. This paper presents a novel method, termed the Condition Diffusion U-Net with Attention (CDUA), which integrates feature engineering and deep learning to address this challenge. The proposed approach employs a diffusion-based generative model for time-series forecasting and incorporates attention mechanisms to enhance predictive performance. Battery capacity is first derived from real-world vehicle operation data. The most relevant features are then identified using the Pearson correlation coefficient and the XGBoost algorithm. These features are used to train the CDUA model, which comprises two core components: (1) a contextual U-Net with self-attention to capture complex temporal dependencies, and (2) a denoising network to reconstruct accurate capacity values from noisy observations. Experimental validation on the real-world vehicle data demonstrates that the proposed CDUA model achieves a relative Mean Absolute Error (MAE) of 0.94% and a relative Root Mean Square Error (RMSE) of 1.14%, with a narrow 95% confidence interval of 3.74% in relative width. These results confirm that CDUA provides both accurate capacity estimation and reliable uncertainty quantification. Comparative experiments further verify its robustness and superior performance over existing mainstream approaches.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "349",
        "title": "Diffusion Models as Dataset Distillation Priors",
        "author": [
            "Duo Su",
            "Huyu Wu",
            "Huanran Chen",
            "Yiming Shi",
            "Yuzhu Wang",
            "Xi Ye",
            "Jun Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17421",
        "abstract": "Dataset distillation aims to synthesize compact yet informative datasets from large ones. A significant challenge in this field is achieving a trifecta of diversity, generalization, and representativeness in a single distilled dataset. Although recent generative dataset distillation methods adopt powerful diffusion models as their foundation models, the inherent representativeness prior in diffusion models is overlooked. Consequently, these approaches often necessitate the integration of external constraints to enhance data quality. To address this, we propose Diffusion As Priors (DAP), which formalizes representativeness by quantifying the similarity between synthetic and real data in feature space using a Mercer kernel. We then introduce this prior as guidance to steer the reverse diffusion process, enhancing the representativeness of distilled samples without any retraining. Extensive experiments on large-scale datasets, such as ImageNet-1K and its subsets, demonstrate that DAP outperforms state-of-the-art methods in generating high-fidelity datasets while achieving superior cross-architecture generalization. Our work not only establishes a theoretical connection between diffusion priors and the objectives of dataset distillation but also provides a practical, training-free framework for improving the quality of the distilled dataset.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "350",
        "title": "DeepDetect: Learning All-in-One Dense Keypoints",
        "author": [
            "Shaharyar Ahmed Khan Tareen",
            "Filza Khan Tareen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17422",
        "abstract": "Keypoint detection is the foundation of many computer vision tasks, including image registration, structure-from motion, 3D reconstruction, visual odometry, and SLAM. Traditional detectors (SIFT, SURF, ORB, BRISK, etc.) and learning based methods (SuperPoint, R2D2, LF-Net, D2-Net, etc.) have shown strong performance yet suffer from key limitations: sensitivity to photometric changes, low keypoint density and repeatability, limited adaptability to challenging scenes, and lack of semantic understanding, often failing to prioritize visually important regions. We present DeepDetect, an intelligent, all-in-one, dense keypoint detector that unifies the strengths of classical detectors using deep learning. Firstly, we create ground-truth masks by fusing outputs of 7 keypoint and 2 edge detectors, extracting diverse visual cues from corners and blobs to prominent edges and textures in the images. Afterwards, a lightweight and efficient model: ESPNet, is trained using these masks as labels, enabling DeepDetect to focus semantically on images while producing highly dense keypoints, that are adaptable to diverse and visually degraded conditions. Evaluations on the Oxford Affine Covariant Regions dataset demonstrate that DeepDetect surpasses other detectors in keypoint density, repeatability, and the number of correct matches, achieving maximum values of 0.5143 (average keypoint density), 0.9582 (average repeatability), and 59,003 (correct matches).",
        "tags": [
            "3D",
            "Detection",
            "SLAM"
        ]
    },
    {
        "id": "351",
        "title": "Quantifying Climate Policy Action and Its Links to Development Outcomes: A Cross-National Data-Driven Analysis",
        "author": [
            "Aditi Dutta"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17425",
        "abstract": "Addressing climate change effectively requires more than cataloguing the number of policies in place; it calls for tools that can reveal their thematic priorities and their tangible impacts on development outcomes. Existing assessments often rely on qualitative descriptions or composite indices, which can mask crucial differences between key domains such as mitigation, adaptation, disaster risk management, and loss and damage. To bridge this gap, we develop a quantitative indicator of climate policy orientation by applying a multilingual transformer-based language model to official national policy documents, achieving a classification accuracy of 0.90 (F1-score). Linking these indicators with World Bank development data in panel regressions reveals that mitigation policies are associated with higher GDP and GNI; disaster risk management correlates with greater GNI and debt but reduced foreign direct investment; adaptation and loss and damage show limited measurable effects. This integrated NLP-econometric framework enables comparable, theme-specific analysis of climate governance, offering a scalable method to monitor progress, evaluate trade-offs, and align policy emphasis with development goals.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "352",
        "title": "Agentic Reinforcement Learning for Search is Unsafe",
        "author": [
            "Yushi Yang",
            "Shreyansh Padarha",
            "Andrew Lee",
            "Adam Mahdi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17431",
        "abstract": "Agentic reinforcement learning (RL) trains large language models to autonomously call tools during reasoning, with search as the most common application. These models excel at multi-step reasoning tasks, but their safety properties are not well understood. In this study, we show that RL-trained search models inherit refusal from instruction tuning and often deflect harmful requests by turning them into safe queries. However, this safety is fragile. Two simple attacks, one that forces the model to begin response with search (Search attack), another that encourages models to repeatedly search (Multi-search attack), trigger cascades of harmful searches and answers. Across two model families (Qwen, Llama) with both local and web search, these attacks lower refusal rates by up to 60.0%, answer safety by 82.5%, and search-query safety by 82.4%. The attacks succeed by triggering models to generate harmful, request-mirroring search queries before they can generate the inherited refusal tokens. This exposes a core weakness of current RL training: it rewards continued generation of effective queries without accounting for their harmfulness. As a result, RL search models have vulnerabilities that users can easily exploit, making it urgent to develop safety-aware agentic RL pipelines optimising for safe search.",
        "tags": [
            "LLM",
            "LLaMA",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": "353",
        "title": "Leveraging AV1 motion vectors for Fast and Dense Feature Matching",
        "author": [
            "Julien Zouein",
            "Hossein Javidnia",
            "FranÃ§ois PitiÃ©",
            "Anil Kokaram"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17434",
        "abstract": "We repurpose AV1 motion vectors to produce dense sub-pixel correspondences and short tracks filtered by cosine consistency. On short videos, this compressed-domain front end runs comparably to sequential SIFT while using far less CPU, and yields denser matches with competitive pairwise geometry. As a small SfM demo on a 117-frame clip, MV matches register all images and reconstruct 0.46-0.62M points at 0.51-0.53,px reprojection error; BA time grows with match density. These results show compressed-domain correspondences are a practical, resource-efficient front end with clear paths to scaling in full pipelines.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": "354",
        "title": "Evaluating Large Language Models on Urdu Idiom Translation",
        "author": [
            "Muhammad Farmal Khan",
            "Mousumi Akter"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17460",
        "abstract": "Idiomatic translation remains a significant challenge in machine translation, especially for low resource languages such as Urdu, and has received limited prior attention. To advance research in this area, we introduce the first evaluation datasets for Urdu to English idiomatic translation, covering both Native Urdu and Roman Urdu scripts and annotated with gold-standard English equivalents. We evaluate multiple open-source Large Language Models (LLMs) and Neural Machine Translation (NMT) systems on this task, focusing on their ability to preserve idiomatic and cultural meaning. Automatic metrics including BLEU, BERTScore, COMET, and XCOMET are used to assess translation quality. Our findings indicate that prompt engineering enhances idiomatic translation compared to direct translation, though performance differences among prompt types are relatively minor. Moreover, cross script comparisons reveal that text representation substantially affects translation quality, with Native Urdu inputs producing more accurate idiomatic translations than Roman Urdu.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "355",
        "title": "Disparities in Multilingual LLM-Based Healthcare Q&A",
        "author": [
            "Ipek Baris Schlicht",
            "Burcu Sayin",
            "Zhixue Zhao",
            "Frederik M. LabontÃ©",
            "Cesare Barbera",
            "Marco Viviani",
            "Paolo Rosso",
            "Lucie Flek"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17476",
        "abstract": "Equitable access to reliable health information is vital when integrating AI into healthcare. Yet, information quality varies across languages, raising concerns about the reliability and consistency of multilingual Large Language Models (LLMs). We systematically examine cross-lingual disparities in pre-training source and factuality alignment in LLM answers for multilingual healthcare Q&A across English, German, Turkish, Chinese (Mandarin), and Italian. We (i) constructed Multilingual Wiki Health Care (MultiWikiHealthCare), a multilingual dataset from Wikipedia; (ii) analyzed cross-lingual healthcare coverage; (iii) assessed LLM response alignment with these references; and (iv) conducted a case study on factual alignment through the use of contextual information and Retrieval-Augmented Generation (RAG). Our findings reveal substantial cross-lingual disparities in both Wikipedia coverage and LLM factual alignment. Across LLMs, responses align more with English Wikipedia, even when the prompts are non-English. Providing contextual excerpts from non-English Wikipedia at inference time effectively shifts factual alignment toward culturally relevant knowledge. These results highlight practical pathways for building more equitable, multilingual AI systems for healthcare.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "356",
        "title": "Towards geological inference with process-based and deep generative modeling, part 2: inversion of fluvial deposits and latent-space disentanglement",
        "author": [
            "Guillaume Rongier",
            "Luk Peeters"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17478",
        "abstract": "High costs and uncertainties make subsurface decision-making challenging, as acquiring new data is rarely scalable. Embedding geological knowledge directly into predictive models offers a valuable alternative. A joint approach enables just that: process-based models that mimic geological processes can help train generative models that make predictions more efficiently. This study explores whether a generative adversarial network (GAN) - a type of deep-learning algorithm for generative modeling - trained to produce fluvial deposits can be inverted to match well and seismic data. Four inversion approaches applied to three test samples with 4, 8, and 20 wells struggled to match these well data, especially as the well number increased or as the test sample diverged from the training data. The key bottleneck lies in the GAN's latent representation: it is entangled, so samples with similar sedimentological features are not necessarily close in the latent space. Label conditioning or latent overparameterization can partially disentangle the latent space during training, although not yet sufficiently for a successful inversion. Fine-tuning the GAN to restructure the latent space locally reduces mismatches to acceptable levels for all test cases, with and without seismic data. But this approach depends on an initial, partially successful inversion step, which influences the quality and diversity of the final samples. Overall, GANs can already handle the tasks required for their integration into geomodeling workflows. We still need to further assess their robustness, and how to best leverage them in support of geological interpretation.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "357",
        "title": "Initialize to Generalize: A Stronger Initialization Pipeline for Sparse-View 3DGS",
        "author": [
            "Feng Zhou",
            "Wenkai Guo",
            "Pu Cao",
            "Zhicheng Zhang",
            "Jianqin Yin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17479",
        "abstract": "Sparse-view 3D Gaussian Splatting (3DGS) often overfits to the training views, leading to artifacts like blurring in novel view rendering. Prior work addresses it either by enhancing the initialization (\\emph{i.e.}, the point cloud from Structure-from-Motion (SfM)) or by adding training-time constraints (regularization) to the 3DGS optimization. Yet our controlled ablations reveal that initialization is the decisive factor: it determines the attainable performance band in sparse-view 3DGS, while training-time constraints yield only modest within-band improvements at extra cost. Given initialization's primacy, we focus our design there. Although SfM performs poorly under sparse views due to its reliance on feature matching, it still provides reliable seed points. Thus, building on SfM, our effort aims to supplement the regions it fails to cover as comprehensively as possible. Specifically, we design: (i) frequency-aware SfM that improves low-texture coverage via low-frequency view augmentation and relaxed multi-view correspondences; (ii) 3DGS self-initialization that lifts photometric supervision into additional points, compensating SfM-sparse regions with learned Gaussian centers; and (iii) point-cloud regularization that enforces multi-view consistency and uniform spatial coverage through simple geometric/visibility priors, yielding a clean and reliable point cloud. Our experiments on LLFF and Mip-NeRF360 demonstrate consistent gains in sparse-view settings, establishing our approach as a stronger initialization strategy. Code is available at https://github.com/zss171999645/ItG-GS.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": "358",
        "title": "ReXMoE: Reusing Experts with Minimal Overhead in Mixture-of-Experts",
        "author": [
            "Zheyue Tan",
            "Zhiyuan Li",
            "Tao Yuan",
            "Dong Zhou",
            "Weilin Liu",
            "Yueqing Zhuang",
            "Yadong Li",
            "Guowei Niu",
            "Cheng Qin",
            "Zhuyu Yao",
            "Congyi Liu",
            "Haiyang Xu",
            "Boxun Li",
            "Guohao Dai",
            "Bo Zhao",
            "Yu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17483",
        "abstract": "Mixture-of-Experts (MoE) architectures have emerged as a promising approach to scale Large Language Models (LLMs). MoE boosts the efficiency by activating a subset of experts per token. Recent works show that fine-grained experts substantially enriches the combinatorial flexibility of active experts and enhances model expressiveness. However, such a design is fundamentally limited by the layer-local routing mechanism: each layer is restricted to its own expert pool. This requires a careful trade-off between expert dimensionality and routing diversity given fixed parameter budgets. We describe ReXMoE, a novel MoE architecture that improves routing beyond the existing layer-local approaches by allowing routers to reuse experts across adjacent layers. ReXMoE decouples expert dimensionality from per-layer budgets, enabling richer expert combinations without sacrificing individual expert capacity or inflating overall parameters. To this end, we propose a new progressive scaling routing (PSR) strategy to gradually increase the candidate expert pool during training. As a result, ReXMoE improves both language modeling and downstream task performance. Extensive experiments on models ranging from 0.5B to 7B parameters across different architectures demonstrate that ReXMoE consistently improves performance under fixed architectural dimensions, confirming ReXMoE as new design paradigm for parameter-efficient and scalable MoE-based LLMs.",
        "tags": [
            "LLM",
            "MoE"
        ]
    },
    {
        "id": "359",
        "title": "DETree: DEtecting Human-AI Collaborative Texts via Tree-Structured Hierarchical Representation Learning",
        "author": [
            "Yongxin He",
            "Shan Zhang",
            "Yixuan Cao",
            "Lei Ma",
            "Ping Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17489",
        "abstract": "Detecting AI-involved text is essential for combating misinformation, plagiarism, and academic misconduct. However, AI text generation includes diverse collaborative processes (AI-written text edited by humans, human-written text edited by AI, and AI-generated text refined by other AI), where various or even new LLMs could be involved. Texts generated through these varied processes exhibit complex characteristics, presenting significant challenges for detection. Current methods model these processes rather crudely, primarily employing binary classification (purely human vs. AI-involved) or multi-classification (treating human-AI collaboration as a new class). We observe that representations of texts generated through different processes exhibit inherent clustering relationships. Therefore, we propose DETree, a novel approach that models the relationships among different processes as a Hierarchical Affinity Tree structure, and introduces a specialized loss function that aligns text representations with this tree. To facilitate this learning, we developed RealBench, a comprehensive benchmark dataset that automatically incorporates a wide spectrum of hybrid texts produced through various human-AI collaboration processes. Our method improves performance in hybrid text detection tasks and significantly enhances robustness and generalization in out-of-distribution scenarios, particularly in few-shot learning conditions, further demonstrating the promise of training-based approaches in OOD settings. Our code and dataset are available at https://github.com/heyongxin233/DETree.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": "360",
        "title": "Empowering Real-World: A Survey on the Technology, Practice, and Evaluation of LLM-driven Industry Agents",
        "author": [
            "Yihong Tang",
            "Kehai Chen",
            "Liang Yue",
            "Jinxin Fan",
            "Caishen Zhou",
            "Xiaoguang Li",
            "Yuyang Zhang",
            "Mingming Zhao",
            "Shixiong Kai",
            "Kaiyang Guo",
            "Xingshan Zeng",
            "Wenjing Cun",
            "Lifeng Shang",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17491",
        "abstract": "With the rise of large language models (LLMs), LLM agents capable of autonomous reasoning, planning, and executing complex tasks have become a frontier in artificial intelligence. However, how to translate the research on general agents into productivity that drives industry transformations remains a significant challenge. To address this, this paper systematically reviews the technologies, applications, and evaluation methods of industry agents based on LLMs. Using an industry agent capability maturity framework, it outlines the evolution of agents in industry applications, from \"process execution systems\" to \"adaptive social systems.\" First, we examine the three key technological pillars that support the advancement of agent capabilities: Memory, Planning, and Tool Use. We discuss how these technologies evolve from supporting simple tasks in their early forms to enabling complex autonomous systems and collective intelligence in more advanced forms. Then, we provide an overview of the application of industry agents in real-world domains such as digital engineering, scientific discovery, embodied intelligence, collaborative business execution, and complex system simulation. Additionally, this paper reviews the evaluation benchmarks and methods for both fundamental and specialized capabilities, identifying the challenges existing evaluation systems face regarding authenticity, safety, and industry specificity. Finally, we focus on the practical challenges faced by industry agents, exploring their capability boundaries, developmental potential, and governance issues in various scenarios, while providing insights into future directions. By combining technological evolution with industry practices, this review aims to clarify the current state and offer a clear roadmap and theoretical foundation for understanding and building the next generation of industry agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "361",
        "title": "I-RAVEN-X: Benchmarking Generalization and Robustness of Analogical and Mathematical Reasoning in Large Language and Reasoning Models",
        "author": [
            "Giacomo Camposampiero",
            "Michael Hersche",
            "Roger Wattenhofer",
            "Abu Sebastian",
            "Abbas Rahimi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17496",
        "abstract": "We introduce I-RAVEN-X, a symbolic benchmark designed to evaluate generalization and robustness in analogical and mathematical reasoning for Large Language Models (LLMs) and Large Reasoning Models (LRMs). I-RAVEN-X extends I-RAVEN by increasing operand complexity, attribute range, and introducing perceptual uncertainty. Compared to LLMs, empirical results show that LRMs achieve improved productivity and systematicity on longer reasoning relations and wider attribute ranges, respectively. However, LRMs are still significantly challenged by reasoning under uncertainty and cannot effectively explore multiple probabilistic outcomes.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "362",
        "title": "Deep Self-Evolving Reasoning",
        "author": [
            "Zihan Liu",
            "Shun Zheng",
            "Xumeng Wen",
            "Yang Wang",
            "Jiang Bian",
            "Mao Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17498",
        "abstract": "Long-form chain-of-thought reasoning has become a cornerstone of advanced reasoning in large language models. While recent verification-refinement frameworks have enabled proprietary models to solve Olympiad-level problems, their effectiveness hinges on strong, reliable verification and correction capabilities, which remain fragile in open-weight, smaller-scale models. This work demonstrates that even with weak verification and refinement capabilities on hard tasks, the reasoning limits of such models can be substantially extended through a probabilistic paradigm we call Deep Self-Evolving Reasoning (DSER). We conceptualize iterative reasoning as a Markov chain, where each step represents a stochastic transition in the solution space. The key insight is that convergence to a correct solution is guaranteed as long as the probability of improvement marginally exceeds that of degradation. By running multiple long-horizon, self-evolving processes in parallel, DSER amplifies these small positive tendencies, enabling the model to asymptotically approach correct answers. Empirically, we apply DSER to the DeepSeek-R1-0528-Qwen3-8B model. On the challenging AIME 2024-2025 benchmark, DSER solves 5 out of 9 previously unsolvable problems and boosts overall performance, enabling this compact model to surpass the single-turn accuracy of its 600B-parameter teacher through majority voting. Beyond its immediate utility for test-time scaling, the DSER framework serves to diagnose the fundamental limitations of current open-weight reasoners. By clearly delineating their shortcomings in self-verification, refinement, and stability, our findings establish a clear research agenda for developing next-generation models with powerful, intrinsic self-evolving capabilities.",
        "tags": [
            "CoT",
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": "363",
        "title": "A non-local model for heterogeneous material flow on conveyor belts",
        "author": [
            "Paola Goatin",
            "Simone GÃ¶ttlich",
            "Fabian Ziegler"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17500",
        "abstract": "In this paper, a finite volume approximation scheme is used to solve a non-local macroscopic material flow model in two space dimensions, accounting for the presence of boundaries in the non-local terms. Based on a previous result for the scalar case, we extend the setting to a system of heterogeneous material on bounded domains. We prove the convergence of the approximate solutions constructed using the Roe scheme with dimensiona splitting, where the major challenge lies in the treatment of the discontinuity occurring in the flux function. Numerical tests show a good agreement with microscopic simulations.",
        "tags": [
            "FLUX"
        ]
    },
    {
        "id": "364",
        "title": "Context-Aware Pseudo-Label Scoring for Zero-Shot Video Summarization",
        "author": [
            "Yuanli Wu",
            "Long Zhang",
            "Yue Du",
            "Bin Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17501",
        "abstract": "With the rapid proliferation of video content across social media, surveillance, and education platforms, efficiently summarizing long videos into concise yet semantically faithful surrogates has become increasingly vital. Existing supervised methods achieve strong in-domain accuracy by learning from dense annotations but suffer from high labeling costs and limited cross-dataset generalization, while unsupervised approaches, though label-free, often fail to capture high-level human semantics and fine-grained narrative cues. More recently, zero-shot prompting pipelines have leveraged large language models (LLMs) for training-free video summarization, yet remain highly sensitive to handcrafted prompt templates and dataset-specific score normalization. To overcome these limitations, we introduce a rubric-guided, pseudo-labeled prompting framework that transforms a small subset of ground-truth annotations into high-confidence pseudo labels, which are aggregated into structured, dataset-adaptive scoring rubrics guiding interpretable scene evaluation. During inference, first and last segments are scored based solely on their descriptions, whereas intermediate ones incorporate brief contextual summaries of adjacent scenes to assess narrative progression and redundancy. This contextual prompting enables the LLM to balance local salience and global coherence without parameter tuning. On SumMe and TVSum, our method achieves F1 scores of \\textbf{57.58} and \\textbf{63.05}, surpassing unsupervised and prior zero-shot baselines while approaching supervised performance. The results demonstrate that rubric-guided pseudo labeling effectively stabilizes LLM-based scoring and establishes a general, interpretable zero-shot paradigm for video summarization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "365",
        "title": "Lingua Custodi's participation at the WMT 2025 Terminology shared task",
        "author": [
            "Jingshu Liu",
            "Raheel Qader",
            "GaÃ«tan Caillaut",
            "Mariam NakhlÃ©"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17504",
        "abstract": "While BERT is an effective method for learning monolingual sentence embeddings for semantic similarity and embedding based transfer learning BERT based cross-lingual sentence embeddings have yet to be explored. We systematically investigate methods for learning multilingual sentence embeddings by combining the best methods for learning monolingual and cross-lingual representations including: masked language modeling (MLM), translation language modeling (TLM), dual encoder translation ranking, and additive margin softmax. We show that introducing a pre-trained multilingual language model dramatically reduces the amount of parallel training data required to achieve good performance by 80%. Composing the best of these methods produces a model that achieves 83.7% bi-text retrieval accuracy over 112 languages on Tatoeba, well above the 65.5 achieved by LASER, while still performing competitively on monolingual transfer learning benchmarks. Parallel data mined from CommonCrawl using our best model is shown to train competitive NMT models for en-zh and en-de. We publicly release our best multilingual sentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "tags": [
            "BERT"
        ]
    },
    {
        "id": "366",
        "title": "Annotation-Efficient Universal Honesty Alignment",
        "author": [
            "Shiyu Ni",
            "Keping Bi",
            "Jiafeng Guo",
            "Minghao Tang",
            "Jingtong Wu",
            "Zengxin Han",
            "Xueqi Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17509",
        "abstract": "Honesty alignment-the ability of large language models (LLMs) to recognize their knowledge boundaries and express calibrated confidence-is essential for trustworthy deployment. Existing methods either rely on training-free confidence estimation (e.g., token probabilities, self-consistency) or training-based calibration with correctness annotations. While effective, achieving universal honesty alignment with training-based calibration requires costly, large-scale labeling. To support annotation-efficient training, we introduce Elicitation-Then-Calibration (EliCal), a two-stage framework that first elicits internal confidence using inexpensive self-consistency supervision, then calibrates this confidence with a small set of correctness annotations. To support a large-scale study, we release HonestyBench, a benchmark covering ten free-form QA datasets with 560k training and 70k evaluation instances annotated with correctness and self-consistency signals. Experiments show that EliCal achieves near-optimal alignment with only 1k correctness annotations (0.18% of full supervision) and better alignment performance on unseen MMLU tasks than the calibration-only baseline, offering a scalable solution toward universal honesty alignment in LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "367",
        "title": "SimBench: Benchmarking the Ability of Large Language Models to Simulate Human Behaviors",
        "author": [
            "Tiancheng Hu",
            "Joachim Baumann",
            "Lorenzo Lupo",
            "Dirk Hovy",
            "Nigel Collier",
            "Paul RÃ¶ttger"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17516",
        "abstract": "Large language model (LLM) simulations of human behavior have the potential to revolutionize the social and behavioral sciences, if and only if they faithfully reflect real human behaviors. Current evaluations are fragmented, based on bespoke tasks and metrics, creating a patchwork of incomparable results. To address this, we introduce SimBench, the first large-scale, standardized benchmark for a robust, reproducible science of LLM simulation. By unifying 20 diverse datasets covering tasks from moral decision-making to economic choice across a large global participant pool, SimBench provides the necessary foundation to ask fundamental questions about when, how, and why LLM simulations succeed or fail. We show that, while even the best LLMs today have limited simulation ability (score: 40.80/100), performance scales log-linearly with model size. Simulation performance is not improved by increased inference-time compute. We demonstrate an alignment-simulation trade-off: instruction-tuning improves performance on low-entropy (consensus) questions but degrades it on high-entropy (diverse) ones. Models particularly struggle when simulating specific demographic groups. Finally, we demonstrate that simulation ability correlates most strongly with deep, knowledge-intensive reasoning (MMLU-Pro, r=0.939). By making progress measurable, we aim to accelerate the development of more faithful LLM simulators.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "368",
        "title": "MUG-V 10B: High-efficiency Training Pipeline for Large Video Generation Models",
        "author": [
            "Yongshun Zhang",
            "Zhongyi Fan",
            "Yonghang Zhang",
            "Zhangzikang Li",
            "Weifeng Chen",
            "Zhongwei Feng",
            "Chaoyue Wang",
            "Peng Hou",
            "Anxiang Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17519",
        "abstract": "In recent years, large-scale generative models for visual content (\\textit{e.g.,} images, videos, and 3D objects/scenes) have made remarkable progress. However, training large-scale video generation models remains particularly challenging and resource-intensive due to cross-modal text-video alignment, the long sequences involved, and the complex spatiotemporal dependencies. To address these challenges, we present a training framework that optimizes four pillars: (i) data processing, (ii) model architecture, (iii) training strategy, and (iv) infrastructure for large-scale video generation models. These optimizations delivered significant efficiency gains and performance improvements across all stages of data preprocessing, video compression, parameter scaling, curriculum-based pretraining, and alignment-focused post-training. Our resulting model, MUG-V 10B, matches recent state-of-the-art video generators overall and, on e-commerce-oriented video generation tasks, surpasses leading open-source baselines in human evaluations. More importantly, we open-source the complete stack, including model weights, Megatron-Core-based large-scale training code, and inference pipelines for video generation and enhancement. To our knowledge, this is the first public release of large-scale video generation training code that exploits Megatron-Core to achieve high training efficiency and near-linear multi-node scaling, details are available in \\href{https://github.com/Shopee-MUG/MUG-V}{our webpage}.",
        "tags": [
            "3D",
            "Video Generation"
        ]
    },
    {
        "id": "369",
        "title": "How role-play shapes relevance judgment in zero-shot LLM rankers",
        "author": [
            "Yumeng Wang",
            "Jirui Qi",
            "Catherine Chen",
            "Panagiotis Eustratiadis",
            "Suzan Verberne"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17535",
        "abstract": "Large Language Models (LLMs) have emerged as promising zero-shot rankers, but their performance is highly sensitive to prompt formulation. In particular, role-play prompts, where the model is assigned a functional role or identity, often give more robust and accurate relevance rankings. However, the mechanisms and diversity of role-play effects remain underexplored, limiting both effective use and interpretability. In this work, we systematically examine how role-play variations influence zero-shot LLM rankers. We employ causal intervention techniques from mechanistic interpretability to trace how role-play information shapes relevance judgments in LLMs. Our analysis reveals that (1) careful formulation of role descriptions have a large effect on the ranking quality of the LLM; (2) role-play signals are predominantly encoded in early layers and communicate with task instructions in middle layers, while receiving limited interaction with query or document representations. Specifically, we identify a group of attention heads that encode information critical for role-conditioned relevance. These findings not only shed light on the inner workings of role-play in LLM ranking but also offer guidance for designing more effective prompts in IR and beyond, pointing toward broader opportunities for leveraging role-play in zero-shot applications.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "370",
        "title": "TrajMamba: An Efficient and Semantic-rich Vehicle Trajectory Pre-training Model",
        "author": [
            "Yichen Liu",
            "Yan Lin",
            "Shengnan Guo",
            "Zeyu Zhou",
            "Youfang Lin",
            "Huaiyu Wan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17545",
        "abstract": "Vehicle GPS trajectories record how vehicles move over time, storing valuable travel semantics, including movement patterns and travel purposes. Learning travel semantics effectively and efficiently is crucial for real-world applications of trajectory data, which is hindered by two major challenges. First, travel purposes are tied to the functions of the roads and points-of-interest (POIs) involved in a trip. Such information is encoded in textual addresses and descriptions and introduces heavy computational burden to modeling. Second, real-world trajectories often contain redundant points, which harm both computational efficiency and trajectory embedding quality. To address these challenges, we propose TrajMamba, a novel approach for efficient and semantically rich vehicle trajectory learning. TrajMamba introduces a Traj-Mamba Encoder that captures movement patterns by jointly modeling both GPS and road perspectives of trajectories, enabling robust representations of continuous travel behaviors. It also incorporates a Travel Purpose-aware Pre-training procedure to integrate travel purposes into the learned embeddings without introducing extra overhead to embedding calculation. To reduce redundancy in trajectories, TrajMamba features a Knowledge Distillation Pre-training scheme to identify key trajectory points through a learnable mask generator and obtain effective compressed trajectory embeddings. Extensive experiments on two real-world datasets and three downstream tasks show that TrajMamba outperforms state-of-the-art baselines in both efficiency and accuracy.",
        "tags": [
            "Mamba"
        ]
    },
    {
        "id": "371",
        "title": "Language Confusion Gate: Language-Aware Decoding Through Model Self-Distillation",
        "author": [
            "Collin Zhang",
            "Fei Huang",
            "Chenhan Yuan",
            "Junyang Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17555",
        "abstract": "Large language models (LLMs) often experience language confusion, which is the unintended mixing of languages during text generation. Current solutions to this problem either necessitate model retraining or cannot differentiate between harmful confusion and acceptable code-switching. This paper introduces the Language Confusion Gate (LCG), a lightweight, plug-in solution that filters tokens during decoding without altering the base LLM. The LCG is trained using norm-adjusted self-distillation to predict appropriate language families and apply masking only when needed. Our method is based on the findings that language confusion is infrequent, correct-language tokens are usually among the top predictions, and output token embedding norms are larger for high-resource languages, which biases sampling. When evaluated across various models, including Qwen3, GPT-OSS, Gemma3, Llama3.1, LCG decreases language confusion significantly, often by an order of magnitude, without negatively impacting task performance. Code is available at https://github.com/collinzrj/language_confusion_gate.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "372",
        "title": "The Free Transformer",
        "author": [
            "FranÃ§ois Fleuret"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17558",
        "abstract": "We propose an extension of the decoder Transformer that conditions its generative process on random latent variables which are learned without supervision thanks to a variational procedure. Experimental evaluations show that allowing such a conditioning translates into substantial improvements on downstream tasks.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "373",
        "title": "An Empirical Study of Lagrangian Methods in Safe Reinforcement Learning",
        "author": [
            "Lindsay Spoor",
            "Ãlvaro Serra-GÃ³mez",
            "Aske Plaat",
            "Thomas Moerland"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17564",
        "abstract": "In safety-critical domains such as robotics, navigation and power systems, constrained optimization problems arise where maximizing performance must be carefully balanced with associated constraints. Safe reinforcement learning provides a framework to address these challenges, with Lagrangian methods being a popular choice. However, the effectiveness of Lagrangian methods crucially depends on the choice of the Lagrange multiplier $\\lambda$, which governs the trade-off between return and constraint cost. A common approach is to update the multiplier automatically during training. Although this is standard in practice, there remains limited empirical evidence on the robustness of an automated update and its influence on overall performance. Therefore, we analyze (i) optimality and (ii) stability of Lagrange multipliers in safe reinforcement learning across a range of tasks. We provide $\\lambda$-profiles that give a complete visualization of the trade-off between return and constraint cost of the optimization problem. These profiles show the highly sensitive nature of $\\lambda$ and moreover confirm the lack of general intuition for choosing the optimal value $\\lambda^*$. Our findings additionally show that automated multiplier updates are able to recover and sometimes even exceed the optimal performance found at $\\lambda^*$ due to the vast difference in their learning trajectories. Furthermore, we show that automated multiplier updates exhibit oscillatory behavior during training, which can be mitigated through PID-controlled updates. However, this method requires careful tuning to achieve consistently better performance across tasks. This highlights the need for further research on stabilizing Lagrangian methods in safe reinforcement learning. The code used to reproduce our results can be found at https://github.com/lindsayspoor/Lagrangian_SafeRL.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "374",
        "title": "PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception",
        "author": [
            "Kaichen Zhou",
            "Yuhan Wang",
            "Grace Chen",
            "Xinhai Chang",
            "Gaspard Beaudouin",
            "Fangneng Zhan",
            "Paul Pu Liang",
            "Mengyu Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17568",
        "abstract": "Recent 3D feed-forward models, such as the Visual Geometry Grounded Transformer (VGGT), have shown strong capability in inferring 3D attributes of static scenes. However, since they are typically trained on static datasets, these models often struggle in real-world scenarios involving complex dynamic elements, such as moving humans or deformable objects like umbrellas. To address this limitation, we introduce PAGE-4D, a feedforward model that extends VGGT to dynamic scenes, enabling camera pose estimation, depth prediction, and point cloud reconstruction -- all without post-processing. A central challenge in multi-task 4D reconstruction is the inherent conflict between tasks: accurate camera pose estimation requires suppressing dynamic regions, while geometry reconstruction requires modeling them. To resolve this tension, we propose a dynamics-aware aggregator that disentangles static and dynamic information by predicting a dynamics-aware mask -- suppressing motion cues for pose estimation while amplifying them for geometry reconstruction. Extensive experiments show that PAGE-4D consistently outperforms the original VGGT in dynamic scenarios, achieving superior results in camera pose estimation, monocular and video depth estimation, and dense point map reconstruction.",
        "tags": [
            "3D",
            "Depth Estimation",
            "Pose Estimation",
            "Transformer"
        ]
    },
    {
        "id": "375",
        "title": "DeTAILS: Deep Thematic Analysis with Iterative LLM Support",
        "author": [
            "Ash Sharma",
            "Karen Cochrane",
            "James R. Wallace"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17575",
        "abstract": "Thematic analysis is widely used in qualitative research but can be difficult to scale because of its iterative, interpretive demands. We introduce DeTAILS, a toolkit that integrates large language model (LLM) assistance into a workflow inspired by Braun and Clarke's thematic analysis framework. DeTAILS supports researchers in generating and refining codes, reviewing clusters, and synthesizing themes through interactive feedback loops designed to preserve analytic agency. We evaluated the system with 18 qualitative researchers analyzing Reddit data. Quantitative results showed strong alignment between LLM-supported outputs and participants' refinements, alongside reduced workload and high perceived usefulness. Qualitatively, participants reported that DeTAILS accelerated analysis, prompted reflexive engagement with AI outputs, and fostered trust through transparency and control. We contribute: (1) an interactive human-LLM workflow for large-scale qualitative analysis, (2) empirical evidence of its feasibility and researcher experience, and (3) design implications for trustworthy AI-assisted qualitative research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "376",
        "title": "Intent-Driven LLM Ensemble Planning for Flexible Multi-Robot Disassembly: Demonstration on EV Batteries",
        "author": [
            "Cansu Erdogan",
            "Cesar Alan Contreras",
            "Alireza Rastegarpanah",
            "Manolis Chiou",
            "Rustam Stolkin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17576",
        "abstract": "This paper addresses the problem of planning complex manipulation tasks, in which multiple robots with different end-effectors and capabilities, informed by computer vision, must plan and execute concatenated sequences of actions on a variety of objects that can appear in arbitrary positions and configurations in unstructured scenes. We propose an intent-driven planning pipeline which can robustly construct such action sequences with varying degrees of supervisory input from a human using simple language instructions. The pipeline integrates: (i) perception-to-text scene encoding, (ii) an ensemble of large language models (LLMs) that generate candidate removal sequences based on the operator's intent, (iii) an LLM-based verifier that enforces formatting and precedence constraints, and (iv) a deterministic consistency filter that rejects hallucinated objects. The pipeline is evaluated on an example task in which two robot arms work collaboratively to dismantle an Electric Vehicle battery for recycling applications. A variety of components must be grasped and removed in specific sequences, determined by human instructions and/or by task-order feasibility decisions made by the autonomous system. On 200 real scenes with 600 operator prompts across five component classes, we used metrics of full-sequence correctness and next-task correctness to evaluate and compare five LLM-based planners (including ablation analyses of pipeline components). We also evaluated the LLM-based human interface in terms of time to execution and NASA TLX with human participant experiments. Results indicate that our ensemble-with-verification approach reliably maps operator intent to safe, executable multi-robot plans while maintaining low user effort.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": "377",
        "title": "MIRAGE: Agentic Framework for Multimodal Misinformation Detection with Web-Grounded Reasoning",
        "author": [
            "Mir Nafis Sharear Shopnil",
            "Sharad Duwal",
            "Abhishek Tyagi",
            "Adiba Mahbub Proma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17590",
        "abstract": "Misinformation spreads across web platforms through billions of daily multimodal posts that combine text and images, overwhelming manual fact-checking capacity. Supervised detection models require domain-specific training data and fail to generalize across diverse manipulation tactics. We present MIRAGE, an inference-time, model-pluggable agentic framework that decomposes multimodal verification into four sequential modules: visual veracity assessment detects AI-generated images, cross-modal consistency analysis identifies out-of-context repurposing, retrieval-augmented factual checking grounds claims in web evidence through iterative question generation, and a calibrated judgment module integrates all signals. MIRAGE orchestrates vision-language model reasoning with targeted web retrieval, outputs structured and citation-linked rationales. On MMFakeBench validation set (1,000 samples), MIRAGE with GPT-4o-mini achieves 81.65% F1 and 75.1% accuracy, outperforming the strongest zero-shot baseline (GPT-4V with MMD-Agent at 74.0% F1) by 7.65 points while maintaining 34.3% false positive rate versus 97.3% for a judge-only baseline. Test set results (5,000 samples) confirm generalization with 81.44% F1 and 75.08% accuracy. Ablation studies show visual verification contributes 5.18 F1 points and retrieval-augmented reasoning contributes 2.97 points. Our results demonstrate that decomposed agentic reasoning with web retrieval can match supervised detector performance without domain-specific training, enabling misinformation detection across modalities where labeled data remains scarce.",
        "tags": [
            "Detection",
            "GPT"
        ]
    },
    {
        "id": "378",
        "title": "LawChain: Modeling Legal Reasoning Chains for Chinese Tort Case Analysis",
        "author": [
            "Huiyuan Xie",
            "Chenyang Li",
            "Huining Zhu",
            "Chubin Zhang",
            "Yuxiao Ye",
            "Zhenghao Liu",
            "Zhiyuan Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17602",
        "abstract": "Legal reasoning is a fundamental component of legal analysis and decision-making. Existing computational approaches to legal reasoning predominantly rely on generic reasoning frameworks such as syllogism and IRAC, which do not comprehensively examine the nuanced processes that underpin legal reasoning. Moreover, current research has largely focused on criminal cases, with insufficient modeling for civil cases. In this work, we present a novel framework for explicitly modeling legal reasoning in the analysis of Chinese tort-related civil cases. We first operationalize the legal reasoning processes used in tort analysis into the LawChain framework. LawChain is a three-module reasoning framework, with each module consisting of multiple finer-grained sub-steps. Informed by the LawChain framework, we introduce the task of tort legal reasoning and construct an evaluation benchmark, LawChain$_{eval}$, to systematically assess the critical steps within analytical reasoning chains for tort analysis. Leveraging this benchmark, we evaluate state-of-the-art large language models for their legal reasoning ability in civil tort contexts. Our results indicate that current models still fall short in accurately handling crucial elements of tort legal reasoning. Furthermore, we introduce several baseline approaches that explicitly incorporate LawChain-style reasoning through prompting or post-training. We conduct further experiments on additional legal analysis tasks, such as Legal Named-Entity Recognition and Criminal Damages Calculation, to verify the generalizability of these baselines. The proposed baseline approaches achieve significant improvements in tort-related legal reasoning and generalize well to related legal analysis tasks, thus demonstrating the value of explicitly modeling legal reasoning chains to enhance the reasoning capabilities of language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "379",
        "title": "ShapeCraft: LLM Agents for Structured, Textured and Interactive 3D Modeling",
        "author": [
            "Shuyuan Zhang",
            "Chenhan Jiang",
            "Zuoou Li",
            "Jiankang Deng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17603",
        "abstract": "3D generation from natural language offers significant potential to reduce expert manual modeling efforts and enhance accessibility to 3D assets. However, existing methods often yield unstructured meshes and exhibit poor interactivity, making them impractical for artistic workflows. To address these limitations, we represent 3D assets as shape programs and introduce ShapeCraft, a novel multi-agent framework for text-to-3D generation. At its core, we propose a Graph-based Procedural Shape (GPS) representation that decomposes complex natural language into a structured graph of sub-tasks, thereby facilitating accurate LLM comprehension and interpretation of spatial relationships and semantic shape details. Specifically, LLM agents hierarchically parse user input to initialize GPS, then iteratively refine procedural modeling and painting to produce structured, textured, and interactive 3D assets. Qualitative and quantitative experiments demonstrate ShapeCraft's superior performance in generating geometrically accurate and semantically rich 3D assets compared to existing LLM-based agents. We further show the versatility of ShapeCraft through examples of animated and user-customized editing, highlighting its potential for broader interactive applications.",
        "tags": [
            "3D",
            "LLM",
            "Text-to-3D"
        ]
    },
    {
        "id": "380",
        "title": "Learned Inertial Odometry for Cycling Based on Mixture of Experts Algorithm",
        "author": [
            "Hao Qiao",
            "Yan Wang",
            "Shuo Yang",
            "Xiaoyao Yu",
            "Jian kuang",
            "Xiaoji Niu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17604",
        "abstract": "With the rapid growth of bike sharing and the increasing diversity of cycling applications, accurate bicycle localization has become essential. traditional GNSS-based methods suffer from multipath effects, while existing inertial navigation approaches rely on precise modeling and show limited robustness. Tight Learned Inertial Odometry (TLIO) achieves low position drift by combining raw IMU data with predicted displacements by neural networks, but its high computational cost restricts deployment on mobile devices. To overcome this, we extend TLIO to bicycle localization and introduce an improved Mixture-of Experts (MoE) model that reduces both training and inference costs. Experiments show that, compared to the state-of-the-art LLIO framework, our method achieves comparable accuracy while reducing parameters by 64.7% and computational cost by 81.8%.",
        "tags": [
            "MoE"
        ]
    },
    {
        "id": "381",
        "title": "Forget to Know, Remember to Use: Context-Aware Unlearning for Large Language Models",
        "author": [
            "Yuefeng Peng",
            "Parnian Afshar",
            "Megan Ganji",
            "Thomas Butler",
            "Amir Houmansadr",
            "Mingxian Wang",
            "Dezhi Hong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17620",
        "abstract": "Large language models may encode sensitive information or outdated knowledge that needs to be removed, to ensure responsible and compliant model responses. Unlearning has emerged as an efficient alternative to full retraining, aiming to remove specific knowledge while preserving overall model utility. Existing evaluations of unlearning methods focus on (1) the extent of forgetting of the target knowledge (forget set) and (2) maintaining performance on the retain set (i.e., utility). However, these evaluations overlook an important usability aspect: users may still want the model to leverage the removed information if it is re-introduced in the prompt. In a systematic evaluation of six state-of-the-art unlearning methods, we find that they consistently impair such contextual utility. To address this, we augment unlearning objectives with a plug-in term that preserves the model's ability to use forgotten knowledge when it is present in context. Extensive experiments demonstrate that our approach restores contextual utility to near original levels while still maintaining effective forgetting and retain-set utility.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "382",
        "title": "GUIDE: Enhancing Gradient Inversion Attacks in Federated Learning with Denoising Models",
        "author": [
            "Vincenzo Carletti",
            "Pasquale Foggia",
            "Carlo Mazzocca",
            "Giuseppe Parrella",
            "Mario Vento"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17621",
        "abstract": "Federated Learning (FL) enables collaborative training of Machine Learning (ML) models across multiple clients while preserving their privacy. Rather than sharing raw data, federated clients transmit locally computed updates to train the global model. Although this paradigm should provide stronger privacy guarantees than centralized ML, client updates remain vulnerable to privacy leakage. Adversaries can exploit them to infer sensitive properties about the training data or even to reconstruct the original inputs via Gradient Inversion Attacks (GIAs). Under the honest-butcurious threat model, GIAs attempt to reconstruct training data by reversing intermediate updates using optimizationbased techniques. We observe that these approaches usually reconstruct noisy approximations of the original inputs, whose quality can be enhanced with specialized denoising models. This paper presents Gradient Update Inversion with DEnoising (GUIDE), a novel methodology that leverages diffusion models as denoising tools to improve image reconstruction attacks in FL. GUIDE can be integrated into any GIAs that exploits surrogate datasets, a widely adopted assumption in GIAs literature. We comprehensively evaluate our approach in two attack scenarios that use different FL algorithms, models, and datasets. Our results demonstrate that GUIDE integrates seamlessly with two state-ofthe- art GIAs, substantially improving reconstruction quality across multiple metrics. Specifically, GUIDE achieves up to 46% higher perceptual similarity, as measured by the DreamSim metric.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "383",
        "title": "SARSteer: Safeguarding Large Audio Language Models via Safe-Ablated Refusal Steering",
        "author": [
            "Weilin Lin",
            "Jianze Li",
            "Hui Xiong",
            "Li Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17633",
        "abstract": "Large Audio-Language Models (LALMs) are becoming essential as a powerful multimodal backbone for real-world applications. However, recent studies show that audio inputs can more easily elicit harmful responses than text, exposing new risks toward deployment. While safety alignment has made initial advances in LLMs and Large Vision-Language Models (LVLMs), we find that vanilla adaptation of these approaches to LALMs faces two key limitations: 1) LLM-based steering fails under audio input due to the large distributional gap between activations, and 2) prompt-based defenses induce over-refusals on benign-speech queries. To address these challenges, we propose Safe-Ablated Refusal Steering (SARSteer), the first inference-time defense framework for LALMs. Specifically, SARSteer leverages text-derived refusal steering to enforce rejection without manipulating audio inputs and introduces decomposed safe-space ablation to mitigate over-refusal. Extensive experiments demonstrate that SARSteer significantly improves harmful-query refusal while preserving benign responses, establishing a principled step toward safety alignment in LALMs.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "384",
        "title": "LLM-as-a-Prophet: Understanding Predictive Intelligence with Prophet Arena",
        "author": [
            "Qingchuan Yang",
            "Simon Mahns",
            "Sida Li",
            "Anri Gu",
            "Jibang Wu",
            "Haifeng Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17638",
        "abstract": "Forecasting is not only a fundamental intellectual pursuit but also is of significant importance to societal systems such as finance and economics. With the rapid advances of large language models (LLMs) trained on Internet-scale data, it raises the promise of employing LLMs to forecast real-world future events, an emerging paradigm we call \"LLM-as-a-Prophet\". This paper systematically investigates such predictive intelligence of LLMs. To this end, we build Prophet Arena, a general evaluation benchmark that continuously collects live forecasting tasks and decomposes each task into distinct pipeline stages, in order to support our controlled and large-scale experimentation. Our comprehensive evaluation reveals that many LLMs already exhibit impressive forecasting capabilities, reflected in, e.g., their small calibration errors, consistent prediction confidence and promising market returns. However, we also uncover key bottlenecks towards achieving superior predictive intelligence via LLM-as-a-Prophet, such as LLMs' inaccurate event recalls, misunderstanding of data sources and slower information aggregation compared to markets when resolution nears.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "385",
        "title": "RESample: A Robust Data Augmentation Framework via Exploratory Sampling for Robotic Manipulation",
        "author": [
            "Yuquan Xue",
            "Guanxing Lu",
            "Zhenyu Wu",
            "Chuanrui Zhang",
            "Bofang Jia",
            "Zhengyi Gu",
            "Yansong Tang",
            "Ziwei Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17640",
        "abstract": "Vision-Language-Action models (VLAs) have demonstrated remarkable performance on complex robotic manipulation tasks through imitation learning. However, existing imitation learning datasets contain only successful trajectories and lack failure or recovery data, especially for out-of-distribution (OOD) states where the robot deviates from the main policy due to minor perturbations or errors, leading VLA models to struggle with states deviating from the training distribution. To this end, we propose an automated OOD data augmentation framework named RESample through exploratory sampling. Specifically, we first leverage offline reinforcement learning to obtain an action-value network that accurately identifies sub-optimal actions under the current manipulation policy. We further sample potential OOD states from trajectories via rollout, and design an exploratory sampling mechanism that adaptively incorporates these action proxies into the training dataset to ensure efficiency. Subsequently, our framework explicitly encourages the VLAs to recover from OOD states and enhances their robustness against distributional shifts. We conduct extensive experiments on the LIBERO benchmark as well as real-world robotic manipulation tasks, demonstrating that RESample consistently improves the stability and generalization ability of VLA models.",
        "tags": [
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "386",
        "title": "Frugal Federated Learning for Violence Detection: A Comparison of LoRA-Tuned VLMs and Personalized CNNs",
        "author": [
            "SÃ©bastien Thuau",
            "Siba Haidar",
            "Ayush Bajracharya",
            "Rachid Chelouah"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17651",
        "abstract": "We examine frugal federated learning approaches to violence detection by comparing two complementary strategies: (i) zero-shot and federated fine-tuning of vision-language models (VLMs), and (ii) personalized training of a compact 3D convolutional neural network (CNN3D). Using LLaVA-7B and a 65.8M parameter CNN3D as representative cases, we evaluate accuracy, calibration, and energy usage under realistic non-IID settings.\nBoth approaches exceed 90% accuracy. CNN3D slightly outperforms Low-Rank Adaptation(LoRA)-tuned VLMs in ROC AUC and log loss, while using less energy. VLMs remain favorable for contextual reasoning and multimodal inference. We quantify energy and CO$_2$ emissions across training and inference, and analyze sustainability trade-offs for deployment.\nTo our knowledge, this is the first comparative study of LoRA-tuned vision-language models and personalized CNNs for federated violence detection, with an emphasis on energy efficiency and environmental metrics.\nThese findings support a hybrid model: lightweight CNNs for routine classification, with selective VLM activation for complex or descriptive scenarios. The resulting framework offers a reproducible baseline for responsible, resource-aware AI in video surveillance, with extensions toward real-time, multimodal, and lifecycle-aware systems.",
        "tags": [
            "3D",
            "Detection",
            "LLaVA",
            "LoRA",
            "VLM"
        ]
    },
    {
        "id": "387",
        "title": "Qomhra: A Bilingual Irish-English Large Language Model",
        "author": [
            "Joseph McInerney"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17652",
        "abstract": "This paper introduces QomhrÃ¡, a bilingual Irish-English large language model (LLM), developed under low-resource constraints presenting a complete pipeline spanning bilingual continued pre-training, instruction tuning, and alignment from human preferences. Newly accessible Irish corpora and English text are mixed and curated to improve Irish performance while preserving English ability. 6 closed-weight LLMs are judged for their Irish text generation by a native speaker, a learner and other LLMs. Google's Gemini-2.5-Pro is ranked the highest and is subsequently used to synthesise instruction tuning and human preference datasets. Two datasets are contributed leveraging Gemini-2.5-Pro: a 30K Irish-English parallel instruction tuning dataset and a 1K human preference dataset, generating accepted and rejected responses that show near perfect alignment with a native Irish speaker. QomhrÃ¡ is comprehensively evaluated across benchmarks testing translation, gender understanding, topic identification and world knowledge with gains of up to 29% in Irish and 44% in English. QomhrÃ¡ also undergoes instruction tuning and demonstrates clear progress in instruction following, crucial for chatbot functionality.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "388",
        "title": "PDE-Free Mass-Constrained Learning of Complex Systems with Hidden States: The crowd dynamics case",
        "author": [
            "Gianmaria Viola",
            "Alessandro Della Pia",
            "Lucia Russo",
            "Ioannis Kevrekidis",
            "Constantinos Siettos"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17657",
        "abstract": "We introduce a machine learning framework for modeling the spatio-temporal dynamics of mass-constrained complex systems with hidden states, whose behavior can, in principle, be described by PDEs but lack explicit models. The method extends the Equation-Free approach, enabling the data-driven reconstruction of reduced-order models (ROMs) without needing to identify governing equations. Using manifold learning, we obtain a latent space representation of system evolution from data via delayed coordinates, in accordance with Takens/Whitney's embedding theorems. Linear (Proper Orthogonal Decomposition, POD) and nonlinear (Diffusion Maps, DMs) methods are employed to extract low-dimensional embeddings that capture the essential dynamics. Predictive ROMs are then learned within this latent space, and their evolution is lifted back to the original high-dimensional space by solving a pre-image problem. We show that both POD and k-nearest neighbor (k-NN) lifting operators preserve mass, a key physical constraint in systems such as computational fluid dynamics and crowd dynamics. Our framework effectively reconstructs the solution operator of the underlying PDE without discovering the PDE itself, by leveraging a manifold-informed objective map that bridges multiple scales. For our illustrations, we use synthetic spatio-temporal data from the Hughes model, which couples a continuity PDE with an Eikonal equation describing optimal path selection in crowds. Results show that DM-based nonlinear embeddings outperform POD in reconstruction accuracy, producing more parsimonious and stable ROMs that remain accurate and integrable over long time horizons.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": "389",
        "title": "Handling Extreme Class Imbalance: Using GANs in Data Augmentation for Suicide Prediction",
        "author": [
            "Vaishnavi Visweswaraiah",
            "Tanvi Banerjee",
            "William Romine"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17661",
        "abstract": "Suicide prediction is the key for prevention, but real data with sufficient positive samples is rare and causes extreme class imbalance. We utilized machine learning (ML) to build the model and deep learning (DL) techniques, like Generative Adversarial Networks (GAN), to generate synthetic data samples to enhance the dataset. The initial dataset contained 656 samples, with only four positive cases, prompting the need for data augmentation. A variety of machine learning models, ranging from interpretable data models to black box algorithmic models, were used. On real test data, Logistic Regression (LR) achieved a weighted precision of 0.99, a weighted recall of 0.85, and a weighted F1 score of 0.91; Random Forest (RF) showed 0.98, 0.99, and 0.99, respectively; and Support Vector Machine (SVM) achieved 0.99, 0.76, and 0.86. LR and SVM correctly identified one suicide attempt case (sensitivity:1.0) and misclassified LR(20) and SVM (31) non-attempts as attempts (specificity: 0.85 & 0.76, respectively). RF identified 0 suicide attempt cases (sensitivity: 0.0) with 0 false positives (specificity: 1.0). These results highlight the models' effectiveness, with GAN playing a key role in generating synthetic data to support suicide prevention modeling efforts.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "390",
        "title": "LILO: Bayesian Optimization with Interactive Natural Language Feedback",
        "author": [
            "Katarzyna Kobalczyk",
            "Zhiyuan Jerry Lin",
            "Benjamin Letham",
            "Zhuokai Zhao",
            "Maximilian Balandat",
            "Eytan Bakshy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17671",
        "abstract": "For many real-world applications, feedback is essential in translating complex, nuanced, or subjective goals into quantifiable optimization objectives. We propose a language-in-the-loop framework that uses a large language model (LLM) to convert unstructured feedback in the form of natural language into scalar utilities to conduct BO over a numeric search space. Unlike preferential BO, which only accepts restricted feedback formats and requires customized models for each domain-specific problem, our approach leverages LLMs to turn varied types of textual feedback into consistent utility signals and to easily include flexible user priors without manual kernel design. At the same time, our method maintains the sample efficiency and principled uncertainty quantification of BO. We show that this hybrid method not only provides a more natural interface to the decision maker but also outperforms conventional BO baselines and LLM-only optimizers, particularly in feedback-limited regimes.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "391",
        "title": "PICABench: How Far Are We from Physically Realistic Image Editing?",
        "author": [
            "Yuandong Pu",
            "Le Zhuo",
            "Songhao Han",
            "Jinbo Xing",
            "Kaiwen Zhu",
            "Shuo Cao",
            "Bin Fu",
            "Si Liu",
            "Hongsheng Li",
            "Yu Qiao",
            "Wenlong Zhang",
            "Xi Chen",
            "Yihao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17681",
        "abstract": "Image editing has achieved remarkable progress recently. Modern editing models could already follow complex instructions to manipulate the original content. However, beyond completing the editing instructions, the accompanying physical effects are the key to the generation realism. For example, removing an object should also remove its shadow, reflections, and interactions with nearby objects. Unfortunately, existing models and benchmarks mainly focus on instruction completion but overlook these physical effects. So, at this moment, how far are we from physically realistic image editing? To answer this, we introduce PICABench, which systematically evaluates physical realism across eight sub-dimension (spanning optics, mechanics, and state transitions) for most of the common editing operations (add, remove, attribute change, etc). We further propose the PICAEval, a reliable evaluation protocol that uses VLM-as-a-judge with per-case, region-level human annotations and questions. Beyond benchmarking, we also explore effective solutions by learning physics from videos and construct a training dataset PICA-100K. After evaluating most of the mainstream models, we observe that physical realism remains a challenging problem with large rooms to explore. We hope that our benchmark and proposed solutions can serve as a foundation for future work moving from naive content editing toward physically consistent realism.",
        "tags": [
            "Image Editing",
            "VLM"
        ]
    },
    {
        "id": "392",
        "title": "Multilingual Text-to-Image Person Retrieval via Bidirectional Relation Reasoning and Aligning",
        "author": [
            "Min Cao",
            "Xinyu Zhou",
            "Ding Jiang",
            "Bo Du",
            "Mang Ye",
            "Min Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17685",
        "abstract": "Text-to-image person retrieval (TIPR) aims to identify the target person using textual descriptions, facing challenge in modality heterogeneity. Prior works have attempted to address it by developing cross-modal global or local alignment strategies. However, global methods typically overlook fine-grained cross-modal differences, whereas local methods require prior information to explore explicit part alignments. Additionally, current methods are English-centric, restricting their application in multilingual contexts. To alleviate these issues, we pioneer a multilingual TIPR task by developing a multilingual TIPR benchmark, for which we leverage large language models for initial translations and refine them by integrating domain-specific knowledge. Correspondingly, we propose Bi-IRRA: a Bidirectional Implicit Relation Reasoning and Aligning framework to learn alignment across languages and modalities. Within Bi-IRRA, a bidirectional implicit relation reasoning module enables bidirectional prediction of masked image and text, implicitly enhancing the modeling of local relations across languages and modalities, a multi-dimensional global alignment module is integrated to bridge the modality heterogeneity. The proposed method achieves new state-of-the-art results on all multilingual TIPR datasets. Data and code are presented in https://github.com/Flame-Chasers/Bi-IRRA.",
        "tags": [
            "LLM",
            "Text-to-Image"
        ]
    },
    {
        "id": "393",
        "title": "Towards 3D Objectness Learning in an Open World",
        "author": [
            "Taichi Liu",
            "Zhenyu Wang",
            "Ruofeng Liu",
            "Guang Wang",
            "Desheng Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17686",
        "abstract": "Recent advancements in 3D object detection and novel category detection have made significant progress, yet research on learning generalized 3D objectness remains insufficient. In this paper, we delve into learning open-world 3D objectness, which focuses on detecting all objects in a 3D scene, including novel objects unseen during training. Traditional closed-set 3D detectors struggle to generalize to open-world scenarios, while directly incorporating 3D open-vocabulary models for open-world ability struggles with vocabulary expansion and semantic overlap. To achieve generalized 3D object discovery, We propose OP3Det, a class-agnostic Open-World Prompt-free 3D Detector to detect any objects within 3D scenes without relying on hand-crafted text prompts. We introduce the strong generalization and zero-shot capabilities of 2D foundation models, utilizing both 2D semantic priors and 3D geometric priors for class-agnostic proposals to broaden 3D object discovery. Then, by integrating complementary information from point cloud and RGB image in the cross-modal mixture of experts, OP3Det dynamically routes uni-modal and multi-modal features to learn generalized 3D objectness. Extensive experiments demonstrate the extraordinary performance of OP3Det, which significantly surpasses existing open-world 3D detectors by up to 16.0% in AR and achieves a 13.5% improvement compared to closed-world 3D detectors.",
        "tags": [
            "3D",
            "Detection",
            "MoE"
        ]
    },
    {
        "id": "394",
        "title": "CrossGuard: Safeguarding MLLMs against Joint-Modal Implicit Malicious Attacks",
        "author": [
            "Xu Zhang",
            "Hao Li",
            "Zhichao Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17687",
        "abstract": "Multimodal Large Language Models (MLLMs) achieve strong reasoning and perception capabilities but are increasingly vulnerable to jailbreak attacks. While existing work focuses on explicit attacks, where malicious content resides in a single modality, recent studies reveal implicit attacks, in which benign text and image inputs jointly express unsafe intent. Such joint-modal threats are difficult to detect and remain underexplored, largely due to the scarcity of high-quality implicit data. We propose ImpForge, an automated red-teaming pipeline that leverages reinforcement learning with tailored reward modules to generate diverse implicit samples across 14 domains. Building on this dataset, we further develop CrossGuard, an intent-aware safeguard providing robust and comprehensive defense against both explicit and implicit threats. Extensive experiments across safe and unsafe benchmarks, implicit and explicit attacks, and multiple out-of-domain settings demonstrate that CrossGuard significantly outperforms existing defenses, including advanced MLLMs and guardrails, achieving stronger security while maintaining high utility. This offers a balanced and practical solution for enhancing MLLM robustness against real-world multimodal threats.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": "395",
        "title": "Quantum Synthetic Data Generation for Industrial Bioprocess Monitoring",
        "author": [
            "Shawn M. Gibford",
            "Mohammad Reza Boskabadi",
            "Christopher J. Savoie",
            "Seyed Soheil Mansouri"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17688",
        "abstract": "Data scarcity and sparsity in bio-manufacturing poses challenges for accurate model\ndevelopment, process monitoring, and optimization. We aim to replicate and capture\nthe complex dynamics of industrial bioprocesses by proposing the use of a Quantum\nWasserstein Generative Adversarial Network with Gradient Penalty (QWGAN-GP) to\ngenerate synthetic time series data for industrially relevant processes. The\ngenerator within our GAN is comprised of a Parameterized Quantum Circuit (PQC). This\nmethodology offers potential advantages in process monitoring, modeling,\nforecasting, and optimization, enabling more efficient bioprocess management by\nreducing the dependence on scarce experimental data. Our results demonstrate\nacceptable performance in capturing the temporal dynamics of real bioprocess data.\nWe focus on Optical Density, a key measurement for Dry Biomass estimation. The data\ngenerated showed high fidelity to the actual historical experimental data. This\nintersection of quantum computing and machine learning has opened new frontiers in\ndata analysis and generation, particularly in computationally intensive fields, for\nuse cases such as increasing prediction accuracy for soft sensor design or for use\nin predictive control.",
        "tags": [
            "GAN"
        ]
    },
    {
        "id": "396",
        "title": "Efficient Algorithms for Mitigating Uncertainty and Risk in Reinforcement Learning",
        "author": [
            "Xihong Su"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17690",
        "abstract": "This dissertation makes three main contributions. First, We identify a new connection between policy gradient and dynamic programming in MMDPs and propose the Coordinate Ascent Dynamic Programming (CADP) algorithm to compute a Markov policy that maximizes the discounted return averaged over the uncertain models. CADP adjusts model weights iteratively to guarantee monotone policy improvements to a local maximum. Second, We establish sufficient and necessary conditions for the exponential ERM Bellman operator to be a contraction and prove the existence of stationary deterministic optimal policies for ERM-TRC and EVaR-TRC. We also propose exponential value iteration, policy iteration, and linear programming algorithms for computing optimal stationary policies for ERM-TRC and EVaR-TRC. Third, We propose model-free Q-learning algorithms for computing policies with risk-averse objectives: ERM-TRC and EVaR-TRC. The challenge is that Q-learning ERM Bellman may not be a contraction. Instead, we use the monotonicity of Q-learning ERM Bellman operators to derive a rigorous proof that the ERM-TRC and the EVaR-TRC Q-learning algorithms converge to the optimal risk-averse value functions. The proposed Q-learning algorithms compute the optimal stationary policy for ERM-TRC and EVaR-TRC.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "397",
        "title": "A Mimamsa Inspired Framework For Instruction Sequencing In AI Agents",
        "author": [
            "Bama Srinivasan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17691",
        "abstract": "This paper presents a formal framework for sequencing instructions in AI agents, inspired by the Indian philosophical system of Mimamsa. The framework formalizes sequencing mechanisms through action object pairs in three distinct ways: direct assertion (Srutikrama) for temporal precedence, purpose driven sequencing (Arthakrama) for functional dependencies, and iterative procedures (Pravrittikrama) for distinguishing between parallel and sequential execution in repetitive tasks. It introduces the syntax and semantics of an action object imperative logic, extending the MIRA formalism (Srinivasan and Parthasarathi, 2021) with explicit deduction rules for sequencing. The correctness of instruction sequencing is established through a validated theorem, which is based on object dependencies across successive instructions. This is further supported by proofs of soundness and completeness. This formal verification enables reliable instruction sequencing, impacting AI applications across areas like task planning and robotics by addressing temporal reasoning and dependency modeling.",
        "tags": [
            "Robotics"
        ]
    },
    {
        "id": "398",
        "title": "A Principle of Targeted Intervention for Multi-Agent Reinforcement Learning",
        "author": [
            "Anjie Liu",
            "Jianhong Wang",
            "Samuel Kaski",
            "Jun Wang",
            "Mengyue Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17697",
        "abstract": "Steering cooperative multi-agent reinforcement learning (MARL) towards desired outcomes is challenging, particularly when the global guidance from a human on the whole multi-agent system is impractical in a large-scale MARL. On the other hand, designing mechanisms to coordinate agents most relies on empirical studies, lacking a easy-to-use research tool. In this work, we employ multi-agent influence diagrams (MAIDs) as a graphical framework to address the above issues. First, we introduce interaction paradigms that leverage MAIDs to analyze and visualize existing approaches in MARL. Then, we design a new interaction paradigm based on MAIDs, referred to as targeted intervention that is applied to only a single targeted agent, so the problem of global guidance can be mitigated. In our implementation, we introduce a causal inference technique-referred to as Pre-Strategy Intervention (PSI)-to realize the targeted intervention paradigm. Since MAIDs can be regarded as a special class of causal diagrams, a composite desired outcome that integrates the primary task goal and an additional desired outcome can be achieved by maximizing the corresponding causal effect through the PSI. Moreover, the bundled relevance graph analysis of MAIDs provides a tool to identify whether an MARL learning paradigm is workable under the design of an interaction paradigm. In experiments, we demonstrate the effectiveness of our proposed targeted intervention, and verify the result of relevance graph analysis.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "399",
        "title": "Towards Mining Effective Pedagogical Strategies from Learner-LLM Educational Dialogues",
        "author": [
            "Liqun He",
            "Manolis Mavrikis",
            "Mutlu Cukurova"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17698",
        "abstract": "Dialogue plays a crucial role in educational settings, yet existing evaluation methods for educational applications of large language models (LLMs) primarily focus on technical performance or learning outcomes, often neglecting attention to learner-LLM interactions. To narrow this gap, this AIED Doctoral Consortium paper presents an ongoing study employing a dialogue analysis approach to identify effective pedagogical strategies from learner-LLM dialogues. The proposed approach involves dialogue data collection, dialogue act (DA) annotation, DA pattern mining, and predictive model building. Early insights are outlined as an initial step toward future research. The work underscores the need to evaluate LLM-based educational applications by focusing on dialogue dynamics and pedagogical strategies.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "400",
        "title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
        "author": [
            "Aleksandr Oganov",
            "Ilya Bykov",
            "Eva Neudachina",
            "Mishan Aliev",
            "Alexander Tolmachev",
            "Alexander Sidorov",
            "Aleksandr Zuev",
            "Andrey Okhotin",
            "Denis Rakitin",
            "Aibek Alanov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17699",
        "abstract": "While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS.",
        "tags": [
            "Diffusion",
            "ODE"
        ]
    },
    {
        "id": "401",
        "title": "Contextual Attention Modulation: Towards Efficient Multi-Task Adaptation in Large Language Models",
        "author": [
            "Dayan Pan",
            "Zhaoyang Fu",
            "Jingyuan Wang",
            "Xiao Han",
            "Yue Zhu",
            "Xiangyu Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17705",
        "abstract": "Large Language Models (LLMs) possess remarkable generalization capabilities but struggle with multi-task adaptation, particularly in balancing knowledge retention with task-specific specialization. Conventional fine-tuning methods suffer from catastrophic forgetting and substantial resource consumption, while existing parameter-efficient methods perform suboptimally in complex multi-task scenarios. To address this, we propose Contextual Attention Modulation (CAM), a novel mechanism that dynamically modulates the representations of self-attention modules in LLMs. CAM enhances task-specific features while preserving general knowledge, thereby facilitating more effective and efficient adaptation. For effective multi-task adaptation, CAM is integrated into our Hybrid Contextual Attention Modulation (HyCAM) framework, which combines a shared, full-parameter CAM module with multiple specialized, lightweight CAM modules, enhanced by a dynamic routing strategy for adaptive knowledge fusion. Extensive experiments on heterogeneous tasks, including question answering, code generation, and logical reasoning, demonstrate that our approach significantly outperforms existing approaches, achieving an average performance improvement of 3.65%. The implemented code and data are available to ease reproducibility at https://github.com/Applied-Machine-Learning-Lab/HyCAM.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "402",
        "title": "Closing the Sim2Real Performance Gap in RL",
        "author": [
            "Akhil S Anand",
            "Shambhuraj Sawant",
            "Jasper Hoffmann",
            "Dirk Reinhardt",
            "Sebastien Gros"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17709",
        "abstract": "Sim2Real aims at training policies in high-fidelity simulation environments and effectively transferring them to the real world. Despite the developments of accurate simulators and Sim2Real RL approaches, the policies trained purely in simulation often suffer significant performance drops when deployed in real environments. This drop is referred to as the Sim2Real performance gap. Current Sim2Real RL methods optimize the simulator accuracy and variability as proxies for real-world performance. However, these metrics do not necessarily correlate with the real-world performance of the policy as established theoretically and empirically in the literature. We propose a novel framework to address this issue by directly adapting the simulator parameters based on real-world performance. We frame this problem as a bi-level RL framework: the inner-level RL trains a policy purely in simulation, and the outer-level RL adapts the simulation model and in-sim reward parameters to maximize real-world performance of the in-sim policy. We derive and validate in simple examples the mathematical tools needed to develop bi-level RL algorithms that close the Sim2Real performance gap.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "403",
        "title": "QueST: Incentivizing LLMs to Generate Difficult Problems",
        "author": [
            "Hanxu Hu",
            "Xingxing Zhang",
            "Jannis Vamvas",
            "Rico Sennrich",
            "Furu Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17715",
        "abstract": "Large Language Models have achieved strong performance on reasoning tasks, solving competition-level coding and math problems. However, their scalability is limited by human-labeled datasets and the lack of large-scale, challenging coding problem training data. Existing competitive coding datasets contain only thousands to tens of thousands of problems. Previous synthetic data generation methods rely on either augmenting existing instruction datasets or selecting challenging problems from human-labeled data. In this paper, we propose QueST, a novel framework which combines difficulty-aware graph sampling and difficulty-aware rejection fine-tuning that directly optimizes specialized generators to create challenging coding problems. Our trained generators demonstrate superior capability compared to even GPT-4o at creating challenging problems that benefit downstream performance. We leverage QueST to generate large-scale synthetic coding problems, which we then use to distill from strong teacher models with long chain-of-thought or to conduct reinforcement learning for smaller models, proving effective in both scenarios. Our distillation experiments demonstrate significant performance gains. Specifically, after fine-tuning Qwen3-8B-base on 100K difficult problems generated by QueST, we surpass the performance of the original Qwen3-8B on LiveCodeBench. With an additional 112K examples (i.e., 28K human-written problems paired with multiple synthetic solutions), our 8B model matches the performance of the much larger DeepSeek-R1-671B. These findings indicate that generating complex problems via QueST offers an effective and scalable approach to advancing the frontiers of competitive coding and reasoning for large language models.",
        "tags": [
            "CoT",
            "DeepSeek",
            "GPT",
            "LLM",
            "RL"
        ]
    },
    {
        "id": "404",
        "title": "Raindrop GS: A Benchmark for 3D Gaussian Splatting under Raindrop Conditions",
        "author": [
            "Zhiqiang Teng",
            "Beibei Lin",
            "Tingting Chen",
            "Zifeng Yuan",
            "Xuanyi Li",
            "Xuanyu Zhang",
            "Shunli Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17719",
        "abstract": "3D Gaussian Splatting (3DGS) under raindrop conditions suffers from severe occlusions and optical distortions caused by raindrop contamination on the camera lens, substantially degrading reconstruction quality. Existing benchmarks typically evaluate 3DGS using synthetic raindrop images with known camera poses (constrained images), assuming ideal conditions. However, in real-world scenarios, raindrops often interfere with accurate camera pose estimation and point cloud initialization. Moreover, a significant domain gap between synthetic and real raindrops further impairs generalization. To tackle these issues, we introduce RaindropGS, a comprehensive benchmark designed to evaluate the full 3DGS pipeline-from unconstrained, raindrop-corrupted images to clear 3DGS reconstructions. Specifically, the whole benchmark pipeline consists of three parts: data preparation, data processing, and raindrop-aware 3DGS evaluation, including types of raindrop interference, camera pose estimation and point cloud initialization, single image rain removal comparison, and 3D Gaussian training comparison. First, we collect a real-world raindrop reconstruction dataset, in which each scene contains three aligned image sets: raindrop-focused, background-focused, and rain-free ground truth, enabling a comprehensive evaluation of reconstruction quality under different focus conditions. Through comprehensive experiments and analyses, we reveal critical insights into the performance limitations of existing 3DGS methods on unconstrained raindrop images and the varying impact of different pipeline components: the impact of camera focus position on 3DGS reconstruction performance, and the interference caused by inaccurate pose and point cloud initialization on reconstruction. These insights establish clear directions for developing more robust 3DGS methods under raindrop conditions.",
        "tags": [
            "3D",
            "Gaussian Splatting",
            "Pose Estimation"
        ]
    },
    {
        "id": "405",
        "title": "PANER: A Paraphrase-Augmented Framework for Low-Resource Named Entity Recognition",
        "author": [
            "Nanda Kumar Rengarajan",
            "Jun Yan",
            "Chun Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17720",
        "abstract": "Named Entity Recognition (NER) is a critical task that requires substantial annotated data, making it challenging in low-resource scenarios where label acquisition is expensive. While zero-shot and instruction-tuned approaches have made progress, they often fail to generalize to domain-specific entities and do not effectively utilize limited available data. We present a lightweight few-shot NER framework that addresses these challenges through two key innovations: (1) a new instruction tuning template with a simplified output format that combines principles from prior IT approaches to leverage the large context window of recent state-of-the-art LLMs; (2) introducing a strategic data augmentation technique that preserves entity information while paraphrasing the surrounding context, thereby expanding our training data without compromising semantic relationships. Experiments on benchmark datasets show that our method achieves performance comparable to state-of-the-art models on few-shot and zero-shot tasks, with our few-shot approach attaining an average F1 score of 80.1 on the CrossNER datasets. Models trained with our paraphrasing approach show consistent improvements in F1 scores of up to 17 points over baseline versions, offering a promising solution for groups with limited NER training data and compute power.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "406",
        "title": "MT-Video-Bench: A Holistic Video Understanding Benchmark for Evaluating Multimodal LLMs in Multi-Turn Dialogues",
        "author": [
            "Yaning Pan",
            "Zekun Wang",
            "Qianqian Xie",
            "Yongqian Wen",
            "Yuanxing Zhang",
            "Guohui Zhang",
            "Haoxuan Hu",
            "Zhiyu Pan",
            "Yibing Huang",
            "Zhidong Gan",
            "Yonghong Lin",
            "An Ping",
            "Tianhao Peng",
            "Jiaheng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17722",
        "abstract": "The recent development of Multimodal Large Language Models (MLLMs) has significantly advanced AI's ability to understand visual modalities. However, existing evaluation benchmarks remain limited to single-turn question answering, overlooking the complexity of multi-turn dialogues in real-world scenarios. To bridge this gap, we introduce MT-Video-Bench, a holistic video understanding benchmark for evaluating MLLMs in multi-turn dialogues. Specifically, our MT-Video-Bench mainly assesses six core competencies that focus on perceptivity and interactivity, encompassing 987 meticulously curated multi-turn dialogues from diverse domains. These capabilities are rigorously aligned with real-world applications, such as interactive sports analysis and multi-turn video-based intelligent tutoring. With MT-Video-Bench, we extensively evaluate various state-of-the-art open-source and closed-source MLLMs, revealing their significant performance discrepancies and limitations in handling multi-turn video dialogues. The benchmark will be publicly available to foster future research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "407",
        "title": "AcademicEval: Live Long-Context LLM Benchmark",
        "author": [
            "Haozhen Zhang",
            "Tao Feng",
            "Pengrui Han",
            "Jiaxuan You"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17725",
        "abstract": "Large Language Models (LLMs) have recently achieved remarkable performance in long-context understanding. However, current long-context LLM benchmarks are limited by rigid context length, labor-intensive annotation, and the pressing challenge of label leakage issues during LLM training. Therefore, we propose \\textsc{AcademicEval}, a live benchmark for evaluating LLMs over long-context generation tasks. \\textsc{AcademicEval} adopts papers on arXiv to introduce several academic writing tasks with long-context inputs, \\textit{i.e.}, \\textsc{Title}, \\textsc{Abstract}, \\textsc{Introduction}, and \\textsc{Related Work}, which cover a wide range of abstraction levels and require no manual labeling. Moreover, \\textsc{AcademicEval} integrates high-quality and expert-curated few-shot demonstrations from a collected co-author graph to enable flexible context length. Especially, \\textsc{AcademicEval} features an efficient live evaluation, ensuring no label leakage. We conduct a holistic evaluation on \\textsc{AcademicEval}, and the results illustrate that LLMs perform poorly on tasks with hierarchical abstraction levels and tend to struggle with long few-shot demonstrations, highlighting the challenge of our benchmark. Through experimental analysis, we also reveal some insights for enhancing LLMs' long-context modeling capabilities. Code is available at https://github.com/ulab-uiuc/AcademicEval",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "408",
        "title": "Rethinking Search: A Study of University Students' Perspectives on Using LLMs and Traditional Search Engines in Academic Problem Solving",
        "author": [
            "Md. Faiyaz Abdullah Sayeedi",
            "Md. Sadman Haque",
            "Zobaer Ibn Razzaque",
            "Robiul Awoul Robin",
            "Sabila Nawshin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17726",
        "abstract": "With the increasing integration of Artificial Intelligence (AI) in academic problem solving, university students frequently alternate between traditional search engines like Google and large language models (LLMs) for information retrieval. This study explores students' perceptions of both tools, emphasizing usability, efficiency, and their integration into academic workflows. Employing a mixed-methods approach, we surveyed 109 students from diverse disciplines and conducted in-depth interviews with 12 participants. Quantitative analyses, including ANOVA and chi-square tests, were used to assess differences in efficiency, satisfaction, and tool preference. Qualitative insights revealed that students commonly switch between GPT and Google: using Google for credible, multi-source information and GPT for summarization, explanation, and drafting. While neither tool proved sufficient on its own, there was a strong demand for a hybrid solution. In response, we developed a prototype, a chatbot embedded within the search interface, that combines GPT's conversational capabilities with Google's reliability to enhance academic research and reduce cognitive load.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": "409",
        "title": "Enabling Fine-Grained Operating Points for Black-Box LLMs",
        "author": [
            "Ege Beyazit",
            "KL Navaneet",
            "Prashant Mathur",
            "Roi Blanco",
            "Vidit Bansal",
            "Karim Bouyarmane"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17727",
        "abstract": "Black-box Large Language Models (LLMs) provide practical and accessible alternatives to other machine learning methods, as they require minimal labeled data and machine learning expertise to develop solutions for various decision making problems. However, for applications that need operating with constraints on specific metrics (e.g., precision $\\geq$ 95%), decision making with black-box LLMs remains unfavorable, due to their low numerical output cardinalities. This results in limited control over their operating points, preventing fine-grained adjustment of their decision making behavior. In this paper, we study using black-box LLMs as classifiers, focusing on efficiently improving their operational granularity without performance loss. Specifically, we first investigate the reasons behind their low-cardinality numerical outputs and show that they are biased towards generating rounded but informative verbalized probabilities. Then, we experiment with standard prompt engineering, uncertainty estimation and confidence elicitation techniques, and observe that they do not effectively improve operational granularity without sacrificing performance or increasing inference cost. Finally, we propose efficient approaches to significantly increase the number and diversity of available operating points. Our proposed approaches provide finer-grained operating points and achieve comparable to or better performance than the benchmark methods across 11 datasets and 3 LLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "410",
        "title": "Can Image-To-Video Models Simulate Pedestrian Dynamics?",
        "author": [
            "Aaron Appelle",
            "Jerome P. Lynch"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17731",
        "abstract": "Recent high-performing image-to-video (I2V) models based on variants of the diffusion transformer (DiT) have displayed remarkable inherent world-modeling capabilities by virtue of training on large scale video datasets. We investigate whether these models can generate realistic pedestrian movement patterns in crowded public scenes. Our framework conditions I2V models on keyframes extracted from pedestrian trajectory benchmarks, then evaluates their trajectory prediction performance using quantitative measures of pedestrian dynamics.",
        "tags": [
            "DiT",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": "411",
        "title": "Train for Truth, Keep the Skills: Binary Retrieval-Augmented Reward Mitigates Hallucinations",
        "author": [
            "Tong Chen",
            "Akari Asai",
            "Luke Zettlemoyer",
            "Hannaneh Hajishirzi",
            "Faeze Brahman"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17733",
        "abstract": "Language models often generate factually incorrect information unsupported by their training data, a phenomenon known as extrinsic hallucination. Existing mitigation approaches often degrade performance on open-ended generation and downstream tasks, limiting their practical utility. We propose an online reinforcement learning method using a novel binary retrieval-augmented reward (RAR) to address this tradeoff. Unlike continuous reward schemes, our approach assigns a reward of one only when the model's output is entirely factually correct, and zero otherwise. We evaluate our method on Qwen3 reasoning models across diverse tasks. For open-ended generation, binary RAR achieves a 39.3% reduction in hallucination rates, substantially outperforming both supervised training and continuous-reward RL baselines. In short-form question answering, the model learns calibrated abstention, strategically outputting \"I don't know\" when faced with insufficient parametric knowledge. This yields 44.4% and 21.7% fewer incorrect answers on PopQA and GPQA, respectively. Crucially, these factuality gains come without performance degradation on instruction following, math, or code, whereas continuous-reward RL, despite improving factuality, induces quality regressions.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "412",
        "title": "This is Going to Sound Crazy, But What If We Used Large Language Models to Boost Automatic Database Tuning Algorithms By Leveraging Prior History? We Will Find Better Configurations More Quickly Than Retraining From Scratch!",
        "author": [
            "William Zhang",
            "Wan Shen Lim",
            "Andrew Pavlo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17748",
        "abstract": "Tuning database management systems (DBMSs) is challenging due to trillions of possible configurations and evolving workloads. Recent advances in tuning have led to breakthroughs in optimizing over the possible configurations. However, due to their design and inability to leverage query-level historical insights, existing automated tuners struggle to adapt and re-optimize the DBMS when the environment changes (e.g., workload drift, schema transfer).\nThis paper presents the Booster framework that assists existing tuners in adapting to environment changes (e.g., drift, cross-schema transfer). Booster structures historical artifacts into query-configuration contexts, prompts large language models (LLMs) to suggest configurations for each query based on relevant contexts, and then composes the query-level suggestions into a holistic configuration with beam search. With multiple OLAP workloads, we evaluate Booster's ability to assist different state-of-the-art tuners (e.g., cost-/machine learning-/LLM-based) in adapting to environment changes. By composing recommendations derived from query-level insights, Booster assists tuners in discovering configurations that are up to 74% better and in up to 4.7x less time than the alternative approach of continuing to tune from historical configurations.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "413",
        "title": "VERA-V: Variational Inference Framework for Jailbreaking Vision-Language Models",
        "author": [
            "Qilin Liao",
            "Anamika Lochab",
            "Ruqi Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17759",
        "abstract": "Vision-Language Models (VLMs) extend large language models with visual reasoning, but their multimodal design also introduces new, underexplored vulnerabilities. Existing multimodal red-teaming methods largely rely on brittle templates, focus on single-attack settings, and expose only a narrow subset of vulnerabilities. To address these limitations, we introduce VERA-V, a variational inference framework that recasts multimodal jailbreak discovery as learning a joint posterior distribution over paired text-image prompts. This probabilistic view enables the generation of stealthy, coupled adversarial inputs that bypass model guardrails. We train a lightweight attacker to approximate the posterior, allowing efficient sampling of diverse jailbreaks and providing distributional insights into vulnerabilities. VERA-V further integrates three complementary strategies: (i) typography-based text prompts that embed harmful cues, (ii) diffusion-based image synthesis that introduces adversarial signals, and (iii) structured distractors to fragment VLM attention. Experiments on HarmBench and HADES benchmarks show that VERA-V consistently outperforms state-of-the-art baselines on both open-source and frontier VLMs, achieving up to 53.75% higher attack success rate (ASR) over the best baseline on GPT-4o.",
        "tags": [
            "Diffusion",
            "GPT",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "414",
        "title": "Seeing but Not Believing: Probing the Disconnect Between Visual Attention and Answer Correctness in VLMs",
        "author": [
            "Zhining Liu",
            "Ziyi Chen",
            "Hui Liu",
            "Chen Luo",
            "Xianfeng Tang",
            "Suhang Wang",
            "Joy Zeng",
            "Zhenwei Dai",
            "Zhan Shi",
            "Tianxin Wei",
            "Benoit Dumoulin",
            "Hanghang Tong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17771",
        "abstract": "Vision-Language Models (VLMs) achieve strong results on multimodal tasks such as visual question answering, yet they can still fail even when the correct visual evidence is present. In this work, we systematically investigate whether these failures arise from not perceiving the evidence or from not leveraging it effectively. By examining layer-wise attention dynamics, we find that shallow layers focus primarily on text, while deeper layers sparsely but reliably attend to localized evidence regions. Surprisingly, VLMs often perceive the visual evidence when outputting incorrect answers, a phenomenon we term ``seeing but not believing'' that widely exists in major VLM families. Building on this, we introduce an inference-time intervention that highlights deep-layer evidence regions through selective attention-based masking. It requires no training and consistently improves accuracy across multiple families, including LLaVA, Qwen, Gemma, and InternVL. These results show that VLMs encode reliable evidence internally but under-utilize it, making such signals explicit can bridge the gap between perception and reasoning, advancing the diagnostic understanding and reliability of VLMs.",
        "tags": [
            "LLaVA",
            "Qwen",
            "VLM"
        ]
    },
    {
        "id": "415",
        "title": "Mapping Post-Training Forgetting in Language Models at Scale",
        "author": [
            "Jackson Harmon",
            "Andreas Hochlehnert",
            "Matthias Bethge",
            "Ameya Prabhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17776",
        "abstract": "Scaled post-training now drives many of the largest capability gains in language models (LMs), yet its effect on pretrained knowledge remains poorly understood. Not all forgetting is equal: Forgetting one fact (e.g., a U.S. president or an API call) does not \"average out\" by recalling another. Hence, we propose a sample-wise paradigm to measure what is forgotten and when backward transfer occurs. Our metric counts 1->0 transitions (correct before post-training, incorrect after) to quantify forgetting and 0->1 transitions to quantify backward transfer. Traditional task averages conflate these effects and obscure large changes. For multiple-choice benchmarks, we add chance-adjusted variants that subtract the expected contribution of random guessing from pre- and post-training accuracies. We apply this framework across post-training stages, model sizes, and data scales. Our large-scale analysis shows that: (1) Domain-continual pretraining induces moderate forgetting with low-to-moderate backward transfer; (2) RL/SFT post-training applied to base models and Instruction tuning yields moderate-to-large backward transfer on math and logic with overall low-to-moderate forgetting; (3) Applying RL/SFT to instruction-tuned models is sensitive on data scale: at small scales, both forgetting and backward transfer are small; at larger scales, effects are mixed and warrant further study with better controls; (4) Model merging does not reliably mitigate forgetting. Overall, our framework offers a practical yardstick for mapping how post-training alters pretrained knowledge at scale -- enabling progress towards generally capable AI systems.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "416",
        "title": "SparseVILA: Decoupling Visual Sparsity for Efficient VLM Inference",
        "author": [
            "Samir Khaki",
            "Junxian Guo",
            "Jiaming Tang",
            "Shang Yang",
            "Yukang Chen",
            "Konstantinos N. Plataniotis",
            "Yao Lu",
            "Song Han",
            "Zhijian Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17777",
        "abstract": "Vision Language Models (VLMs) have rapidly advanced in integrating visual and textual reasoning, powering applications across high-resolution image understanding, long-video analysis, and multi-turn conversation. However, their scalability remains limited by the growing number of visual tokens that dominate inference latency. We present SparseVILA, a new paradigm for efficient VLM inference that decouples visual sparsity across the prefilling and decoding stages. SparseVILA distributes sparsity across stages by pruning redundant visual tokens during prefill and retrieving only query-relevant tokens during decoding. This decoupled design matches leading prefill pruning methods while preserving multi-turn fidelity by retaining most of the visual cache so that query-aware tokens can be retrieved at each conversation round. Built on an AWQ-optimized inference pipeline, SparseVILA achieves up to 4.0 times faster prefilling, 2.5 times faster decoding, and an overall 2.6 times end-to-end speedup on long-context video tasks -- while improving accuracy on document-understanding and reasoning tasks. By decoupling query-agnostic pruning and query-aware retrieval, SparseVILA establishes a new direction for efficient multimodal inference, offering a training-free, architecture-agnostic framework for accelerating large VLMs without sacrificing capability.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": "417",
        "title": "Botany-Bot: Digital Twin Monitoring of Occluded and Underleaf Plant Structures with Gaussian Splats",
        "author": [
            "Simeon Adebola",
            "Chung Min Kim",
            "Justin Kerr",
            "Shuangyu Xie",
            "Prithvi Akella",
            "Jose Luis Susa Rincon",
            "Eugen Solowjow",
            "Ken Goldberg"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17783",
        "abstract": "Commercial plant phenotyping systems using fixed cameras cannot perceive many plant details due to leaf occlusion. In this paper, we present Botany-Bot, a system for building detailed \"annotated digital twins\" of living plants using two stereo cameras, a digital turntable inside a lightbox, an industrial robot arm, and 3D segmentated Gaussian Splat models. We also present robot algorithms for manipulating leaves to take high-resolution indexable images of occluded details such as stem buds and the underside/topside of leaves. Results from experiments suggest that Botany-Bot can segment leaves with 90.8% accuracy, detect leaves with 86.2% accuracy, lift/push leaves with 77.9% accuracy, and take detailed overside/underside images with 77.3% accuracy. Code, videos, and datasets are available at https://berkeleyautomation.github.io/Botany-Bot/.",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": "418",
        "title": "UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action",
        "author": [
            "Yuhao Yang",
            "Zhen Yang",
            "Zi-Yi Dou",
            "Anh Nguyen",
            "Keen You",
            "Omar Attia",
            "Andrew Szot",
            "Michael Feng",
            "Ram Ramrakhya",
            "Alexander Toshev",
            "Chao Huang",
            "Yinfei Yang",
            "Zhe Gan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17790",
        "abstract": "Multimodal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid action -- seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing over 17,000 verifiable tasks spanning real-world computer-use scenarios; (3) a large-scale high-quality hybrid action trajectory collection with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 22% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid action mechanism proves critical, reducing error propagation while maintaining execution efficiency.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "419",
        "title": "SoftMimic: Learning Compliant Whole-body Control from Examples",
        "author": [
            "Gabriel B. Margolis",
            "Michelle Wang",
            "Nolan Fey",
            "Pulkit Agrawal"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17792",
        "abstract": "We introduce SoftMimic, a framework for learning compliant whole-body control policies for humanoid robots from example motions. Imitating human motions with reinforcement learning allows humanoids to quickly learn new skills, but existing methods incentivize stiff control that aggressively corrects deviations from a reference motion, leading to brittle and unsafe behavior when the robot encounters unexpected contacts. In contrast, SoftMimic enables robots to respond compliantly to external forces while maintaining balance and posture. Our approach leverages an inverse kinematics solver to generate an augmented dataset of feasible compliant motions, which we use to train a reinforcement learning policy. By rewarding the policy for matching compliant responses rather than rigidly tracking the reference motion, SoftMimic learns to absorb disturbances and generalize to varied tasks from a single motion clip. We validate our method through simulations and real-world experiments, demonstrating safe and effective interaction with the environment.",
        "tags": [
            "CLIP",
            "RL",
            "Robotics"
        ]
    },
    {
        "id": "420",
        "title": "Foundational Automatic Evaluators: Scaling Multi-Task Generative Evaluator Training for Reasoning-Centric Domains",
        "author": [
            "Austin Xu",
            "Xuan-Phi Nguyen",
            "Yilun Zhou",
            "Chien-Sheng Wu",
            "Caiming Xiong",
            "Shafiq Joty"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17793",
        "abstract": "Finetuning specialized generative evaluators has emerged as a popular paradigm to meet the increasing demand for scalable evaluation during both training and test-time. However, recent work has largely focused on applying new methodology, such as reinforcement learning (RL), to training evaluators, shying away from large-scale, data-driven development. In this work, we focus on data scaling, curating a set of 2.5M samples spanning five unique evaluation tasks (pairwise, step-level, reference-free and reference-based verification, and single rating) and multiple domains focused on reasoning evaluation. With our data, we train Foundational Automatic Reasoning Evaluators (FARE), a family of 8B and 20B (with 3.6B active) parameter evaluators, with a simple iterative rejection-sampling supervised finetuning (SFT) approach. FARE-8B challenges larger specialized RL-trained evaluators and FARE-20B sets the new standard for open-source evaluators, surpassing specialized 70B+ evaluators. Beyond static benchmarks, we evaluate FARE in real-world tasks: As inference-time rerankers, FARE-20B achieves near-oracle performance on MATH. As verifiers in RL training, FARE improves the downstream RL-trained model performance by up to 14.1% vs. string-matching verifiers. When initialized from FARE, a continually-finetuned FARE-Code outperforms gpt-oss-20B by 65% on evaluating test-case quality.",
        "tags": [
            "GPT",
            "RL"
        ]
    },
    {
        "id": "421",
        "title": "Executable Knowledge Graphs for Replicating AI Research",
        "author": [
            "Yujie Luo",
            "Zhuoyun Yu",
            "Xuehai Wang",
            "Yuqi Zhu",
            "Ningyu Zhang",
            "Lanning Wei",
            "Lun Du",
            "Da Zheng",
            "Huajun Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17795",
        "abstract": "Replicating AI research is a crucial yet challenging task for large language model (LLM) agents. Existing approaches often struggle to generate executable code, primarily due to insufficient background knowledge and the limitations of retrieval-augmented generation (RAG) methods, which fail to capture latent technical details hidden in referenced papers. Furthermore, previous approaches tend to overlook valuable implementation-level code signals and lack structured knowledge representations that support multi-granular retrieval and reuse. To overcome these challenges, we propose Executable Knowledge Graphs (xKG), a modular and pluggable knowledge base that automatically integrates technical insights, code snippets, and domain-specific knowledge extracted from scientific literature. When integrated into three agent frameworks with two different LLMs, xKG shows substantial performance gains (10.9% with o3-mini) on PaperBench, demonstrating its effectiveness as a general and extensible solution for automated AI research replication. Code will released at https://github.com/zjunlp/xKG.",
        "tags": [
            "LLM",
            "RAG"
        ]
    },
    {
        "id": "422",
        "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
        "author": [
            "Jiale Cheng",
            "Yusen Liu",
            "Xinyu Zhang",
            "Yulin Fei",
            "Wenyi Hong",
            "Ruiliang Lyu",
            "Weihan Wang",
            "Zhe Su",
            "Xiaotao Gu",
            "Xiao Liu",
            "Yushi Bai",
            "Jie Tang",
            "Hongning Wang",
            "Minlie Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17800",
        "abstract": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.",
        "tags": [
            "LLM",
            "VLM"
        ]
    },
    {
        "id": "423",
        "title": "Robobench: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models as Embodied Brain",
        "author": [
            "Yulin Luo",
            "Chun-Kai Fan",
            "Menghang Dong",
            "Jiayu Shi",
            "Mengdi Zhao",
            "Bo-Wen Zhang",
            "Cheng Chi",
            "Jiaming Liu",
            "Gaole Dai",
            "Rongyu Zhang",
            "Ruichuan An",
            "Kun Wu",
            "Zhengping Che",
            "Shaoxuan Xie",
            "Guocai Yao",
            "Zhongxia Zhao",
            "Pengwei Wang",
            "Guang Liu",
            "Zhongyuan Wang",
            "Tiejun Huang",
            "Shanghang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17801",
        "abstract": "Building robots that can perceive, reason, and act in dynamic, unstructured environments remains a core challenge. Recent embodied systems often adopt a dual-system paradigm, where System 2 handles high-level reasoning while System 1 executes low-level control. In this work, we refer to System 2 as the embodied brain, emphasizing its role as the cognitive core for reasoning and decision-making in manipulation tasks. Given this role, systematic evaluation of the embodied brain is essential. Yet existing benchmarks emphasize execution success, or when targeting high-level reasoning, suffer from incomplete dimensions and limited task realism, offering only a partial picture of cognitive capability. To bridge this gap, we introduce RoboBench, a benchmark that systematically evaluates multimodal large language models (MLLMs) as embodied brains. Motivated by the critical roles across the full manipulation pipeline, RoboBench defines five dimensions-instruction comprehension, perception reasoning, generalized planning, affordance prediction, and failure analysis-spanning 14 capabilities, 25 tasks, and 6092 QA pairs. To ensure realism, we curate datasets across diverse embodiments, attribute-rich objects, and multi-view scenes, drawing from large-scale real robotic data. For planning, RoboBench introduces an evaluation framework, MLLM-as-world-simulator. It evaluate embodied feasibility by simulating whether predicted plans can achieve critical object-state changes. Experiments on 14 MLLMs reveal fundamental limitations: difficulties with implicit instruction comprehension, spatiotemporal reasoning, cross-scenario planning, fine-grained affordance understanding, and execution failure diagnosis. RoboBench provides a comprehensive scaffold to quantify high-level cognition, and guide the development of next-generation embodied MLLMs. The project page is in https://robo-bench.github.io.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "424",
        "title": "Unbiased Gradient Low-Rank Projection",
        "author": [
            "Rui Pan",
            "Yang Luo",
            "Yuxing Liu",
            "Yang You",
            "Tong Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17802",
        "abstract": "Memory-efficient optimization is critical for training increasingly large language models (LLMs). A popular strategy involves gradient low-rank projection, storing only the projected optimizer states, with GaLore being a representative example. However, a significant drawback of many such methods is their lack of convergence guarantees, as various low-rank projection approaches introduce inherent biases relative to the original optimization algorithms, which contribute to performance gaps compared to full-parameter training. Aiming to tackle this problem, this paper investigates the layerwise sampling technique for debiasing low-rank projection mechanisms. In particular, an instantiation of the paradigm gives rise to a novel and unbiased low-rank optimization method built upon GaLore's mechanism and the Muon algorithm, named GaLore Unbiased with Muon (GUM). We theoretically prove our method matches the convergence guarantees of the base Muon algorithm while preserving the memory efficiency of low-rank techniques. Empirical experiments on LLM fine-tuning and pretraining also demonstrate non-trivial improvements over GaLore and even better performance than full-parameter training. Further investigation shows that the improvement of this technique comes from a more uniform distribution of knowledge inside layers, leading to more efficient utilization of the model parameter space and better memorization.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "425",
        "title": "ConsistEdit: Highly Consistent and Precise Training-free Visual Editing",
        "author": [
            "Zixin Yin",
            "Ling-Hao Chen",
            "Lionel Ni",
            "Xili Dai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17803",
        "abstract": "Recent advances in training-free attention control methods have enabled flexible and efficient text-guided editing capabilities for existing generation models. However, current approaches struggle to simultaneously deliver strong editing strength while preserving consistency with the source. This limitation becomes particularly critical in multi-round and video editing, where visual errors can accumulate over time. Moreover, most existing methods enforce global consistency, which limits their ability to modify individual attributes such as texture while preserving others, thereby hindering fine-grained editing. Recently, the architectural shift from U-Net to MM-DiT has brought significant improvements in generative performance and introduced a novel mechanism for integrating text and vision modalities. These advancements pave the way for overcoming challenges that previous methods failed to resolve. Through an in-depth analysis of MM-DiT, we identify three key insights into its attention mechanisms. Building on these, we propose ConsistEdit, a novel attention control method specifically tailored for MM-DiT. ConsistEdit incorporates vision-only attention control, mask-guided pre-attention fusion, and differentiated manipulation of the query, key, and value tokens to produce consistent, prompt-aligned edits. Extensive experiments demonstrate that ConsistEdit achieves state-of-the-art performance across a wide range of image and video editing tasks, including both structure-consistent and structure-inconsistent scenarios. Unlike prior methods, it is the first approach to perform editing across all inference steps and attention layers without handcraft, significantly enhancing reliability and consistency, which enables robust multi-round and multi-region editing. Furthermore, it supports progressive adjustment of structural consistency, enabling finer control.",
        "tags": [
            "DiT",
            "Video Editing"
        ]
    },
    {
        "id": "426",
        "title": "FinFlowRL: An Imitation-Reinforcement Learning Framework for Adaptive Stochastic Control in Finance",
        "author": [
            "Yang Li",
            "Zhi Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15883",
        "abstract": "Traditional stochastic control methods in finance struggle in real world markets due to their reliance on simplifying assumptions and stylized frameworks. Such methods typically perform well in specific, well defined environments but yield suboptimal results in changed, non stationary ones. We introduce FinFlowRL, a novel framework for financial optimal stochastic control. The framework pretrains an adaptive meta policy learning from multiple expert strategies, then finetunes through reinforcement learning in the noise space to optimize the generative process. By employing action chunking generating action sequences rather than single decisions, it addresses the non Markovian nature of markets. FinFlowRL consistently outperforms individually optimized experts across diverse market conditions.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "427",
        "title": "Quantum and Classical Machine Learning in Decentralized Finance: Comparative Evidence from Multi-Asset Backtesting of Automated Market Makers",
        "author": [
            "Chi-Sheng Chen",
            "Aidan Hung-Wen Tsai"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15903",
        "abstract": "This study presents a comprehensive empirical comparison between quantum machine learning (QML) and classical machine learning (CML) approaches in Automated Market Makers (AMM) and Decentralized Finance (DeFi) trading strategies through extensive backtesting on 10 models across multiple cryptocurrency assets. Our analysis encompasses classical ML models (Random Forest, Gradient Boosting, Logistic Regression), pure quantum models (VQE Classifier, QNN, QSVM), hybrid quantum-classical models (QASA Hybrid, QASA Sequence, QuantumRWKV), and transformer models. The results demonstrate that hybrid quantum models achieve superior overall performance with 11.2\\% average return and 1.42 average Sharpe ratio, while classical ML models show 9.8\\% average return and 1.47 average Sharpe ratio. The QASA Sequence hybrid model achieves the highest individual return of 13.99\\% with the best Sharpe ratio of 1.76, demonstrating the potential of quantum-classical hybrid approaches in AMM and DeFi trading strategies.",
        "tags": [
            "Transformer"
        ]
    },
    {
        "id": "428",
        "title": "Comparing LLMs for Sentiment Analysis in Financial Market News",
        "author": [
            "Lucas Eduardo Pereira Teles",
            "Carlos M. S. Figueiredo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15929",
        "abstract": "This article presents a comparative study of large language models (LLMs) in the task of sentiment analysis of financial market news. This work aims to analyze the performance difference of these models in this important natural language processing task within the context of finance. LLM models are compared with classical approaches, allowing for the quantification of the benefits of each tested model or approach. Results show that large language models outperform classical models in the vast majority of cases.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "429",
        "title": "ATLAS: Adaptive Trading with LLM AgentS Through Dynamic Prompt Optimization and Multi-Agent Coordination",
        "author": [
            "Charidimos Papadakis",
            "Angeliki Dimitriou",
            "Giorgos Filandrianos",
            "Maria Lymperaiou",
            "Konstantinos Thomas",
            "Giorgos Stamou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15949",
        "abstract": "Large language models show promise for financial decision-making, yet deploying them as autonomous trading agents raises fundamental challenges: how to adapt instructions when rewards arrive late and obscured by market noise, how to synthesize heterogeneous information streams into coherent decisions, and how to bridge the gap between model outputs and executable market actions. We present ATLAS (Adaptive Trading with LLM AgentS), a unified multi-agent framework that integrates structured information from markets, news, and corporate fundamentals to support robust trading decisions. Within ATLAS, the central trading agent operates in an order-aware action space, ensuring that outputs correspond to executable market orders rather than abstract signals. The agent can incorporate feedback while trading using Adaptive-OPRO, a novel prompt-optimization technique that dynamically adapts the prompt by incorporating real-time, stochastic feedback, leading to increasing performance over time. Across regime-specific equity studies and multiple LLM families, Adaptive-OPRO consistently outperforms fixed prompts, while reflection-based feedback fails to provide systematic gains.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "430",
        "title": "Convolutional Attention in Betting Exchange Markets",
        "author": [
            "Rui GonÃ§alves",
            "Vitor Miguel Ribeiro",
            "Roman Chertovskih",
            "AntÃ³nio Pedro Aguiar"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16008",
        "abstract": "This study presents the implementation of a short-term forecasting system for price movements in exchange markets, using market depth data and a systematic procedure to enable a fully automated trading system. The case study focuses on the UK to Win Horse Racing market during the pre-live stage on the world's leading betting exchange, Betfair. Innovative convolutional attention mechanisms are introduced and applied to multiple recurrent neural networks and bi-dimensional convolutional recurrent neural network layers. Additionally, a novel padding method for convolutional layers is proposed, specifically designed for multivariate time series processing. These innovations are thoroughly detailed, along with their execution process. The proposed architectures follow a standard supervised learning approach, involving model training and subsequent testing on new data, which requires extensive pre-processing and data analysis. The study also presents a complete end-to-end framework for automated feature engineering and market interactions using the developed models in production. The key finding of this research is that all proposed innovations positively impact the performance metrics of the classification task under examination, thereby advancing the current state-of-the-art in convolutional attention mechanisms and padding methods applied to multivariate time series problems.",
        "tags": [
            "RNN"
        ]
    },
    {
        "id": "431",
        "title": "TriAgent: Automated Biomarker Discovery with Deep Research Grounding for Triage in Acute Care by LLM-Based Multi-Agent Collaboration",
        "author": [
            "Kerem Delikoyun",
            "Qianyu Chen",
            "Win Sen Kuan",
            "John Tshon Yit Soong",
            "Matthew Edward Cove",
            "Oliver Hayden"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16080",
        "abstract": "Emergency departments worldwide face rising patient volumes, workforce shortages, and variability in triage decisions that threaten the delivery of timely and accurate care. Current triage methods rely primarily on vital signs, routine laboratory values, and clinicians' judgment, which, while effective, often miss emerging biological signals that could improve risk prediction for infection typing or antibiotic administration in acute conditions. To address this challenge, we introduce TriAgent, a large language model (LLM)-based multi-agent framework that couples automated biomarker discovery with deep research for literature-grounded validation and novelty assessment. TriAgent employs a supervisor research agent to generate research topics and delegate targeted queries to specialized sub-agents for evidence retrieval from various data sources. Findings are synthesized to classify biomarkers as either grounded in existing knowledge or flagged as novel candidates, offering transparent justification and highlighting unexplored pathways in acute care risk stratification. Unlike prior frameworks limited to existing routine clinical biomarkers, TriAgent aims to deliver an end-to-end framework from data analysis to literature grounding to improve transparency, explainability and expand the frontier of potentially actionable clinical biomarkers. Given a user's clinical query and quantitative triage data, TriAgent achieved a topic adherence F1 score of 55.7 +/- 5.0%, surpassing the CoT-ReAct agent by over 10%, and a faithfulness score of 0.42 +/- 0.39, exceeding all baselines by more than 50%. Across experiments, TriAgent consistently outperformed state-of-the-art LLM-based agentic frameworks in biomarker justification and literature-grounded novelty assessment. We share our repo: https://github.com/CellFace/TriAgent.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "432",
        "title": "Interpretable RNA-Seq Clustering with an LLM-Based Agentic Evidence-Grounded Framework",
        "author": [
            "Elias Hossain",
            "Mehrdad Shoeibi",
            "Ivan Garibay",
            "Niloofar Yousefi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16082",
        "abstract": "We propose CITE V.1, an agentic, evidence-grounded framework that leverages Large Language Models (LLMs) to provide transparent and reproducible interpretations of RNA-seq clusters. Unlike existing enrichment-based approaches that reduce results to broad statistical associations and LLM-only models that risk unsupported claims or fabricated citations, CITE V.1 transforms cluster interpretation by producing biologically coherent explanations explicitly anchored in the biomedical literature. The framework orchestrates three specialized agents: a Retriever that gathers domain knowledge from PubMed and UniProt, an Interpreter that formulates functional hypotheses, and Critics that evaluate claims, enforce evidence grounding, and qualify uncertainty through confidence and reliability indicators. Applied to Salmonella enterica RNA-seq data, CITE V.1 generated biologically meaningful insights supported by the literature, while an LLM-only Gemini baseline frequently produced speculative results with false citations. By moving RNA-seq analysis from surface-level enrichment to auditable, interpretable, and evidence-based hypothesis generation, CITE V.1 advances the transparency and reliability of AI in biomedicine.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "433",
        "title": "Learning density ratios in causal inference using Bregman-Riesz regression",
        "author": [
            "Oliver J. Hines",
            "Caleb H. Miles"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16127",
        "abstract": "The ratio of two probability density functions is a fundamental quantity that appears in many areas of statistics and machine learning, including causal inference, reinforcement learning, covariate shift, outlier detection, independence testing, importance sampling, and diffusion modeling. Naively estimating the numerator and denominator densities separately using, e.g., kernel density estimators, can lead to unstable performance and suffers from the curse of dimensionality as the number of covariates increases. For this reason, several methods have been developed for estimating the density ratio directly based on (a) Bregman divergences or (b) recasting the density ratio as the odds in a probabilistic classification model that predicts whether an observation is sampled from the numerator or denominator distribution. Additionally, the density ratio can be viewed as the Riesz representer of a continuous linear map, making it amenable to estimation via (c) minimization of the so-called Riesz loss, which was developed to learn the Riesz representer in the Riesz regression procedure in causal inference. In this paper we show that all three of these methods can be unified in a common framework, which we call Bregman-Riesz regression. We further show how data augmentation techniques can be used to apply density ratio learning methods to causal problems, where the numerator distribution typically represents an unobserved intervention. We show through simulations how the choice of Bregman divergence and data augmentation strategy can affect the performance of the resulting density ratio learner. A Python package is provided for researchers to apply Bregman-Riesz regression in practice using gradient boosting, neural networks, and kernel methods.",
        "tags": [
            "Detection",
            "Diffusion",
            "RL"
        ]
    },
    {
        "id": "434",
        "title": "AsyncVoice Agent: Real-Time Explanation for LLM Planning and Reasoning",
        "author": [
            "Yueqian Lin",
            "Zhengmian Hu",
            "Jayakumar Subramanian",
            "Qinsi Wang",
            "Nikos Vlassis",
            "Hai \"Helen\" Li",
            "Yiran Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16156",
        "abstract": "Effective human-AI collaboration on complex reasoning tasks requires that users understand and interact with the model's process, not just receive an output. However, the monolithic text from methods like Chain-of-Thought (CoT) prevents this, as current interfaces lack real-time verbalization and robust user barge-in. We present AsyncVoice Agent, a system whose asynchronous architecture decouples a streaming LLM backend from a conversational voice frontend. This design allows narration and inference to run in parallel, empowering users to interrupt, query, and steer the model's reasoning process at any time. Objective benchmarks show this approach reduces interaction latency by more than 600x compared to monolithic baselines while ensuring high fidelity and competitive task accuracy. By enabling a two-way dialogue with a model's thought process, AsyncVoice Agent offers a new paradigm for building more effective, steerable, and trustworthy human-AI systems for high-stakes tasks.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": "435",
        "title": "Synergizing chemical and AI communities for advancing laboratories of the future",
        "author": [
            "Saejin Oh",
            "Xinyi Fang",
            "I-Hsin Lin",
            "Paris Dee",
            "Christopher S. Dunham",
            "Stacy M. Copp",
            "Abigail G. Doyle",
            "Javier Read de Alaniz",
            "Mengyang Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16293",
        "abstract": "The development of automated experimental facilities and the digitization of experimental data have introduced numerous opportunities to radically advance chemical laboratories. As many laboratory tasks involve predicting and understanding previously unknown chemical relationships, machine learning (ML) approaches trained on experimental data can substantially accelerate the conventional design-build-test-learn process. This outlook article aims to help chemists understand and begin to adopt ML predictive models for a variety of laboratory tasks, including experimental design, synthesis optimization, and materials characterization. Furthermore, this article introduces how artificial intelligence (AI) agents based on large language models can help researchers acquire background knowledge in chemical or data science and accelerate various aspects of the discovery process. We present three case studies in distinct areas to illustrate how ML models and AI agents can be leveraged to reduce time-consuming experiments and manual data analysis. Finally, we highlight existing challenges that require continued synergistic effort from both experimental and computational communities to address.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "436",
        "title": "From Reviews to Actionable Insights: An LLM-Based Approach for Attribute and Feature Extraction",
        "author": [
            "Khaled Boughanmi",
            "Kamel Jedidi",
            "Nour Jedidi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16551",
        "abstract": "This research proposes a systematic, large language model (LLM) approach for extracting product and service attributes, features, and associated sentiments from customer reviews. Grounded in marketing theory, the framework distinguishes perceptual attributes from actionable features, producing interpretable and managerially actionable insights. We apply the methodology to 20,000 Yelp reviews of Starbucks stores and evaluate eight prompt variants on a random subset of reviews. Model performance is assessed through agreement with human annotations and predictive validity for customer ratings. Results show high consistency between LLMs and human coders and strong predictive validity, confirming the reliability of the approach. Human coders required a median of six minutes per review, whereas the LLM processed each in two seconds, delivering comparable insights at a scale unattainable through manual coding. Managerially, the analysis identifies attributes and features that most strongly influence customer satisfaction and their associated sentiments, enabling firms to pinpoint \"joy points,\" address \"pain points,\" and design targeted interventions. We demonstrate how structured review data can power an actionable marketing dashboard that tracks sentiment over time and across stores, benchmarks performance, and highlights high-leverage features for improvement. Simulations indicate that enhancing sentiment for key service features could yield 1-2% average revenue gains per store.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": "437",
        "title": "Adversarial Reinforcement Learning for Robust Control of Fixed-Wing Aircraft under Model Uncertainty",
        "author": [
            "Dennis J. Marquis",
            "Blake Wilhelm",
            "Devaprakash Muniraj",
            "Mazen Farhood"
        ],
        "pdf": "https://arxiv.org/pdf/2510.16650",
        "abstract": "This paper presents a reinforcement learning-based path-following controller for a fixed-wing small uncrewed aircraft system (sUAS) that is robust to uncertainties in the aerodynamic model of the sUAS. The controller is trained using the Robust Adversarial Reinforcement Learning framework, where an adversary perturbs the environment (aerodynamic model) to expose the agent (sUAS) to demanding scenarios. In our formulation, the adversary introduces rate-bounded perturbations to the aerodynamic model coefficients. We demonstrate that adversarial training improves robustness compared to controllers trained using stochastic model uncertainty. The learned controller is also benchmarked against a switched uncertain initial condition controller. The effectiveness of the approach is validated through high-fidelity simulations using a realistic six-degree-of-freedom fixed-wing aircraft model, showing accurate and robust path-following performance under a variety of uncertain aerodynamic conditions.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "438",
        "title": "Estimating Orbital Parameters of Direct Imaging Exoplanet Using Neural Network",
        "author": [
            "Bo Liang",
            "Hanlin Song",
            "Chang Liu",
            "Tianyu Zhao",
            "Yuxiang Xu",
            "Zihao Xiao",
            "Manjia Liang",
            "Minghui Du",
            "Wei-Liang Qian",
            "Li-e Qiang",
            "Peng Xu",
            "Ziren Luo"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17459",
        "abstract": "In this work, we propose a new flow-matching Markov chain Monte Carlo (FM-MCMC) algorithm for estimating the orbital parameters of exoplanetary systems, especially for those only one exoplanet is involved. Compared to traditional methods that rely on random sampling within the Bayesian framework, our approach first leverages flow matching posterior estimation (FMPE) to efficiently constrain the prior range of physical parameters, and then employs MCMC to accurately infer the posterior distribution. For example, in the orbital parameter inference of beta Pictoris b, our model achieved a substantial speed-up while maintaining comparable accuracy-running 77.8 times faster than Parallel Tempered MCMC (PTMCMC) and 365.4 times faster than nested sampling. Moreover, our FM-MCMC method also attained the highest average log-likelihood among all approaches, demonstrating its superior sampling efficiency and accuracy. This highlights the scalability and efficiency of our approach, making it well-suited for processing the massive datasets expected from future exoplanet surveys. Beyond astrophysics, our methodology establishes a versatile paradigm for synergizing deep generative models with traditional sampling, which can be adopted to tackle complex inference problems in other fields, such as cosmology, biomedical imaging, and particle physics.",
        "tags": [
            "Flow Matching"
        ]
    },
    {
        "id": "439",
        "title": "Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs",
        "author": [
            "Paula Cordero-Encinar",
            "Andrew B. Duncan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17472",
        "abstract": "Recent advances such as self-consistency and test-time reinforcement learning (TTRL) improve the reliability of large language models (LLMs) without additional supervision, yet their underlying mechanisms and statistical guarantees remain poorly understood. We present a unified framework for certifiable inference in LLMs, showing that majority voting provides a statistical certificate of self-consistency: under mild assumptions, the aggregated answer coincides with the mode of the model's terminal distribution with high probability. We derive finite-sample and anytime-valid concentration bounds that quantify this confidence, and introduce the Martingale Majority Certificate (MMC), a sequential stopping rule that adaptively determines when sufficient samples have been drawn. We further prove that label-free post-training methods such as TTRL implicitly sharpen the answer distribution by exponentially tilting it toward its mode, thereby reducing the number of samples required for certification. Building on this insight, we propose new post-training objectives that explicitly optimise this trade-off between sharpness and bias. Together, these results explain and connect two central test-time scaling strategies, self-consistency and TTRL, within a single statistical framework for label-free, certifiable reliability in reasoning LLMs.",
        "tags": [
            "LLM",
            "RL",
            "TTT"
        ]
    },
    {
        "id": "440",
        "title": "Plasma Shape Control via Zero-shot Generative Reinforcement Learning",
        "author": [
            "Niannian Wu",
            "Rongpeng Li",
            "Zongyu Yang",
            "Yong Xiao",
            "Ning Wei",
            "Yihang Chen",
            "Bo Li",
            "Zhifeng Zhao",
            "Wulyu Zhong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17531",
        "abstract": "Traditional PID controllers have limited adaptability for plasma shape control, and task-specific reinforcement learning (RL) methods suffer from limited generalization and the need for repetitive retraining. To overcome these challenges, this paper proposes a novel framework for developing a versatile, zero-shot control policy from a large-scale offline dataset of historical PID-controlled discharges. Our approach synergistically combines Generative Adversarial Imitation Learning (GAIL) with Hilbert space representation learning to achieve dual objectives: mimicking the stable operational style of the PID data and constructing a geometrically structured latent space for efficient, goal-directed control. The resulting foundation policy can be deployed for diverse trajectory tracking tasks in a zero-shot manner without any task-specific fine-tuning. Evaluations on the HL-3 tokamak simulator demonstrate that the policy excels at precisely and stably tracking reference trajectories for key shape parameters across a range of plasma scenarios. This work presents a viable pathway toward developing highly flexible and data-efficient intelligent control systems for future fusion reactors.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": "441",
        "title": "Non-asymptotic error bounds for probability flow ODEs under weak log-concavity",
        "author": [
            "Gitte Kremling",
            "Francesco Iafrate",
            "Mahsa Taheri",
            "Johannes Lederer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.17608",
        "abstract": "Score-based generative modeling, implemented through probability flow ODEs, has shown impressive results in numerous practical settings. However, most convergence guarantees rely on restrictive regularity assumptions on the target distribution -- such as strong log-concavity or bounded support. This work establishes non-asymptotic convergence bounds in the 2-Wasserstein distance for a general class of probability flow ODEs under considerably weaker assumptions: weak log-concavity and Lipschitz continuity of the score function. Our framework accommodates non-log-concave distributions, such as Gaussian mixtures, and explicitly accounts for initialization errors, score approximation errors, and effects of discretization via an exponential integrator scheme. Bridging a key theoretical challenge in diffusion-based generative modeling, our results extend convergence theory to more realistic data distributions and practical ODE solvers. We provide concrete guarantees for the efficiency and correctness of the sampling algorithm, complementing the empirical success of diffusion models with rigorous theory. Moreover, from a practical perspective, our explicit rates might be helpful in choosing hyperparameters, such as the step size in the discretization.",
        "tags": [
            "Diffusion",
            "ODE",
            "Score-Based Generative"
        ]
    }
]