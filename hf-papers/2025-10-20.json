[
    {
        "id": 1,
        "title": "A Theoretical Study on Bridging Internal Probability and Self-Consistency for LLM Reasoning",
        "author": [
            "Zhi Zhou",
            "Yuhao Tan",
            "Zenan Li",
            "Yuan Yao",
            "Lan-Zhe Guo",
            "Yu-Feng Li",
            "Xiaoxing Ma"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15444",
        "abstract": "Test-time scaling seeks to improve the reasoning performance of large language models (LLMs) by adding computational resources. A prevalent approach within the field is sampling-based test-time scaling methods, which enhance reasoning by generating multiple reasoning paths for a given input during inference. However, despite its practical success, the theoretical foundations remain underexplored. In this paper, we provide the first theoretical framework for analyzing sampling-based test-time scaling methods, grounded in the perspective of confidence estimation. Based on the framework, we analyze two dominant paradigms: self-consistency and perplexity, and reveal key limitations: self-consistency suffers from high estimation error while perplexity exhibits substantial modeling error and possible degradation of the estimation error convergence. To address these limitations, we introduce RPC, a hybrid method that leverages our theoretical insights through two key components: Perplexity Consistency and Reasoning Pruning. Perplexity Consistency combines the strengths of self-consistency and perplexity, boosting the convergence rate of estimation error from linear to exponential while preserving model error. Reasoning Pruning prevents degradation by eliminating low-probability reasoning paths. Both theoretical analysis and empirical results across seven benchmark datasets demonstrate that RPC has a strong potential for reducing reasoning error. Notably, RPC achieves reasoning performance comparable to self-consistency while not only enhancing confidence reliability but also reducing sampling costs by 50%. The code and resources are available at https://wnjxyk.github.io/RPC.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 2,
        "title": "OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM",
        "author": [
            "Hanrong Ye",
            "Chao-Han Huck Yang",
            "Arushi Goel",
            "Wei Huang",
            "Ligeng Zhu",
            "Yuanhang Su",
            "Sean Lin",
            "An-Chieh Cheng",
            "Zhen Wan",
            "Jinchuan Tian",
            "Yuming Lou",
            "Dong Yang",
            "Zhijian Liu",
            "Yukang Chen",
            "Ambrish Dantrey",
            "Ehsan Jahangiri",
            "Sreyan Ghosh",
            "Daguang Xu",
            "Ehsan Hosseini-Asl",
            "Danial Mohseni Taheri",
            "Vidya Murali",
            "Sifei Liu",
            "Jason Lu",
            "Oluwatobi Olabiyi",
            "Frank Wang",
            "Rafael Valle",
            "Bryan Catanzaro",
            "Andrew Tao",
            "Song Han",
            "Jan Kautz",
            "Hongxu Yin",
            "Pavlo Molchanov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15870",
        "abstract": "Advancing machine intelligence requires developing the ability to perceive across multiple modalities, much as humans sense the world. We introduce OmniVinci, an initiative to build a strong, open-source, omni-modal LLM. We carefully study the design choices across model architecture and data curation. For model architecture, we present three key innovations: (i) OmniAlignNet for strengthening alignment between vision and audio embeddings in a shared omni-modal latent space; (ii) Temporal Embedding Grouping for capturing relative temporal alignment between vision and audio signals; and (iii) Constrained Rotary Time Embedding for encoding absolute temporal information in omni-modal embeddings. We introduce a curation and synthesis pipeline that generates 24M single-modal and omni-modal conversations. We find that modalities reinforce one another in both perception and reasoning. Our model, OmniVinci, outperforms Qwen2.5-Omni with +19.05 on DailyOmni (cross-modal understanding), +1.7 on MMAR (audio), and +3.9 on Video-MME (vision), while using just 0.2T training tokens - a 6 times reduction compared to Qwen2.5-Omni's 1.2T. We finally demonstrate omni-modal advantages in downstream applications spanning robotics, medical AI, and smart factory.",
        "tags": [
            "LLM",
            "Robotics"
        ]
    },
    {
        "id": 3,
        "title": "NANO3D: A Training-Free Approach for Efficient 3D Editing Without Masks",
        "author": [
            "Junliang Ye",
            "Shenghao Xie",
            "Ruowen Zhao",
            "Zhengyi Wang",
            "Hongyu Yan",
            "Wenqiang Zu",
            "Lei Ma",
            "Jun Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15019",
        "abstract": "3D object editing is essential for interactive content creation in gaming, animation, and robotics, yet current approaches remain inefficient, inconsistent, and often fail to preserve unedited regions. Most methods rely on editing multi-view renderings followed by reconstruction, which introduces artifacts and limits practicality. To address these challenges, we propose Nano3D, a training-free framework for precise and coherent 3D object editing without masks. Nano3D integrates FlowEdit into TRELLIS to perform localized edits guided by front-view renderings, and further introduces region-aware merging strategies, Voxel/Slat-Merge, which adaptively preserve structural fidelity by ensuring consistency between edited and unedited areas. Experiments demonstrate that Nano3D achieves superior 3D consistency and visual quality compared with existing methods. Based on this framework, we construct the first large-scale 3D editing datasets Nano3D-Edit-100k, which contains over 100,000 high-quality 3D editing pairs. This work addresses long-standing challenges in both algorithm design and data availability, significantly improving the generality and reliability of 3D editing, and laying the groundwork for the development of feed-forward 3D editing models. Project Page:https://jamesyjl.github.io/Nano3D",
        "tags": [
            "3D",
            "Robotics"
        ]
    },
    {
        "id": 4,
        "title": "Emergent Misalignment via In-Context Learning: Narrow in-context examples can produce broadly misaligned LLMs",
        "author": [
            "Nikita Afonin",
            "Nikita Andriyanov",
            "Nikhil Bageshpura",
            "Kyle Liu",
            "Kevin Zhu",
            "Sunishchal Dev",
            "Ashwinee Panda",
            "Alexander Panchenko",
            "Oleg Rogov",
            "Elena Tutubalina",
            "Mikhail Seleznyov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11288",
        "abstract": "Recent work has shown that narrow finetuning can produce broadly misaligned LLMs, a phenomenon termed emergent misalignment (EM). While concerning, these findings were limited to finetuning and activation steering, leaving out in-context learning (ICL). We therefore ask: does EM emerge in ICL? We find that it does: across three datasets, three frontier models produce broadly misaligned responses at rates between 2% and 17% given 64 narrow in-context examples, and up to 58% with 256 examples. We also examine mechanisms of EM by eliciting step-by-step reasoning (while leaving in-context examples unchanged). Manual analysis of the resulting chain-of-thought shows that 67.5% of misaligned traces explicitly rationalize harmful outputs by adopting a reckless or dangerous ''persona'', echoing prior results on finetuning-induced EM.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": 5,
        "title": "Scaling Instruction-Based Video Editing with a High-Quality Synthetic Dataset",
        "author": [
            "Qingyan Bai",
            "Qiuyu Wang",
            "Hao Ouyang",
            "Yue Yu",
            "Hanlin Wang",
            "Wen Wang",
            "Ka Leong Cheng",
            "Shuailei Ma",
            "Yanhong Zeng",
            "Zichen Liu",
            "Yinghao Xu",
            "Yujun Shen",
            "Qifeng Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15742",
        "abstract": "Instruction-based video editing promises to democratize content creation, yet its progress is severely hampered by the scarcity of large-scale, high-quality training data. We introduce Ditto, a holistic framework designed to tackle this fundamental challenge. At its heart, Ditto features a novel data generation pipeline that fuses the creative diversity of a leading image editor with an in-context video generator, overcoming the limited scope of existing models. To make this process viable, our framework resolves the prohibitive cost-quality trade-off by employing an efficient, distilled model architecture augmented by a temporal enhancer, which simultaneously reduces computational overhead and improves temporal coherence. Finally, to achieve full scalability, this entire pipeline is driven by an intelligent agent that crafts diverse instructions and rigorously filters the output, ensuring quality control at scale. Using this framework, we invested over 12,000 GPU-days to build Ditto-1M, a new dataset of one million high-fidelity video editing examples. We trained our model, Editto, on Ditto-1M with a curriculum learning strategy. The results demonstrate superior instruction-following ability and establish a new state-of-the-art in instruction-based video editing.",
        "tags": [
            "Video Editing"
        ]
    },
    {
        "id": 6,
        "title": "Skyfall-GS: Synthesizing Immersive 3D Urban Scenes from Satellite Imagery",
        "author": [
            "Jie-Ying Lee",
            "Yi-Ruei Liu",
            "Shr-Ruei Tsai",
            "Wei-Cheng Chang",
            "Chung-Ho Wu",
            "Jiewen Chan",
            "Zhenjun Zhao",
            "Chieh Hubert Lin",
            "Yu-Lun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15869",
        "abstract": "Synthesizing large-scale, explorable, and geometrically accurate 3D urban scenes is a challenging yet valuable task in providing immersive and embodied applications. The challenges lie in the lack of large-scale and high-quality real-world 3D scans for training generalizable generative models. In this paper, we take an alternative route to create large-scale 3D scenes by synergizing the readily available satellite imagery that supplies realistic coarse geometry and the open-domain diffusion model for creating high-quality close-up appearances. We propose \\textbf{Skyfall-GS}, the first city-block scale 3D scene creation framework without costly 3D annotations, also featuring real-time, immersive 3D exploration. We tailor a curriculum-driven iterative refinement strategy to progressively enhance geometric completeness and photorealistic textures. Extensive experiments demonstrate that Skyfall-GS provides improved cross-view consistent geometry and more realistic textures compared to state-of-the-art approaches. Project page: https://skyfall-gs.jayinnn.dev/",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": 7,
        "title": "Latent Diffusion Model without Variational Autoencoder",
        "author": [
            "Minglei Shi",
            "Haolin Wang",
            "Wenzhao Zheng",
            "Ziyang Yuan",
            "Xiaoshi Wu",
            "Xintao Wang",
            "Pengfei Wan",
            "Jie Zhou",
            "Jiwen Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15301",
        "abstract": "Recent progress in diffusion-based visual generation has largely relied on latent diffusion models with variational autoencoders (VAEs). While effective for high-fidelity synthesis, this VAE+diffusion paradigm suffers from limited training efficiency, slow inference, and poor transferability to broader vision tasks. These issues stem from a key limitation of VAE latent spaces: the lack of clear semantic separation and strong discriminative structure. Our analysis confirms that these properties are crucial not only for perception and understanding tasks, but also for the stable and efficient training of latent diffusion models. Motivated by this insight, we introduce SVG, a novel latent diffusion model without variational autoencoders, which leverages self-supervised representations for visual generation. SVG constructs a feature space with clear semantic discriminability by leveraging frozen DINO features, while a lightweight residual branch captures fine-grained details for high-fidelity reconstruction. Diffusion models are trained directly on this semantically structured latent space to facilitate more efficient learning. As a result, SVG enables accelerated diffusion training, supports few-step sampling, and improves generative quality. Experimental results further show that SVG preserves the semantic and discriminative capabilities of the underlying self-supervised representations, providing a principled pathway toward task-general, high-quality visual representations.",
        "tags": [
            "Diffusion",
            "VAE"
        ]
    },
    {
        "id": 8,
        "title": "LightsOut: Diffusion-based Outpainting for Enhanced Lens Flare Removal",
        "author": [
            "Shr-Ruei Tsai",
            "Wei-Cheng Chang",
            "Jie-Ying Lee",
            "Chih-Hai Su",
            "Yu-Lun Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15868",
        "abstract": "Lens flare significantly degrades image quality, impacting critical computer vision tasks like object detection and autonomous driving. Recent Single Image Flare Removal (SIFR) methods perform poorly when off-frame light sources are incomplete or absent. We propose LightsOut, a diffusion-based outpainting framework tailored to enhance SIFR by reconstructing off-frame light sources. Our method leverages a multitask regression module and LoRA fine-tuned diffusion model to ensure realistic and physically consistent outpainting results. Comprehensive experiments demonstrate LightsOut consistently boosts the performance of existing SIFR methods across challenging scenarios without additional retraining, serving as a universally applicable plug-and-play preprocessing solution. Project page: https://ray-1026.github.io/lightsout/",
        "tags": [
            "Detection",
            "Diffusion",
            "LoRA"
        ]
    },
    {
        "id": 9,
        "title": "Paper2Web: Let's Make Your Paper Alive!",
        "author": [
            "Yuhang Chen",
            "Tianpeng Lv",
            "Siyi Zhang",
            "Yixiang Yin",
            "Yao Wan",
            "Philip S. Yu",
            "Dongping Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15842",
        "abstract": "Academic project websites can more effectively disseminate research when they clearly present core content and enable intuitive navigation and interaction. However, current approaches such as direct Large Language Model (LLM) generation, templates, or direct HTML conversion struggle to produce layout-aware, interactive sites, and a comprehensive evaluation suite for this task has been lacking. In this paper, we introduce Paper2Web, a benchmark dataset and multi-dimensional evaluation framework for assessing academic webpage generation. It incorporates rule-based metrics like Connectivity, Completeness and human-verified LLM-as-a-Judge (covering interactivity, aesthetics, and informativeness), and PaperQuiz, which measures paper-level knowledge retention. We further present PWAgent, an autonomous pipeline that converts scientific papers into interactive and multimedia-rich academic homepages. The agent iteratively refines both content and layout through MCP tools that enhance emphasis, balance, and presentation quality. Our experiments show that PWAgent consistently outperforms end-to-end baselines like template-based webpages and arXiv/alphaXiv versions by a large margin while maintaining low cost, achieving the Pareto-front in academic webpage generation.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 10,
        "title": "A$^2$FM: An Adaptive Agent Foundation Model for Tool-Aware Hybrid Reasoning",
        "author": [
            "Qianben Chen",
            "Jingyi Cao",
            "Jiayu Zhang",
            "Tianrui Qin",
            "Xiaowan Li",
            "King Zhu",
            "Dingfeng Shi",
            "He Zhu",
            "Minghao Liu",
            "Xiaobo Liang",
            "Xin Gui",
            "Ge Zhang",
            "Jian Yang",
            "Yuchen Eleanor Jiang",
            "Wangchunshu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12838",
        "abstract": "Large language models split into two families: reasoning-centric LLMs, which strengthen internal chain-of-thought reasoning but cannot invoke external tools, and agentic LLMs, which learn to interact with environments and leverage tools but often lag in deep reasoning. This divide arises from fundamentally different training objectives, leading to mismatched strengths and inefficiency on simple queries, where both families tend to overthink or over-call tools. In this work, we present Adaptive Agent Foundation Model (A$^2$FM), a unified framework that follows a route-then-align principle: the model first learns task-aware routing and then aligns mode-specific trajectories under a shared backbone. To address the inefficiency gap, we introduce a third mode-instant-that handles simple queries directly, preventing unnecessary reasoning or tool calls while complementing the agentic and reasoning modes. To jointly enhance accuracy and efficiency, we propose Adaptive Policy Optimization (APO), which enforces adaptive sampling across modes and applies a cost-regularized reward. On the 32B scale, A$^2$FM achieves 13.4% on BrowseComp, 70.4% on AIME25, and 16.7% on HLE, setting new SOTA among comparable models and performing competitively with frontier LLMs across agentic, reasoning, and general benchmarks. Notably, the adaptive execution achieves a cost of pass of only $0.00487 per correct answer-cutting cost by 45.2% relative to reasoning and 33.5% relative to agentic, thus delivering substantially higher cost efficiency while maintaining comparable accuracy.",
        "tags": [
            "CoT",
            "LLM"
        ]
    },
    {
        "id": 11,
        "title": "MorphoBench: A Benchmark with Difficulty Adaptive to Model Reasoning",
        "author": [
            "Xukai Wang",
            "Xuanbo Liu",
            "Mingrui Chen",
            "Haitian Zhong",
            "Xuanlin Yang",
            "Bohan Zeng",
            "Jinbo Hu",
            "Hao Liang",
            "Junbo Niu",
            "Xuchen Li",
            "Ruitao Wu",
            "Ruichuan An",
            "Yang Shi",
            "Liu Liu",
            "Xu-Yao Zhang",
            "Qiang Liu",
            "Zhouchen Lin",
            "Wentao Zhang",
            "Bin Dong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14265",
        "abstract": "With the advancement of powerful large-scale reasoning models, effectively evaluating the reasoning capabilities of these models has become increasingly important. However, existing benchmarks designed to assess the reasoning abilities of large models tend to be limited in scope and lack the flexibility to adapt their difficulty according to the evolving reasoning capacities of the models. To address this, we propose MorphoBench, a benchmark that incorporates multidisciplinary questions to evaluate the reasoning capabilities of large models and can adjust and update question difficulty based on the reasoning abilities of advanced models. Specifically, we curate the benchmark by selecting and collecting complex reasoning questions from existing benchmarks and sources such as Olympiad-level competitions. Additionally, MorphoBench adaptively modifies the analytical challenge of questions by leveraging key statements generated during the model's reasoning process. Furthermore, it includes questions generated using simulation software, enabling dynamic adjustment of benchmark difficulty with minimal resource consumption. We have gathered over 1,300 test questions and iteratively adjusted the difficulty of MorphoBench based on the reasoning capabilities of models such as o3 and GPT-5. MorphoBench enhances the comprehensiveness and validity of model reasoning evaluation, providing reliable guidance for improving both the reasoning abilities and scientific robustness of large models. The code has been released in https://github.com/OpenDCAI/MorphoBench.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": 12,
        "title": "Language Models Model Language",
        "author": [
            "Åukasz Borchmann"
        ],
        "pdf": "https://arxiv.org/pdf/2510.12766",
        "abstract": "Linguistic commentary on LLMs, heavily influenced by the theoretical frameworks of de Saussure and Chomsky, is often speculative and unproductive. Critics challenge whether LLMs can legitimately model language, citing the need for \"deep structure\" or \"grounding\" to achieve an idealized linguistic \"competence.\" We argue for a radical shift in perspective towards the empiricist principles of Witold MaÅczak, a prominent general and historical linguist. He defines language not as a \"system of signs\" or a \"computational system of the brain\" but as the totality of all that is said and written. Above all, he identifies frequency of use of particular language elements as language's primary governing principle. Using his framework, we challenge prior critiques of LLMs and provide a constructive guide for designing, evaluating, and interpreting language models.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 13,
        "title": "BLIP3o-NEXT: Next Frontier of Native Image Generation",
        "author": [
            "Jiuhai Chen",
            "Le Xue",
            "Zhiyang Xu",
            "Xichen Pan",
            "Shusheng Yang",
            "Can Qin",
            "An Yan",
            "Honglu Zhou",
            "Zeyuan Chen",
            "Lifu Huang",
            "Tianyi Zhou",
            "Junnan Li",
            "Silvio Savarese",
            "Caiming Xiong",
            "Ran Xu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15857",
        "abstract": "We present BLIP3o-NEXT, a fully open-source foundation model in the BLIP3 series that advances the next frontier of native image generation. BLIP3o-NEXT unifies text-to-image generation and image editing within a single architecture, demonstrating strong image generation and image editing capabilities. In developing the state-of-the-art native image generation model, we identify four key insights: (1) Most architectural choices yield comparable performance; an architecture can be deemed effective provided it scales efficiently and supports fast inference; (2) The successful application of reinforcement learning can further push the frontier of native image generation; (3) Image editing still remains a challenging task, yet instruction following and the consistency between generated and reference images can be significantly enhanced through post-training and data engine; (4) Data quality and scale continue to be decisive factors that determine the upper bound of model performance. Building upon these insights, BLIP3o-NEXT leverages an Autoregressive + Diffusion architecture in which an autoregressive model first generates discrete image tokens conditioned on multimodal inputs, whose hidden states are then used as conditioning signals for a diffusion model to generate high-fidelity images. This architecture integrates the reasoning strength and instruction following of autoregressive models with the fine-detail rendering ability of diffusion models, achieving a new level of coherence and realism. Extensive evaluations of various text-to-image and image-editing benchmarks show that BLIP3o-NEXT achieves superior performance over existing models.",
        "tags": [
            "Diffusion",
            "Image Editing",
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": 14,
        "title": "Foundation Models for Scientific Discovery: From Paradigm Enhancement to Paradigm Transition",
        "author": [
            "Fan Liu",
            "Jindong Han",
            "Tengfei Lyu",
            "Weijia Zhang",
            "Zhe-Rui Yang",
            "Lu Dai",
            "Cancheng Liu",
            "Hao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15280",
        "abstract": "Foundation models (FMs), such as GPT-4 and AlphaFold, are reshaping the landscape of scientific research. Beyond accelerating tasks such as hypothesis generation, experimental design, and result interpretation, they prompt a more fundamental question: Are FMs merely enhancing existing scientific methodologies, or are they redefining the way science is conducted? In this paper, we argue that FMs are catalyzing a transition toward a new scientific paradigm. We introduce a three-stage framework to describe this evolution: (1) Meta-Scientific Integration, where FMs enhance workflows within traditional paradigms; (2) Hybrid Human-AI Co-Creation, where FMs become active collaborators in problem formulation, reasoning, and discovery; and (3) Autonomous Scientific Discovery, where FMs operate as independent agents capable of generating new scientific knowledge with minimal human intervention. Through this lens, we review current applications and emerging capabilities of FMs across existing scientific paradigms. We further identify risks and future directions for FM-enabled scientific discovery. This position paper aims to support the scientific community in understanding the transformative role of FMs and to foster reflection on the future of scientific discovery. Our project is available at https://github.com/usail-hkust/Awesome-Foundation-Models-for-Scientific-Discovery.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": 15,
        "title": "VISTA: A Test-Time Self-Improving Video Generation Agent",
        "author": [
            "Do Xuan Long",
            "Xingchen Wan",
            "Hootan Nakhost",
            "Chen-Yu Lee",
            "Tomas Pfister",
            "Sercan Ã. ArÄ±k"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15831",
        "abstract": "Despite rapid advances in text-to-video synthesis, generated video quality remains critically dependent on precise user prompts. Existing test-time optimization methods, successful in other domains, struggle with the multi-faceted nature of video. In this work, we introduce VISTA (Video Iterative Self-improvemenT Agent), a novel multi-agent system that autonomously improves video generation through refining prompts in an iterative loop. VISTA first decomposes a user idea into a structured temporal plan. After generation, the best video is identified through a robust pairwise tournament. This winning video is then critiqued by a trio of specialized agents focusing on visual, audio, and contextual fidelity. Finally, a reasoning agent synthesizes this feedback to introspectively rewrite and enhance the prompt for the next generation cycle. Experiments on single- and multi-scene video generation scenarios show that while prior methods yield inconsistent gains, VISTA consistently improves video quality and alignment with user intent, achieving up to 60% pairwise win rate against state-of-the-art baselines. Human evaluators concur, preferring VISTA outputs in 66.4% of comparisons.",
        "tags": [
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": 16,
        "title": "Build Your Personalized Research Group: A Multiagent Framework for Continual and Interactive Science Automation",
        "author": [
            "Ed Li",
            "Junyu Ren",
            "Xintian Pan",
            "Cat Yan",
            "Chuanhao Li",
            "Dirk Bergemann",
            "Zhuoran Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15624",
        "abstract": "The automation of scientific discovery represents a critical milestone in Artificial Intelligence (AI) research. However, existing agentic systems for science suffer from two fundamental limitations: rigid, pre-programmed workflows that cannot adapt to intermediate findings, and inadequate context management that hinders long-horizon research. We present \\texttt{freephdlabor}, an open-source multiagent framework featuring \\textit{fully dynamic workflows} determined by real-time agent reasoning and a \\coloremph{\\textit{modular architecture}} enabling seamless customization -- users can modify, add, or remove agents to address domain-specific requirements. The framework provides comprehensive infrastructure including \\textit{automatic context compaction}, \\textit{workspace-based communication} to prevent information degradation, \\textit{memory persistence} across sessions, and \\textit{non-blocking human intervention} mechanisms. These features collectively transform automated research from isolated, single-run attempts into \\textit{continual research programs} that build systematically on prior explorations and incorporate human feedback. By providing both the architectural principles and practical implementation for building customizable co-scientist systems, this work aims to facilitate broader adoption of automated research across scientific domains, enabling practitioners to deploy interactive multiagent systems that autonomously conduct end-to-end research -- from ideation through experimentation to publication-ready manuscripts.",
        "tags": []
    },
    {
        "id": 17,
        "title": "Explore to Evolve: Scaling Evolved Aggregation Logic via Proactive Online Exploration for Deep Research Agents",
        "author": [
            "Rui Wang",
            "Ce Zhang",
            "Jun-Yu Ma",
            "Jianshu Zhang",
            "Hongru Wang",
            "Yi Chen",
            "Boyang Xue",
            "Tianqing Fang",
            "Zhisong Zhang",
            "Hongming Zhang",
            "Haitao Mi",
            "Dong Yu",
            "Kam-Fai Wong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14438",
        "abstract": "Deep research web agents not only retrieve information from diverse sources such as web environments, files, and multimodal inputs, but more importantly, they need to rigorously analyze and aggregate knowledge for insightful research. However, existing open-source deep research agents predominantly focus on enhancing information-seeking capabilities of web agents to locate specific information, while overlooking the essential need for information aggregation, which would limit their ability to support in-depth research. We propose an Explore to Evolve paradigm to scalably construct verifiable training data for web agents. Begins with proactive online exploration, an agent sources grounded information by exploring the real web. Using the collected evidence, the agent then self-evolves an aggregation program by selecting, composing, and refining operations from 12 high-level logical types to synthesize a verifiable QA pair. This evolution from high-level guidance to concrete operations allowed us to scalably produce WebAggregatorQA, a dataset of 10K samples across 50K websites and 11 domains. Based on an open-source agent framework, SmolAgents, we collect supervised fine-tuning trajectories to develop a series of foundation models, WebAggregator. WebAggregator-8B matches the performance of GPT-4.1, while the 32B variant surpasses GPT-4.1 by more than 10% on GAIA-text and closely approaches Claude-3.7-sonnet. Moreover, given the limited availability of benchmarks that evaluate web agents' information aggregation abilities, we construct a human-annotated evaluation split of WebAggregatorQA as a challenging test set. On this benchmark, Claude-3.7-sonnet only achieves 28%, and GPT-4.1 scores 25.8%. Even when agents manage to retrieve all references, they still struggle on WebAggregatorQA, highlighting the need to strengthen the information aggregation capabilities of web agent foundations.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": 18,
        "title": "InfiMed-ORBIT: Aligning LLMs on Open-Ended Complex Tasks via Rubric-Based Incremental Training",
        "author": [
            "Pengkai Wang",
            "Qi Zuo",
            "Pengwei Liu",
            "Zhijie Sang",
            "Congkai Xie",
            "Hongxia Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15859",
        "abstract": "Large Language Models (LLMs) have shown substantial advances through reinforcement learning (RL), particularly in domains where rewards can be programmatically verified, such as mathematics and code. In these areas, models benefit from a well-defined operational base guided by explicit rule-based objectives. However, this progress reveals a significant limitation: in open-ended domains where rewards are ambiguous, subjective, or context-dependent, such as creative writing, scientific reasoning, and notably medical consultation, robust reward functions are lacking, making these areas challenging for current RL strategies. To bridge this gap, we introduce ORBIT, an open-ended rubric-based incremental training framework specifically designed for high-stakes medical dialogue. ORBIT integrates syn- thetic dialogue generation with the dynamic creation of rubrics, employing these rubrics to direct an incremental RL process. In particular, this approach does not depend on external medical knowledge or manual rules, instead utilizing rubric-guided feedback to shape learning. When implemented on the Qwen3-4B-Instruct model, our method can greatly enhance its performance on the HealthBench-Hard benchmark from 7.0 to 27.2 using only 2k samples, thus achieving state-of-the-art results for models of this scale. Our analysis confirms that rubric-driven RL fos-ters consistent performance gains across diverse consultation scenarios, going beyond simple numerical improvements. These findings underscore rubric-based feedback as a scalable strategy for advancing LLMs in intricate, open-ended tasks.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": 19,
        "title": "Imaginarium: Vision-guided High-Quality 3D Scene Layout Generation",
        "author": [
            "Xiaoming Zhu",
            "Xu Huang",
            "Qinghongbing Xie",
            "Zhi Deng",
            "Junsheng Yu",
            "Yirui Guan",
            "Zhongyuan Liu",
            "Lin Zhu",
            "Qijun Zhao",
            "Ligang Liu",
            "Long Zeng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15564",
        "abstract": "Generating artistic and coherent 3D scene layouts is crucial in digital content creation. Traditional optimization-based methods are often constrained by cumbersome manual rules, while deep generative models face challenges in producing content with richness and diversity. Furthermore, approaches that utilize large language models frequently lack robustness and fail to accurately capture complex spatial relationships. To address these challenges, this paper presents a novel vision-guided 3D layout generation system. We first construct a high-quality asset library containing 2,037 scene assets and 147 3D scene layouts. Subsequently, we employ an image generation model to expand prompt representations into images, fine-tuning it to align with our asset library. We then develop a robust image parsing module to recover the 3D layout of scenes based on visual semantics and geometric information. Finally, we optimize the scene layout using scene graphs and overall visual semantics to ensure logical coherence and alignment with the images. Extensive user testing demonstrates that our algorithm significantly outperforms existing methods in terms of layout richness and quality. The code and dataset will be available at https://github.com/HiHiAllen/Imaginarium.",
        "tags": [
            "3D",
            "LLM"
        ]
    },
    {
        "id": 20,
        "title": "DLER: Doing Length pEnalty Right - Incentivizing More Intelligence per Token via Reinforcement Learning",
        "author": [
            "Shih-Yang Liu",
            "Xin Dong",
            "Ximing Lu",
            "Shizhe Diao",
            "Mingjie Liu",
            "Min-Hung Chen",
            "Hongxu Yin",
            "Yu-Chiang Frank Wang",
            "Kwang-Ting Cheng",
            "Yejin Choi",
            "Jan Kautz",
            "Pavlo Molchanov"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15110",
        "abstract": "Reasoning language models such as OpenAI-o1, DeepSeek-R1, and Qwen achieve strong performance via extended chains of thought but often generate unnecessarily long outputs. Maximizing intelligence per token--accuracy relative to response length--remains an open problem. We revisit reinforcement learning (RL) with the simplest length penalty--truncation--and show that accuracy degradation arises not from the lack of sophisticated penalties but from inadequate RL optimization. We identify three key challenges: (i) large bias in advantage estimation, (ii) entropy collapse, and (iii) sparse reward signal. We address them with Doing Length pEnalty Right (DLER), a training recipe combining batch-wise reward normalization, higher clipping, dynamic sampling, and a simple truncation length penalty. DLER achieves state-of-the-art accuracy--efficiency trade-offs, cutting output length by over 70 percent while surpassing all previous baseline accuracy. It also improves test-time scaling: compared to DeepSeek-R1-7B, DLER-7B generates multiple concise responses in parallel with 28 percent higher accuracy and lower latency. We further introduce Difficulty-Aware DLER, which adaptively tightens truncation on easier questions for additional efficiency gains. We also propose an update-selective merging method that preserves baseline accuracy while retaining the concise reasoning ability of the DLER model, which is useful for scenarios where RL training data is scarce.",
        "tags": [
            "DeepSeek",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": 21,
        "title": "FinTrust: A Comprehensive Benchmark of Trustworthiness Evaluation in Finance Domain",
        "author": [
            "Tiansheng Hu",
            "Tongyan Hu",
            "Liuyang Bai",
            "Yilun Zhao",
            "Arman Cohan",
            "Chen Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15232",
        "abstract": "Recent LLMs have demonstrated promising ability in solving finance related problems. However, applying LLMs in real-world finance application remains challenging due to its high risk and high stakes property. This paper introduces FinTrust, a comprehensive benchmark specifically designed for evaluating the trustworthiness of LLMs in finance applications. Our benchmark focuses on a wide range of alignment issues based on practical context and features fine-grained tasks for each dimension of trustworthiness evaluation. We assess eleven LLMs on FinTrust and find that proprietary models like o4-mini outperforms in most tasks such as safety while open-source models like DeepSeek-V3 have advantage in specific areas like industry-level fairness. For challenging task like fiduciary alignment and disclosure, all LLMs fall short, showing a significant gap in legal awareness. We believe that FinTrust can be a valuable benchmark for LLMs' trustworthiness evaluation in finance domain.",
        "tags": [
            "DeepSeek",
            "LLM"
        ]
    },
    {
        "id": 22,
        "title": "Rewiring Experts on the Fly:Continuous Rerouting for Better Online Adaptation in Mixture-of-Expert models",
        "author": [
            "Guinan Su",
            "Yanwu Yang",
            "Li Shen",
            "Lu Yin",
            "Shiwei Liu",
            "Jonas Geiping"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14853",
        "abstract": "Mixture-of-Experts (MoE) models achieve efficient scaling through sparse expert activation, but often suffer from suboptimal routing decisions due to distribution shifts in deployment. While existing test-time adaptation methods could potentially address these issues, they primarily focus on dense models and require access to external data, limiting their practical applicability to MoE architectures. However, we find that, instead of relying on reference data, we can optimize MoE expert selection on-the-fly based only on input context. As such, we propose \\textit{a data-free, online test-time framework} that continuously adapts MoE routing decisions during text generation without external supervision or data. Our method cycles between two phases: During the prefill stage, and later in regular intervals, we optimize the routing decisions of the model using self-supervision based on the already generated sequence. Then, we generate text as normal, maintaining the modified router until the next adaption. We implement this through lightweight additive vectors that only update router logits in selected layers, maintaining computational efficiency while preventing over-adaptation. The experimental results show consistent performance gains on challenging reasoning tasks while maintaining robustness to context shifts. For example, our method achieves a 5.5\\% improvement on HumanEval with OLMoE. Furthermore, owing to its plug-and-play property, our method naturally complements existing test-time scaling techniques, e.g., achieving 6\\% average gains when incorporated with self-consistency on DeepSeek-V2-Lite.",
        "tags": [
            "DeepSeek",
            "MoE"
        ]
    },
    {
        "id": 23,
        "title": "Do LLMs \"Feel\"? Emotion Circuits Discovery and Control",
        "author": [
            "Chenxi Wang",
            "Yixuan Zhang",
            "Ruiji Yu",
            "Yufei Zheng",
            "Lang Gao",
            "Zirui Song",
            "Zixiang Xu",
            "Gus Xia",
            "Huishuai Zhang",
            "Dongyan Zhao",
            "Xiuying Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11328",
        "abstract": "As the demand for emotional intelligence in large language models (LLMs) grows, a key challenge lies in understanding the internal mechanisms that give rise to emotional expression and in controlling emotions in generated text. This study addresses three core questions: (1) Do LLMs contain context-agnostic mechanisms shaping emotional expression? (2) What form do these mechanisms take? (3) Can they be harnessed for universal emotion control? We first construct a controlled dataset, SEV (Scenario-Event with Valence), to elicit comparable internal states across emotions. Subsequently, we extract context-agnostic emotion directions that reveal consistent, cross-context encoding of emotion (Q1). We identify neurons and attention heads that locally implement emotional computation through analytical decomposition and causal analysis, and validate their causal roles via ablation and enhancement interventions. Next, we quantify each sublayer's causal influence on the model's final emotion representation and integrate the identified local components into coherent global emotion circuits that drive emotional expression (Q2). Directly modulating these circuits achieves 99.65% emotion-expression accuracy on the test set, surpassing prompting- and steering-based methods (Q3). To our knowledge, this is the first systematic study to uncover and validate emotion circuits in LLMs, offering new insights into interpretability and controllable emotional intelligence.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 24,
        "title": "Robust Layerwise Scaling Rules by Proper Weight Decay Tuning",
        "author": [
            "Zhiyuan Fan",
            "Yifeng Liu",
            "Qingyue Zhao",
            "Angela Yuan",
            "Quanquan Gu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15262",
        "abstract": "Empirical scaling laws prescribe how to allocate parameters, data, and compute, while maximal-update parameterization ($\\mu$P) enables learning-rate transfer across widths by equalizing early-time update magnitudes. However, in modern scale-invariant architectures, training quickly enters an optimizer-governed steady state where normalization layers create backward scale sensitivity and the effective learning rate becomes width dependent, degrading $\\mu$P transfer. We address this by introducing a weight-decay scaling rule for AdamW that preserves sublayer gain across widths. Empirically, the singular-value spectrum of each matrix parameter scales in norm as $\\sqrt{\\eta/\\lambda}$ with an approximately invariant shape; under width scaling $d$, we observe that the top singular value scales approximately as $\\sqrt{\\eta/\\lambda}\\cdot d^{0.75}$. Combining this observation with the $\\mu$P learning-rate rule $\\eta_2\\propto d^{-1}$ for matrix-like parameters implies an empirical weight-decay scaling rule $\\lambda_2\\propto \\sqrt{d}$ that approximately keeps sublayer gains width invariant. Together with vector-like parameters trained at $\\eta_1=\\Theta_d(1)$ and $\\lambda_1=0$, this yields \\emph{zero-shot} transfer of both learning rate and weight decay from proxy to target widths, removing per-width sweeps. We validate the rule on LLaMA-style Transformers and in a minimal synthetic setting, and we provide a simple diagnostic, matching top singular values, to check sublayer-gain invariance. Our results extend $\\mu$P beyond the near-init regime by explicitly controlling steady-state scales set by the optimizer, offering a practical recipe for width-robust hyperparameter transfer under AdamW.",
        "tags": [
            "LLaMA"
        ]
    },
    {
        "id": 25,
        "title": "ERGO: Entropy-guided Resetting for Generation Optimization in Multi-turn Language Models",
        "author": [
            "Haziq Mohammad Khalid",
            "Athikash Jeyaganthan",
            "Timothy Do",
            "Yicheng Fu",
            "Sean O'Brien",
            "Vasu Sharma",
            "Kevin Zhu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14077",
        "abstract": "Large Language Models (LLMs) suffer significant performance degradation in multi-turn conversations when information is presented incrementally. Given that multi-turn conversations characterize everyday interactions with LLMs, this degradation poses a severe challenge to real world usability. We hypothesize that abrupt increases in model uncertainty signal misalignment in multi-turn LLM interactions, and we exploit this insight to dynamically realign conversational context. We introduce ERGO (Entropy-guided Resetting for Generation Optimization), which continuously quantifies internal uncertainty via Shannon entropy over next token distributions and triggers adaptive prompt consolidation when a sharp spike in entropy is detected. By treating uncertainty as a first class signal rather than a nuisance to eliminate, ERGO embraces variability in language and modeling, representing and responding to uncertainty. In multi-turn tasks with incrementally revealed instructions, ERGO yields a 56.6% average performance gain over standard baselines, increases aptitude (peak performance capability) by 24.7%, and decreases unreliability (variability in performance) by 35.3%, demonstrating that uncertainty aware interventions can improve both accuracy and reliability in conversational AI.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 26,
        "title": "DriveGen3D: Boosting Feed-Forward Driving Scene Generation with Efficient Video Diffusion",
        "author": [
            "Weijie Wang",
            "Jiagang Zhu",
            "Zeyu Zhang",
            "Xiaofeng Wang",
            "Zheng Zhu",
            "Guosheng Zhao",
            "Chaojun Ni",
            "Haoxiao Wang",
            "Guan Huang",
            "Xinze Chen",
            "Yukun Zhou",
            "Wenkang Qin",
            "Duochao Shi",
            "Haoyun Li",
            "Guanghong Jia",
            "Jiwen Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15264",
        "abstract": "We present DriveGen3D, a novel framework for generating high-quality and highly controllable dynamic 3D driving scenes that addresses critical limitations in existing methodologies. Current approaches to driving scene synthesis either suffer from prohibitive computational demands for extended temporal generation, focus exclusively on prolonged video synthesis without 3D representation, or restrict themselves to static single-scene reconstruction. Our work bridges this methodological gap by integrating accelerated long-term video generation with large-scale dynamic scene reconstruction through multimodal conditional control. DriveGen3D introduces a unified pipeline consisting of two specialized components: FastDrive-DiT, an efficient video diffusion transformer for high-resolution, temporally coherent video synthesis under text and Bird's-Eye-View (BEV) layout guidance; and FastRecon3D, a feed-forward reconstruction module that rapidly builds 3D Gaussian representations across time, ensuring spatial-temporal consistency. Together, these components enable real-time generation of extended driving videos (up to $424\\times800$ at 12 FPS) and corresponding dynamic 3D scenes, achieving SSIM of 0.811 and PSNR of 22.84 on novel view synthesis, all while maintaining parameter efficiency.",
        "tags": [
            "3D",
            "DiT",
            "Diffusion",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": 27,
        "title": "Train a Unified Multimodal Data Quality Classifier with Synthetic Data",
        "author": [
            "Weizhi Wang",
            "Rongmei Lin",
            "Shiyang Li",
            "Colin Lockard",
            "Ritesh Sarkhel",
            "Sanket Lokegaonkar",
            "Jingbo Shang",
            "Xifeng Yan",
            "Nasser Zalmout",
            "Xian Li"
        ],
        "pdf": "https://arxiv.org/pdf/2510.15162",
        "abstract": "The Multimodal Large Language Models (MLLMs) are continually pre-trained on a mixture of image-text caption data and interleaved document data, while the high-quality data filtering towards image-text interleaved document data is under-explored. We propose to train an efficient MLLM as a Unified Mulitmodal Data Quality Classifier to Filter both high-quality image-text caption and interleaved data (UniFilter). To address the challenge of collecting diverse labeled multimodal data, we introduce a semi-synthetic approach that leverages readily available raw images and generates corresponding text across four quality levels. This method enables efficient creation of sample-score pairs for both caption and interleaved document data to train UniFilter. We apply UniFilter to curate high-quality caption data from DataComp caption dataset and interleaved data from the OBELICS image-text interleaved dataset. MLLMs pre-trained on the filtered data demonstrate significantly enhanced capabilities compared to those trained on baseline-filtered data, achieving stronger zero-shot reasoning and in-context learning capabilities. After visual supervised fine-tuning, these UniFilter-induced MLLMs achieve stronger performance on various benchmarks, highlighting the downstream benefits of high-quality multimodal pre-training. We release the synthetic training data used for training UniFilter, the UniFilter model checkpoints, and the high-quality interleaved document subset OBELICS-HQ, curated by UniFilter, to the community for reproduction and further development.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 28,
        "title": "Adapting Self-Supervised Representations as a Latent Space for Efficient Generation",
        "author": [
            "Ming Gui",
            "Johannes Schusterbauer",
            "Timy Phan",
            "Felix Krause",
            "Josh Susskind",
            "Miguel Angel Bautista",
            "BjÃ¶rn Ommer"
        ],
        "pdf": "https://arxiv.org/pdf/2510.14630",
        "abstract": "We introduce Representation Tokenizer (RepTok), a generative modeling framework that represents an image using a single continuous latent token obtained from self-supervised vision transformers. Building on a pre-trained SSL encoder, we fine-tune only the semantic token embedding and pair it with a generative decoder trained jointly using a standard flow matching objective. This adaptation enriches the token with low-level, reconstruction-relevant details, enabling faithful image reconstruction. To preserve the favorable geometry of the original SSL space, we add a cosine-similarity loss that regularizes the adapted token, ensuring the latent space remains smooth and suitable for generation. Our single-token formulation resolves spatial redundancies of 2D latent spaces and significantly reduces training costs. Despite its simplicity and efficiency, RepTok achieves competitive results on class-conditional ImageNet generation and naturally extends to text-to-image synthesis, reaching competitive zero-shot performance on MS-COCO under extremely limited training budgets. Our findings highlight the potential of fine-tuned SSL representations as compact and effective latent spaces for efficient generative modeling.",
        "tags": [
            "Flow Matching",
            "Text-to-Image"
        ]
    }
]