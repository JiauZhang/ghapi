[
    {
        "id": 1,
        "index": 42,
        "title": "dLLM: Simple Diffusion Language Modeling",
        "author": [
            "Zhanhui Zhou",
            "Lingjie Chen",
            "Hanghang Tong",
            "Dawn Song"
        ],
        "pdf": "https://arxiv.org/pdf/2602.22661",
        "abstract": "Although diffusion language models (DLMs) are evolving quickly, many recent models converge on a set of shared components. These components, however, are distributed across ad-hoc research codebases or lack transparent implementations, making them difficult to reproduce or extend. As the field accelerates, there is a clear need for a unified framework that standardizes these common components while remaining flexible enough to support new methods and architectures.\nTo address this gap, we introduce dLLM, an open-source framework that unifies the core components of diffusion language modeling -- training, inference, and evaluation -- and makes them easy to customize for new designs. With dLLM, users can reproduce, finetune, deploy, and evaluate open-source large DLMs such as LLaDA and Dream through a standardized pipeline. The framework also provides minimal, reproducible recipes for building small DLMs from scratch with accessible compute, including converting any BERT-style encoder or autoregressive LM into a DLM. We also release the checkpoints of these small DLMs to make DLMs more accessible and accelerate future research.",
        "tags": [
            "BERT",
            "Diffusion"
        ]
    },
    {
        "id": 2,
        "index": 34,
        "title": "Enhancing Spatial Understanding in Image Generation via Reward Modeling",
        "author": [
            "Zhenyu Tang",
            "Chaoran Feng",
            "Yufan Deng",
            "Jie Wu",
            "Xiaojie Li",
            "Rui Wang",
            "Yunpeng Chen",
            "Daquan Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2602.24233",
        "abstract": "Recent progress in text-to-image generation has greatly advanced visual fidelity and creativity, but it has also imposed higher demands on prompt complexity-particularly in encoding intricate spatial relationships. In such cases, achieving satisfactory results often requires multiple sampling attempts. To address this challenge, we introduce a novel method that strengthens the spatial understanding of current image generation models. We first construct the SpatialReward-Dataset with over 80k preference pairs. Building on this dataset, we build SpatialScore, a reward model designed to evaluate the accuracy of spatial relationships in text-to-image generation, achieving performance that even surpasses leading proprietary models on spatial evaluation. We further demonstrate that this reward model effectively enables online reinforcement learning for the complex spatial generation. Extensive experiments across multiple benchmarks show that our specialized reward model yields significant and consistent gains in spatial understanding for image generation.",
        "tags": [
            "RL",
            "Text-to-Image"
        ]
    },
    {
        "id": 3,
        "index": 33,
        "title": "Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets",
        "author": [
            "Hanna Yukhymenko",
            "Anton Alexandrov",
            "Martin Vechev"
        ],
        "pdf": "https://arxiv.org/pdf/2602.22207",
        "abstract": "The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 4,
        "index": 16,
        "title": "CUDA Agent: Large-Scale Agentic RL for High-Performance CUDA Kernel Generation",
        "author": [
            "Weinan Dai",
            "Hanlin Wu",
            "Qiying Yu",
            "Huan-ang Gao",
            "Jiahao Li",
            "Chengquan Jiang",
            "Weiqiang Lou",
            "Yufan Song",
            "Hongli Yu",
            "Jiaze Chen",
            "Wei-Ying Ma",
            "Ya-Qin Zhang",
            "Jingjing Liu",
            "Mingxuan Wang",
            "Xin Liu",
            "Hao Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2602.24286",
        "abstract": "GPU kernel optimization is fundamental to modern deep learning but remains a highly specialized task requiring deep hardware expertise. Despite strong performance in general programming, large language models (LLMs) remain uncompetitive with compiler-based systems such as http://torch.compile for CUDA kernel generation. Existing CUDA code generation approaches either rely on training-free refinement or fine-tune models within fixed multi-turn execution-feedback loops, but both paradigms fail to fundamentally improve the model's intrinsic CUDA optimization ability, resulting in limited performance gains. We present CUDA Agent, a large-scale agentic reinforcement learning system that develops CUDA kernel expertise through three components: a scalable data synthesis pipeline, a skill-augmented CUDA development environment with automated verification and profiling to provide reliable reward signals, and reinforcement learning algorithmic techniques enabling stable training. CUDA Agent achieves state-of-the-art results on KernelBench, delivering 100\\%, 100\\%, and 92\\% faster rate over http://torch.compile on KernelBench Level-1, Level-2, and Level-3 splits, outperforming the strongest proprietary models such as Claude Opus 4.5 and Gemini 3 Pro by about 40\\% on the hardest Level-3 setting.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": 5,
        "index": 15,
        "title": "Mode Seeking meets Mean Seeking for Fast Long Video Generation",
        "author": [
            "Shengqu Cai",
            "Weili Nie",
            "Chao Liu",
            "Julius Berner",
            "Lvmin Zhang",
            "Nanye Ma",
            "Hansheng Chen",
            "Maneesh Agrawala",
            "Leonidas Guibas",
            "Gordon Wetzstein",
            "Arash Vahdat"
        ],
        "pdf": "https://arxiv.org/pdf/2602.24289",
        "abstract": "Scaling video generation from seconds to minutes faces a critical bottleneck: while short-video data is abundant and high-fidelity, coherent long-form data is scarce and limited to narrow domains. To address this, we propose a training paradigm where Mode Seeking meets Mean Seeking, decoupling local fidelity from long-term coherence based on a unified representation via a Decoupled Diffusion Transformer. Our approach utilizes a global Flow Matching head trained via supervised learning on long videos to capture narrative structure, while simultaneously employing a local Distribution Matching head that aligns sliding windows to a frozen short-video teacher via a mode-seeking reverse-KL divergence. This strategy enables the synthesis of minute-scale videos that learns long-range coherence and motions from limited long videos via supervised flow matching, while inheriting local realism by aligning every sliding-window segment of the student to a frozen short-video teacher, resulting in a few-step fast long video generator. Evaluations show that our method effectively closes the fidelity-horizon gap by jointly improving local sharpness, motion and long-range consistency. Project website: https://primecai.github.io/mmm/.",
        "tags": [
            "DiT",
            "Diffusion",
            "Flow Matching",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": 6,
        "index": 9,
        "title": "CiteAudit: You Cited It, But Did You Read It? A Benchmark for Verifying Scientific References in the LLM Era",
        "author": [
            "Zhengqing Yuan",
            "Kaiwen Shi",
            "Zheyuan Zhang",
            "Lichao Sun",
            "Nitesh V. Chawla",
            "Yanfang Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2602.23452",
        "abstract": "Scientific research relies on accurate citation for attribution and integrity, yet large language models (LLMs) introduce a new risk: fabricated references that appear plausible but correspond to no real publications. Such hallucinated citations have already been observed in submissions and accepted papers at major machine learning venues, exposing vulnerabilities in peer review. Meanwhile, rapidly growing reference lists make manual verification impractical, and existing automated tools remain fragile to noisy and heterogeneous citation formats and lack standardized evaluation. We present the first comprehensive benchmark and detection framework for hallucinated citations in scientific writing. Our multi-agent verification pipeline decomposes citation checking into claim extraction, evidence retrieval, passage matching, reasoning, and calibrated judgment to assess whether a cited source truly supports its claim. We construct a large-scale human-validated dataset across domains and define unified metrics for citation faithfulness and evidence alignment. Experiments with state-of-the-art LLMs reveal substantial citation errors and show that our framework significantly outperforms prior methods in both accuracy and interpretability. This work provides the first scalable infrastructure for auditing citations in the LLM era and practical tools to improve the trustworthiness of scientific references.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": 7,
        "index": 7,
        "title": "Accelerating Masked Image Generation by Learning Latent Controlled Dynamics",
        "author": [
            "Kaiwen Zhu",
            "Quansheng Zeng",
            "Yuandong Pu",
            "Shuo Cao",
            "Xiaohui Li",
            "Yi Xin",
            "Qi Qin",
            "Jiayang Li",
            "Yu Qiao",
            "Jinjin Gu",
            "Yihao Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2602.23996",
        "abstract": "Masked Image Generation Models (MIGMs) have achieved great success, yet their efficiency is hampered by the multiple steps of bi-directional attention. In fact, there exists notable redundancy in their computation: when sampling discrete tokens, the rich semantics contained in the continuous features are lost. Some existing works attempt to cache the features to approximate future features. However, they exhibit considerable approximation error under aggressive acceleration rates. We attribute this to their limited expressivity and the failure to account for sampling information. To fill this gap, we propose to learn a lightweight model that incorporates both previous features and sampled tokens, and regresses the average velocity field of feature evolution. The model has moderate complexity that suffices to capture the subtle dynamics while keeping lightweight compared to the original base model. We apply our method, MIGM-Shortcut, to two representative MIGM architectures and tasks. In particular, on the state-of-the-art Lumina-DiMOO, it achieves over 4x acceleration of text-to-image generation while maintaining quality, significantly pushing the Pareto frontier of masked image generation. The code and model weights are available at https://github.com/Kaiwen-Zhu/MIGM-Shortcut.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": 8,
        "index": 5,
        "title": "Memory Caching: RNNs with Growing Memory",
        "author": [
            "Ali Behrouz",
            "Zeman Li",
            "Yuan Deng",
            "Peilin Zhong",
            "Meisam Razaviyayn",
            "Vahab Mirrokni"
        ],
        "pdf": "https://arxiv.org/pdf/2602.24281",
        "abstract": "Transformers have been established as the de-facto backbones for most recent advances in sequence modeling, mainly due to their growing memory capacity that scales with the context length. While plausible for retrieval tasks, it causes quadratic complexity and so has motivated recent studies to explore viable subquadratic recurrent alternatives. Despite showing promising preliminary results in diverse domains, such recurrent architectures underperform Transformers in recall-intensive tasks, often attributed to their fixed-size memory. In this paper, we introduce Memory Caching (MC), a simple yet effective technique that enhances recurrent models by caching checkpoints of their memory states (a.k.a. hidden states). Memory Caching allows the effective memory capacity of RNNs to grow with sequence length, offering a flexible trade-off that interpolates between the fixed memory (i.e., $O(L)$ complexity) of RNNs and the growing memory (i.e., $O(L^2)$ complexity) of Transformers. We propose four variants of MC, including gated aggregation and sparse selective mechanisms, and discuss their implications on both linear and deep memory modules. Our experimental results on language modeling, and long-context understanding tasks show that MC enhances the performance of recurrent models, supporting its effectiveness. The results of in-context recall tasks indicate that while Transformers achieve the best accuracy, our MC variants show competitive performance, close the gap with Transformers, and performs better than state-of-the-art recurrent models.",
        "tags": []
    },
    {
        "id": 9,
        "index": 5,
        "title": "LK Losses: Direct Acceptance Rate Optimization for Speculative Decoding",
        "author": [
            "Alexander Samarin",
            "Sergei Krutikov",
            "Anton Shevtsov",
            "Sergei Skvortsov",
            "Filipp Fisin",
            "Alexander Golubev"
        ],
        "pdf": "https://arxiv.org/pdf/2602.23881",
        "abstract": "Speculative decoding accelerates autoregressive large language model (LLM) inference by using a lightweight draft model to propose candidate tokens that are then verified in parallel by the target model. The speedup is significantly determined by the acceptance rate, yet standard training minimizes Kullback-Leibler (KL) divergence as a proxy objective. While KL divergence and acceptance rate share the same global optimum, small draft models, having limited capacity, typically converge to suboptimal solutions where minimizing KL does not guarantee maximizing acceptance rate. To address this issue, we propose LK losses, special training objectives that directly target acceptance rate. Comprehensive experiments across four draft architectures and six target models, ranging from 8B to 685B parameters, demonstrate consistent improvements in acceptance metrics across all configurations compared to the standard KL-based training. We evaluate our approach on general, coding and math domains and report gains of up to 8-10% in average acceptance length. LK losses are easy to implement, introduce no computational overhead and can be directly integrated into any existing speculator training framework, making them a compelling alternative to the existing draft training objectives.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 10,
        "index": 4,
        "title": "Compositional Generalization Requires Linear, Orthogonal Representations in Vision Embedding Models",
        "author": [
            "Arnas Uselis",
            "Andrea Dittadi",
            "Seong Joon Oh"
        ],
        "pdf": "https://arxiv.org/pdf/2602.24264",
        "abstract": "Compositional generalization, the ability to recognize familiar parts in novel contexts, is a defining property of intelligent systems. Although modern models are trained on massive datasets, they still cover only a tiny fraction of the combinatorial space of possible inputs, raising the question of what structure representations must have to support generalization to unseen combinations. We formalize three desiderata for compositional generalization under standard training (divisibility, transferability, stability) and show they impose necessary geometric constraints: representations must decompose linearly into per-concept components, and these components must be orthogonal across concepts. This provides theoretical grounding for the Linear Representation Hypothesis: the linear structure widely observed in neural representations is a necessary consequence of compositional generalization. We further derive dimension bounds linking the number of composable concepts to the embedding geometry. Empirically, we evaluate these predictions across modern vision models (CLIP, SigLIP, DINO) and find that representations exhibit partial linear factorization with low-rank, near-orthogonal per-concept factors, and that the degree of this structure correlates with compositional generalization on unseen combinations. As models continue to scale, these conditions predict the representational geometry they may converge to. Code is available at https://github.com/oshapio/necessary-compositionality.",
        "tags": [
            "CLIP"
        ]
    },
    {
        "id": 11,
        "index": 4,
        "title": "InfoNCE Induces Gaussian Distribution",
        "author": [
            "Roy Betser",
            "Eyal Gofer",
            "Meir Yossef Levi",
            "Guy Gilboa"
        ],
        "pdf": "https://arxiv.org/pdf/2602.24012",
        "abstract": "Contrastive learning has become a cornerstone of modern representation learning, allowing training with massive unlabeled data for both task-specific and general (foundation) models. A prototypical loss in contrastive training is InfoNCE and its variants. In this work, we show that the InfoNCE objective induces Gaussian structure in representations that emerge from contrastive training. We establish this result in two complementary regimes. First, we show that under certain alignment and concentration assumptions, projections of the high-dimensional representation asymptotically approach a multivariate Gaussian distribution. Next, under less strict assumptions, we show that adding a small asymptotically vanishing regularization term that promotes low feature norm and high feature entropy leads to similar asymptotic results. We support our analysis with experiments on synthetic and CIFAR-10 datasets across multiple encoder architectures and sizes, demonstrating consistent Gaussian behavior. This perspective provides a principled explanation for commonly observed Gaussianity in contrastive representations. The resulting Gaussian model enables principled analytical treatment of learned representations and is expected to support a wide range of applications in contrastive learning.",
        "tags": []
    },
    {
        "id": 12,
        "index": 4,
        "title": "Ref-Adv: Exploring MLLM Visual Reasoning in Referring Expression Tasks",
        "author": [
            "Qihua Dong",
            "Kuo Yang",
            "Lin Ju",
            "Handong Zhao",
            "Yitian Zhang",
            "Yizhou Wang",
            "Huimin Zeng",
            "Jianglin Lu",
            "Yun Fu"
        ],
        "pdf": "https://arxiv.org/pdf/2602.23898",
        "abstract": "Referring Expression Comprehension (REC) links language to region level visual perception. Standard benchmarks (RefCOCO, RefCOCO+, RefCOCOg) have progressed rapidly with multimodal LLMs but remain weak tests of visual reasoning and grounding: (i) many expressions are very short, leaving little reasoning demand; (ii) images often contain few distractors, making the target easy to find; and (iii) redundant descriptors enable shortcut solutions that bypass genuine text understanding and visual reasoning. We introduce Ref-Adv, a modern REC benchmark that suppresses shortcuts by pairing linguistically nontrivial expressions with only the information necessary to uniquely identify the target. The dataset contains referring expressions on real images, curated with hard distractors and annotated with reasoning facets including negation. We conduct comprehensive ablations (word order perturbations and descriptor deletion sufficiency) to show that solving Ref-Adv requires reasoning beyond simple cues, and we evaluate a broad suite of contemporary multimodal LLMs on Ref-Adv. Despite strong results on RefCOCO, RefCOCO+, and RefCOCOg, models drop markedly on Ref-Adv, revealing reliance on shortcuts and gaps in visual reasoning and grounding. We provide an in depth failure analysis and aim for Ref-Adv to guide future work on visual reasoning and grounding in MLLMs.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 13,
        "index": 3,
        "title": "SenCache: Accelerating Diffusion Model Inference via Sensitivity-Aware Caching",
        "author": [
            "Yasaman Haghighi",
            "Alexandre Alahi"
        ],
        "pdf": "https://arxiv.org/pdf/2602.24208",
        "abstract": "Diffusion models achieve state-of-the-art video generation quality, but their inference remains expensive due to the large number of sequential denoising steps. This has motivated a growing line of research on accelerating diffusion inference. Among training-free acceleration methods, caching reduces computation by reusing previously computed model outputs across timesteps. Existing caching methods rely on heuristic criteria to choose cache/reuse timesteps and require extensive tuning. We address this limitation with a principled sensitivity-aware caching framework. Specifically, we formalize the caching error through an analysis of the model output sensitivity to perturbations in the denoising inputs, i.e., the noisy latent and the timestep, and show that this sensitivity is a key predictor of caching error. Based on this analysis, we propose Sensitivity-Aware Caching (SenCache), a dynamic caching policy that adaptively selects caching timesteps on a per-sample basis. Our framework provides a theoretical basis for adaptive caching, explains why prior empirical heuristics can be partially effective, and extends them to a dynamic, sample-specific approach. Experiments on Wan 2.1, CogVideoX, and LTX-Video show that SenCache achieves better visual quality than existing caching methods under similar computational budgets.",
        "tags": [
            "CogVideo",
            "Diffusion",
            "Video Generation"
        ]
    },
    {
        "id": 14,
        "index": 2,
        "title": "LongVideo-R1: Smart Navigation for Low-cost Long Video Understanding",
        "author": [
            "Jihao Qiu",
            "Lingxi Xie",
            "Xinyue Huo",
            "Qi Tian",
            "Qixiang Ye"
        ],
        "pdf": "https://arxiv.org/pdf/2602.20913",
        "abstract": "This paper addresses the critical and underexplored challenge of long video understanding with low computational budgets. We propose LongVideo-R1, an active, reasoning-equipped multimodal large language model (MLLM) agent designed for efficient video context navigation, avoiding the redundancy of exhaustive search. At the core of LongVideo-R1 lies a reasoning module that leverages high-level visual cues to infer the most informative video clip for subsequent processing. During inference, the agent initiates traversal from top-level visual summaries and iteratively refines its focus, immediately halting the exploration process upon acquiring sufficient knowledge to answer the query. To facilitate training, we first extract hierarchical video captions from CGBench, a video corpus with grounding annotations, and guide GPT-5 to generate 33K high-quality chain-of-thought-with-tool trajectories. The LongVideo-R1 agent is fine-tuned upon the Qwen-3-8B model through a two-stage paradigm: supervised fine-tuning (SFT) followed by reinforcement learning (RL), where RL employs a specifically designed reward function to maximize selective and efficient clip navigation. Experiments on multiple long video benchmarks validate the effectiveness of name, which enjoys superior tradeoff between QA accuracy and efficiency. All curated data and source code are provided in the supplementary material and will be made publicly available. Code and data are available at: https://github.com/qiujihao19/LongVideo-R1",
        "tags": [
            "CLIP",
            "CoT",
            "GPT",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": 15,
        "index": 1,
        "title": "Vectorizing the Trie: Efficient Constrained Decoding for LLM-based Generative Retrieval on Accelerators",
        "author": [
            "Zhengyang Su",
            "Isay Katsman",
            "Yueqi Wang",
            "Ruining He",
            "Lukasz Heldt",
            "Raghunandan Keshavan",
            "Shao-Chuan Wang",
            "Xinyang Yi",
            "Mingyan Gao",
            "Onkar Dalal",
            "Lichan Hong",
            "Ed Chi",
            "Ningren Han"
        ],
        "pdf": "https://arxiv.org/pdf/2602.22647",
        "abstract": "Generative retrieval has emerged as a powerful paradigm for LLM-based recommendation. However, industrial recommender systems often benefit from restricting the output space to a constrained subset of items based on business logic (e.g. enforcing content freshness or product category), which standard autoregressive decoding cannot natively support. Moreover, existing constrained decoding methods that make use of prefix trees (Tries) incur severe latency penalties on hardware accelerators (TPUs/GPUs). In this work, we introduce STATIC (Sparse Transition Matrix-Accelerated Trie Index for Constrained Decoding), an efficient and scalable constrained decoding technique designed specifically for high-throughput LLM-based generative retrieval on TPUs/GPUs. By flattening the prefix tree into a static Compressed Sparse Row (CSR) matrix, we transform irregular tree traversals into fully vectorized sparse matrix operations, unlocking massive efficiency gains on hardware accelerators. We deploy STATIC on a large-scale industrial video recommendation platform serving billions of users. STATIC produces significant product metric impact with minimal latency overhead (0.033 ms per step and 0.25% of inference time), achieving a 948x speedup over a CPU trie implementation and a 47-1033x speedup over a hardware-accelerated binary-search baseline. Furthermore, the runtime overhead of STATIC remains extremely low across a wide range of practical configurations. To the best of our knowledge, STATIC enables the first production-scale deployment of strictly constrained generative retrieval. In addition, evaluation on academic benchmarks demonstrates that STATIC can considerably improve cold-start performance for generative retrieval. Our code is available at https://github.com/youtube/static-constraint-decoding.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 16,
        "index": 0,
        "title": "DLEBench: Evaluating Small-scale Object Editing Ability for Instruction-based Image Editing Model",
        "author": [
            "Shibo Hong",
            "Boxian Ai",
            "Jun Kuang",
            "Wei Wang",
            "FengJiao Chen",
            "Zhongyuan Peng",
            "Chenhao Huang",
            "Yixin Cao"
        ],
        "pdf": "https://arxiv.org/pdf/2602.23622",
        "abstract": "Significant progress has been made in the field of Instruction-based Image Editing Models (IIEMs). However, while these models demonstrate plausible adherence to instructions and strong reasoning ability on current benchmarks, their ability to edit small objects remains underexplored, despite its importance for precise local editing and refining details in both real and generated images. In this paper, we introduce DeepLookEditBench (DLEBench), the first benchmark dedicated to assessing the abilities of IIEMs in editing small-scale objects. Specifically, we construct a challenging testbed comprising 1889 samples across seven instruction types. In these samples, target objects occupy only 1%-10% of the image area, covering complex scenarios such as partial occlusion and multi-object editing. To ensure robust evaluation on this benchmark, we propose an evaluation protocol with refined score rubrics to minimize subjectivity and ambiguity in two criteria: Instruction Following and Visual Consistency. This protocol also introduces a dual-mode evaluation framework (Tool-driven and Oracle-guided Modes) addressing the misalignment between LMM-as-a-Judge and human judgements on DLEBench. Empirical results on 10 IIEMs reveal significant performance gaps in small-scale object editing, highlighting the need for specialized benchmarks to advance this ability.",
        "tags": [
            "Image Editing"
        ]
    }
]