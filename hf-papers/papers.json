[
    {
        "id": 1,
        "title": "QeRL: Beyond Efficiency -- Quantization-enhanced Reinforcement Learning for LLMs",
        "author": [
            "Wei Huang",
            "Yi Ge",
            "Shuai Yang",
            "Yicheng Xiao",
            "Huizi Mao",
            "Yujun Lin",
            "Hanrong Ye",
            "Sifei Liu",
            "Ka Chun Cheung",
            "Hongxu Yin",
            "Yao Lu",
            "Xiaojuan Qi",
            "Song Han",
            "Yukang Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11696",
        "abstract": "We propose QeRL, a Quantization-enhanced Reinforcement Learning framework for large language models (LLMs). While RL is essential for LLMs' reasoning capabilities, it is resource-intensive, requiring substantial GPU memory and long rollout durations. QeRL addresses these issues by combining NVFP4 quantization with Low-Rank Adaptation (LoRA), accelerating rollout phase of RL while reducing memory overhead. Beyond efficiency, our findings show that quantization noise increases policy entropy, enhancing exploration, and enabling the discovery of better strategies during RL. To further optimize exploration, QeRL introduces an Adaptive Quantization Noise (AQN) mechanism, which dynamically adjusts noise during training. Experiments demonstrate that QeRL delivers over 1.5 times speedup in the rollout phase. Moreover, this is the first framework to enable RL training of a 32B LLM on a single H100 80GB GPU, while delivering overall speedups for RL training. It also achieves faster reward growth and higher final accuracy than 16-bit LoRA and QLoRA, while matching the performance of full-parameter fine-tuning on mathematical benchmarks such as GSM8K (90.8%) and MATH 500 (77.4%) in the 7B model. These results establish QeRL as an efficient and effective framework for RL training in LLMs.",
        "tags": [
            "LLM",
            "LoRA",
            "RL"
        ]
    },
    {
        "id": 2,
        "title": "Diffusion Transformers with Representation Autoencoders",
        "author": [
            "Boyang Zheng",
            "Nanye Ma",
            "Shengbang Tong",
            "Saining Xie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11690",
        "abstract": "Latent generative modeling, where a pretrained autoencoder maps pixels into a latent space for the diffusion process, has become the standard strategy for Diffusion Transformers (DiT); however, the autoencoder component has barely evolved. Most DiTs continue to rely on the original VAE encoder, which introduces several limitations: outdated backbones that compromise architectural simplicity, low-dimensional latent spaces that restrict information capacity, and weak representations that result from purely reconstruction-based training and ultimately limit generative quality. In this work, we explore replacing the VAE with pretrained representation encoders (e.g., DINO, SigLIP, MAE) paired with trained decoders, forming what we term Representation Autoencoders (RAEs). These models provide both high-quality reconstructions and semantically rich latent spaces, while allowing for a scalable transformer-based architecture. Since these latent spaces are typically high-dimensional, a key challenge is enabling diffusion transformers to operate effectively within them. We analyze the sources of this difficulty, propose theoretically motivated solutions, and validate them empirically. Our approach achieves faster convergence without auxiliary representation alignment losses. Using a DiT variant equipped with a lightweight, wide DDT head, we achieve strong image generation results on ImageNet: 1.51 FID at 256x256 (no guidance) and 1.13 at both 256x256 and 512x512 (with guidance). RAE offers clear advantages and should be the new default for diffusion transformer training.",
        "tags": [
            "DiT",
            "Diffusion",
            "Transformer",
            "VAE"
        ]
    },
    {
        "id": 3,
        "title": "OmniVideoBench: Towards Audio-Visual Understanding Evaluation for Omni MLLMs",
        "author": [
            "Caorui Li",
            "Yu Chen",
            "Yiyan Ji",
            "Jin Xu",
            "Zhenyu Cui",
            "Shihao Li",
            "Yuanxing Zhang",
            "Jiafu Tang",
            "Zhenghao Song",
            "Dingling Zhang",
            "Ying He",
            "Haoxiang Liu",
            "Yuxuan Wang",
            "Qiufeng Wang",
            "Zhenhe Wu",
            "Jiehui Luo",
            "Zhiyu Pan",
            "Weihao Xie",
            "Chenchen Zhang",
            "Zhaohui Wang",
            "Jiayi Tian",
            "Yanghai Wang",
            "Zhe Cao",
            "Minxin Dai",
            "Ke Wang",
            "Runzhe Wen",
            "Yinghao Ma",
            "Yaning Pan",
            "Sungkyun Chang",
            "Termeh Taheri",
            "Haiwen Xia",
            "Christos Plachouras",
            "Emmanouil Benetos",
            "Yizhi Li",
            "Ge Zhang",
            "Jian Yang",
            "Tianhao Peng",
            "Zili Wang",
            "Minghao Liu",
            "Junran Peng",
            "Zhaoxiang Zhang",
            "Jiaheng Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10689",
        "abstract": "Recent advances in multimodal large language models (MLLMs) have demonstrated substantial potential in video understanding. However, existing benchmarks fail to comprehensively evaluate synergistic reasoning capabilities across audio and visual modalities, often neglecting either one of the modalities or integrating them in a logically inconsistent manner. To bridge this gap, we introduce OmniVideoBench, a large-scale and rigorously designed benchmark dedicated to assessing synergistic audio-visual understanding, with a strong emphasis on modality complementarity and logical consistency. Specifically, OmniVideoBench comprises 1000 high-quality question-answer(QA) pairs, each annotated with step-by-step reasoning traces, derived from 628 diverse videos ranging from several seconds to 30 minutes, and manually verified to guarantee complete correctness and uniqueness. Moreover, OmniVideoBench encompasses 13 carefully designed question types, covering temporal reasoning, spatial localization, counting, causal inference, summarization, and beyond, thereby capturing the essential challenges of video understanding. Evaluation of multiple MLLMs on OmniVideoBench reveals a pronounced gap between model performance and human reasoning, with open-source models lagging significantly behind their closed-source counterparts, underscoring the inherent difficulty of genuine audio-visual reasoning. We will release OmniVideoBench to foster the development of MLLMs with stronger and more generalizable reasoning capabilities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 4,
        "title": "Latent Refinement Decoding: Enhancing Diffusion-Based Language Models by Refining Belief States",
        "author": [
            "Qinglin Zhu",
            "Yizhen Yao",
            "Runcong Zhao",
            "Yanzheng Xiang",
            "Amrutha Saseendran",
            "Chen Jin",
            "Philip Alexander Teare",
            "Bin Liang",
            "Yulan He",
            "Lin Gui"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11052",
        "abstract": "Autoregressive (AR) models remain the standard for natural language generation but still suffer from high latency due to strictly sequential decoding. Recent diffusion-inspired approaches, such as LlaDA and Dream, mitigate this by generating in parallel, yet they suffer from two core limitations: information loss, as predictive distributions for non-finalized tokens are discarded at each step, and premature commitment, where local decisions are made without sufficient global coordination. We introduce Latent Refinement Decoding (LRD), a two-stage framework with Latent Refinement and a Predictive Feedback Loop. The first stage maintains masked positions as distributional mixtures of predicted tokens and the mask embedding, allowing the model to establish more globally consistent beliefs. The second stage progressively finalizes confident tokens while retaining uncertain ones for iterative feedback. KL-divergence dynamics provide a principled and reliable criterion for convergence and early stopping. Experiments across coding (HumanEval +6.3, MBPP +2.6) and reasoning (GSM8K +2.9, MATH500 +3.8) show that LRD improves accuracy while delivering speedups of up to 10.6x, making it a strong and versatile alternative for parallel sequence generation.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": 5,
        "title": "RLFR: Extending Reinforcement Learning for LLMs with Flow Environment",
        "author": [
            "Jinghao Zhang",
            "Naishan Zheng",
            "Ruilin Li",
            "Dongzhou Cheng",
            "Zheming Liang",
            "Feng Zhao",
            "Jiaqi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10201",
        "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently emerged as a promising framework for improving reasoning abilities in Large Language Models (LLMs). However, policy optimized with binary verification prone to overlook potential valuable exploration in reasoning trajectory. In view of heavy annotation cost of golden Process Reward Models (PRMs), recent works attempt using auxiliary signals for reward shaping of process tokens, involving entropy and likelihood collected from logit space. In this work, we offer a novel perspective on shaping RLVR with flow rewards derived from latent space, and propose RLFR, where the flow fields of model latents are constructed from either off-policy high-quality data and on-policy rejection sampling data, and the velocity deviations of policy latents within it are quantified to serve as a reward signal. RLFR first demonstrates that a well-established flow field can be a sound environment for reward signal collection, highlighting the expressive latent space is much underexplored. Moreover, RLFR is able to compress any off-policy expert data as reference for constituting reward signals, and we show that the efficient context dependence compressed within the hidden states are utilized, rather than individual token-level denotation for context comprehending. Experiments on both language and multimodal reasoning benchmarks demonstrate the reliability of flow rewards, and suggesting a promising paradigm for reward shaping with auxiliary signals.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": 6,
        "title": "Spotlight on Token Perception for Multimodal Reinforcement Learning",
        "author": [
            "Siyuan Huang",
            "Xiaoye Qu",
            "Yafu Li",
            "Yun Luo",
            "Zefeng He",
            "Daizong Liu",
            "Yu Cheng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09285",
        "abstract": "While Reinforcement Learning with Verifiable Rewards (RLVR) has advanced the reasoning capabilities of Large Vision-Language Models (LVLMs), most existing methods in multimodal reasoning neglect the critical role of visual perception within the RLVR optimization process. In this paper, we undertake a pioneering exploration of multimodal RLVR through the novel perspective of token perception, which measures the visual dependency of each generated token. With a granular analysis of Chain-of-Thought (CoT) processes, we uncover two key insights: first, token perception in a rollout trajectory is sparsely distributed, where only a small fraction of tokens have high visual dependency for visually-grounded reasoning; second, different trajectories exhibit significant divergence in their overall visual dependency. Based on these observations, we propose Visually-Perceptive Policy Optimization (VPPO), a novel policy gradient algorithm that explicitly leverages token perception to refine the learning signal. Specifically, VPPO achieves this through a dual mechanism: it reweights a trajectory's advantage by its overall visual dependency, and focuses policy updates exclusively on perceptually pivotal tokens. On a comprehensive suite of eight perception and reasoning benchmarks, VPPO demonstrates substantial gains over leading open-source RL-tuned models, with its effectiveness consistently validated across 7B and 32B model scales. Our findings not only establish a new token-level perceptual perspective for analyzing multimodal RLVR but also present a novel and effective optimization strategy to significantly enhance the multimodal reasoning capabilities of LVLMs.",
        "tags": [
            "CoT",
            "RL",
            "VLM"
        ]
    },
    {
        "id": 7,
        "title": "AVoCaDO: An Audiovisual Video Captioner Driven by Temporal Orchestration",
        "author": [
            "Xinlong Chen",
            "Yue Ding",
            "Weihong Lin",
            "Jingyun Hua",
            "Linli Yao",
            "Yang Shi",
            "Bozhou Li",
            "Yuanxing Zhang",
            "Qiang Liu",
            "Pengfei Wan",
            "Liang Wang",
            "Tieniu Tan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10395",
        "abstract": "Audiovisual video captioning aims to generate semantically rich descriptions with temporal alignment between visual and auditory events, thereby benefiting both video understanding and generation. In this paper, we present AVoCaDO, a powerful audiovisual video captioner driven by the temporal orchestration between audio and visual modalities. We propose a two-stage post-training pipeline: (1) AVoCaDO SFT, which fine-tunes the model on a newly curated dataset of 107K high-quality, temporally-aligned audiovisual captions; and (2) AVoCaDO GRPO, which leverages tailored reward functions to further enhance temporal coherence and dialogue accuracy while regularizing caption length and reducing collapse. Experimental results demonstrate that AVoCaDO significantly outperforms existing open-source models across four audiovisual video captioning benchmarks, and also achieves competitive performance on the VDC and DREAM-1K benchmark under visual-only settings.",
        "tags": [
            "GRPO"
        ]
    },
    {
        "id": 8,
        "title": "DiT360: High-Fidelity Panoramic Image Generation via Hybrid Training",
        "author": [
            "Haoran Feng",
            "Dizhe Zhang",
            "Xiangtai Li",
            "Bo Du",
            "Lu Qi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11712",
        "abstract": "In this work, we propose DiT360, a DiT-based framework that performs hybrid training on perspective and panoramic data for panoramic image generation. For the issues of maintaining geometric fidelity and photorealism in generation quality, we attribute the main reason to the lack of large-scale, high-quality, real-world panoramic data, where such a data-centric view differs from prior methods that focus on model design. Basically, DiT360 has several key modules for inter-domain transformation and intra-domain augmentation, applied at both the pre-VAE image level and the post-VAE token level. At the image level, we incorporate cross-domain knowledge through perspective image guidance and panoramic refinement, which enhance perceptual quality while regularizing diversity and photorealism. At the token level, hybrid supervision is applied across multiple modules, which include circular padding for boundary continuity, yaw loss for rotational robustness, and cube loss for distortion awareness. Extensive experiments on text-to-panorama, inpainting, and outpainting tasks demonstrate that our method achieves better boundary consistency and image fidelity across eleven quantitative metrics. Our code is available at https://github.com/Insta360-Research-Team/DiT360.",
        "tags": [
            "DiT",
            "Inpainting",
            "VAE"
        ]
    },
    {
        "id": 9,
        "title": "Demystifying Reinforcement Learning in Agentic Reasoning",
        "author": [
            "Zhaochen Yu",
            "Ling Yang",
            "Jiaru Zou",
            "Shuicheng Yan",
            "Mengdi Wang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11701",
        "abstract": "Recently, the emergence of agentic RL has showcased that RL could also effectively improve the agentic reasoning ability of LLMs, yet the key design principles and optimal practices remain unclear. In this work, we conduct a comprehensive and systematic investigation to demystify reinforcement learning in agentic reasoning from three key perspectives: data, algorithm, and reasoning mode. We highlight our key insights: (i) Replacing stitched synthetic trajectories with real end-to-end tool-use trajectories yields a far stronger SFT initialization; high-diversity, model-aware datasets sustain exploration and markedly improve RL performance. (ii) Exploration-friendly techniques are crucial for agentic RL, such as clip higher, overlong reward shaping, and maintaining adequate policy entropy could improve the training efficiency. (iii) A deliberative strategy with fewer tool calls outperforms frequent tool calls or verbose self-reasoning, improving tool efficiency and final accuracy. Together, these simple practices consistently enhance agentic reasoning and training efficiency, achieving strong results on challenging benchmarks with smaller models, and establishing a practical baseline for future agentic RL research. Beyond these empirical insights, we further contribute a high-quality, real end-to-end agentic SFT dataset along with a high-quality RL dataset, and demonstrate the effectiveness of our insights in boosting the agentic reasoning ability of LLMs across four challenging benchmarks, including AIME2024/AIME2025, GPQA-Diamond, and LiveCodeBench-v6. With our recipes, 4B-sized models could also achieve superior agentic reasoning performance compared to 32B-sized models. Code and models: https://github.com/Gen-Verse/Open-AgentRL",
        "tags": [
            "CLIP",
            "LLM",
            "RL"
        ]
    },
    {
        "id": 10,
        "title": "Making Mathematical Reasoning Adaptive",
        "author": [
            "Zhejian Lai",
            "Xiang Geng",
            "Zhijun Wang",
            "Yang Bai",
            "Jiahuan Li",
            "Rongxiang Weng",
            "Jingang Wang",
            "Xuezhi Cao",
            "Xunliang Cai",
            "Shujian Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.04617",
        "abstract": "Mathematical reasoning is a primary indicator of large language models (LLMs) intelligence. However, existing LLMs exhibit failures of robustness and generalization. This paper attributes these deficiencies to spurious reasoning, i.e., producing answers from superficial features. To address this challenge, we propose the AdaR framework to enable adaptive reasoning, wherein models rely on problem-solving logic to produce answers. AdaR synthesizes logically equivalent queries by varying variable values, and trains models with RLVR on these data to penalize spurious logic while encouraging adaptive logic. To improve data quality, we extract the problem-solving logic from the original query and generate the corresponding answer by code execution, then apply a sanity check. Experimental results demonstrate that AdaR improves robustness and generalization, achieving substantial improvement in mathematical reasoning while maintaining high data efficiency. Analysis indicates that data synthesis and RLVR function in a coordinated manner to enable adaptive reasoning in LLMs. Subsequent analyses derive key design insights into the effect of critical factors and the applicability to instruct LLMs. Our project is available at https://github.com/NJUNLP/AdaR.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 11,
        "title": "Building a Foundational Guardrail for General Agentic Systems via Synthetic Data",
        "author": [
            "Yue Huang",
            "Hang Hua",
            "Yujun Zhou",
            "Pengcheng Jing",
            "Manish Nagireddy",
            "Inkit Padhi",
            "Greta Dolcetti",
            "Zhangchen Xu",
            "Subhajit Chaudhury",
            "Ambrish Rawat",
            "Liubov Nedoshivina",
            "Pin-Yu Chen",
            "Prasanna Sattigeri",
            "Xiangliang Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09781",
        "abstract": "While LLM agents can plan multi-step tasks, intervening at the planning stage-before any action is executed-is often the safest way to prevent harm, since certain risks can lead to severe consequences once carried out. However, existing guardrails mostly operate post-execution, which is difficult to scale and leaves little room for controllable supervision at the plan level. To address this challenge, we highlight three critical gaps in current research: data gap, model gap, and evaluation gap. To close the data gap, we introduce AuraGen, a controllable engine that (i) synthesizes benign trajectories, (ii) injects category-labeled risks with calibrated difficulty, and (iii) filters outputs via an automated reward model, producing large and reliable corpora for pre-execution safety. To close the guardian model gap, we propose a foundational guardrail Safiron, combining a cross-planner adapter with a compact guardian model. The adapter unifies different input formats, while Safiron flags risky cases, assigns risk types, and generates rationales; trained in two stages with a broadly explored data recipe, Safiron achieves robust transfer across settings. To close the evaluation gap, we release Pre-Exec Bench, a realistic benchmark covering diverse tools and branching trajectories, which measures detection, fine-grained categorization, explanation, and cross-planner generalization in human-verified scenarios. Extensive experiments demonstrate consistent gains of the proposed guardrail over strong baselines on Pre-Exec Bench, and ablations further distill actionable practices, providing a practical template for safer agentic systems.",
        "tags": [
            "Detection",
            "LLM"
        ]
    },
    {
        "id": 12,
        "title": "InternSVG: Towards Unified SVG Tasks with Multimodal Large Language Models",
        "author": [
            "Haomin Wang",
            "Jinhui Yin",
            "Qi Wei",
            "Wenguang Zeng",
            "Lixin Gu",
            "Shenglong Ye",
            "Zhangwei Gao",
            "Yaohui Wang",
            "Yanting Zhang",
            "Yuanqi Li",
            "Yanwen Guo",
            "Wenhai Wang",
            "Kai Chen",
            "Yu Qiao",
            "Hongjie Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11341",
        "abstract": "General SVG modeling remains challenging due to fragmented datasets, limited transferability of methods across tasks, and the difficulty of handling structural complexity. In response, we leverage the strong transfer and generalization capabilities of multimodal large language models (MLLMs) to achieve unified modeling for SVG understanding, editing, and generation. We present the InternSVG family, an integrated data-benchmark-model suite. At its core is SAgoge, the largest and most comprehensive multimodal dataset for SVG tasks, encompassing both static graphics and dynamic animations. It covers icons, long-sequence illustrations, scientific diagrams, and dynamic animations, supporting tasks of varied difficulty levels and providing deeper hierarchies with richer attributes compared to previous datasets. Based on this resource, we introduce SArena, a companion benchmark with comprehensive task definitions and standardized evaluation that aligns with the domains and difficulty spectrum covered by SAgoge. Building on these foundations, we propose InternSVG, a unified MLLM for SVG understanding, editing, and generation with SVG-specific special tokens, subword-based embedding initialization, and a two-stage training strategy that progresses from short static SVGs to long-sequence illustrations and complex animations. This unified formulation induces positive transfer and improves overall performance. Experiments on SArena and prior benchmark confirm that InternSVG achieves substantial gains and consistently outperforms leading open and proprietary counterparts.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 13,
        "title": "BrowserAgent: Building Web Agents with Human-Inspired Web Browsing Actions",
        "author": [
            "Zhengbo Zhang",
            "Zhiheng Lyu",
            "Junhao Gong",
            "Hongzhu Yi",
            "Xinming Wang",
            "Yuxuan Zhou",
            "Jiabing Yang",
            "Ping Nie",
            "Yan Huang",
            "Wenhu Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10666",
        "abstract": "Efficiently solving real-world problems with LLMs increasingly hinges on their ability to interact with dynamic web environments and autonomously acquire external information. While recent research like Search-R1 and WebDancer demonstrates strong performance in solving web tasks, they heavily rely on additional tools to convert the interactive web environment into static text content. This is in contrast to human browsing behaviors, which involve diverse interactions with the browser, such as scrolling, clicking, and typing. In this paper, we propose BrowserAgent, a more interactive agent that solves complex tasks through human-inspired browser actions. BrowserAgent operates directly on raw web pages via Playwright through a set of predefined browser actions. We adopt a two-stage training (Supervised Fine-Tuning (SFT) and Rejection Fine-Tuning (RFT)) to improve the model's generalization abilities. Despite using significantly less training data than Search-R1, BrowserAgent achieves more competitive results across different Open-QA tasks. Additionally, we introduce an explicit memory mechanism to store key conclusions across steps, further enhancing the model's reasoning capabilities for long-horizon tasks. Notably, BrowserAgent-7B can achieve around 20\\% improvement over Search-R1 on multi-hop QA tasks like HotpotQA, 2Wiki, and Bamboogle. These results indicate that BrowserAgent can serve as a more advanced framework for more interactive and scalable web agents.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 14,
        "title": "ACADREASON: Exploring the Limits of Reasoning Models with Academic Research Problems",
        "author": [
            "Xin Gui",
            "King Zhu",
            "JinCheng Ren",
            "Qianben Chen",
            "Zekun Moore Wang",
            "Yizhi LI",
            "Xinpeng Liu",
            "Xiaowan Li",
            "Wenli Ren",
            "Linyu Miao",
            "Tianrui Qin",
            "Ziqi Shu",
            "He Zhu",
            "Xiangru Tang",
            "Dingfeng Shi",
            "Jiaheng Liu",
            "Yuchen Eleanor Jiang",
            "Minghao Liu",
            "Ge Zhang",
            "Wangchunshu Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11652",
        "abstract": "In recent years, the research focus of large language models (LLMs) and agents has shifted increasingly from demonstrating novel capabilities to complex reasoning and tackling challenging tasks. However, existing evaluations focus mainly on math/code contests or general tasks, while existing multi-domain academic benchmarks lack sufficient reasoning depth, leaving the field without a rigorous benchmark for high-level reasoning. To fill this gap, we introduce the Acadreason benchmark, designed to evaluate the ability of LLMs and agents to acquire and reason over academic knowledge. It consists of 50 expert-annotated academic problems across five high-reasoning domains, including computer science, economics, law, mathematics, and philosophy. All questions are sourced from top-tier publications in recent years and undergo rigorous annotation and quality control to ensure they are both challenging and answerable. We conduct systematic evaluations of over 10 mainstream LLMs and agents. The results show that most LLMs scored below 20 points, with even the cutting-edge GPT-5 achieving only 16 points. While agents achieved higher scores, none exceeded 40 points. This demonstrates the current capability gap between LLMs and agents in super-intelligent academic research tasks and highlights the challenges of Acadreason.",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": 15,
        "title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs",
        "author": [
            "Yan Wang",
            "Keyi Wang",
            "Shanshan Yang",
            "Jaisal Patel",
            "Jeff Zhao",
            "Fengran Mo",
            "Xueqing Peng",
            "Lingfei Qian",
            "Jimin Huang",
            "Guojun Xiong",
            "Xiao-Yang Liu",
            "Jian-Yun Nie"
        ],
        "pdf": "https://arxiv.org/pdf/2510.08886",
        "abstract": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 16,
        "title": "DocReward: A Document Reward Model for Structuring and Stylizing",
        "author": [
            "Junpeng Liu",
            "Yuzhong Zhao",
            "Bowen Cao",
            "Jiayu Ding",
            "Yilin Jia",
            "Tengchao Lv",
            "Yupan Huang",
            "Shaohan Huang",
            "Nan Yang",
            "Li Dong",
            "Lei Cui",
            "Tao Ge",
            "Xun Wang",
            "Huitian Jiao",
            "Sun Mao",
            "FNU Kartik",
            "Si-Qing Chen",
            "Wai Lam",
            "Furu Wei"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11391",
        "abstract": "Recent advances in agentic workflows have enabled the automation of tasks such as professional document generation. However, they primarily focus on textual quality, neglecting visual structure and style, which are crucial for readability and engagement. This gap arises mainly from the absence of suitable reward models to guide agentic workflows toward producing documents with stronger structural and stylistic quality. To address this, we propose DocReward, a document reward model that evaluates documents based on their structure and style. We construct a multi-domain dataset DocPair of 117K paired documents, covering 32 domains and 267 document types, each including a high- and low-professionalism document with identical content but different structure and style. This enables the model to evaluate professionalism comprehensively, and in a textual-quality-agnostic way. DocReward is trained using the Bradley-Terry loss to score documents, penalizing predictions that contradict the annotated ranking. To assess the performance of reward models, we create a test dataset containing document bundles ranked by well-educated human evaluators. Notably, DocReward outperforms GPT-4o and GPT-5 in accuracy by 30.6 and 19.4 percentage points, respectively, demonstrating its superiority over baselines. In an extrinsic evaluation of document generation, DocReward achieves a significantly higher win rate of 60.8%, compared to GPT-5's 37.7% win rate, demonstrating its utility in guiding generation agents toward producing human-preferred documents.",
        "tags": [
            "GPT"
        ]
    },
    {
        "id": 17,
        "title": "Don't Just Fine-tune the Agent, Tune the Environment",
        "author": [
            "Siyuan Lu",
            "Zechuan Wang",
            "Hongxuan Zhang",
            "Qintong Wu",
            "Leilei Gan",
            "Chenyi Zhuang",
            "Jinjie Gu",
            "Tao Lin"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10197",
        "abstract": "Large Language Model (LLM) agents show great promise for complex, multi-turn tool-use tasks, but their development is often hampered by the extreme scarcity of high-quality training data. Supervised fine-tuning (SFT) on synthetic data leads to overfitting, whereas standard reinforcement learning (RL) struggles with a critical cold-start problem and training instability. To address these challenges, we introduce $\\textbf{Environment Tuning}$, a novel training paradigm that enables agents to learn complex behaviors directly from problem instances without relying on pre-collected expert trajectories. $\\textbf{Environment Tuning}$ orchestrates this learning process through a structured curriculum, actionable environment augmentation that provides corrective feedback, and fine-grained progress rewards to ensure stable and efficient exploration. Using only 400 problem instances from Berkeley Function-Calling Leaderboard (BFCL) benchmark, our method not only achieves competitive in-distribution performance against strong baselines but also demonstrates superior out-of-distribution generalization, overcoming the performance collapse common to SFT-based approaches. Our work presents a paradigm shift from supervised fine-tuning on static trajectories to dynamic, environment-based exploration, paving the way for training more robust and data-efficient agents.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": 18,
        "title": "GIR-Bench: Versatile Benchmark for Generating Images with Reasoning",
        "author": [
            "Hongxiang Li",
            "Yaowei Li",
            "Bin Lin",
            "Yuwei Niu",
            "Yuhang Yang",
            "Xiaoshuang Huang",
            "Jiayin Cai",
            "Xiaolong Jiang",
            "Yao Hu",
            "Long Chen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11026",
        "abstract": "Unified multimodal models integrate the reasoning capacity of large language models with both image understanding and generation, showing great promise for advanced multimodal intelligence. However, the community still lacks a rigorous reasoning-centric benchmark to systematically evaluate the alignment between understanding and generation, and their generalization potential in complex visual tasks. To this end, we introduce \\textbf{GIR-Bench}, a comprehensive benchmark that evaluates unified models across three complementary perspectives. Firstly, we investigate understanding-generation consistency (GIR-Bench-UGC), asking whether models can consistently leverage the same knowledge in both understanding and generation tasks. Secondly, we investigate whether models can perform reasoning-centric text-to-image generation that requires applying logical constraints and implicit knowledge to generate faithful visual content (GIR-Bench-T2I). Thirdly, we evaluate whether models can handle multi-step reasoning in editing (GIR-Bench-Edit). For each subset, we carefully design different task-specific evaluation pipelines tailored for each task. This enables fine-grained and interpretable evaluation while mitigating biases from the prevalent MLLM-as-a-Judge paradigm. Extensive ablations over various unified models and generation-only systems have shown that: Although unified models are more capable of reasoning-driven visual tasks, they still exhibit a persistent gap between understanding and generation. The data and code for GIR-Bench are available at \\href{https://hkust-longgroup.github.io/GIR-Bench}{https://hkust-longgroup.github.io/GIR-Bench}.",
        "tags": [
            "LLM",
            "Text-to-Image"
        ]
    },
    {
        "id": 19,
        "title": "AdaViewPlanner: Adapting Video Diffusion Models for Viewpoint Planning in 4D Scenes",
        "author": [
            "Yu Li",
            "Menghan Xia",
            "Gongye Liu",
            "Jianhong Bai",
            "Xintao Wang",
            "Conglang Zhang",
            "Yuxuan Lin",
            "Ruihang Chu",
            "Pengfei Wan",
            "Yujiu Yang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10670",
        "abstract": "Recent Text-to-Video (T2V) models have demonstrated powerful capability in visual simulation of real-world geometry and physical laws, indicating its potential as implicit world models. Inspired by this, we explore the feasibility of leveraging the video generation prior for viewpoint planning from given 4D scenes, since videos internally accompany dynamic scenes with natural viewpoints. To this end, we propose a two-stage paradigm to adapt pre-trained T2V models for viewpoint prediction, in a compatible manner. First, we inject the 4D scene representation into the pre-trained T2V model via an adaptive learning branch, where the 4D scene is viewpoint-agnostic and the conditional generated video embeds the viewpoints visually. Then, we formulate viewpoint extraction as a hybrid-condition guided camera extrinsic denoising process. Specifically, a camera extrinsic diffusion branch is further introduced onto the pre-trained T2V model, by taking the generated video and 4D scene as input. Experimental results show the superiority of our proposed method over existing competitors, and ablation studies validate the effectiveness of our key technical designs. To some extent, this work proves the potential of video generation models toward 4D interaction in real world.",
        "tags": [
            "Diffusion",
            "Text-to-Video",
            "Video Generation"
        ]
    },
    {
        "id": 20,
        "title": "Vlaser: Vision-Language-Action Model with Synergistic Embodied Reasoning",
        "author": [
            "Ganlin Yang",
            "Tianyi Zhang",
            "Haoran Hao",
            "Weiyun Wang",
            "Yibin Liu",
            "Dehui Wang",
            "Guanzhou Chen",
            "Zijian Cai",
            "Junting Chen",
            "Weijie Su",
            "Wengang Zhou",
            "Yu Qiao",
            "Jifeng Dai",
            "Jiangmiao Pang",
            "Gen Luo",
            "Wenhai Wang",
            "Yao Mu",
            "Zhi Hou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11027",
        "abstract": "While significant research has focused on developing embodied reasoning capabilities using Vision-Language Models (VLMs) or integrating advanced VLMs into Vision-Language-Action (VLA) models for end-to-end robot control, few studies directly address the critical gap between upstream VLM-based reasoning and downstream VLA policy learning. In this work, we take an initial step toward bridging embodied reasoning with VLA policy learning by introducing Vlaser - a Vision-Language-Action Model with synergistic embodied reasoning capability, which is a foundational vision-language model designed to integrate high-level reasoning with low-level control for embodied agents. Built upon the high-quality Vlaser-6M dataset, Vlaser achieves state-of-the-art performance across a range of embodied reasoning benchmarks - including spatial reasoning, embodied grounding, embodied QA, and task planning. Furthermore, we systematically examine how different VLM initializations affect supervised VLA fine-tuning, offering novel insights into mitigating the domain shift between internet-scale pre-training data and embodied-specific policy learning data. Based on these insights, our approach achieves state-of-the-art results on the WidowX benchmark and competitive performance on the Google Robot benchmark.",
        "tags": [
            "Robotics",
            "VLM"
        ]
    },
    {
        "id": 21,
        "title": "SPG: Sandwiched Policy Gradient for Masked Diffusion Language Models",
        "author": [
            "Chenyu Wang",
            "Paria Rashidinejad",
            "DiJia Su",
            "Song Jiang",
            "Sid Wang",
            "Siyan Zhao",
            "Cai Zhou",
            "Shannon Zejiang Shen",
            "Feiyu Chen",
            "Tommi Jaakkola",
            "Yuandong Tian",
            "Bo Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09541",
        "abstract": "Diffusion large language models (dLLMs) are emerging as an efficient alternative to autoregressive models due to their ability to decode multiple tokens in parallel. However, aligning dLLMs with human preferences or task-specific rewards via reinforcement learning (RL) is challenging because their intractable log-likelihood precludes the direct application of standard policy gradient methods. While prior work uses surrogates like the evidence lower bound (ELBO), these one-sided approximations can introduce significant policy gradient bias. To address this, we propose the Sandwiched Policy Gradient (SPG) that leverages both an upper and a lower bound of the true log-likelihood. Experiments show that SPG significantly outperforms baselines based on ELBO or one-step estimation. Specifically, SPG improves the accuracy over state-of-the-art RL methods for dLLMs by 3.6% in GSM8K, 2.6% in MATH500, 18.4% in Countdown and 27.0% in Sudoku.",
        "tags": [
            "Diffusion",
            "LLM",
            "RL"
        ]
    },
    {
        "id": 22,
        "title": "CodePlot-CoT: Mathematical Visual Reasoning by Thinking with Code-Driven Images",
        "author": [
            "Chengqi Duan",
            "Kaiyue Sun",
            "Rongyao Fang",
            "Manyuan Zhang",
            "Yan Feng",
            "Ying Luo",
            "Yufang Liu",
            "Ke Wang",
            "Peng Pei",
            "Xunliang Cai",
            "Hongsheng Li",
            "Yi Ma",
            "Xihui Liu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11718",
        "abstract": "Recent advances in Large Language Models (LLMs) and Vision Language Models (VLMs) have shown significant progress in mathematical reasoning, yet they still face a critical bottleneck with problems requiring visual assistance, such as drawing auxiliary lines or plotting functions to solve the problems. Most LLMs and VLMs are constrained to text-only reasoning chains, while multimodal unified models that can generate interleaved text and images lack the necessary precision and controllability for such tasks. To address this, we propose CodePlot-CoT, a code-driven Chain-of-Thought paradigm for \"thinking with images\" in mathematics. Our approach leverages the VLM to generate text reasoning as well as executable plotting code, which is then rendered into images as \"visual thought\", to solve mathematical problems. To achieve this, we first construct Math-VR, the first large-scale, bilingual dataset and benchmark for Mathematics problems with Visual Reasoning, comprising 178K samples. Second, to create high-quality training data, we develop a state-of-the-art image-to-code converter specialized for parsing complex mathematical figures into codes. Finally, using these training data, we train the CodePlot-CoT model for solving mathematical problems. Experimental results show that our model achieves up to 21% increase over base model on our new benchmark, fully validating the efficacy of our proposed code-driven reasoning paradigm. Our work opens a new direction for multimodal mathematical reasoning and provides the community with the first large-scale dataset, comprehensive benchmark, and strong approach for such problems. To facilitate future research, we make our datasets, code, and pretrained models publicly available at https://github.com/HKU-MMLab/Math-VR-CodePlot-CoT.",
        "tags": [
            "CoT",
            "LLM",
            "VLM"
        ]
    },
    {
        "id": 23,
        "title": "On Epistemic Uncertainty of Visual Tokens for Object Hallucinations in Large Vision-Language Models",
        "author": [
            "Hoigi Seo",
            "Dong Un Kang",
            "Hyunjin Cho",
            "Joohoon Lee",
            "Se Young Chun"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09008",
        "abstract": "Large vision-language models (LVLMs), which integrate a vision encoder (VE) with a large language model, have achieved remarkable success across various tasks. However, there are still crucial challenges in LVLMs such as object hallucination, generating descriptions of objects that are not in the input image. Here, we argue that uncertain visual tokens within the VE is a key factor that contributes to object hallucination. Our statistical analysis found that there are positive correlations between visual tokens with high epistemic uncertainty and the occurrence of hallucinations. Furthermore, we show theoretically and empirically that visual tokens in early VE layers that exhibit large representation deviations under small adversarial perturbations indicate high epistemic uncertainty. Based on these findings, we propose a simple yet effective strategy to mitigate object hallucination by modifying the VE only. Our method comprises a proxy method with adversarial perturbations for identifying uncertain visual tokens efficiently and a method to mask these uncertain visual tokens during the self-attention process in the middle layers of the VE, suppressing their influence on visual encoding and thus alleviating hallucinations. Extensive experiments show that our method significantly reduces object hallucinations in LVLMs and can synergistically work with other prior arts.",
        "tags": [
            "VLM"
        ]
    },
    {
        "id": 24,
        "title": "High-Fidelity Simulated Data Generation for Real-World Zero-Shot Robotic Manipulation Learning with Gaussian Splatting",
        "author": [
            "Haoyu Zhao",
            "Cheng Zeng",
            "Linghao Zhuang",
            "Yaxi Zhao",
            "Shengke Xue",
            "Hao Wang",
            "Xingyue Zhao",
            "Zhongyu Li",
            "Kehan Li",
            "Siteng Huang",
            "Mingxiu Chen",
            "Xin Li",
            "Deli Zhao",
            "Hua Zou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10637",
        "abstract": "The scalability of robotic learning is fundamentally bottlenecked by the significant cost and labor of real-world data collection. While simulated data offers a scalable alternative, it often fails to generalize to the real world due to significant gaps in visual appearance, physical properties, and object interactions. To address this, we propose RoboSimGS, a novel Real2Sim2Real framework that converts multi-view real-world images into scalable, high-fidelity, and physically interactive simulation environments for robotic manipulation. Our approach reconstructs scenes using a hybrid representation: 3D Gaussian Splatting (3DGS) captures the photorealistic appearance of the environment, while mesh primitives for interactive objects ensure accurate physics simulation. Crucially, we pioneer the use of a Multi-modal Large Language Model (MLLM) to automate the creation of physically plausible, articulated assets. The MLLM analyzes visual data to infer not only physical properties (e.g., density, stiffness) but also complex kinematic structures (e.g., hinges, sliding rails) of objects. We demonstrate that policies trained entirely on data generated by RoboSimGS achieve successful zero-shot sim-to-real transfer across a diverse set of real-world manipulation tasks. Furthermore, data from RoboSimGS significantly enhances the performance and generalization capabilities of SOTA methods. Our results validate RoboSimGS as a powerful and scalable solution for bridging the sim-to-real gap.",
        "tags": [
            "3D",
            "Gaussian Splatting"
        ]
    },
    {
        "id": 25,
        "title": "Skill-Targeted Adaptive Training",
        "author": [
            "Yinghui He",
            "Abhishek Panigrahi",
            "Yong Lin",
            "Sanjeev Arora"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10023",
        "abstract": "Language models often show little to no improvement (i.e., \"saturation\") when trained via vanilla supervised fine-tuning (SFT) on data similar to what they saw in their training set (e.g., MATH). We introduce a new fine-tuning strategy, STAT, to train such a student model by using the metacognition ability of a stronger large language model (LLM) as the teacher. The teacher uses the task dataset to create a list of skills needed for the task, and then labels each data point with its required skills (Didolkar et al., 2024). By monitoring the student's answers, the teacher creates a Missing-Skill-Profile for the student, tracking how often they failed to apply each skill in their responses. We use this idea to build a modified training set in one of two ways. In STAT-Sel, the teacher uses an existing set of training examples but adaptively reweights them according to the Missing-Skill-Profile. In STAT-Syn, the teacher synthesizes additional examples involving missing skills. Across extensive experiments on Llama and Qwen models, our methods yield improvements of up to 7.5% on MATH, whereas SFT provides only limited gains. Furthermore, STAT enhances performance on out-of-distribution benchmarks (e.g., AIME24/25, AMC23, etc.) by an average of 4.6%. Crucially, we find that STAT is complementary to RL via GRPO (Shao et al., 2024): after the model is improved using STAT to address skill gaps, GRPO continues to add further gains. We conclude that skill-targeted adaptive training should broadly improve current training pipelines. Our code is available at: https://github.com/princeton-pli/STAT.",
        "tags": [
            "GRPO",
            "LLM",
            "LLaMA",
            "Qwen",
            "RL"
        ]
    },
    {
        "id": 26,
        "title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding",
        "author": [
            "Yuhang Li",
            "Chenchen Zhang",
            "Ruilin Lv",
            "Ao Liu",
            "Ken Deng",
            "Yuanxing Zhang",
            "Jiaheng Liu",
            "Wiggin Zhou",
            "Bo Zhou"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11498",
        "abstract": "While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": 27,
        "title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
        "author": [
            "Adnan El Assadi",
            "Isaac Chung",
            "Roman Solomatin",
            "Niklas Muennighoff",
            "Kenneth Enevoldsen"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10062",
        "abstract": "Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb.",
        "tags": []
    },
    {
        "id": 28,
        "title": "PEAR: Phase Entropy Aware Reward for Efficient Reasoning",
        "author": [
            "Chen Huang",
            "Wei Lu",
            "Wenxuan Zhang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.08026",
        "abstract": "Large Reasoning Models (LRMs) have achieved impressive performance on complex reasoning tasks by generating detailed chain-of-thought (CoT) explanations. However, these responses are often excessively long, containing redundant reasoning steps that inflate inference cost and reduce usability. Controlling the length of generated reasoning without sacrificing accuracy remains an open challenge. Through a systematic empirical analysis, we reveal a consistent positive correlation between model entropy and response length at different reasoning stages across diverse LRMs: the thinking phase exhibits higher entropy, reflecting exploratory behavior of longer responses, while the final answer phase shows lower entropy, indicating a more deterministic solution. This observation suggests that entropy at different reasoning stages can serve as a control knob for balancing conciseness and performance. Based on this insight, this paper introduces Phase Entropy Aware Reward (PEAR), a reward mechanism that incorporating phase-dependent entropy into the reward design. Instead of treating all tokens uniformly, PEAR penalize excessive entropy during the thinking phase and allowing moderate exploration at the final answer phase, which encourages models to generate concise reasoning traces that retain sufficient flexibility to solve the task correctly. This enables adaptive control of response length without relying on explicit length targets or rigid truncation rules. Extensive experiments across four benchmarks demonstrate that PEAR consistently reduces response length while sustaining competitive accuracy across model scales. In addition, PEAR demonstrates strong out-of-distribution (OOD) robustness beyond the training distribution. Our code is available at: https://github.com/iNLP-Lab/PEAR.",
        "tags": [
            "CoT"
        ]
    },
    {
        "id": 29,
        "title": "Self-Improving LLM Agents at Test-Time",
        "author": [
            "Emre Can Acikgoz",
            "Cheng Qian",
            "Heng Ji",
            "Dilek Hakkani-TÃ¼r",
            "Gokhan Tur"
        ],
        "pdf": "https://arxiv.org/pdf/2510.07841",
        "abstract": "One paradigm of language model (LM) fine-tuning relies on creating large training datasets, under the assumption that high quantity and diversity will enable models to generalize to novel tasks after post-training. In practice, gathering large sets of data is inefficient, and training on them is prohibitively expensive; worse, there is no guarantee that the resulting model will handle complex scenarios or generalize better. Moreover, existing techniques rarely assess whether a training sample provides novel information or is redundant with the knowledge already acquired by the model, resulting in unnecessary costs. In this work, we explore a new test-time self-improvement method to create more effective and generalizable agentic LMs on-the-fly. The proposed algorithm can be summarized in three steps: (i) first it identifies the samples that model struggles with (self-awareness), (ii) then generates similar examples from detected uncertain samples (self-data augmentation), and (iii) uses these newly generated samples at test-time fine-tuning (self-improvement). We study two variants of this approach: Test-Time Self-Improvement (TT-SI), where the same model generates additional training examples from its own uncertain cases and then learns from them, and contrast this approach with Test-Time Distillation (TT-D), where a stronger model generates similar examples for uncertain cases, enabling student to adapt using distilled supervision. Empirical evaluations across different agent benchmarks demonstrate that TT-SI improves the performance with +5.48% absolute accuracy gain on average across all benchmarks and surpasses other standard learning methods, yet using 68x less training samples. Our findings highlight the promise of TT-SI, demonstrating the potential of self-improvement algorithms at test-time as a new paradigm for building more capable agents toward self-evolution.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 30,
        "title": "FastHMR: Accelerating Human Mesh Recovery via Token and Layer Merging with Diffusion Decoding",
        "author": [
            "Soroush Mehraban",
            "Andrea Iaboni",
            "Babak Taati"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10868",
        "abstract": "Recent transformer-based models for 3D Human Mesh Recovery (HMR) have achieved strong performance but often suffer from high computational cost and complexity due to deep transformer architectures and redundant tokens. In this paper, we introduce two HMR-specific merging strategies: Error-Constrained Layer Merging (ECLM) and Mask-guided Token Merging (Mask-ToMe). ECLM selectively merges transformer layers that have minimal impact on the Mean Per Joint Position Error (MPJPE), while Mask-ToMe focuses on merging background tokens that contribute little to the final prediction. To further address the potential performance drop caused by merging, we propose a diffusion-based decoder that incorporates temporal context and leverages pose priors learned from large-scale motion capture datasets. Experiments across multiple benchmarks demonstrate that our method achieves up to 2.3x speed-up while slightly improving performance over the baseline.",
        "tags": [
            "3D",
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": 31,
        "title": "SwarmSys: Decentralized Swarm-Inspired Agents for Scalable and Adaptive Reasoning",
        "author": [
            "Ruohao Li",
            "Hongjun Liu",
            "Leyi Zhao",
            "Zisu Li",
            "Jiawei Li",
            "Jiajun Jiang",
            "Linning Xu",
            "Chen Zhao",
            "Mingming Fan",
            "Chen Liang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10047",
        "abstract": "Large language model (LLM) agents have shown remarkable reasoning abilities. However, existing multi-agent frameworks often rely on fixed roles or centralized control, limiting scalability and adaptability in long-horizon reasoning. We introduce SwarmSys, a closed-loop framework for distributed multi-agent reasoning inspired by swarm intelligence. Coordination in SwarmSys emerges through iterative interactions among three specialized roles, Explorers, Workers, and Validators, that continuously cycle through exploration, exploitation, and validation. To enable scalable and adaptive collaboration, we integrate adaptive agent and event profiles, embedding-based probabilistic matching, and a pheromone-inspired reinforcement mechanism, supporting dynamic task allocation and self-organizing convergence without global supervision. Across symbolic reasoning, research synthesis, and scientific programming tasks, SwarmSys consistently outperforms baselines, improving both accuracy and reasoning stability. These findings highlight swarm-inspired coordination as a promising paradigm for scalable, robust, and adaptive multi-agent reasoning, suggesting that coordination scaling may rival model scaling in advancing LLM intelligence.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 32,
        "title": "The Personalization Trap: How User Memory Alters Emotional Reasoning in LLMs",
        "author": [
            "Xi Fang",
            "Weijie Xu",
            "Yuchong Zhang",
            "Stephanie Eckman",
            "Scott Nickleach",
            "Chandan K. Reddy"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09905",
        "abstract": "When an AI assistant remembers that Sarah is a single mother working two jobs, does it interpret her stress differently than if she were a wealthy executive? As personalized AI systems increasingly incorporate long-term user memory, understanding how this memory shapes emotional reasoning is critical. We investigate how user memory affects emotional intelligence in large language models (LLMs) by evaluating 15 models on human validated emotional intelligence tests. We find that identical scenarios paired with different user profiles produce systematically divergent emotional interpretations. Across validated user independent emotional scenarios and diverse user profiles, systematic biases emerged in several high-performing LLMs where advantaged profiles received more accurate emotional interpretations. Moreover, LLMs demonstrate significant disparities across demographic factors in emotion understanding and supportive recommendations tasks, indicating that personalization mechanisms can embed social hierarchies into models emotional reasoning. These results highlight a key challenge for memory enhanced AI: systems designed for personalization may inadvertently reinforce social inequalities.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 33,
        "title": "Stable Video Infinity: Infinite-Length Video Generation with Error Recycling",
        "author": [
            "Wuyang Li",
            "Wentao Pan",
            "Po-Chien Luan",
            "Yang Gao",
            "Alexandre Alahi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09212",
        "abstract": "We propose Stable Video Infinity (SVI) that is able to generate infinite-length videos with high temporal consistency, plausible scene transitions, and controllable streaming storylines. While existing long-video methods attempt to mitigate accumulated errors via handcrafted anti-drifting (e.g., modified noise scheduler, frame anchoring), they remain limited to single-prompt extrapolation, producing homogeneous scenes with repetitive motions. We identify that the fundamental challenge extends beyond error accumulation to a critical discrepancy between the training assumption (seeing clean data) and the test-time autoregressive reality (conditioning on self-generated, error-prone outputs). To bridge this hypothesis gap, SVI incorporates Error-Recycling Fine-Tuning, a new type of efficient training that recycles the Diffusion Transformer (DiT)'s self-generated errors into supervisory prompts, thereby encouraging DiT to actively identify and correct its own errors. This is achieved by injecting, collecting, and banking errors through closed-loop recycling, autoregressively learning from error-injected feedback. Specifically, we (i) inject historical errors made by DiT to intervene on clean inputs, simulating error-accumulated trajectories in flow matching; (ii) efficiently approximate predictions with one-step bidirectional integration and calculate errors with residuals; (iii) dynamically bank errors into replay memory across discretized timesteps, which are resampled for new input. SVI is able to scale videos from seconds to infinite durations with no additional inference cost, while remaining compatible with diverse conditions (e.g., audio, skeleton, and text streams). We evaluate SVI on three benchmarks, including consistent, creative, and conditional settings, thoroughly verifying its versatility and state-of-the-art role.",
        "tags": [
            "DiT",
            "Diffusion",
            "Flow Matching",
            "Transformer",
            "Video Generation"
        ]
    },
    {
        "id": 34,
        "title": "LikePhys: Evaluating Intuitive Physics Understanding in Video Diffusion Models via Likelihood Preference",
        "author": [
            "Jianhao Yuan",
            "Fabio Pizzati",
            "Francesco Pinto",
            "Lars Kunze",
            "Ivan Laptev",
            "Paul Newman",
            "Philip Torr",
            "Daniele De Martini"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11512",
        "abstract": "Intuitive physics understanding in video diffusion models plays an essential role in building general-purpose physically plausible world simulators, yet accurately evaluating such capacity remains a challenging task due to the difficulty in disentangling physics correctness from visual appearance in generation. To the end, we introduce LikePhys, a training-free method that evaluates intuitive physics in video diffusion models by distinguishing physically valid and impossible videos using the denoising objective as an ELBO-based likelihood surrogate on a curated dataset of valid-invalid pairs. By testing on our constructed benchmark of twelve scenarios spanning over four physics domains, we show that our evaluation metric, Plausibility Preference Error (PPE), demonstrates strong alignment with human preference, outperforming state-of-the-art evaluator baselines. We then systematically benchmark intuitive physics understanding in current video diffusion models. Our study further analyses how model design and inference settings affect intuitive physics understanding and highlights domain-specific capacity variations across physical laws. Empirical results show that, despite current models struggling with complex and chaotic dynamics, there is a clear trend of improvement in physics understanding as model capacity and inference settings scale.",
        "tags": [
            "Diffusion"
        ]
    },
    {
        "id": 35,
        "title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation",
        "author": [
            "Abdelhakim Benechehab",
            "Gabriel Singer",
            "Corentin LÃ©ger",
            "Youssef Attia El Hili",
            "Giuseppe Paolo",
            "Albert Thomas",
            "Maurizio Filippone",
            "BalÃ¡zs KÃ©gl"
        ],
        "pdf": "https://arxiv.org/pdf/2510.07624",
        "abstract": "Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only high-quality datasets are accessible. In this work, we address this challenge via a Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. We then conduct a theoretical analysis of this optimization problem in a tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning. We release the code at https://github.com/abenechehab/nll_to_po .",
        "tags": [
            "RL"
        ]
    },
    {
        "id": 36,
        "title": "InfiniHuman: Infinite 3D Human Creation with Precise Control",
        "author": [
            "Yuxuan Xue",
            "Xianghui Xie",
            "Margaret Kostyrko",
            "Gerard Pons-Moll"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11650",
        "abstract": "Generating realistic and controllable 3D human avatars is a long-standing challenge, particularly when covering broad attribute ranges such as ethnicity, age, clothing styles, and detailed body shapes. Capturing and annotating large-scale human datasets for training generative models is prohibitively expensive and limited in scale and diversity. The central question we address in this paper is: Can existing foundation models be distilled to generate theoretically unbounded, richly annotated 3D human data? We introduce InfiniHuman, a framework that synergistically distills these models to produce richly annotated human data at minimal cost and with theoretically unlimited scalability. We propose InfiniHumanData, a fully automatic pipeline that leverages vision-language and image generation models to create a large-scale multi-modal dataset. User study shows our automatically generated identities are undistinguishable from scan renderings. InfiniHumanData contains 111K identities spanning unprecedented diversity. Each identity is annotated with multi-granularity text descriptions, multi-view RGB images, detailed clothing images, and SMPL body-shape parameters. Building on this dataset, we propose InfiniHumanGen, a diffusion-based generative pipeline conditioned on text, body shape, and clothing assets. InfiniHumanGen enables fast, realistic, and precisely controllable avatar generation. Extensive experiments demonstrate significant improvements over state-of-the-art methods in visual quality, generation speed, and controllability. Our approach enables high-quality avatar generation with fine-grained control at effectively unbounded scale through a practical and affordable solution. We will publicly release the automatic data generation pipeline, the comprehensive InfiniHumanData dataset, and the InfiniHumanGen models at https://yuxuan-xue.com/infini-human.",
        "tags": [
            "3D",
            "Diffusion"
        ]
    },
    {
        "id": 37,
        "title": "LLaMAX2: Your Translation-Enhanced Model also Performs Well in Reasoning",
        "author": [
            "Changjiang Gao",
            "Zixian Huang",
            "Jingyang Gong",
            "Shujian Huang",
            "Lei Li",
            "Fei Yuan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09189",
        "abstract": "General Large Language Models (LLMs) excel in reasoning, but those enhanced for translation struggle with reasoning tasks. To address this, we propose a novel translationenhanced recipe that begins with instruct models and applies layer-selective tuning only on parallel data. Following this pipeline, we introduce the Qwen3-XPlus models, which demonstrate significant improvements in translation performance across both high- and lowresource languages, achieving 15+ spBLEU and 40+ xComet in low-resource languages, like Swahili. Interestingly, training only with small parallel datasets, Qwen3-XPlus achieves an average improvement of 1+ points on 7 multilingual tasks while maintaining proficiency comparable to the Qwen3 instruct model in 15 popular reasoning datasets. This work offers a promising approach to multilingual enhancement, significantly reducing complexity and enhancing accessibility for a wider range of languages. The code and model are publicly available.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 38,
        "title": "oMeBench: Towards Robust Benchmarking of LLMs in Organic Mechanism Elucidation and Reasoning",
        "author": [
            "Ruiling Xu",
            "Yifan Zhang",
            "Qingyun Wang",
            "Carl Edwards",
            "Heng Ji"
        ],
        "pdf": "https://arxiv.org/pdf/2510.07731",
        "abstract": "Organic reaction mechanisms are the stepwise elementary reactions by which reactants form intermediates and products, and are fundamental to understanding chemical reactivity and designing new molecules and reactions. Although large language models (LLMs) have shown promise in understanding chemical tasks such as synthesis design, it is unclear to what extent this reflects genuine chemical reasoning capabilities, i.e., the ability to generate valid intermediates, maintain chemical consistency, and follow logically coherent multi-step pathways. We address this by introducing oMeBench, the first large-scale, expert-curated benchmark for organic mechanism reasoning in organic chemistry. It comprises over 10,000 annotated mechanistic steps with intermediates, type labels, and difficulty ratings. Furthermore, to evaluate LLM capability more precisely and enable fine-grained scoring, we propose oMeS, a dynamic evaluation framework that combines step-level logic and chemical similarity. We analyze the performance of state-of-the-art LLMs, and our results show that although current models display promising chemical intuition, they struggle with correct and consistent multi-step reasoning. Notably, we find that using prompting strategy and fine-tuning a specialist model on our proposed dataset increases performance by 50% over the leading closed-source model. We hope that oMeBench will serve as a rigorous foundation for advancing AI systems toward genuine chemical reasoning.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 39,
        "title": "World-To-Image: Grounding Text-to-Image Generation with Agent-Driven World Knowledge",
        "author": [
            "Moo Hyun Son",
            "Jintaek Oh",
            "Sun Bin Mun",
            "Jaechul Roh",
            "Sehyun Choi"
        ],
        "pdf": "https://arxiv.org/pdf/2510.04201",
        "abstract": "While text-to-image (T2I) models can synthesize high-quality images, their performance degrades significantly when prompted with novel or out-of-distribution (OOD) entities due to inherent knowledge cutoffs. We introduce World-To-Image, a novel framework that bridges this gap by empowering T2I generation with agent-driven world knowledge. We design an agent that dynamically searches the web to retrieve images for concepts unknown to the base model. This information is then used to perform multimodal prompt optimization, steering powerful generative backbones toward an accurate synthesis. Critically, our evaluation goes beyond traditional metrics, utilizing modern assessments like LLMGrader and ImageReward to measure true semantic fidelity. Our experiments show that World-To-Image substantially outperforms state-of-the-art methods in both semantic alignment and visual aesthetics, achieving +8.1% improvement in accuracy-to-prompt on our curated NICE benchmark. Our framework achieves these results with high efficiency in less than three iterations, paving the way for T2I systems that can better reflect the ever-changing real world. Our demo code is available here\\footnote{https://github.com/mhson-kyle/World-To-Image}.",
        "tags": [
            "Text-to-Image"
        ]
    },
    {
        "id": 40,
        "title": "RePro: Training Language Models to Faithfully Recycle the Web for Pretraining",
        "author": [
            "Zichun Yu",
            "Chenyan Xiong"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10681",
        "abstract": "High-quality pretraining data is the fossil fuel of large language models (LLMs), yet its reserves are running low for frontier models. In this paper, we introduce RePro, a novel web recycling method that trains a relatively small LM with reinforcement learning to generate effective and faithful rephrasings of pretraining data. Specifically, we design one quality reward and three faithfulness rewards, optimizing the LM rephraser to convert organic data into high-quality rephrasings while maintaining its core semantics and structure. In our experiment, we train a 4B rephraser to recycle 72B tokens sampled from DCLM-RefinedWeb. Pretraining results on 400M and 1.4B models demonstrate that RePro delivers 4.7%-14.0% relative accuracy gains over organic-only baseline on 22 downstream tasks. RePro also outperforms ReWire, the state-of-the-art web recycling method that prompts a 70B rephraser, as well as the organic baseline with a 4x larger data pool. Experiments with different amounts of recycled data highlight that RePro improves organic data efficiency by 2-3x. Individual and distributional analyses validate that RePro preserves more critical information and faithfully reflects the characteristics of organic data compared to prompting-based methods. Together, these results show that RePro provides an efficient and controllable path to effectively harness the fossil fuel of LLM pretraining. We open-source our code, rephraser, and recycled data at https://github.com/cxcscmu/RePro.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": 41,
        "title": "Multimodal Policy Internalization for Conversational Agents",
        "author": [
            "Zhenhailong Wang",
            "Jiateng Liu",
            "Amin Fazel",
            "Ritesh Sarkhel",
            "Xing Fan",
            "Xiang Li",
            "Chenlei Guo",
            "Heng Ji",
            "Ruhi Sarikaya"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09474",
        "abstract": "Modern conversational agents like ChatGPT and Alexa+ rely on predefined policies specifying metadata, response styles, and tool-usage rules. As these LLM-based systems expand to support diverse business and user queries, such policies, often implemented as in-context prompts, are becoming increasingly complex and lengthy, making faithful adherence difficult and imposing large fixed computational costs. With the rise of multimodal agents, policies that govern visual and multimodal behaviors are critical but remain understudied. Prior prompt-compression work mainly shortens task templates and demonstrations, while existing policy-alignment studies focus only on text-based safety rules. We introduce Multimodal Policy Internalization (MPI), a new task that internalizes reasoning-intensive multimodal policies into model parameters, enabling stronger policy-following without including the policy during inference. MPI poses unique data and algorithmic challenges. We build two datasets spanning synthetic and real-world decision-making and tool-using tasks and propose TriMPI, a three-stage training framework. TriMPI first injects policy knowledge via continual pretraining, then performs supervised finetuning, and finally applies PolicyRollout, a GRPO-style reinforcement learning extension that augments rollouts with policy-aware responses for grounded exploration. TriMPI achieves notable gains in end-to-end accuracy, generalization, and robustness to forgetting. As the first work on multimodal policy internalization, we provide datasets, training recipes, and comprehensive evaluations to foster future research. Project page: https://mikewangwzhl.github.io/TriMPI.",
        "tags": [
            "GPT",
            "GRPO",
            "LLM",
            "RL"
        ]
    },
    {
        "id": 42,
        "title": "The Attacker Moves Second: Stronger Adaptive Attacks Bypass Defenses Against Llm Jailbreaks and Prompt Injections",
        "author": [
            "Milad Nasr",
            "Nicholas Carlini",
            "Chawin Sitawarin",
            "Sander V. Schulhoff",
            "Jamie Hayes",
            "Michael Ilie",
            "Juliette Pluto",
            "Shuang Song",
            "Harsh Chaudhari",
            "Ilia Shumailov",
            "Abhradeep Thakurta",
            "Kai Yuanqing Xiao",
            "Andreas Terzis",
            "Florian TramÃ¨r"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09023",
        "abstract": "How should we evaluate the robustness of language model defenses? Current defenses against jailbreaks and prompt injections (which aim to prevent an attacker from eliciting harmful knowledge or remotely triggering malicious actions, respectively) are typically evaluated either against a static set of harmful attack strings, or against computationally weak optimization methods that were not designed with the defense in mind. We argue that this evaluation process is flawed.\nInstead, we should evaluate defenses against adaptive attackers who explicitly modify their attack strategy to counter a defense's design while spending considerable resources to optimize their objective. By systematically tuning and scaling general optimization techniques-gradient descent, reinforcement learning, random search, and human-guided exploration-we bypass 12 recent defenses (based on a diverse set of techniques) with attack success rate above 90% for most; importantly, the majority of defenses originally reported near-zero attack success rates. We believe that future defense work must consider stronger attacks, such as the ones we describe, in order to make reliable and convincing claims of robustness.",
        "tags": [
            "LLM",
            "RL"
        ]
    },
    {
        "id": 43,
        "title": "Graph Diffusion Transformers are In-Context Molecular Designers",
        "author": [
            "Gang Liu",
            "Jie Chen",
            "Yihan Zhu",
            "Michael Sun",
            "Tengfei Luo",
            "Nitesh V Chawla",
            "Meng Jiang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.08744",
        "abstract": "In-context learning allows large models to adapt to new tasks from a few demonstrations, but it has shown limited success in molecular design. Existing databases such as ChEMBL contain molecular properties spanning millions of biological assays, yet labeled data for each property remain scarce. To address this limitation, we introduce demonstration-conditioned diffusion models (DemoDiff), which define task contexts using a small set of molecule-score examples instead of text descriptions. These demonstrations guide a denoising Transformer to generate molecules aligned with target properties. For scalable pretraining, we develop a new molecular tokenizer with Node Pair Encoding that represents molecules at the motif level, requiring 5.5$\\times$ fewer nodes. We curate a dataset containing millions of context tasks from multiple sources covering both drugs and materials, and pretrain a 0.7-billion-parameter model on it. Across 33 design tasks in six categories, DemoDiff matches or surpasses language models 100-1000$\\times$ larger and achieves an average rank of 3.63 compared to 5.25-10.20 for domain-specific approaches. These results position DemoDiff as a molecular foundation model for in-context molecular design. Our code is available at https://github.com/liugangcode/DemoDiff.",
        "tags": [
            "Diffusion",
            "Transformer"
        ]
    },
    {
        "id": 44,
        "title": "VER: Vision Expert Transformer for Robot Learning via Foundation Distillation and Dynamic Routing",
        "author": [
            "Yixiao Wang",
            "Mingxiao Huo",
            "Zhixuan Liang",
            "Yushi Du",
            "Lingfeng Sun",
            "Haotian Lin",
            "Jinghuan Shang",
            "Chensheng Peng",
            "Mohit Bansal",
            "Mingyu Ding",
            "Masayoshi Tomizuka"
        ],
        "pdf": "https://arxiv.org/pdf/2510.05213",
        "abstract": "Pretrained vision foundation models (VFMs) advance robotic learning via rich visual representations, yet individual VFMs typically excel only in specific domains, limiting generality across tasks. Distilling multiple VFMs into a unified representation for policy can mitigate this limitation but often yields inflexible task-specific feature selection and requires costly full re-training to incorporate robot-domain knowledge. We propose VER, a Vision Expert transformer for Robot learning. During pretraining, VER distills multiple VFMs into a vision expert library. It then fine-tunes only a lightweight routing network (fewer than 0.4% of parameters) to dynamically select task-relevant experts from the pretrained library for downstream robot tasks. We further introduce Patchwise Expert Routing with Curriculum Top-K Annealing to improve both flexibility and precision of dynamic expert selection. Moreover, VER supports parameter-efficient finetuning for scalable expert utilization and adaptive robot-domain knowledge integration. Across 17 diverse robotic tasks and multiple policy heads, VER achieves state-of-the-art performance. We find that VER reduces large-norm outliers in task-irrelevant regions (e.g., background) and concentrates on task-critical regions. Visualizations and codes can be found in https://yixiaowang7.github.io/ver_page/.",
        "tags": [
            "Robotics",
            "Transformer"
        ]
    },
    {
        "id": 45,
        "title": "A Tale of LLMs and Induced Small Proxies: Scalable Agents for Knowledge Mining",
        "author": [
            "Sipeng Zhang",
            "Longfei Yun",
            "Zilong Wang",
            "Jingbo Shang",
            "Letian Peng"
        ],
        "pdf": "https://arxiv.org/pdf/2510.01427",
        "abstract": "At the core of Deep Research is knowledge mining, the task of extracting structured information from massive unstructured text in response to user instructions. Large language models (LLMs) excel at interpreting such instructions but are prohibitively expensive to deploy at scale, while traditional pipelines of classifiers and extractors remain efficient yet brittle and unable to generalize to new tasks. We introduce Falconer, a collaborative framework that combines the agentic reasoning of LLMs with lightweight proxy models for scalable knowledge mining. In Falconer, LLMs act as planners, decomposing user instructions into executable pipelines, and as annotators, generating supervision to train small proxies. The framework unifies classification and extraction into two atomic operations, get label and get span, enabling a single instruction-following model to replace multiple task-specific components. To evaluate the consistency between proxy models incubated by Falconer and annotations provided by humans and large models, we construct new benchmarks covering both planning and end-to-end execution. Experiments show that Falconer closely matches state-of-the-art LLMs in instruction-following accuracy while reducing inference cost by up to 90% and accelerating large-scale knowledge mining by more than 20x, offering an efficient and scalable foundation for Deep Research.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 46,
        "title": "Are Large Reasoning Models Interruptible?",
        "author": [
            "Tsung-Han Wu",
            "Mihran Miroyan",
            "David M. Chan",
            "Trevor Darrell",
            "Narges Norouzi",
            "Joseph E. Gonzalez"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11713",
        "abstract": "Large Reasoning Models (LRMs) excel at complex reasoning but are traditionally evaluated in static, \"frozen world\" settings: model responses are assumed to be instantaneous, and the context of a request is presumed to be immutable over the duration of the response. While generally true for short-term tasks, the \"frozen world\" assumption breaks down in modern reasoning tasks such as assistive programming, where models may take hours to think through problems and code may change dramatically from the time the model starts thinking to the model's final output. In this work, we challenge the frozen world assumption and evaluate LRM robustness under two realistic dynamic scenarios: interruptions, which test the quality of the model's partial outputs on a limited budget, and dynamic context, which tests model adaptation to in-flight changes. Across mathematics and programming benchmarks that require long-form reasoning, static evaluations consistently overestimate robustness: even state-of-the-art LRMs, which achieve high accuracy in static settings, can fail unpredictably when interrupted or exposed to changing context, with performance dropping by up to 60% when updates are introduced late in the reasoning process. Our analysis further reveals several novel failure modes, including reasoning leakage, where models fold the reasoning into their final answer when interrupted; panic, where under time pressure models abandon reasoning entirely and return incorrect answers; and self-doubt, where performance degrades while incorporating updated information.",
        "tags": []
    },
    {
        "id": 47,
        "title": "IVEBench: Modern Benchmark Suite for Instruction-Guided Video Editing Assessment",
        "author": [
            "Yinan Chen",
            "Jiangning Zhang",
            "Teng Hu",
            "Yuxiang Zeng",
            "Zhucun Xue",
            "Qingdong He",
            "Chengjie Wang",
            "Yong Liu",
            "Xiaobin Hu",
            "Shuicheng Yan"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11647",
        "abstract": "Instruction-guided video editing has emerged as a rapidly advancing research direction, offering new opportunities for intuitive content transformation while also posing significant challenges for systematic evaluation. Existing video editing benchmarks fail to support the evaluation of instruction-guided video editing adequately and further suffer from limited source diversity, narrow task coverage and incomplete evaluation metrics. To address the above limitations, we introduce IVEBench, a modern benchmark suite specifically designed for instruction-guided video editing assessment. IVEBench comprises a diverse database of 600 high-quality source videos, spanning seven semantic dimensions, and covering video lengths ranging from 32 to 1,024 frames. It further includes 8 categories of editing tasks with 35 subcategories, whose prompts are generated and refined through large language models and expert review. Crucially, IVEBench establishes a three-dimensional evaluation protocol encompassing video quality, instruction compliance and video fidelity, integrating both traditional metrics and multimodal large language model-based assessments. Extensive experiments demonstrate the effectiveness of IVEBench in benchmarking state-of-the-art instruction-guided video editing methods, showing its ability to provide comprehensive and human-aligned evaluation outcomes.",
        "tags": [
            "LLM",
            "Video Editing"
        ]
    },
    {
        "id": 48,
        "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large Language Model",
        "author": [
            "Zhiwei Jin",
            "Xiaohui Song",
            "Nan Wang",
            "Yafei Liu",
            "Chao Li",
            "Xin Li",
            "Ruichen Wang",
            "Zhihao Li",
            "Qi Qi",
            "Long Cheng",
            "Dongze Hao",
            "Quanlong Zheng",
            "Yanhao Zhang",
            "Haobo Ji",
            "Jian Ma",
            "Zhitong Zheng",
            "Zhenyi Lin",
            "Haolin Deng",
            "Xin Zou",
            "Xiaojie Yin",
            "Ruilin Wang",
            "Liankai Cai",
            "Haijing Liu",
            "Yuqing Qiu",
            "Ke Chen",
            "Zixian Li",
            "Chi Xie",
            "Huafei Li",
            "Chenxing Li",
            "Chuangchuang Wang",
            "Kai Tang",
            "Zhiguang Zhu",
            "Kai Tang",
            "Wenmei Gao",
            "Rui Wang",
            "Jun Wu",
            "Chao Liu",
            "Qin Xie",
            "Chen Chen",
            "Haonan Lu"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11496",
        "abstract": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o, Gemini, and Claude Sonnet have demonstrated outstanding performance with enormous model sizes reaching hundreds of billions of parameters, they significantly surpass the limitations in memory, power consumption, and computing capacity of edge devices such as mobile phones. This paper introduces AndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on Qwen3's LLM and various visual encoders. We comprehensively outline the model architectures, training pipeline, and training data of AndesVL, which achieves first-tier performance across a wide range of open-source benchmarks, including fields such as text-rich image understanding, reasoning and math, multi-image comprehension, general VQA, hallucination mitigation, multilingual understanding, and GUI-related tasks when compared with state-of-the-art models of a similar scale. Furthermore, we introduce a 1+N LoR",
        "tags": [
            "GPT",
            "LLM"
        ]
    },
    {
        "id": 49,
        "title": "ViSurf: Visual Supervised-and-Reinforcement Fine-Tuning for Large Vision-and-Language Models",
        "author": [
            "Yuqi Liu",
            "Liangyu Chen",
            "Jiazhen Liu",
            "Mingkang Zhu",
            "Zhisheng Zhong",
            "Bei Yu",
            "Jiaya Jia"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10606",
        "abstract": "Typical post-training paradigms for Large Vision-and-Language Models (LVLMs) include Supervised Fine-Tuning (SFT) and Reinforcement Learning with Verifiable Rewards (RLVR). SFT leverages external guidance to inject new knowledge, whereas RLVR utilizes internal reinforcement to enhance reasoning capabilities and overall performance. However, our analysis reveals that SFT often leads to sub-optimal performance, while RLVR struggles with tasks that exceed the model's internal knowledge base. To address these limitations, we propose ViSurf (\\textbf{Vi}sual \\textbf{Su}pervised-and-\\textbf{R}einforcement \\textbf{F}ine-Tuning), a unified post-training paradigm that integrates the strengths of both SFT and RLVR within a single stage. We analyze the derivation of the SFT and RLVR objectives to establish the ViSurf objective, providing a unified perspective on these two paradigms. The core of ViSurf involves injecting ground-truth labels into the RLVR rollouts, thereby providing simultaneous external supervision and internal reinforcement. Furthermore, we introduce three novel reward control strategies to stabilize and optimize the training process. Extensive experiments across several diverse benchmarks demonstrate the effectiveness of ViSurf, outperforming both individual SFT, RLVR, and two-stage SFT \\textrightarrow RLVR. In-depth analysis corroborates these findings, validating the derivation and design principles of ViSurf.",
        "tags": [
            "RL"
        ]
    },
    {
        "id": 50,
        "title": "The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution",
        "author": [
            "Norbert Tihanyi",
            "Bilel Cherif",
            "Richard A. Dubniczky",
            "Mohamed Amine Ferrag",
            "TamÃ¡s Bisztray"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10493",
        "abstract": "In this paper, we present the first large-scale study exploring whether JavaScript code generated by Large Language Models (LLMs) can reveal which model produced it, enabling reliable authorship attribution and model fingerprinting. With the rapid rise of AI-generated code, attribution is playing a critical role in detecting vulnerabilities, flagging malicious content, and ensuring accountability. While AI-vs-human detection usually treats AI as a single category we show that individual LLMs leave unique stylistic signatures, even among models belonging to the same family or parameter size. To this end, we introduce LLM-NodeJS, a dataset of 50,000 http://Node.js back-end programs from 20 large language models. Each has four transformed variants, yielding 250,000 unique JavaScript samples and two additional representations (JSIR and AST) for diverse research applications. Using this dataset, we benchmark traditional machine learning classifiers against fine-tuned Transformer encoders and introduce CodeT5-JSA, a custom architecture derived from the 770M-parameter CodeT5 model with its decoder removed and a modified classification head. It achieves 95.8% accuracy on five-class attribution, 94.6% on ten-class, and 88.5% on twenty-class tasks, surpassing other tested models such as BERT, CodeBERT, and Longformer. We demonstrate that classifiers capture deeper stylistic regularities in program dataflow and structure, rather than relying on surface-level features. As a result, attribution remains effective even after mangling, comment removal, and heavy code transformations. To support open science and reproducibility, we release the LLM-NodeJS dataset, Google Colab training scripts, and all related materials on GitHub: https://github.com/LLM-NodeJS-dataset.",
        "tags": [
            "BERT",
            "Detection",
            "LLM",
            "Transformer"
        ]
    },
    {
        "id": 51,
        "title": "CoBia: Constructed Conversations Can Trigger Otherwise Concealed Societal Biases in LLMs",
        "author": [
            "Nafiseh Nikeghbal",
            "Amir Hossein Kargaran",
            "Jana Diesner"
        ],
        "pdf": "https://arxiv.org/pdf/2510.09871",
        "abstract": "Improvements in model construction, including fortified safety guardrails, allow Large language models (LLMs) to increasingly pass standard safety checks. However, LLMs sometimes slip into revealing harmful behavior, such as expressing racist viewpoints, during conversations. To analyze this systematically, we introduce CoBia, a suite of lightweight adversarial attacks that allow us to refine the scope of conditions under which LLMs depart from normative or ethical behavior in conversations. CoBia creates a constructed conversation where the model utters a biased claim about a social group. We then evaluate whether the model can recover from the fabricated bias claim and reject biased follow-up questions. We evaluate 11 open-source as well as proprietary LLMs for their outputs related to six socio-demographic categories that are relevant to individual safety and fair treatment, i.e., gender, race, religion, nationality, sex orientation, and others. Our evaluation is based on established LLM-based bias metrics, and we compare the results against human judgments to scope out the LLMs' reliability and alignment. The results suggest that purposefully constructed conversations reliably reveal bias amplification and that LLMs often fail to reject biased follow-up questions during dialogue. This form of stress-testing highlights deeply embedded biases that can be surfaced through interaction. Code and artifacts are available at https://github.com/nafisenik/CoBia.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 52,
        "title": "Through the Perspective of LiDAR: A Feature-Enriched and Uncertainty-Aware Annotation Pipeline for Terrestrial Point Cloud Segmentation",
        "author": [
            "Fei Zhang",
            "Rob Chancia",
            "Josie Clapp",
            "Amirhossein Hassanzadeh",
            "Dimah Dera",
            "Richard MacKenzie",
            "Jan van Aardt"
        ],
        "pdf": "https://arxiv.org/pdf/2510.06582",
        "abstract": "Accurate semantic segmentation of terrestrial laser scanning (TLS) point clouds is limited by costly manual annotation. We propose a semi-automated, uncertainty-aware pipeline that integrates spherical projection, feature enrichment, ensemble learning, and targeted annotation to reduce labeling effort, while sustaining high accuracy. Our approach projects 3D points to a 2D spherical grid, enriches pixels with multi-source features, and trains an ensemble of segmentation networks to produce pseudo-labels and uncertainty maps, the latter guiding annotation of ambiguous regions. The 2D outputs are back-projected to 3D, yielding densely annotated point clouds supported by a three-tier visualization suite (2D feature maps, 3D colorized point clouds, and compact virtual spheres) for rapid triage and reviewer guidance. Using this pipeline, we build Mangrove3D, a semantic segmentation TLS dataset for mangrove forests. We further evaluate data efficiency and feature importance to address two key questions: (1) how much annotated data are needed and (2) which features matter most. Results show that performance saturates after ~12 annotated scans, geometric features contribute the most, and compact nine-channel stacks capture nearly all discriminative power, with the mean Intersection over Union (mIoU) plateauing at around 0.76. Finally, we confirm the generalization of our feature-enrichment strategy through cross-dataset tests on ForestSemantic and Semantic3D.\nOur contributions include: (i) a robust, uncertainty-aware TLS annotation pipeline with visualization tools; (ii) the Mangrove3D dataset; and (iii) empirical guidance on data efficiency and feature importance, thus enabling scalable, high-quality segmentation of TLS point clouds for ecological monitoring and beyond. The dataset and processing scripts are publicly available at https://fz-rit.github.io/through-the-lidars-eye/.",
        "tags": [
            "3D",
            "Segmentation"
        ]
    },
    {
        "id": 53,
        "title": "Pathology-CoT: Learning Visual Chain-of-Thought Agent from Expert Whole Slide Image Diagnosis Behavior",
        "author": [
            "Sheng Wang",
            "Ruiming Wu",
            "Charles Herndon",
            "Yihang Liu",
            "Shunsuke Koga",
            "Jeanne Shen",
            "Zhi Huang"
        ],
        "pdf": "https://arxiv.org/pdf/2510.04587",
        "abstract": "Diagnosing a whole-slide image is an interactive, multi-stage process involving changes in magnification and movement between fields. Although recent pathology foundation models are strong, practical agentic systems that decide what field to examine next, adjust magnification, and deliver explainable diagnoses are still lacking. The blocker is data: scalable, clinically aligned supervision of expert viewing behavior that is tacit and experience-based, not written in textbooks or online, and therefore absent from large language model training. We introduce the AI Session Recorder, which works with standard WSI viewers to unobtrusively record routine navigation and convert the viewer logs into standardized behavioral commands (inspect or peek at discrete magnifications) and bounding boxes. A lightweight human-in-the-loop review turns AI-drafted rationales into the Pathology-CoT dataset, a form of paired \"where to look\" and \"why it matters\" supervision produced at roughly six times lower labeling time. Using this behavioral data, we build Pathologist-o3, a two-stage agent that first proposes regions of interest and then performs behavior-guided reasoning. On gastrointestinal lymph-node metastasis detection, it achieved 84.5% precision, 100.0% recall, and 75.4% accuracy, exceeding the state-of-the-art OpenAI o3 model and generalizing across backbones. To our knowledge, this constitutes one of the first behavior-grounded agentic systems in pathology. Turning everyday viewer logs into scalable, expert-validated supervision, our framework makes agentic pathology practical and establishes a path to human-aligned, upgradeable clinical AI.",
        "tags": [
            "CoT",
            "Detection"
        ]
    },
    {
        "id": 54,
        "title": "The Curious Case of Factual (Mis)Alignment between LLMs' Short- and Long-Form Answers",
        "author": [
            "Saad Obaid ul Islam",
            "Anne Lauscher",
            "Goran GlavaÅ¡"
        ],
        "pdf": "https://arxiv.org/pdf/2510.11218",
        "abstract": "Large language models (LLMs) can correctly answer \"When was Einstein born?\" yet fail to provide the same date when writing about Einstein's life revealing a fundamental inconsistency in how models access factual knowledge across task complexities. While models display impressive accuracy on factual question-answering benchmarks, the reliability gap between simple and complex queries remains poorly understood, eroding their trustworthiness. In this work, we introduce Short-Long Form Alignment for Factual Question Answering (SLAQ), a controlled evaluation framework that compares LLMs' answers to the same factual questions asked (a) in isolation (short) vs. (b) integrated into complex queries (long). Looking at 16 LLMs across 600 queries, we find a systematic misalignment of answers to the corresponding short and long queries. We further uncover position-dependent accuracy loss and momentum effects where consecutive correct or incorrect answers create self-reinforcing patterns. Through mechanistic analysis, we find that aligned facts activate overlapping model internals, and that metrics based on mechanistic similarity can predict short-long answer alignment with up to 78% accuracy. Our work establishes factual consistency over query complexity as an important aspect of LLMs' trustworthiness and challenges current evaluation practices, which implicitly assume that good performance for simple factual queries implies reliability in more complex knowledge-seeking tasks too.",
        "tags": [
            "LLM"
        ]
    },
    {
        "id": 55,
        "title": "VLM-Guided Adaptive Negative Prompting for Creative Generation",
        "author": [
            "Shelly Golan",
            "Yotam Nitzan",
            "Zongze Wu",
            "Or Patashnik"
        ],
        "pdf": "https://arxiv.org/pdf/2510.10715",
        "abstract": "Creative generation is the synthesis of new, surprising, and valuable samples that reflect user intent yet cannot be envisioned in advance. This task aims to extend human imagination, enabling the discovery of visual concepts that exist in the unexplored spaces between familiar domains. While text-to-image diffusion models excel at rendering photorealistic scenes that faithfully match user prompts, they still struggle to generate genuinely novel content. Existing approaches to enhance generative creativity either rely on interpolation of image features, which restricts exploration to predefined categories, or require time-intensive procedures such as embedding optimization or model fine-tuning. We propose VLM-Guided Adaptive Negative-Prompting, a training-free, inference-time method that promotes creative image generation while preserving the validity of the generated object. Our approach utilizes a vision-language model (VLM) that analyzes intermediate outputs of the generation process and adaptively steers it away from conventional visual concepts, encouraging the emergence of novel and surprising outputs. We evaluate creativity through both novelty and validity, using statistical metrics in the CLIP embedding space. Through extensive experiments, we show consistent gains in creative novelty with negligible computational overhead. Moreover, unlike existing methods that primarily generate single objects, our approach extends to complex scenarios, such as generating coherent sets of creative objects and preserving creativity within elaborate compositional prompts. Our method integrates seamlessly into existing diffusion pipelines, offering a practical route to producing creative outputs that venture beyond the constraints of textual descriptions.",
        "tags": [
            "CLIP",
            "Diffusion",
            "Text-to-Image",
            "VLM"
        ]
    },
    {
        "id": 56,
        "title": "MultiCOIN: Multi-Modal COntrollable Video INbetweening",
        "author": [
            "Maham Tanveer",
            "Yang Zhou",
            "Simon Niklaus",
            "Ali Mahdavi Amiri",
            "Hao Zhang",
            "Krishna Kumar Singh",
            "Nanxuan Zhao"
        ],
        "pdf": "https://arxiv.org/pdf/2510.08561",
        "abstract": "Video inbetweening creates smooth and natural transitions between two image frames, making it an indispensable tool for video editing and long-form video synthesis. Existing works in this domain are unable to generate large, complex, or intricate motions. In particular, they cannot accommodate the versatility of user intents and generally lack fine control over the details of intermediate frames, leading to misalignment with the creative mind. To fill these gaps, we introduce MultiCOIN, a video inbetweening framework that allows multi-modal controls, including depth transition and layering, motion trajectories, text prompts, and target regions for movement localization, while achieving a balance between flexibility, ease of use, and precision for fine-grained video interpolation. To achieve this, we adopt the Diffusion Transformer (DiT) architecture as our video generative model, due to its proven capability to generate high-quality long videos. To ensure compatibility between DiT and our multi-modal controls, we map all motion controls into a common sparse and user-friendly point-based representation as the video/noise input. Further, to respect the variety of controls which operate at varying levels of granularity and influence, we separate content controls and motion controls into two branches to encode the required features before guiding the denoising process, resulting in two generators, one for motion and the other for content. Finally, we propose a stage-wise training strategy to ensure that our model learns the multi-modal controls smoothly. Extensive qualitative and quantitative experiments demonstrate that multi-modal controls enable a more dynamic, customizable, and contextually accurate visual narrative.",
        "tags": [
            "DiT",
            "Diffusion",
            "Transformer",
            "Video Editing"
        ]
    }
]